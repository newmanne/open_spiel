{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.2.min.js\"];\n",
       "  const css_urls = [];\n",
       "  \n",
       "\n",
       "  const inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.2.min.js\"];\n  const css_urls = [];\n  \n\n  const inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import scipy.stats\n",
    "import time\n",
    "\n",
    "from open_spiel.python.algorithms.exploitability import nash_conv, best_response\n",
    "from open_spiel.python.examples.ubc_plotting_utils import *\n",
    "from open_spiel.python.examples.ubc_sample_game_tree import sample_game_tree, flatten_trees, flatten_tree\n",
    "from open_spiel.python.examples.ubc_clusters import projectPCA, fitGMM\n",
    "from open_spiel.python.examples.ubc_utils import *\n",
    "import open_spiel.python.examples.ubc_dispatch as dispatch\n",
    "\n",
    "from auctions.webutils import *\n",
    "\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "\n",
    "from open_spiel.python.examples.ubc_cma import *\n",
    "\n",
    "output_notebook()\n",
    "from open_spiel.python.games.clock_auction_base import InformationPolicy, ActivityPolicy, UndersellPolicy\n",
    "from open_spiel.python.algorithms.exploitability import nash_conv, best_response\n",
    "from open_spiel.python.examples.ubc_decorators import TakeSingleActionDecorator, TremblingAgentDecorator, ModalAgentDecorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 450 runs\n"
     ]
    }
   ],
   "source": [
    "# load runs from experiments\n",
    "experiments = []\n",
    "experiments += ['jun24_cfr_gamut_6'] \n",
    "experiments += ['jun24_cfr_gamut_7']\n",
    "# experiments += ['jun26_port']\n",
    "runs = []\n",
    "for experiment in experiments:\n",
    "    runs += Experiment.objects.get(name=experiment).equilibriumsolverrun_set.all()\n",
    "print(f\"Found {len(runs)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_combo_specific(evaluation):\n",
    "    r = dict()\n",
    "    for k,v in evaluation.samples.items():\n",
    "        if isinstance(v, dict):\n",
    "            for k2, v2 in v.items():\n",
    "                r[f'{k}_{k2}'] = v2\n",
    "        else:\n",
    "            r[k] = v\n",
    "    del r['rewards']\n",
    "\n",
    "    c_df = pd.DataFrame(r)\n",
    "    type_columns = [c for c in c_df if 'types_' in c]\n",
    "    c_df['combo'] = list(map(tuple, c_df[type_columns].values))\n",
    "    mean_lengths = c_df.groupby('combo')['auction_lengths'].mean()\n",
    "    ### A given type combo where tie-breaking (non-final) seems to matter\n",
    "    for v in mean_lengths.values:\n",
    "        if not v.is_integer() and v < 4:\n",
    "            print(\"FOUND YOU\", v)\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 450/450 [01:38<00:00,  4.55it/s]\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "for run in tqdm(runs):\n",
    "    try:\n",
    "        checkpoints =  run.equilibriumsolverruncheckpoint_set.all()\n",
    "        if len(checkpoints) == 0:\n",
    "            print(f\"No checkpoints for {run}!\")\n",
    "        e = checkpoints.last()\n",
    "        evaluation = e.get_modal_eval()\n",
    "        record = dict(run=run.name, game=run.game.name, t=e.t, modal_nash_conv=evaluation.nash_conv, walltime=e.walltime, alg=get_algorithm_from_run(run), rewards=evaluation.mean_rewards, improvements=evaluation.player_improvements)\n",
    "\n",
    "        x = dict(run.config)\n",
    "        # del x['seed']\n",
    "        del x['use_wandb']\n",
    "        del x['solver_type']\n",
    "        record.update(x)\n",
    "\n",
    "\n",
    "        ### For Greg: convergence test\n",
    "#             if run.config['seed'] != 100:\n",
    "#                 continue\n",
    "#             try:\n",
    "#                 game, final_checkpoint, policy = get_results(run, skip_single_chance_nodes=False, load_policy=True)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Skipping run {run.name} because of error {e}\")\n",
    "#                 continue\n",
    "\n",
    "#             total_entropy = analyze_samples(final_checkpoint.get_old_eval().samples, game)['total_entropy'] # Use old eval since modal is stupid for this and it's a convergence test\n",
    "#             record['total_entropy'] = total_entropy\n",
    "        ### End\n",
    "\n",
    "\n",
    "        records.append(record)\n",
    "    except Exception as e:\n",
    "        print(f\"Run {run} facing error {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = pd.DataFrame.from_records(records)\n",
    "df_time['variant'] = list(map(str,(zip(df_time.sampling_method, df_time.linear_averaging, df_time.regret_matching_plus, df_time.explore_prob, df_time.tremble_prob, df_time.regret_init, df_time.regret_init_strength))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(9,9))\n",
    "\n",
    "# # Does longer = better?\n",
    "# dt = df_time.query('~modal_nash_conv.isnull()')\n",
    "# dt = dt.groupby('variant').filter(lambda grp: len(grp) > 25)\n",
    "# dt = dt.query('seed == 100 and t > 1_000')\n",
    "# dt['port'] = dt['run'].str.contains('port')\n",
    "# # TODO: Limit to same comparison\n",
    "# dt = dt.groupby(['game', 'variant']).filter(lambda grp: len(grp) == 2)\n",
    "# # dt.groupby(['variant', 'port'])['modal_nash_conv'].describe()\n",
    "# dt.pivot(values=['modal_nash_conv'], columns=['port'], index=['variant', 'game']).plot(kind='scatter', x=(\"modal_nash_conv\", False), y=(\"modal_nash_conv\", True))\n",
    "# plt.axline((0, 0), slope=1)\n",
    "\n",
    "# # dt.groupby(['variant', 'game'])['modal_nash_conv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_time.groupby(['sampling_method', 'linear_averaging', 'regret_matching_plus', 'explore_prob'])['modal_nash_conv'].mean()\n",
    "# Why are 25 rows missing?\n",
    "\n",
    "# df_time.pivot(['sampling_method', 'linear_averaging', 'regret_matching_plus', 'explore_prob'], index='run', values='modal_nash_conv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    21.000000\n",
       "mean      7.364062\n",
       "std       6.436483\n",
       "min       0.000000\n",
       "25%       2.400000\n",
       "50%       5.592050\n",
       "75%      10.648000\n",
       "max      22.025000\n",
       "Name: modal_nash_conv, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time.query(\"\"\"variant == \"('outcome', True, True, nan, nan, 'straightforward_clock', 100.0)\" and t > 1000\"\"\")['modal_nash_conv'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "game\n",
       "jun24/jun24_0_base.json                 18\n",
       "jun24/jun24_0_high_speed.json           18\n",
       "jun24/jun24_0_medium_speed.json         18\n",
       "jun24/jun24_0_no_activity.json          18\n",
       "jun24/jun24_0_undersell_allowed.json    18\n",
       "jun24/jun24_1_base.json                 18\n",
       "jun24/jun24_1_high_speed.json           18\n",
       "jun24/jun24_1_medium_speed.json         18\n",
       "jun24/jun24_1_no_activity.json          18\n",
       "jun24/jun24_1_undersell_allowed.json    18\n",
       "jun24/jun24_2_base.json                 18\n",
       "jun24/jun24_2_high_speed.json           18\n",
       "jun24/jun24_2_medium_speed.json         18\n",
       "jun24/jun24_2_no_activity.json          18\n",
       "jun24/jun24_2_undersell_allowed.json    18\n",
       "jun24/jun24_3_base.json                 18\n",
       "jun24/jun24_3_high_speed.json           18\n",
       "jun24/jun24_3_medium_speed.json         18\n",
       "jun24/jun24_3_no_activity.json          18\n",
       "jun24/jun24_3_undersell_allowed.json    18\n",
       "jun24/jun24_4_base.json                 18\n",
       "jun24/jun24_4_high_speed.json           18\n",
       "jun24/jun24_4_medium_speed.json         18\n",
       "jun24/jun24_4_no_activity.json          18\n",
       "jun24/jun24_4_undersell_allowed.json    18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time.groupby('game').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_variants = df_time['variant'].nunique()\n",
    "df_all = df_time.groupby('game').filter(lambda grp: len(grp) == 18 and grp['modal_nash_conv'].isnull().sum() == 0) # Only look at games where ALL of them had runs and everyone finished NashConv calcs for fairness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [count, mean, std, min, 25%, 50%, 75%, max]\n",
       "Index: []"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.groupby(['variant'])['modal_nash_conv'].describe().sort_values('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('external', nan, True, nan, nan, nan, nan) 89.64118749999997\n",
      "1 ('outcome', True, True, nan, nan, nan, nan) 47.383837499999956\n",
      "2 ('outcome', True, nan, nan, nan, nan, nan) 39.95759999999996\n",
      "3 ('outcome', True, True, nan, nan, 'straightforward_clock', 100.0) 33.80759999999998\n",
      "4 ('external', True, nan, nan, nan, nan, nan) 30.080400000000015\n"
     ]
    }
   ],
   "source": [
    "#### PORTFOLIO LOGIC ####\n",
    "portfolio = []\n",
    "algs = df_time['variant'].unique()\n",
    "df_port = df_all[['variant', 'game', 'modal_nash_conv']].rename(columns={'variant': 'alg', 'game': 'instance', 'modal_nash_conv': 'runtime'})\n",
    "\n",
    "# Greedy portfolio agg\n",
    "for i in range(5):\n",
    "    alg2score = dict()\n",
    "    for alg in algs:\n",
    "        new_port = portfolio + [alg]\n",
    "        score = df_port.query('alg in @new_port').groupby('instance')['runtime'].min().sum()\n",
    "        alg2score[alg] = score\n",
    "\n",
    "    best_alg = pd.Series(alg2score).sort_values().idxmin()\n",
    "    best_score = pd.Series(alg2score).sort_values().min()\n",
    "    print(i, best_alg, best_score)\n",
    "    portfolio.append(best_alg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heatmap = df_all[['variant', 'modal_nash_conv', 'game']].pivot_table(\n",
    "    values='modal_nash_conv',\n",
    "    columns=['game'],\n",
    "    index='variant'\n",
    ")\n",
    "df_heatmap_norm=(df_heatmap / df_heatmap.max())\n",
    "sns.heatmap(df_heatmap_norm, annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[['variant', 'modal_nash_conv', 'game']].pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = df_all.query(\"\"\" variant == \"('outcome', True, True, 0.4)\" \"\"\")\n",
    "(z['modal_nash_conv'] / z['rewards'].apply(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time['reward_sum'] = df_time['rewards'].apply(lambda x: sum(x))\n",
    "df_time['nash_conv_frac'] = df_time['modal_nash_conv'] / df_time['reward_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time.loc[df_time.groupby('run')['t'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I only want the LAST nashconv, this is dumb\n",
    "# df.query('~modal_nash_conv.isnull() and alg == \"cfr_outcome\"').groupby('run')['modal_nash_conv'].describe().sort_values('mean', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.groupby('run')['t'].idxmax()].set_index('run')['nash_conv_frac'].sort_values(ascending=False).describe() # Why 69 and not 75?\n",
    "# In summary, NashConv is usually less than 10%. Hopefully this is also true on a per-player basis. There are a 13 bad cases with > 10%, 7 NaN, and 49 \"good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if you took the \"best\" t? Can get up to 53 that way... \n",
    "(df.groupby('run')['nash_conv_frac'].min() < 0.1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "plt_df = df_time.query('~modal_nash_conv.isnull() and alg == \"cfr_external\"').groupby('run').filter(lambda grp: len(grp) > 1).sort_values('t')\n",
    "for r, sub_df in plt_df.groupby('run'):\n",
    "    # display(sub_df)\n",
    "    plt.plot(sub_df['t'].values, sub_df['modal_nash_conv'].values)\n",
    "# plt.legend()\n",
    "# plt.semilogy()\n",
    "# sns.lineplot(data=plt_df, y='modal_nash_conv', x='t', hue='run', legend=False)\n",
    "\n",
    "# How many lines are changing downwards? Why is almost no progress ever made after 500K iterations?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_df['run'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = df.loc[df.groupby('run')['t'].idxmin()].set_index('run')['modal_nash_conv'] - df.loc[df.groupby('run')['t'].idxmax()].set_index('run')['modal_nash_conv']\n",
    "deltas[deltas > 0]\n",
    "# Most of the time, the deltas are very weakly positive. Can definitely move in either direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(e.get_model()._infostates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empirical = True\n",
    "# USE_MODAL = True\n",
    "\n",
    "records = []\n",
    "for run in tqdm(runs):\n",
    "    try:\n",
    "    \n",
    "        try:\n",
    "            game, final_checkpoint, policy = get_results(run, skip_single_chance_nodes=False, load_policy=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping run {run.name} because of error {e}\")\n",
    "            continue\n",
    "\n",
    "#         if USE_MODAL:\n",
    "#             evaluation = final_checkpoint.get_modal_eval()\n",
    "#             # if make_combo_specific(evaluation):\n",
    "#             #     print(run.name)\n",
    "#         else:\n",
    "#             evaluation = final_checkpoint.get_old_eval()\n",
    "            \n",
    "            \n",
    "        # total_entropy = analyze_samples(final_checkpoint.get_old_eval().samples, game)['total_entropy'] # Use old eval since modal is stupid for this and it's a convergence test\n",
    "\n",
    "        # if empirical: \n",
    "        #     history_dists = empirical_history_distribution(final_checkpoint.get_old_eval()) # Always use the old eval here\n",
    "        #     history_entropies = [scipy.stats.entropy(list(history_dists.loc[type_combo].values)) for type_combo in np.unique(history_dists.index.get_level_values(0))]\n",
    "        # else:\n",
    "        #     history_dists = compute_per_type_combo(history_distribution, policy, game, min_prob=0.01, history_type='processed')\n",
    "        #     history_entropies = [scipy.stats.entropy(list(history_dists[type_combo].values())) for type_combo in history_dists]\n",
    "\n",
    "        # try:\n",
    "        #     # TODO: Broken until you remove \"Tremble\" agents\n",
    "        #     _, _, approx_nash_conv = find_best_checkpoint(run)\n",
    "        # except:\n",
    "        #     # TODO: Fix bug where this actually happens because your BRs failed to do better and you have an Empirical Nash Conv of 0, very different from NaN\n",
    "        #     # ev_df = parse_run(run, None)\n",
    "        #     # display(ev_df)\n",
    "        #     approx_nash_conv = np.nan\n",
    "\n",
    "        record = {\n",
    "            # clock auction params\n",
    "            'game_name': run.game.name, \n",
    "            'potential': run.config.get('potential_function', 'None'),\n",
    "            'seed': run.config.get('seed'), \n",
    "            'run_name': run.name,\n",
    "            'experiment': run.experiment.name,\n",
    "            'config': run.get_config_name(),\n",
    "            't': final_checkpoint.t,\n",
    "\n",
    "            # CMA knobs\n",
    "            'information_policy': InformationPolicy(game.auction_params.information_policy).name,\n",
    "            'activity_policy': ActivityPolicy(game.auction_params.activity_policy).name,\n",
    "            'undersell_policy': UndersellPolicy(game.auction_params.undersell_policy).name,\n",
    "            'clock_speed': game.auction_params.increment,\n",
    "            'base_game_name': '_'.join(run.game.name.split('/')[1].split('_')[:2]), # Stupid naming convention that will surely bite us later\n",
    "\n",
    "            # solver information\n",
    "            'alg': get_algorithm_from_run(run),\n",
    "            'walltime': run.walltime(),\n",
    "\n",
    "            # metrics from eval\n",
    "            # 'total_entropy': total_entropy,\n",
    "\n",
    "            # stats about history distribution\n",
    "            # 'avg_distinct_histories': np.mean([len(d) for d in history_dists.values()]),\n",
    "            # 'avg_history_entropy': np.mean(history_entropies),\n",
    "        } \n",
    "        \n",
    "        evaluation = final_checkpoint.get_modal_eval()\n",
    "        record['nash_conv'] = evaluation.nash_conv\n",
    "        record['rewards'] = evaluation.mean_rewards\n",
    "        record['nash_conv_frac'] = evaluation.nash_conv / sum(evaluation.mean_rewards) if not pd.isnull(evaluation.nash_conv) else np.nan\n",
    "        for i in range(game.num_players()):\n",
    "            record[f'player_improvements_{i}'] = evaluation.player_improvements[i] if not pd.isnull(evaluation.nash_conv) else np.nan\n",
    "            record[f'player_improvements_frac_{i}'] = (evaluation.player_improvements[i] / evaluation.mean_rewards[i]) if not pd.isnull(evaluation.nash_conv) else np.nan\n",
    "        record.update(**analyze_samples(evaluation.samples, game))\n",
    "\n",
    "        # Slowwwwww you may want to comment this out when not using it\n",
    "#         start = time.time()\n",
    "#         c = final_checkpoint.equilibrium_solver_run.config\n",
    "#         cfr = c.get('solver_type') == 'cfr'\n",
    "#         env_and_policy = ppo_db_checkpoint_loader(final_checkpoint, cfr=cfr)\n",
    "#         for player in range(game.num_players()):\n",
    "#             env_and_policy.agents[player] = ModalAgentDecorator(env_and_policy.agents[player])\n",
    "#         modal_policy = env_and_policy.make_policy()\n",
    "        \n",
    "#         worked, nc = time_bounded_run(30, nash_conv, game, modal_policy)\n",
    "#         if not worked:\n",
    "#             print(\"Aborted run because time\")\n",
    "#         record['nash_conv'] = nc if worked else np.nan\n",
    "#         record['nash_conv_time'] = time.time() - start\n",
    "#         print(time.time() - start)\n",
    "        records.append(record)\n",
    "    except Exception as e:\n",
    "        print(f\"Something wrong with {run}. Skipping. {e}\")\n",
    "        raise e\n",
    "print(len(records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(records)\n",
    "# df['imperfect'] = df['game_name'].str.contains('imperfect')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('config')['walltime'].describe() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('~imperfect')['config'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('~imperfect').groupby('config')['nash_conv'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('~nash_conv.isnull()').sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['run_name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('experiment')['avg_history_entropy'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupers = ['base_game_name', 'clock_speed', 'information_policy']\n",
    "df.sort_values(groupers).set_index([df.index]+groupers, drop=True).to_csv('jun5_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(groupers).set_index([df.index]+groupers, drop=True)\n",
    "df.to_csv('greg3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['variant'] = list(map(str,(zip(df.information_policy, df.clock_speed, df.activity_policy, df.undersell_policy))))\n",
    "palette = dict()\n",
    "\n",
    "colors = ['red', 'blue', 'magenta', 'green', 'orange']\n",
    "\n",
    "for i, v in enumerate(df['variant'].unique()):\n",
    "    palette[v] = colors[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to a) Remove \"bad\" entries b) Be careful about comparisons that are missing datapoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove bad entries\n",
    "good_thresh = 0.1\n",
    "good_thresh_abs = 5\n",
    "df_plt = df.query(f'player_improvements_0 < {good_thresh_abs} and player_improvements_1 < {good_thresh_abs}')\n",
    "# df_plt = df.query(f'player_improvements_frac_0 < {good_thresh} and player_improvements_frac_1 < {good_thresh}')\n",
    "# df_plt = df.query(f'nash_conv_frac < {good_thresh}')\n",
    "len(df), len(df_plt)\n",
    "\n",
    "(df.groupby('game_name').size() - df_plt.groupby('game_name').size()).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    df[f'rewards_{i}'] = df['rewards'].apply(lambda x: x[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rewards_0'].describe(), df['rewards_1'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['rewards']].explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rewards'].explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rewards'].explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_plt), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.query('activity_policy == \"OFF\"')['nash_conv_frac']\n",
    "df.query('undersell_policy == \"UNDERSELL\" and information_policy == \"SHOW_DEMAND\" and activity_policy == \"ON\" and clock_speed == 0.3 ')[['t', 'nash_conv', 'nash_conv_frac']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTRIBUTE = 'variant'\n",
    "NORMALIZED = False\n",
    "plot_type = 'scatter'\n",
    "box_df = df_plt.copy()\n",
    "markers = ['o', 'X', '^']\n",
    "\n",
    "# Vanilla for clock\n",
    "# box_df = box_df.query('information_policy == \"SHOW_DEMAND\" and activity_policy == \"ON\" and undersell_policy == \"UNDERSELL\"')\n",
    "\n",
    "# # Check undersell\n",
    "box_df = box_df.query('information_policy == \"SHOW_DEMAND\" and activity_policy == \"ON\" and clock_speed == 0.3')\n",
    "\n",
    "# # Check activity\n",
    "# box_df = box_df.query('information_policy == \"SHOW_DEMAND\" and undersell_policy == \"UNDERSELL\" and clock_speed == 0.3')\n",
    "\n",
    "box_df = box_df.sort_values(['clock_speed', 'game_name', 'config'])\n",
    "\n",
    "STATS = ['total_revenue', 'total_welfare', 'auction_lengths']\n",
    "# Normalize by the mean of the \"base\" version, so the scales line up better in the same graph\n",
    "if NORMALIZED: \n",
    "    for stat in STATS:\n",
    "        base = str(('SHOW_DEMAND', 0.3))\n",
    "        base_stat = box_df.pivot_table(index=['base_game_name'], values=stat, aggfunc='mean', columns=ATTRIBUTE)\n",
    "        box_df[f'normalized_{stat}'] = box_df.apply(lambda x: x[stat] / base_stat.loc[x['base_game_name'], base], axis=1)\n",
    "\n",
    "for stat in STATS:\n",
    "    if NORMALIZED:\n",
    "        stat = f'normalized_{stat}'\n",
    "    plt.figure(figsize=(20,10))\n",
    "    if plot_type == 'box':\n",
    "        ax = sns.boxplot(data=box_df, x='base_game_name', y=stat, hue=ATTRIBUTE)\n",
    "        # ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
    "    else:\n",
    "        strip_kwargs = dict(x='base_game_name', y=stat, hue=ATTRIBUTE, s=10, alpha=0.5, dodge=True, jitter=False, palette=palette)\n",
    "        for i, config in enumerate(box_df['config'].unique()):\n",
    "            # print(config, markers[i])\n",
    "            ax = sns.stripplot(data=box_df.query(f'config == \"{config}\"'), marker=markers[i], **strip_kwargs)\n",
    "            if i == 0:\n",
    "                old_handles, old_labels = ax.get_legend_handles_labels()\n",
    "            # ax = sns.stripplot(data=box_df.query('alg.str.contains(\"PPO\") and imperfect'), marker='X', **strip_kwargs)\n",
    "            # ax = sns.stripplot(data=box_df.query('alg.str.contains(\"cfr\")'), marker='^', **strip_kwargs)\n",
    "        ax.legend(old_handles, old_labels)\n",
    "\n",
    "    plt.title(f\"{stat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('information_policy == \"SHOW_DEMAND\" and undersell_policy == \"UNDERSELL\" and clock_speed == 0.3')[['nash_conv_frac']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ### Code to rerun evals for a failed experiment\n",
    "# from open_spiel.python.examples.ubc_dispatch import dispatch_eval_database\n",
    "# exp = 'jun5outcome'\n",
    "\n",
    "# for e in Evaluation.objects.filter(checkpoint__equilibrium_solver_run__experiment__name=exp):\n",
    "#     experiment_name = e.checkpoint.equilibrium_solver_run.experiment.name\n",
    "#     run_name = e.checkpoint.equilibrium_solver_run.name\n",
    "#     t = e.checkpoint.t\n",
    "#     dispatch_eval_database(experiment_name, run_name, t, None, None)\n",
    "# print(Evaluation.objects.filter(checkpoint__equilibrium_solver_run__experiment__name=exp).delete())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('alg == \"PPO\"').groupby(['experiment', 'imperfect']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for run in tqdm(runs):\n",
    "    try:\n",
    "        ev_df = parse_run(run, expected_additional_br=0)\n",
    "        if ev_df is None:\n",
    "            continue\n",
    "        ev_df = ev_df.query('name == \"modal\"')\n",
    "        if not ev_df.empty:\n",
    "            val = ((ev_df['Regret'] / ev_df['Baseline']).mean() * 100)\n",
    "            record = dict(val=val, alg=get_algorithm_from_run(run), game=run.game.name)\n",
    "            records.append(record)\n",
    "    except KeyError as e:\n",
    "        continue\n",
    "val_df = pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df.groupby(['game', 'alg']).describe()\n",
    "val_df.groupby(['game', 'alg']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open_spiel.python.examples.ubc_dispatch as dispatch\n",
    "# for run in runs:\n",
    "#     try:\n",
    "#         game, final_checkpoint, policy = get_results(run, skip_single_chance_nodes=False)\n",
    "#     except:\n",
    "#         print(\"Skipping\", run)\n",
    "#         continue\n",
    "#     final_checkpoint.bestresponse_set.all().delete()\n",
    "#     for player in range(game.num_players()):\n",
    "#         dispatch.dispatch_eval_database(run.experiment.name, run.name, final_checkpoint.t, player, 'modal') \n",
    "#         dispatch.dispatch_eval_database(run.experiment.name, run.name, final_checkpoint.t, player, 'tremble')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the cell you run when you want to rerun evals \n",
    "bad_count = 0\n",
    "for run in tqdm(runs):\n",
    "    # print(run)\n",
    "    try:\n",
    "        game, final_checkpoint, policy = get_results(run, skip_single_chance_nodes=False, load_policy=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping run {run.name} because of error {e}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        ev = final_checkpoint.get_modal_eval()\n",
    "        if not pd.isnull(ev.nash_conv):\n",
    "            continue\n",
    "        else:\n",
    "            ev.delete()\n",
    "    except Exception as e:\n",
    "        ev = None\n",
    "\n",
    "    bad_count += 1\n",
    "    br_mapping = {p: 'modal' for p in range(2)}\n",
    "    dispatch.dispatch_eval_database(final_checkpoint.t, run.experiment.name, run.name, str(br_mapping))\n",
    "    \n",
    "print(bad_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pyspiel.load_game('python_clock_auction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
