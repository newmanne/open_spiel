{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pyspiel\n",
    "import open_spiel.python.examples.ubc_utils\n",
    "from open_spiel.python.visualizations import ubc_treeviz\n",
    "from open_spiel.python.examples.ubc_utils import *\n",
    "from auctions.webutils import *\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)\n",
    "import os\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "from open_spiel.python.examples.ubc_cma import *\n",
    "from open_spiel.python.examples.ubc_decorators import TakeSingleActionDecorator, TremblingAgentDecorator, ModalAgentDecorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = EquilibriumSolverRun.objects.get(name='jun9_jun9_0_base-cfr_outcomemccfr_outcome-101')\n",
    "game, checkpoint, policy = get_results(run)\n",
    "\n",
    "c = checkpoint.equilibrium_solver_run.config\n",
    "cfr = c.get('solver_type') == 'cfr'\n",
    "\n",
    "env_and_policy = ppo_db_checkpoint_loader(checkpoint, cfr=cfr)\n",
    "for player in range(game.num_players()):\n",
    "    env_and_policy.agents[player] = ModalAgentDecorator(env_and_policy.agents[player])\n",
    "policy = env_and_policy.make_policy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Responses + NashConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<open_spiel.python.rl_agent_policy.JointRLAgentPolicy at 0x7faf643df040>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 1.0}\n",
      "{0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 1.0}\n",
      "{0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 1.0, 7: 0.0, 8: 0.0, 9: 0.0}\n",
      "{0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 1.0, 9: 0.0}\n",
      "{0: 0.0, 1: 0.0, 2: 0.0, 5: 0.0, 6: 1.0}\n"
     ]
    }
   ],
   "source": [
    "state = game.new_initial_state().child(0).child(0)\n",
    "\n",
    "print(policy.action_probabilities(state))\n",
    "print(policy.action_probabilities(state.child(9)))\n",
    "print(policy.action_probabilities(state.child(9).child(9)))\n",
    "print(policy.action_probabilities(state.child(9).child(9).child(6)))\n",
    "print(policy.action_probabilities(state.child(9).child(9).child(6).child(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing best-responder...\n",
      "Computing best reponse...\n",
      "110.2971\n",
      "Initializing best-responder...\n",
      "Computing best reponse...\n",
      "214.127\n"
     ]
    }
   ],
   "source": [
    "from open_spiel.python.algorithms.best_response import BestResponsePolicy\n",
    "for player_num in range(game.num_players()):\n",
    "    print(\"Initializing best-responder...\")\n",
    "    br = BestResponsePolicy(game, player_num, policy)\n",
    "    print(\"Computing best reponse...\")\n",
    "    print(br.value(game.new_initial_state()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.963800000000006"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from open_spiel.python.algorithms.exploitability import nash_conv\n",
    "nash_conv(game, policy, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External Sampling CFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = EquilibriumSolverRun.objects.get(name='jun9_jun9_0_base-cfr_outcomemccfr_outcome-101')\n",
    "game, checkpoint, policy = get_results(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_spiel.python.algorithms.external_sampling_mccfr import ExternalSamplingSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = ExternalSamplingSolver(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                            | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████████████████████████████████████████▌                                                                                                                | 306/1000 [03:28<07:53,  1.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/apps/open_spiel/notebooks/greg/2023-06-15-nash_conv_cfr_performance.ipynb Cell 12\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcfrgpu22/apps/open_spiel/notebooks/greg/2023-06-15-nash_conv_cfr_performance.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m)):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcfrgpu22/apps/open_spiel/notebooks/greg/2023-06-15-nash_conv_cfr_performance.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     solver\u001b[39m.\u001b[39;49miteration()\n",
      "File \u001b[0;32m/apps/open_spiel/open_spiel/python/algorithms/external_sampling_mccfr.py:65\u001b[0m, in \u001b[0;36mExternalSamplingSolver.iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m\"\"\"Performs one iteration of external sampling.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[39mAn iteration consists of one episode for each player as the update\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mplayer.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m player \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_players):\n\u001b[0;32m---> 65\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_regrets(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_game\u001b[39m.\u001b[39;49mnew_initial_state(), player)\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_average_type \u001b[39m==\u001b[39m AverageType\u001b[39m.\u001b[39mFULL:\n\u001b[1;32m     67\u001b[0m   reach_probs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_players, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n",
      "File \u001b[0;32m/apps/open_spiel/open_spiel/python/algorithms/external_sampling_mccfr.py:130\u001b[0m, in \u001b[0;36mExternalSamplingSolver._update_regrets\u001b[0;34m(self, state, player)\u001b[0m\n\u001b[1;32m    128\u001b[0m   outcome \u001b[39m=\u001b[39m fast_choice(outcomes, probs)\n\u001b[1;32m    129\u001b[0m   \u001b[39m# outcome = np.random.choice(outcomes, p=probs)\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_regrets(state\u001b[39m.\u001b[39;49mchild(outcome), player)\n\u001b[1;32m    132\u001b[0m cur_player \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39mcurrent_player()\n\u001b[1;32m    133\u001b[0m info_state_key \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39minformation_state_string(cur_player)\n",
      "File \u001b[0;32m/apps/open_spiel/open_spiel/python/algorithms/external_sampling_mccfr.py:130\u001b[0m, in \u001b[0;36mExternalSamplingSolver._update_regrets\u001b[0;34m(self, state, player)\u001b[0m\n\u001b[1;32m    128\u001b[0m   outcome \u001b[39m=\u001b[39m fast_choice(outcomes, probs)\n\u001b[1;32m    129\u001b[0m   \u001b[39m# outcome = np.random.choice(outcomes, p=probs)\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_regrets(state\u001b[39m.\u001b[39;49mchild(outcome), player)\n\u001b[1;32m    132\u001b[0m cur_player \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39mcurrent_player()\n\u001b[1;32m    133\u001b[0m info_state_key \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39minformation_state_string(cur_player)\n",
      "File \u001b[0;32m/apps/open_spiel/open_spiel/python/algorithms/external_sampling_mccfr.py:153\u001b[0m, in \u001b[0;36mExternalSamplingSolver._update_regrets\u001b[0;34m(self, state, player)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m   \u001b[39m# Walk over all actions at my node\u001b[39;00m\n\u001b[1;32m    152\u001b[0m   \u001b[39mfor\u001b[39;00m action_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_legal_actions):\n\u001b[0;32m--> 153\u001b[0m     child_values[action_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_regrets(\n\u001b[1;32m    154\u001b[0m         state\u001b[39m.\u001b[39;49mchild(legal_actions[action_idx]), player)\n\u001b[1;32m    155\u001b[0m     value \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m policy[action_idx] \u001b[39m*\u001b[39m child_values[action_idx]\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m cur_player \u001b[39m==\u001b[39m player:\n\u001b[1;32m    158\u001b[0m   \u001b[39m# Update regrets.\u001b[39;00m\n",
      "File \u001b[0;32m/apps/open_spiel/open_spiel/python/algorithms/external_sampling_mccfr.py:148\u001b[0m, in \u001b[0;36mExternalSamplingSolver._update_regrets\u001b[0;34m(self, state, player)\u001b[0m\n\u001b[1;32m    146\u001b[0m   action_idx \u001b[39m=\u001b[39m fast_choice(np\u001b[39m.\u001b[39marange(num_legal_actions), policy)\n\u001b[1;32m    147\u001b[0m   \u001b[39m# action_idx = np.random.choice(np.arange(num_legal_actions), p=policy)\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_regrets(\n\u001b[1;32m    149\u001b[0m       state\u001b[39m.\u001b[39;49mchild(legal_actions[action_idx]), player)\n\u001b[1;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m   \u001b[39m# Walk over all actions at my node\u001b[39;00m\n\u001b[1;32m    152\u001b[0m   \u001b[39mfor\u001b[39;00m action_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_legal_actions):\n",
      "File \u001b[0;32m/apps/open_spiel/open_spiel/python/algorithms/external_sampling_mccfr.py:153\u001b[0m, in \u001b[0;36mExternalSamplingSolver._update_regrets\u001b[0;34m(self, state, player)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m   \u001b[39m# Walk over all actions at my node\u001b[39;00m\n\u001b[1;32m    152\u001b[0m   \u001b[39mfor\u001b[39;00m action_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_legal_actions):\n\u001b[0;32m--> 153\u001b[0m     child_values[action_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_regrets(\n\u001b[1;32m    154\u001b[0m         state\u001b[39m.\u001b[39;49mchild(legal_actions[action_idx]), player)\n\u001b[1;32m    155\u001b[0m     value \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m policy[action_idx] \u001b[39m*\u001b[39m child_values[action_idx]\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m cur_player \u001b[39m==\u001b[39m player:\n\u001b[1;32m    158\u001b[0m   \u001b[39m# Update regrets.\u001b[39;00m\n",
      "File \u001b[0;32m/apps/open_spiel/open_spiel/python/algorithms/external_sampling_mccfr.py:148\u001b[0m, in \u001b[0;36mExternalSamplingSolver._update_regrets\u001b[0;34m(self, state, player)\u001b[0m\n\u001b[1;32m    146\u001b[0m   action_idx \u001b[39m=\u001b[39m fast_choice(np\u001b[39m.\u001b[39marange(num_legal_actions), policy)\n\u001b[1;32m    147\u001b[0m   \u001b[39m# action_idx = np.random.choice(np.arange(num_legal_actions), p=policy)\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_regrets(\n\u001b[1;32m    149\u001b[0m       state\u001b[39m.\u001b[39;49mchild(legal_actions[action_idx]), player)\n\u001b[1;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m   \u001b[39m# Walk over all actions at my node\u001b[39;00m\n\u001b[1;32m    152\u001b[0m   \u001b[39mfor\u001b[39;00m action_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_legal_actions):\n",
      "File \u001b[0;32m/apps/open_spiel/open_spiel/python/algorithms/external_sampling_mccfr.py:160\u001b[0m, in \u001b[0;36mExternalSamplingSolver._update_regrets\u001b[0;34m(self, state, player)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m cur_player \u001b[39m==\u001b[39m player:\n\u001b[1;32m    158\u001b[0m   \u001b[39m# Update regrets.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m   \u001b[39mfor\u001b[39;00m action_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_legal_actions):\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_regret(info_state_key, action_idx,\n\u001b[1;32m    161\u001b[0m                      child_values[action_idx] \u001b[39m-\u001b[39m value)\n\u001b[1;32m    162\u001b[0m \u001b[39m# Simple average does averaging on the opponent node. To do this in a game\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m# with more than two players, we only update the player + 1 mod num_players,\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m# which reduces to the standard rule in 2 players.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_average_type \u001b[39m==\u001b[39m AverageType\u001b[39m.\u001b[39mSIMPLE \u001b[39mand\u001b[39;00m cur_player \u001b[39m==\u001b[39m (\n\u001b[1;32m    166\u001b[0m     player \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_players:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    solver.iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.08"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 * 3600 * 12 / 1e6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
