{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4d1ec4-edc2-4274-9c59-b38d8d3edda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.2.min.js\"];\n",
       "  const css_urls = [];\n",
       "  \n",
       "\n",
       "  const inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.2.min.js\"];\n  const css_urls = [];\n  \n\n  const inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from open_spiel.python.algorithms.exploitability import nash_conv, best_response\n",
    "from open_spiel.python.algorithms.ubc_exploitability import game_tree_size\n",
    "\n",
    "from open_spiel.python.examples.ubc_plotting_utils import *\n",
    "from open_spiel.python.examples.ubc_sample_game_tree import sample_game_tree, flatten_trees, flatten_tree\n",
    "from open_spiel.python.examples.ubc_sample_game_tree import NodeType\n",
    "\n",
    "from auctions.webutils import *\n",
    "\n",
    "from open_spiel.python.algorithms.random_agent import RandomAgent\n",
    "import bokeh\n",
    "from bokeh.layouts import row, column\n",
    "from bokeh.plotting import figure, show, output_file, save\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, ColorBar, LogColorMapper, LinearColorMapper\n",
    "from bokeh.transform import linear_cmap, log_cmap\n",
    "from bokeh.palettes import Category10_10, Magma256, Spectral10, Category20_20\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "from collections import defaultdict\n",
    "from open_spiel.python.examples.ppo_eval import run_eval\n",
    "from open_spiel.python.examples.ppo_utils import run_ppo\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e45bdbb7-f028-47fc-a1ff-01ea4b7ddcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONS\n",
    "EXPERIMENT_NAME = 'jan12_2'\n",
    "RUN_NAME = 'jan12_regional_vs_national_reveal-dec15ppo_3-102'\n",
    "### Load checkpoint\n",
    "checkpoint = get_checkpoint_by_name(EXPERIMENT_NAME, RUN_NAME)\n",
    "checkpoint.equilibrium_solver_run.config['device'] = 'cpu'\n",
    "checkpoint.equilibrium_solver_run.config['use_wandb'] = False\n",
    "env_and_policy = ppo_db_checkpoint_loader(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "949ad2e1-7d1e-47c6-a6db-7f2ecc8ce8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prof():\n",
    "    return run_eval(env_and_policy, num_samples=10_000, report_freq=1000)\n",
    "\n",
    "def prof_training():\n",
    "    timesteps = 100_000\n",
    "    return run_ppo(env_and_policy, 50_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb5665da-bc76-4c37-9843-2f9f66cd13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile, pstats\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "# eval_output = prof()\n",
    "eval_output = prof_training()\n",
    "profiler.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89dae0c4-a245-4b29-b793-8a1ed9d0a653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         65434710 function calls (64616505 primitive calls) in 154.747 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000  154.916   77.458 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3215(run_code)\n",
      "        2    0.000    0.000  154.916   77.458 {built-in method builtins.exec}\n",
      "        1    0.000    0.000  154.916  154.916 /tmp/ipykernel_27885/2878371160.py:5(<module>)\n",
      "        1    0.000    0.000  154.916  154.916 /tmp/ipykernel_27885/1631146118.py:4(prof_training)\n",
      "        1    0.000    0.000  154.916  154.916 /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:348(run_ppo)\n",
      "        1    1.168    1.168  154.915  154.915 /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:249(training_loop)\n",
      "    24960    0.151    0.000  110.459    0.004 /apps/open_spiel/open_spiel/python/vector_env.py:20(step)\n",
      "   299524   12.249    0.000   75.414    0.000 /apps/open_spiel/open_spiel/python/rl_environment.py:237(get_time_step)\n",
      "    24960    0.589    0.000   72.013    0.003 /apps/open_spiel/open_spiel/python/vector_env.py:27(<listcomp>)\n",
      "    99840    0.202    0.000   71.424    0.001 /apps/open_spiel/open_spiel/python/env_decorator.py:29(step)\n",
      "   199684    5.119    0.000   57.697    0.000 /apps/open_spiel/open_spiel/python/env_decorator.py:48(get_time_step)\n",
      "   648936   27.038    0.000   43.052    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:833(set_from)\n",
      "    99840    1.139    0.000   41.343    0.000 /apps/open_spiel/open_spiel/python/rl_environment.py:302(step)\n",
      "    24961    0.061    0.000   38.191    0.002 /apps/open_spiel/open_spiel/python/vector_env.py:37(reset)\n",
      "    24961    0.244    0.000   38.125    0.002 /apps/open_spiel/open_spiel/python/vector_env.py:41(<listcomp>)\n",
      "      390    2.630    0.007   24.336    0.062 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:287(learn)\n",
      "    31590    0.136    0.000   18.018    0.001 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:237(get_action_and_value)\n",
      "    31590    0.595    0.000   17.833    0.001 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:69(get_action_and_value)\n",
      "    24960    1.148    0.000   17.689    0.001 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:240(step)\n",
      "    24944    0.052    0.000   16.241    0.001 /apps/open_spiel/open_spiel/python/env_decorator.py:33(reset)\n",
      "   124784    2.297    0.000   11.214    0.000 /apps/open_spiel/open_spiel/python/rl_environment.py:398(_sample_external_events)\n",
      "   274588    5.626    0.000   10.818    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:322(_legal_actions)\n",
      "    24944    1.413    0.000   10.010    0.000 /apps/open_spiel/open_spiel/python/rl_environment.py:344(reset)\n",
      "  1572460    7.684    0.000    8.148    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction_bidders.py:25(get_profits)\n",
      "   648936    0.785    0.000    7.495    0.000 /apps/open_spiel/open_spiel/python/rl_environment.py:60(fastcopy)\n",
      "   174708    1.854    0.000    6.838    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:370(_apply_action)\n",
      "381420/63570    0.703    0.000    6.780    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1124(_call_impl)\n",
      "   648936    2.449    0.000    6.600    0.000 /apps/open_spiel/open_spiel/python/rl_environment.py:62(<dictcomp>)\n",
      "    63570    0.575    0.000    6.522    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/container.py:137(forward)\n",
      "     6240    0.052    0.000    6.090    0.001 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:108(wrapper)\n",
      "     6240    0.030    0.000    5.824    0.001 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:24(decorate_context)\n",
      "     6240    0.241    0.000    5.710    0.001 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/adam.py:105(step)\n",
      "  5591726    5.406    0.000    5.406    0.000 {built-in method numpy.array}\n",
      "     6240    0.020    0.000    5.357    0.001 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:340(backward)\n",
      "     6240    0.036    0.000    5.335    0.001 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:85(backward)\n",
      "     6240    0.022    0.000    5.240    0.001 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/adam.py:176(adam)\n",
      "     6240    5.211    0.001    5.211    0.001 {method 'run_backward' of 'torch._C._EngineBase' objects}\n",
      "     6240    1.395    0.000    5.189    0.001 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/adam.py:229(_single_tensor_adam)\n",
      "    74868    1.591    0.000    4.988    0.000 /apps/open_spiel/open_spiel/python/examples/ubc_utils.py:156(__call__)\n",
      "  1698916    4.856    0.000    4.856    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "  1173128    0.906    0.000    4.590    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
      "    31590    0.120    0.000    4.478    0.000 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:20(__init__)\n",
      " 10208388    4.423    0.000    4.423    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
      "   190710    0.482    0.000    4.092    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:113(forward)\n",
      "    31590    0.507    0.000    3.949    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/categorical.py:49(__init__)\n",
      "  1173128    0.520    0.000    3.684    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py:54(_any)\n",
      "   249604    3.527    0.000    3.527    0.000 {built-in method torch.tensor}\n",
      "   190710    3.426    0.000    3.426    0.000 {built-in method torch._C._nn.linear}\n",
      "774388/624668    1.283    0.000    3.393    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "     6240    0.323    0.000    3.354    0.001 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py:9(clip_grad_norm_)\n",
      "    49880    1.195    0.000    3.197    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:454(returns)\n",
      "    31590    0.363    0.000    3.031    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/categorical.py:119(log_prob)\n",
      "    74868    0.611    0.000    2.675    0.000 /apps/open_spiel/open_spiel/python/examples/ubc_math_utils.py:4(fast_choice)\n",
      "    24980    0.935    0.000    2.617    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:581(_process_bids)\n",
      "    49920    1.309    0.000    2.152    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:512(_post_process)\n",
      "   199680    2.075    0.000    2.075    0.000 /apps/open_spiel/open_spiel/python/rl_environment.py:472(is_turn_based)\n",
      "    31590    1.796    0.000    1.796    0.000 {method 'logsumexp' of 'torch._C._TensorBase' objects}\n",
      "   424468    0.277    0.000    1.734    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
      "    49920    0.121    0.000    1.687    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:423(_handle_bids)\n",
      "    24960    1.513    0.000    1.685    0.000 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:119(legal_actions_to_mask)\n",
      "    31590    0.282    0.000    1.680    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/distribution.py:255(_validate_sample)\n",
      "    99840    0.066    0.000    1.599    0.000 /apps/open_spiel/open_spiel/python/rl_environment.py:467(num_actions_per_step)\n",
      "    31590    0.342    0.000    1.574    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/distribution.py:34(__init__)\n",
      "   424468    0.160    0.000    1.457    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py:46(_sum)\n",
      "     6240    0.148    0.000    1.452    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py:42(<listcomp>)\n",
      "   149736    0.189    0.000    1.446    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:51(_wrapfunc)\n",
      "    74868    0.071    0.000    1.444    0.000 <__array_function__ internals>:177(cumsum)\n",
      "    99760    0.585    0.000    1.327    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction_bidders.py:62(value_for_package)\n",
      "    31590    0.525    0.000    1.295    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/categorical.py:127(entropy)\n",
      "    74868    0.089    0.000    1.282    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2495(cumsum)\n",
      "    24960    0.425    0.000    1.240    0.000 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:279(post_step)\n",
      "   224640    1.236    0.000    1.236    0.000 {method 'mul_' of 'torch._C._TensorBase' objects}\n",
      "    74880    1.207    0.000    1.207    0.000 {method 'sqrt' of 'torch._C._TensorBase' objects}\n",
      "    31590    1.116    0.000    1.116    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/constraints.py:256(check)\n",
      "  2564377    1.092    0.000    1.092    0.000 {built-in method builtins.min}\n",
      "   374348    0.322    0.000    1.086    0.000 <__array_function__ internals>:177(where)\n",
      "    81120    0.356    0.000    1.083    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/functional.py:1345(norm)\n",
      "    24960    0.188    0.000    1.046    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/categorical.py:112(sample)\n",
      "    74868    0.211    0.000    1.035    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:38(_wrapit)\n",
      "    74860    0.079    0.000    0.929    0.000 <__array_function__ internals>:177(zeros_like)\n",
      "   324468    0.873    0.000    0.873    0.000 /apps/open_spiel/open_spiel/python/rl_environment.py:463(num_players)\n",
      "    31590    0.204    0.000    0.872    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/constraints.py:208(check)\n",
      "     6240    0.051    0.000    0.780    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py:32(<listcomp>)\n",
      "    63180    0.165    0.000    0.770    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/utils.py:106(__get__)\n",
      "    74860    0.185    0.000    0.749    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/numeric.py:76(zeros_like)\n",
      "   648936    0.735    0.000    0.735    0.000 {method 'fill' of 'numpy.ndarray' objects}\n",
      "   409048    0.691    0.000    0.726    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:554(_apply_bid)\n",
      "4662088/4662087    0.724    0.000    0.724    0.000 {built-in method builtins.len}\n",
      "  2770640    0.711    0.000    0.711    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:309(current_player)\n",
      "    74868    0.633    0.000    0.689    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:498(chance_outcomes)\n",
      "    81120    0.033    0.000    0.680    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1621(parameters)\n",
      "     6240    0.190    0.000    0.650    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:216(zero_grad)\n",
      "    81120    0.033    0.000    0.647    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1645(named_parameters)\n",
      "  1747168    0.642    0.000    0.642    0.000 {built-in method numpy.asarray}\n",
      "    24944    0.038    0.000    0.640    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:272(new_initial_state)\n",
      "    81120    0.154    0.000    0.615    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1608(_named_members)\n",
      "    24944    0.264    0.000    0.602    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:290(__init__)\n",
      "    81120    0.596    0.000    0.596    0.000 {built-in method torch.norm}\n",
      "   149760    0.596    0.000    0.596    0.000 {method 'add_' of 'torch._C._TensorBase' objects}\n",
      "    63180    0.071    0.000    0.572    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:97(forward)\n",
      "    94770    0.542    0.000    0.542    0.000 {method 'all' of 'torch._C._TensorBase' objects}\n",
      "    74880    0.534    0.000    0.534    0.000 {method 'addcdiv_' of 'torch._C._TensorBase' objects}\n",
      "    99760    0.102    0.000    0.527    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
      "    63180    0.064    0.000    0.500    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/functional.py:1446(relu)\n",
      "   349304    0.494    0.000    0.494    0.000 {built-in method numpy.zeros}\n",
      "    63960    0.046    0.000    0.464    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:353(forward)\n",
      "    31590    0.163    0.000    0.462    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/functional.py:44(broadcast_tensors)\n",
      "    43680    0.451    0.000    0.451    0.000 {method 'mean' of 'torch._C._TensorBase' objects}\n",
      "    74868    0.055    0.000    0.450    0.000 <__array_function__ internals>:177(searchsorted)\n",
      "    31590    0.026    0.000    0.439    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/categorical.py:92(probs)\n",
      "  3673212    0.435    0.000    0.435    0.000 {method 'append' of 'list' objects}\n",
      "   829920    0.337    0.000    0.429    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:1071(grad)\n",
      "    99760    0.039    0.000    0.426    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py:60(_all)\n",
      "    63180    0.423    0.000    0.423    0.000 {built-in method torch.relu}\n",
      "    50310    0.423    0.000    0.423    0.000 {built-in method torch.clamp}\n",
      "    63960    0.419    0.000    0.419    0.000 {built-in method torch.tanh}\n",
      "    49920    0.088    0.000    0.418    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:713(__iter__)\n",
      "    31590    0.039    0.000    0.412    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/utils.py:65(logits_to_probs)\n",
      "    43680    0.040    0.000    0.400    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:26(wrapped)\n",
      "    31590    0.035    0.000    0.373    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/functional.py:1804(softmax)\n",
      "    31590    0.370    0.000    0.370    0.000 {built-in method torch.where}\n",
      "    74868    0.367    0.000    0.367    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
      "   508560    0.353    0.000    0.353    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194(__getattr__)\n",
      "    85020    0.352    0.000    0.352    0.000 {method 'reshape' of 'torch._C._TensorBase' objects}\n",
      "    31590    0.331    0.000    0.331    0.000 {method 'softmax' of 'torch._C._TensorBase' objects}\n",
      "    24960    0.179    0.000    0.330    0.000 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:275(<listcomp>)\n",
      "  1247984    0.328    0.000    0.328    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:450(is_terminal)\n",
      "    99804    0.100    0.000    0.327    0.000 <__array_function__ internals>:177(copyto)\n",
      "    74868    0.067    0.000    0.320    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:1319(searchsorted)\n",
      "   149786    0.312    0.000    0.312    0.000 {method 'detach' of 'torch._C._TensorBase' objects}\n",
      "   199684    0.312    0.000    0.312    0.000 {method 'tolist' of 'torch._C._TensorBase' objects}\n",
      "   250380    0.293    0.000    0.293    0.000 {method 'to' of 'torch._C._TensorBase' objects}\n",
      "    74880    0.292    0.000    0.292    0.000 {method 'addcmul_' of 'torch._C._TensorBase' objects}\n",
      "    49920    0.282    0.000    0.282    0.000 {method 'unbind' of 'torch._C._TensorBase' objects}\n",
      "393120/93600    0.252    0.000    0.278    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1775(named_modules)\n",
      "    31590    0.275    0.000    0.275    0.000 {method 'any' of 'torch._C._TensorBase' objects}\n",
      "  1572460    0.274    0.000    0.274    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction_bidders.py:22(get_values)\n",
      "   431340    0.271    0.000    0.271    0.000 {built-in method torch._C._get_tracing_state}\n",
      "    51090    0.127    0.000    0.271    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1210(__setattr__)\n",
      "    74860    0.089    0.000    0.268    0.000 <__array_function__ internals>:177(empty_like)\n",
      "693050/641960    0.195    0.000    0.267    0.000 {built-in method builtins.isinstance}\n",
      "    31590    0.258    0.000    0.258    0.000 {method 'gather' of 'torch._C._TensorBase' objects}\n",
      "    31590    0.253    0.000    0.253    0.000 {built-in method torch.isnan}\n",
      "    24980    0.122    0.000    0.253    0.000 /apps/open_spiel/open_spiel/python/examples/ubc_utils.py:206(permute_array)\n",
      "    31590    0.252    0.000    0.252    0.000 {method 'sum' of 'torch._C._TensorBase' objects}\n",
      "    31590    0.246    0.000    0.246    0.000 {built-in method torch.broadcast_tensors}\n",
      "    37440    0.242    0.000    0.242    0.000 {built-in method torch.zeros}\n",
      "   400525    0.133    0.000    0.229    0.000 {built-in method builtins.getattr}\n",
      "    31590    0.220    0.000    0.220    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/constraints.py:307(check)\n",
      "   274588    0.220    0.000    0.220    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
      "   798816    0.220    0.000    0.220    0.000 {built-in method builtins.max}\n",
      "   424308    0.211    0.000    0.211    0.000 {built-in method __new__ of type object at 0x55e11fb0dfc0}\n",
      "    24960    0.028    0.000    0.208    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_ops.py:138(__call__)\n",
      "    24980    0.192    0.000    0.204    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:413(_generate_processing_queue)\n",
      "    24944    0.041    0.000    0.197    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/numeric.py:149(ones)\n",
      "    12480    0.046    0.000    0.188    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/profiler.py:445(__enter__)\n",
      "    24960    0.185    0.000    0.185    0.000 {built-in method torch.multinomial}\n",
      "    56160    0.185    0.000    0.185    0.000 {method 'view' of 'torch._C._TensorBase' objects}\n",
      "    74868    0.183    0.000    0.183    0.000 {method 'searchsorted' of 'numpy.ndarray' objects}\n",
      "   181350    0.174    0.000    0.174    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
      "    24960    0.011    0.000    0.171    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:637(__rsub__)\n",
      "    74868    0.169    0.000    0.169    0.000 {method 'rand' of 'numpy.random.mtrand.RandomState' objects}\n",
      "    37830    0.078    0.000    0.162    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:131(__enter__)\n",
      "    24960    0.159    0.000    0.159    0.000 {built-in method torch.rsub}\n",
      "   224640    0.093    0.000    0.156    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:731(__hash__)\n",
      "    75660    0.113    0.000    0.152    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:226(__init__)\n",
      "  1236690    0.150    0.000    0.150    0.000 {built-in method torch._C._has_torch_function_unary}\n",
      "    37830    0.064    0.000    0.148    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:135(__exit__)\n",
      "    12480    0.126    0.000    0.126    0.000 {built-in method torch._ops.profiler._record_function_enter}\n",
      "    63570    0.083    0.000    0.125    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/container.py:129(__iter__)\n",
      "    37830    0.117    0.000    0.125    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:126(__init__)\n",
      "    31590    0.118    0.000    0.118    0.000 {method 'squeeze' of 'torch._C._TensorBase' objects}\n",
      "   680527    0.117    0.000    0.117    0.000 {method 'items' of 'dict' objects}\n",
      "    31590    0.117    0.000    0.117    0.000 {method 'unsqueeze' of 'torch._C._TensorBase' objects}\n",
      "    24960    0.114    0.000    0.114    0.000 {method 'flatten' of 'torch._C._TensorBase' objects}\n",
      "    74880    0.109    0.000    0.109    0.000 {method 'zero_' of 'torch._C._TensorBase' objects}\n",
      "     6240    0.103    0.000    0.103    0.000 {built-in method torch.stack}\n",
      "    12480    0.025    0.000    0.101    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/profiler.py:436(__init__)\n",
      "   112710    0.069    0.000    0.100    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_VF.py:25(__getattr__)\n",
      "    12480    0.031    0.000    0.096    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/profiler.py:449(__exit__)\n",
      "     6240    0.062    0.000    0.091    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:641(__rdiv__)\n",
      "   162270    0.045    0.000    0.090    0.000 {method 'add' of 'set' objects}\n",
      "    31590    0.042    0.000    0.089    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/categorical.py:84(support)\n",
      "    12480    0.087    0.000    0.087    0.000 {method 'pow' of 'torch._C._TensorBase' objects}\n",
      "     6240    0.086    0.000    0.086    0.000 {method 'std' of 'torch._C._TensorBase' objects}\n",
      "    51090    0.063    0.000    0.083    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/parameter.py:9(__instancecheck__)\n",
      "   157950    0.082    0.000    0.082    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
      "     6240    0.032    0.000    0.079    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:30(_make_grads)\n",
      "    24973    0.060    0.000    0.078    0.000 /apps/open_spiel/open_spiel/python/examples/ubc_utils.py:191(convert_seed_to_swaps)\n",
      "    24960    0.038    0.000    0.077    0.000 /apps/open_spiel/open_spiel/python/vector_env.py:29(<listcomp>)\n",
      "    12480    0.076    0.000    0.076    0.000 {built-in method torch.max}\n",
      "    38220    0.076    0.000    0.076    0.000 {method 'long' of 'torch._C._TensorBase' objects}\n",
      "   374348    0.074    0.000    0.074    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/multiarray.py:341(where)\n",
      "    99840    0.037    0.000    0.071    0.000 <string>:1(__new__)\n",
      "    31590    0.048    0.000    0.070    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:177(__enter__)\n",
      "    63180    0.028    0.000    0.061    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/constraints.py:142(is_dependent)\n",
      "   225810    0.057    0.000    0.057    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
      "    31590    0.055    0.000    0.055    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/utils.py:121(__init__)\n",
      "    12480    0.054    0.000    0.054    0.000 {built-in method torch._ops.profiler._record_function_exit}\n",
      "      780    0.001    0.000    0.050    0.000 <__array_function__ internals>:177(var)\n",
      "    87360    0.039    0.000    0.050    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1666(<lambda>)\n",
      "    24960    0.046    0.000    0.050    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/distribution.py:241(_extended_shape)\n",
      "    49888    0.049    0.000    0.049    0.000 <string>:2(__init__)\n",
      "   145080    0.047    0.000    0.047    0.000 {built-in method torch.is_grad_enabled}\n",
      "    31590    0.047    0.000    0.047    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/constraints.py:251(__init__)\n",
      "      780    0.004    0.000    0.047    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3624(var)\n",
      "     6240    0.045    0.000    0.045    0.000 {method 'exp' of 'torch._C._TensorBase' objects}\n",
      "     6630    0.044    0.000    0.044    0.000 {method 'abs' of 'torch._C._TensorBase' objects}\n",
      "      780    0.027    0.000    0.043    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py:195(_var)\n",
      "      390    0.001    0.000    0.042    0.000 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:234(get_value)\n",
      "    24960    0.041    0.000    0.041    0.000 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:261(<listcomp>)\n",
      "     6240    0.041    0.000    0.041    0.000 {built-in method torch.ones_like}\n",
      "      390    0.001    0.000    0.040    0.000 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:66(get_value)\n",
      "    31590    0.039    0.000    0.039    0.000 {method 'bool' of 'torch._C._TensorBase' objects}\n",
      "    99840    0.039    0.000    0.039    0.000 /apps/open_spiel/open_spiel/python/rl_environment.py:94(last)\n",
      "   138840    0.036    0.000    0.036    0.000 {built-in method torch._C._set_grad_enabled}\n",
      "   224640    0.036    0.000    0.036    0.000 {built-in method builtins.id}\n",
      "   113490    0.036    0.000    0.036    0.000 {built-in method builtins.iter}\n",
      "    24944    0.032    0.000    0.032    0.000 {built-in method numpy.empty}\n",
      "    31590    0.024    0.000    0.030    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:181(__exit__)\n",
      "     6240    0.010    0.000    0.030    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:82(clone)\n",
      "     6240    0.029    0.000    0.029    0.000 {method 'reciprocal' of 'torch._C._TensorBase' objects}\n",
      "    74880    0.029    0.000    0.029    0.000 {method 'conj' of 'torch._C._TensorBase' objects}\n",
      "     6240    0.029    0.000    0.029    0.000 {method 'float' of 'torch._C._TensorBase' objects}\n",
      "    24973    0.019    0.000    0.028    0.000 /apps/open_spiel/open_spiel/python/examples/ubc_utils.py:188(factorial)\n",
      "     6240    0.019    0.000    0.027    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/adam.py:198(<listcomp>)\n",
      "    74880    0.026    0.000    0.026    0.000 {method 'requires_grad_' of 'torch._C._TensorBase' objects}\n",
      "    49953    0.025    0.000    0.025    0.000 {built-in method math.factorial}\n",
      "     1560    0.025    0.000    0.025    0.000 {method 'shuffle' of 'numpy.random.mtrand.RandomState' objects}\n",
      "     6240    0.009    0.000    0.025    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:84(_cuda_graph_capture_health_check)\n",
      "    24960    0.024    0.000    0.024    0.000 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:263(<listcomp>)\n",
      "    49888    0.022    0.000    0.022    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:504(<listcomp>)\n",
      "   174798    0.022    0.000    0.022    0.000 {method 'items' of 'collections.OrderedDict' objects}\n",
      "    49880    0.022    0.000    0.022    0.000 /apps/open_spiel/open_spiel/python/rl_environment.py:278(<listcomp>)\n",
      "    81120    0.020    0.000    0.020    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/functional.py:1450(<listcomp>)\n",
      "    75270    0.020    0.000    0.020    0.000 {built-in method torch._C._has_torch_function}\n",
      "   159511    0.020    0.000    0.020    0.000 {method 'get' of 'dict' objects}\n",
      "    99804    0.019    0.000    0.019    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/multiarray.py:1071(copyto)\n",
      "    24960    0.018    0.000    0.018    0.000 /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:273(<listcomp>)\n",
      "    74860    0.018    0.000    0.018    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/numeric.py:72(_zeros_like_dispatcher)\n",
      "    63596    0.018    0.000    0.018    0.000 {method 'values' of 'collections.OrderedDict' objects}\n",
      "    24960    0.017    0.000    0.017    0.000 /apps/open_spiel/open_spiel/python/vector_env.py:28(<listcomp>)\n",
      "    74868    0.017    0.000    0.017    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:1315(_searchsorted_dispatcher)\n",
      "    74868    0.016    0.000    0.016    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2491(_cumsum_dispatcher)\n",
      "    74880    0.016    0.000    0.016    0.000 {built-in method math.sqrt}\n",
      "     6240    0.007    0.000    0.015    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:77(is_available)\n",
      "    24973    0.015    0.000    0.015    0.000 /apps/open_spiel/open_spiel/python/examples/ubc_utils.py:225(<listcomp>)\n",
      "    24980    0.013    0.000    0.013    0.000 /apps/open_spiel/open_spiel/python/games/clock_auction.py:584(<listcomp>)\n",
      "    74860    0.012    0.000    0.012    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/multiarray.py:80(empty_like)\n",
      "    31590    0.011    0.000    0.011    0.000 {built-in method builtins.setattr}\n",
      "     6296    0.010    0.000    0.010    0.000 {method 'format' of 'str' objects}\n",
      "    51090    0.009    0.000    0.009    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f4897069280}\n",
      "    31590    0.009    0.000    0.009    0.000 {method 'ndimension' of 'torch._C._TensorBase' objects}\n",
      "    24960    0.008    0.000    0.008    0.000 {method 'numel' of 'torch.Size' objects}\n",
      "    37830    0.008    0.000    0.008    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_jit_internal.py:958(is_scripting)\n",
      "      780    0.006    0.000    0.007    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py:66(_count_reduce_items)\n",
      "    13286    0.006    0.000    0.006    0.000 {built-in method builtins.hasattr}\n",
      "     6240    0.005    0.000    0.005    0.000 {built-in method torch._C._cuda_getDeviceCount}\n",
      "      390    0.005    0.000    0.005    0.000 {method 'max' of 'torch._C._TensorBase' objects}\n",
      "     6240    0.004    0.000    0.004    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:77(_tensor_or_tensors_to_tuple)\n",
      "     6240    0.003    0.000    0.003    0.000 {built-in method builtins.all}\n",
      "     6240    0.003    0.000    0.003    0.000 {method 'numel' of 'torch._C._TensorBase' objects}\n",
      "      390    0.003    0.000    0.003    0.000 {built-in method torch.zeros_like}\n",
      "      390    0.002    0.000    0.002    0.000 {built-in method numpy.arange}\n",
      "      780    0.001    0.000    0.001    0.000 {method 'numpy' of 'torch._C._TensorBase' objects}\n",
      "      390    0.001    0.000    0.001    0.000 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:293(<listcomp>)\n",
      "      780    0.001    0.000    0.001    0.000 {method 'cpu' of 'torch._C._TensorBase' objects}\n",
      "     1560    0.001    0.000    0.001    0.000 {built-in method builtins.issubclass}\n",
      "        1    0.000    0.000    0.001    0.001 /apps/open_spiel/open_spiel/python/examples/ubc_utils.py:43(fix_seeds)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/random.py:26(manual_seed)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/cuda/random.py:98(manual_seed_all)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:154(_lazy_call)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/traceback.py:193(format_stack)\n",
      "      780    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
      "     1560    0.000    0.000    0.000    0.000 {built-in method numpy.asanyarray}\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/traceback.py:200(extract_stack)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/traceback.py:321(extract)\n",
      "        1    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:356(eval_hook)\n",
      "        1    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:331(ppo_checkpoint)\n",
      "      780    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3619(_var_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/rl_agent_policy.py:82(save)\n",
      "        2    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:429(save)\n",
      "     26/2    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1316(state_dict)\n",
      "      390    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/pytorch/ppo.py:439(get_max_policy_diff)\n",
      "        1    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/examples/ubc_utils.py:116(pretty_time)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/humanize/time.py:399(precisedelta)\n",
      "       19    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/IPython/core/compilerop.py:189(check_linecache_ipython)\n",
      "       26    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1279(_save_to_state_dict)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/traceback.py:27(format_list)\n",
      "       19    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/linecache.py:53(checkcache)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/traceback.py:388(format)\n",
      "       19    0.000    0.000    0.000    0.000 {built-in method posix.stat}\n",
      "       84    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/traceback.py:285(line)\n",
      "        2    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/codeop.py:135(__call__)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "       28    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/linecache.py:15(getline)\n",
      "        1    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/examples/env_and_policy.py:13(make_policy)\n",
      "        1    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:227(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/rl_agent_policy.py:32(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/logging/__init__.py:2062(info)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/humanize/time.py:62(_date_and_delta)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/absl/logging/__init__.py:413(info)\n",
      "       28    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/linecache.py:147(lazycache)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/humanize/time.py:383(_suppress_lower_units)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/absl/logging/__init__.py:531(log)\n",
      "        2    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/humanize/i18n.py:106(_ngettext)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/humanize/time.py:43(_now)\n",
      "       28    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/linecache.py:37(getlines)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/traceback.py:369(from_list)\n",
      "       29    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/traceback.py:292(walk_stack)\n",
      "        5    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/humanize/time.py:289(_quotient_and_remainder)\n",
      "        7    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/logging/__init__.py:1424(info)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method now}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'seed' of 'numpy.random.mtrand.RandomState' objects}\n",
      "       56    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
      "       28    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/traceback.py:243(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/humanize/i18n.py:27(get_translation)\n",
      "        2    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/contextlib.py:238(helper)\n",
      "       19    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/contextlib.py:108(__enter__)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/absl/logging/__init__.py:1118(log)\n",
      "        9    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/enum.py:641(__hash__)\n",
      "       13    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/enum.py:347(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'manual_seed' of 'torch._C.Generator' objects}\n",
      "        2    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/contextlib.py:82(__init__)\n",
      "       29    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n",
      "        8    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/logging/__init__.py:1677(isEnabledFor)\n",
      "        4    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/IPython/core/compilerop.py:174(extra_flags)\n",
      "        2    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/contextlib.py:117(__exit__)\n",
      "        1    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:363(report_hook)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:50(queue_seed_all)\n",
      "        2    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/traitlets/traitlets.py:566(__get__)\n",
      "        3    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/humanize/time.py:322(_carry)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/logging/__init__.py:1485(log)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/humanize/i18n.py:69(_gettext)\n",
      "        1    0.000    0.000    0.000    0.000 /tmp/ipykernel_27885/2878371160.py:6(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch._C._cuda_isInBadFork}\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/absl/logging/converter.py:138(absl_to_standard)\n",
      "        2    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/enum.py:346(__iter__)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}\n",
      "        7    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/enum.py:366(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3167(compare)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/enum.py:343(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/enum.py:365(__reversed__)\n",
      "        2    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/traitlets/traitlets.py:535(get)\n",
      "        2    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/gettext.py:290(ngettext)\n",
      "        1    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/policy.py:109(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:1142(user_global_ns)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
      "        2    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/enum.py:349(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/examples/env_and_policy.py:16(<dictcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/vector_env.py:10(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/humanize/time.py:47(_abs_timedelta)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method math.modf}\n",
      "        1    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:243(add_report_hook)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/humanize/time.py:353(_suitable_minimum_unit)\n",
      "        1    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/vector_env.py:39(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:246(add_eval_hook)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:149(is_initialized)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/humanize/time.py:470(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/gettext.py:272(gettext)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'reverse' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method sys._getframe}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7f4891acfac0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "stats.print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d7904ec-eb72-461c-b0ef-76ec6bc95f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprofile\n",
    "profiler = pprofile.Profile()\n",
    "with profiler:\n",
    "    prof_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14a7b8a2-6d41-4b08-8b4c-7861fe757402",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration: 621.077s\n",
      "File: /apps/open_spiel/open_spiel/python/games/clock_auction.py\n",
      "File duration: 286.258s (46.09%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|from collections import defaultdict\n",
      "     2|     49916|     0.104117|  2.08585e-06|  0.02%|from dataclasses import dataclass, field\n",
      "     3|     49916|     0.118459|  2.37316e-06|  0.02%|from distutils.command.build import build\n",
      "     4|     49916|    0.0955076|  1.91337e-06|  0.02%|from multiprocessing.sharedctypes import Value\n",
      "     5|     49916|    0.0942366|   1.8879e-06|  0.02%|from tkinter.messagebox import NO\n",
      "     6|     49916|    0.0896423|  1.79586e-06|  0.01%|import numpy as np\n",
      "     7|     49916|     0.089618|  1.79538e-06|  0.01%|import pandas as pd\n",
      "     8|     49916|    0.0925539|  1.85419e-06|  0.01%|from open_spiel.python.examples.ubc_utils import players_not_me, pulp_solve, random_string, permute_array, num_to_letter\n",
      "     9|         0|            0|            0|  0.00%|import pyspiel\n",
      "    10|         0|            0|            0|  0.00%|import json\n",
      "    11|         0|            0|            0|  0.00%|import os\n",
      "    12|         0|            0|            0|  0.00%|import logging\n",
      "    13|         0|            0|            0|  0.00%|import enum\n",
      "    14|         0|            0|            0|  0.00%|import itertools\n",
      "    15|         0|            0|            0|  0.00%|from typing import List, Dict, Tuple, Optional, Any, Union, Iterable\n",
      "    16|         0|            0|            0|  0.00%|import math\n",
      "    17|         0|            0|            0|  0.00%|from open_spiel.python.games import clock_auction_bidders\n",
      "    18|         0|            0|            0|  0.00%|from functools import cached_property\n",
      "    19|         0|            0|            0|  0.00%|from pulp import LpProblem, LpMinimize, LpVariable, LpStatus, LpBinary, lpSum, lpDot, LpMaximize, LpInteger, value\n",
      "    20|         0|            0|            0|  0.00%|\n",
      "    21|         0|            0|            0|  0.00%|DEFAULT_MAX_ROUNDS = 100\n",
      "    22|         0|            0|            0|  0.00%|DEFAULT_AGENT_MEMORY = 3\n",
      "    23|         0|            0|            0|  0.00%|\n",
      "    24|         0|            0|            0|  0.00%|class ActivityPolicy(enum.IntEnum):\n",
      "    25|         0|            0|            0|  0.00%|  ON = 0\n",
      "    26|         0|            0|            0|  0.00%|  OFF = 1\n",
      "    27|         0|            0|            0|  0.00%|\n",
      "    28|         0|            0|            0|  0.00%|class UndersellPolicy(enum.IntEnum):\n",
      "    29|         0|            0|            0|  0.00%|  UNDERSELL = 0\n",
      "    30|         0|            0|            0|  0.00%|  UNDERSELL_ALLOWED = 1\n",
      "    31|         0|            0|            0|  0.00%|\n",
      "    32|         0|            0|            0|  0.00%|class InformationPolicy(enum.IntEnum):\n",
      "    33|         0|            0|            0|  0.00%|  SHOW_DEMAND = 0\n",
      "    34|         0|            0|            0|  0.00%|  HIDE_DEMAND = 1\n",
      "    35|         0|            0|            0|  0.00%|\n",
      "    36|         0|            0|            0|  0.00%|class InformationPolicyConstants:\n",
      "    37|         0|            0|            0|  0.00%|  OVER_DEMAND = 1\n",
      "    38|         0|            0|            0|  0.00%|  AT_SUPPLY = 0\n",
      "    39|         0|            0|            0|  0.00%|  UNDER_DEMAND = 3\n",
      "    40|         0|            0|            0|  0.00%|\n",
      "    41|         0|            0|            0|  0.00%|class ValueFormat(enum.IntEnum):\n",
      "    42|         0|            0|            0|  0.00%|  LINEAR = 0\n",
      "    43|         0|            0|            0|  0.00%|  FULL = 1\n",
      "    44|         0|            0|            0|  0.00%|  MARGINAL = 2\n",
      "    45|         0|            0|            0|  0.00%|\n",
      "    46|         0|            0|            0|  0.00%|@dataclass\n",
      "    47|         0|            0|            0|  0.00%|class AuctionParams:\n",
      "    48|         0|            0|            0|  0.00%|  opening_prices: List[float] = field(default_factory=lambda : [])\n",
      "    49|         0|            0|            0|  0.00%|  licenses: List[int] = field(default_factory=lambda : [])\n",
      "    50|         0|            0|            0|  0.00%|  license_names: List[str] = field(default_factory=lambda : [])\n",
      "    51|         0|            0|            0|  0.00%|  activity: List[int] = field(default_factory=lambda : [])\n",
      "    52|         0|            0|            0|  0.00%|  num_products: int = 0\n",
      "    53|         0|            0|            0|  0.00%|  increment: float = 0.1\n",
      "    54|         0|            0|            0|  0.00%|  reveal_type_round: int = None\n",
      "    55|         0|            0|            0|  0.00%|\n",
      "    56|         0|            0|            0|  0.00%|  max_round: int = DEFAULT_MAX_ROUNDS\n",
      "    57|         0|            0|            0|  0.00%|  player_types: Dict = None\n",
      "    58|         0|            0|            0|  0.00%|\n",
      "    59|         0|            0|            0|  0.00%|  all_bids: List[List[int]] = None\n",
      "    60|         0|            0|            0|  0.00%|  all_bids_activity: List[int] = None\n",
      "    61|         0|            0|            0|  0.00%|\n",
      "    62|         0|            0|            0|  0.00%|  activity_policy: ActivityPolicy = ActivityPolicy.ON\n",
      "    63|         0|            0|            0|  0.00%|  undersell_policy: UndersellPolicy = UndersellPolicy.UNDERSELL\n",
      "    64|         0|            0|            0|  0.00%|  information_policy: InformationPolicy = InformationPolicy.SHOW_DEMAND\n",
      "    65|         0|            0|            0|  0.00%|\n",
      "    66|         0|            0|            0|  0.00%|  agent_memory: int = DEFAULT_AGENT_MEMORY\n",
      "    67|         0|            0|            0|  0.00%|\n",
      "    68|         0|            0|            0|  0.00%|  @cached_property\n",
      "    69|         0|            0|            0|  0.00%|  def max_activity(self):\n",
      "    70|         0|            0|            0|  0.00%|    return np.array(self.activity) @ np.array(self.licenses)\n",
      "    71|         0|            0|            0|  0.00%|\n",
      "    72|         0|            0|            0|  0.00%|  def max_budget_for_player(self, player_id):\n",
      "    73|         0|            0|            0|  0.00%|    return max([t['bidder'].get_budget() for t in self.player_types[player_id]])\n",
      "    74|         0|            0|            0|  0.00%|\n",
      "    75|         0|            0|            0|  0.00%|  @cached_property\n",
      "    76|         0|            0|            0|  0.00%|  def max_budget(self):\n",
      "    77|         0|            0|            0|  0.00%|    return max([self.max_budget_for_player(p) for p in range(len(self.player_types))])\n",
      "    78|         0|            0|            0|  0.00%|\n",
      "    79|         0|            0|            0|  0.00%|  @cached_property\n",
      "    80|         0|            0|            0|  0.00%|  def max_total_spend(self):\n",
      "    81|         0|            0|            0|  0.00%|    return sum([self.max_budget_for_player(p) for p in range(len(self.player_types))])\n",
      "    82|         0|            0|            0|  0.00%|\n",
      "    83|         0|            0|            0|  0.00%|  def max_opponent_spend(self, player_id):\n",
      "    84|         0|            0|            0|  0.00%|    return sum([self.max_budget_for_player(p) for p in range(len(self.player_types)) if p != player_id])\n",
      "    85|         0|            0|            0|  0.00%|\n",
      "    86|         0|            0|            0|  0.00%|  @cached_property\n",
      "    87|         0|            0|            0|  0.00%|  def max_opponent_spends(self):\n",
      "    88|         0|            0|            0|  0.00%|    return np.array([self.max_opponent_spend(p) for p in range(len(self.player_types))])\n",
      "    89|         0|            0|            0|  0.00%|\n",
      "    90|         0|            0|            0|  0.00%|@dataclass\n",
      "    91|         0|            0|            0|  0.00%|class BidderState:\n",
      "    92|         0|            0|            0|  0.00%|  processed_demand: List[List[int]] = field(default_factory=lambda : [])\n",
      "    93|         0|            0|            0|  0.00%|  submitted_demand: List[List[int]] = field(default_factory=lambda : [])\n",
      "    94|         0|            0|            0|  0.00%|  activity: int = None\n",
      "    95|         0|            0|            0|  0.00%|  bidder: clock_auction_bidders.Bidder = None\n",
      "    96|         0|            0|            0|  0.00%|  type_index: int = None\n",
      "    97|         0|            0|            0|  0.00%|  drop_out_heuristic: bool = True\n",
      "    98|         0|            0|            0|  0.00%|\n",
      "    99|         0|            0|            0|  0.00%|def action_to_bundles(licenses):\n",
      "   100|         0|            0|            0|  0.00%|    bids = []\n",
      "   101|         0|            0|            0|  0.00%|    for n in licenses:\n",
      "   102|         0|            0|            0|  0.00%|        b = []\n",
      "   103|         0|            0|            0|  0.00%|        for i in range(n + 1):\n",
      "   104|         0|            0|            0|  0.00%|            b.append(i)\n",
      "   105|         0|            0|            0|  0.00%|        bids.append(b)\n",
      "   106|         0|            0|            0|  0.00%|    actions = np.array(list(itertools.product(*bids)))\n",
      "   107|         0|            0|            0|  0.00%|    return actions\n",
      "   108|         0|            0|            0|  0.00%|\n",
      "   109|         0|            0|            0|  0.00%|_GAME_TYPE = pyspiel.GameType(\n",
      "   110|         0|            0|            0|  0.00%|    short_name=\"python_clock_auction\",\n",
      "   111|         0|            0|            0|  0.00%|    long_name=\"Python Clock Auction\",\n",
      "   112|         0|            0|            0|  0.00%|    dynamics=pyspiel.GameType.Dynamics.SEQUENTIAL,\n",
      "   113|         0|            0|            0|  0.00%|    chance_mode=pyspiel.GameType.ChanceMode.EXPLICIT_STOCHASTIC,\n",
      "   114|         0|            0|            0|  0.00%|    information=pyspiel.GameType.Information.IMPERFECT_INFORMATION,\n",
      "   115|         0|            0|            0|  0.00%|    utility=pyspiel.GameType.Utility.GENERAL_SUM,\n",
      "   116|         0|            0|            0|  0.00%|    reward_model=pyspiel.GameType.RewardModel.TERMINAL,\n",
      "   117|         0|            0|            0|  0.00%|    max_num_players=10,\n",
      "   118|         0|            0|            0|  0.00%|    min_num_players=2,\n",
      "   119|         0|            0|            0|  0.00%|    provides_information_state_string=False,\n",
      "   120|         0|            0|            0|  0.00%|    provides_information_state_tensor=False,\n",
      "   121|         0|            0|            0|  0.00%|    provides_observation_string=True,\n",
      "   122|         0|            0|            0|  0.00%|    provides_observation_tensor=True,\n",
      "   123|         0|            0|            0|  0.00%|    provides_factored_observation_string=False,\n",
      "   124|         0|            0|            0|  0.00%|    parameter_specification={\n",
      "   125|         0|            0|            0|  0.00%|      \"filename\": 'parameters.json'\n",
      "   126|         0|            0|            0|  0.00%|      }\n",
      "   127|         0|            0|            0|  0.00%|    )\n",
      "   128|         0|            0|            0|  0.00%|\n",
      "   129|         0|            0|            0|  0.00%|def parse_auction_params(file_name):\n",
      "   130|         0|            0|            0|  0.00%|  if file_name.startswith('/'):\n",
      "   131|         0|            0|            0|  0.00%|    full_path = file_name\n",
      "   132|         0|            0|            0|  0.00%|  else:\n",
      "   133|         0|            0|            0|  0.00%|    logging.info(\"Reading from env variable CLOCK_AUCTION_CONFIG_DIR. If it is not set, there will be trouble.\")\n",
      "   134|         0|            0|            0|  0.00%|    config_dir = os.environ.get('CLOCK_AUCTION_CONFIG_DIR')\n",
      "   135|         0|            0|            0|  0.00%|    if config_dir is None:\n",
      "   136|         0|            0|            0|  0.00%|      raise ValueError(\"CLOCK_AUCTION_CONFIG_DIR env variable is not set.\")\n",
      "   137|         0|            0|            0|  0.00%|    logging.info(f\"CLOCK_AUCTION_CONFIG_DIR={config_dir}\")\n",
      "   138|         0|            0|            0|  0.00%|    full_path = f'{config_dir}/{file_name}';\n",
      "   139|         0|            0|            0|  0.00%|\n",
      "   140|         0|            0|            0|  0.00%|  logging.info(f\"Parsing configuration from {full_path}\")\n",
      "   141|         0|            0|            0|  0.00%|  with open(full_path, 'r') as f:\n",
      "   142|         0|            0|            0|  0.00%|    game_params = json.load(f)\n",
      "   143|         0|            0|            0|  0.00%|\n",
      "   144|         0|            0|            0|  0.00%|    players = game_params['players']\n",
      "   145|         0|            0|            0|  0.00%|\n",
      "   146|         0|            0|            0|  0.00%|    opening_prices = game_params['opening_price']\n",
      "   147|         0|            0|            0|  0.00%|    licenses = np.array(game_params['licenses'])\n",
      "   148|         0|            0|            0|  0.00%|    if 'license_names' in game_params:\n",
      "   149|         0|            0|            0|  0.00%|      license_names = game_params['license_names']\n",
      "   150|         0|            0|            0|  0.00%|    else:\n",
      "   151|         0|            0|            0|  0.00%|      license_names = [num_to_letter(i) for i in range(len(licenses))]\n",
      "   152|         0|            0|            0|  0.00%|\n",
      "   153|         0|            0|            0|  0.00%|    num_products = len(licenses)\n",
      "   154|         0|            0|            0|  0.00%|\n",
      "   155|         0|            0|            0|  0.00%|    if len(opening_prices) != num_products:\n",
      "   156|         0|            0|            0|  0.00%|      raise ValueError(\"Number of opening prices must match number of products.\")\n",
      "   157|         0|            0|            0|  0.00%|\n",
      "   158|         0|            0|            0|  0.00%|    activity = game_params['activity']\n",
      "   159|         0|            0|            0|  0.00%|    if len(activity) != num_products:\n",
      "   160|         0|            0|            0|  0.00%|      raise ValueError(\"Number of activity must match number of products.\")\n",
      "   161|         0|            0|            0|  0.00%|\n",
      "   162|         0|            0|            0|  0.00%|    activity_policy = game_params.get('activity_policy', ActivityPolicy.ON)\n",
      "   163|         0|            0|            0|  0.00%|    if isinstance(activity_policy, bool):\n",
      "   164|         0|            0|            0|  0.00%|      activity_policy = ActivityPolicy.ON if activity_policy else ActivityPolicy.OFF\n",
      "   165|         0|            0|            0|  0.00%|\n",
      "   166|         0|            0|            0|  0.00%|    information_policy = game_params.get('information_policy', InformationPolicy.SHOW_DEMAND)\n",
      "   167|         0|            0|            0|  0.00%|    if isinstance(information_policy, str):\n",
      "   168|         0|            0|            0|  0.00%|      information_policy = InformationPolicy[information_policy.upper()]\n",
      "   169|         0|            0|            0|  0.00%|\n",
      "   170|         0|            0|            0|  0.00%|    undersell_policy = game_params.get('undersell_rule', UndersellPolicy.UNDERSELL)\n",
      "   171|         0|            0|            0|  0.00%|    if isinstance(undersell_policy, str):\n",
      "   172|         0|            0|            0|  0.00%|      undersell_policy = UndersellPolicy[undersell_policy.upper()]\n",
      "   173|         0|            0|            0|  0.00%|\n",
      "   174|         0|            0|            0|  0.00%|    reveal_type_round = int(game_params.get('reveal_type_round', -1))\n",
      "   175|         0|            0|            0|  0.00%|\n",
      "   176|         0|            0|            0|  0.00%|    all_bids = action_to_bundles(licenses)\n",
      "   177|         0|            0|            0|  0.00%|    all_bids_activity = np.array([activity @ bid for bid in all_bids])\n",
      "   178|         0|            0|            0|  0.00%|\n",
      "   179|         0|            0|            0|  0.00%|    types = defaultdict(list)\n",
      "   180|         0|            0|            0|  0.00%|\n",
      "   181|         0|            0|            0|  0.00%|    for player_id, player in enumerate(players):\n",
      "   182|         0|            0|            0|  0.00%|      player_types = player['type']\n",
      "   183|         0|            0|            0|  0.00%|      for player_type in player_types:\n",
      "   184|         0|            0|            0|  0.00%|        values = player_type['value']\n",
      "   185|         0|            0|            0|  0.00%|        if np.array(values).ndim == 2:\n",
      "   186|         0|            0|            0|  0.00%|          default_assumption = ValueFormat.MARGINAL\n",
      "   187|         0|            0|            0|  0.00%|        else:\n",
      "   188|         0|            0|            0|  0.00%|          default_assumption = ValueFormat.LINEAR\n",
      "   189|         0|            0|            0|  0.00%|        value_format = player_type.get('value_format', default_assumption)\n",
      "   190|         0|            0|            0|  0.00%|        if isinstance(value_format, str):\n",
      "   191|         0|            0|            0|  0.00%|          value_format = ValueFormat[value_format.upper()]\n",
      "   192|         0|            0|            0|  0.00%|        budget = player_type['budget']\n",
      "   193|         0|            0|            0|  0.00%|        prob = player_type['prob']\n",
      "   194|         0|            0|            0|  0.00%|        pricing_bonus = player_type.get('pricing_bonus', 0)\n",
      "   195|         0|            0|            0|  0.00%|        drop_out_heuristic = player_type.get('drop_out_heuristic', True)\n",
      "   196|         0|            0|            0|  0.00%|        name = player_type.get('name', None)\n",
      "   197|         0|            0|            0|  0.00%|\n",
      "   198|         0|            0|            0|  0.00%|        if value_format == ValueFormat.LINEAR:\n",
      "   199|         0|            0|            0|  0.00%|          if len(values) != num_products:\n",
      "   200|         0|            0|            0|  0.00%|            raise ValueError(\"Number of values must match number of products.\")\n",
      "   201|         0|            0|            0|  0.00%|          bidder = clock_auction_bidders.LinearBidder(values, budget, pricing_bonus, all_bids, drop_out_heuristic)\n",
      "   202|         0|            0|            0|  0.00%|        elif value_format == ValueFormat.FULL:\n",
      "   203|         0|            0|            0|  0.00%|          if len(values) != len(all_bids):\n",
      "   204|         0|            0|            0|  0.00%|            raise ValueError(\"Number of values must match number of bids.\")\n",
      "   205|         0|            0|            0|  0.00%|          bidder = clock_auction_bidders.EnumeratedValueBidder(values, budget, pricing_bonus, all_bids, drop_out_heuristic, name)\n",
      "   206|         0|            0|            0|  0.00%|        elif value_format == ValueFormat.MARGINAL:\n",
      "   207|         0|            0|            0|  0.00%|          bidder = clock_auction_bidders.MarginalValueBidder(values, budget, pricing_bonus, all_bids, drop_out_heuristic)\n",
      "   208|         0|            0|            0|  0.00%|        else:\n",
      "   209|         0|            0|            0|  0.00%|          raise ValueError(\"Unknown value format\")\n",
      "   210|         0|            0|            0|  0.00%|\n",
      "   211|         0|            0|            0|  0.00%|        types[player_id].append(dict(prob=prob, bidder=bidder))\n",
      "   212|         0|            0|            0|  0.00%|\n",
      "   213|         0|            0|            0|  0.00%|  logging.info(\"Done config parsing\")\n",
      "   214|         0|            0|            0|  0.00%|  return AuctionParams(\n",
      "   215|         0|            0|            0|  0.00%|      opening_prices=opening_prices,\n",
      "   216|         0|            0|            0|  0.00%|      licenses=licenses,\n",
      "   217|         0|            0|            0|  0.00%|      license_names=license_names,\n",
      "   218|         0|            0|            0|  0.00%|      num_products=num_products,\n",
      "   219|         0|            0|            0|  0.00%|      activity=activity,\n",
      "   220|         0|            0|            0|  0.00%|      increment=game_params.get('increment', 0.1),\n",
      "   221|         0|            0|            0|  0.00%|      max_round=game_params.get('max_rounds', DEFAULT_MAX_ROUNDS),\n",
      "   222|         0|            0|            0|  0.00%|      player_types=types,\n",
      "   223|         0|            0|            0|  0.00%|      all_bids=all_bids,\n",
      "   224|         0|            0|            0|  0.00%|      all_bids_activity=all_bids_activity,\n",
      "   225|         0|            0|            0|  0.00%|      activity_policy=activity_policy,\n",
      "   226|         0|            0|            0|  0.00%|      undersell_policy=undersell_policy,\n",
      "   227|         0|            0|            0|  0.00%|      information_policy=information_policy,\n",
      "   228|         0|            0|            0|  0.00%|      reveal_type_round=reveal_type_round,\n",
      "   229|         0|            0|            0|  0.00%|      agent_memory=game_params.get('agent_memory', DEFAULT_AGENT_MEMORY),\n",
      "   230|         0|            0|            0|  0.00%|    )\n",
      "   231|         0|            0|            0|  0.00%|\n",
      "   232|         0|            0|            0|  0.00%|class ClockAuctionGame(pyspiel.Game):\n",
      "   233|         0|            0|            0|  0.00%|\n",
      "   234|         0|            0|            0|  0.00%|  def __init__(self, params=None):\n",
      "   235|         0|            0|            0|  0.00%|    file_name = params.get('filename', 'parameters.json')\n",
      "   236|         0|            0|            0|  0.00%|    self.auction_params = parse_auction_params(file_name)\n",
      "   237|         0|            0|            0|  0.00%|    num_players = len(self.auction_params.player_types)\n",
      "   238|         0|            0|            0|  0.00%|\n",
      "   239|         0|            0|            0|  0.00%|    # Max of # of type draws and tie-breaking\n",
      "   240|         0|            0|            0|  0.00%|    max_chance_outcomes = max(max([len(v) for v in self.auction_params.player_types.values()]), math.factorial(num_players))\n",
      "   241|         0|            0|            0|  0.00%|\n",
      "   242|         0|            0|            0|  0.00%|    # You can bid for [0...M_j] for any of the j products\n",
      "   243|         0|            0|            0|  0.00%|    num_actions = (1 + np.array(self.auction_params.licenses)).prod()\n",
      "   244|         0|            0|            0|  0.00%|\n",
      "   245|         0|            0|            0|  0.00%|    # MAX AND MIN UTILITY\n",
      "   246|         0|            0|            0|  0.00%|    self.upper_bounds = []\n",
      "   247|         0|            0|            0|  0.00%|    self.lower_bounds = []\n",
      "   248|         0|            0|            0|  0.00%|    for player_id, types in self.auction_params.player_types.items():\n",
      "   249|         0|            0|            0|  0.00%|      player_upper_bounds = []\n",
      "   250|         0|            0|            0|  0.00%|      player_lower_bounds = []\n",
      "   251|         0|            0|            0|  0.00%|      for t in types:\n",
      "   252|         0|            0|            0|  0.00%|        bidder = t['bidder']\n",
      "   253|         0|            0|            0|  0.00%|        # What if you won your favorite package at opening prices?\n",
      "   254|         0|            0|            0|  0.00%|        bound = bidder.get_profits(self.auction_params.opening_prices).max()\n",
      "   255|         0|            0|            0|  0.00%|        player_upper_bounds.append(bound)\n",
      "   256|         0|            0|            0|  0.00%|        # What if you spent your entire budget and got nothing? (A tighter not implemented bound: if you got the single worst item for you, since you must be paying for something)\n",
      "   257|         0|            0|            0|  0.00%|        player_lower_bounds.append(-bidder.budget)\n",
      "   258|         0|            0|            0|  0.00%|      self.upper_bounds.append(max(player_upper_bounds))\n",
      "   259|         0|            0|            0|  0.00%|      self.lower_bounds.append(min(player_lower_bounds))\n",
      "   260|         0|            0|            0|  0.00%|\n",
      "   261|         0|            0|            0|  0.00%|    game_info = pyspiel.GameInfo(\n",
      "   262|         0|            0|            0|  0.00%|        num_distinct_actions=num_actions,\n",
      "   263|         0|            0|            0|  0.00%|        max_chance_outcomes=max_chance_outcomes,\n",
      "   264|         0|            0|            0|  0.00%|        num_players=num_players,\n",
      "   265|         0|            0|            0|  0.00%|        min_utility=-99999,\n",
      "   266|         0|            0|            0|  0.00%|        max_utility=max(self.upper_bounds),\n",
      "   267|         0|            0|            0|  0.00%|        utility_sum=-99999,\n",
      "   268|         0|            0|            0|  0.00%|        max_game_length=9999)\n",
      "   269|         0|            0|            0|  0.00%|\n",
      "   270|         0|            0|            0|  0.00%|    super().__init__(_GAME_TYPE, game_info, params)\n",
      "   271|         0|            0|            0|  0.00%|\n",
      "   272|     24958|    0.0410721|  1.64565e-06|  0.01%|  def new_initial_state(self):\n",
      "   273|         0|            0|            0|  0.00%|    \"\"\"Returns a state corresponding to the start of a game.\"\"\"\n",
      "   274|     24958|     0.173511|  6.95213e-06|  0.03%|    return ClockAuctionState(self)\n",
      "(call)|     24958|      2.24438|  8.99263e-05|  0.36%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:290 __init__\n",
      "   275|         0|            0|            0|  0.00%|\n",
      "   276|         0|            0|            0|  0.00%|  def make_py_observer(self, iig_obs_type=None, params=None):\n",
      "   277|         0|            0|            0|  0.00%|    \"\"\"Returns an object used for observing game state.\"\"\"\n",
      "   278|         0|            0|            0|  0.00%|    if params is None:\n",
      "   279|         0|            0|            0|  0.00%|      params = dict()\n",
      "   280|         0|            0|            0|  0.00%|\n",
      "   281|         0|            0|            0|  0.00%|    params['auction_params'] = self.auction_params\n",
      "   282|         0|            0|            0|  0.00%|\n",
      "   283|         0|            0|            0|  0.00%|    return ClockAuctionObserver(\n",
      "   284|         0|            0|            0|  0.00%|        iig_obs_type or pyspiel.IIGObservationType(perfect_recall=False),\n",
      "   285|         0|            0|            0|  0.00%|        params)\n",
      "   286|         0|            0|            0|  0.00%|\n",
      "   287|         0|            0|            0|  0.00%|class ClockAuctionState(pyspiel.State):\n",
      "   288|         0|            0|            0|  0.00%|  \"\"\"A python version of the Atari Game state.\"\"\"\n",
      "   289|         0|            0|            0|  0.00%|\n",
      "   290|     24958|     0.045892|  1.83877e-06|  0.01%|  def __init__(self, game):\n",
      "   291|         0|            0|            0|  0.00%|    \"\"\"Constructor; should only be called by Game.new_initial_state.\"\"\"\n",
      "   292|     24958|     0.204786|  8.20522e-06|  0.03%|    super().__init__(game)\n",
      "   293|     24958|    0.0607488|  2.43404e-06|  0.01%|    self.auction_params = game.auction_params\n",
      "   294|     24958|    0.0452459|  1.81288e-06|  0.01%|    self._game_over = False\n",
      "   295|     24958|    0.0445819|  1.78628e-06|  0.01%|    self._auction_finished = False\n",
      "   296|     24958|    0.0444267|  1.78006e-06|  0.01%|    self._is_chance = True\n",
      "   297|     24958|     0.044486|  1.78244e-06|  0.01%|    self.bidders = []\n",
      "   298|     24958|     0.109704|  4.39554e-06|  0.02%|    self.posted_prices = [np.array(self.auction_params.opening_prices)]\n",
      "   299|     24958|    0.0684094|  2.74098e-06|  0.01%|    self.sor_prices = [np.array(self.auction_params.opening_prices)]\n",
      "   300|     24958|    0.0646198|  2.58914e-06|  0.01%|    self.clock_prices = [np.array(self.auction_params.opening_prices)]\n",
      "   301|     24958|     0.110142|   4.4131e-06|  0.02%|    self.aggregate_demand = [np.zeros(self.auction_params.num_products, dtype=int)]\n",
      "   302|     24958|    0.0469527|  1.88127e-06|  0.01%|    self.round = 1\n",
      "   303|     24958|     0.042757|  1.71316e-06|  0.01%|    self._cur_player = 0\n",
      "   304|     24958|     0.043978|  1.76208e-06|  0.01%|    self._final_payments = None\n",
      "   305|     24958|    0.0725849|  2.90828e-06|  0.01%|    self.price_increments = np.zeros(self.auction_params.num_products, dtype=int)\n",
      "   306|     24958|      0.19022|   7.6216e-06|  0.03%|    self.legal_action_mask = np.ones(len(self.auction_params.all_bids), dtype=bool)\n",
      "(call)|     24958|     0.954702|  3.82524e-05|  0.15%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/numeric.py:149 ones\n",
      "   307|     24958|    0.0501428|  2.00909e-06|  0.01%|    self.processing_queue = None\n",
      "   308|         0|            0|            0|  0.00%|\n",
      "   309|   2770644|      4.31018|  1.55566e-06|  0.69%|  def current_player(self) -> pyspiel.PlayerId:\n",
      "   310|         0|            0|            0|  0.00%|    \"\"\"Returns the current player.\n",
      "   311|         0|            0|            0|  0.00%|\n",
      "   312|         0|            0|            0|  0.00%|    If the game is over, TERMINAL is returned. If the game is at a chance\n",
      "   313|         0|            0|            0|  0.00%|    node then CHANCE is returned. Otherwise SIMULTANEOUS is returned.\n",
      "   314|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   315|   2770644|      4.43032|  1.59902e-06|  0.71%|    if self._game_over:\n",
      "   316|    124770|     0.222982|  1.78715e-06|  0.04%|      return pyspiel.PlayerId.TERMINAL\n",
      "   317|   2645874|       3.7843|  1.43026e-06|  0.61%|    elif self._is_chance:\n",
      "   318|    324478|     0.530974|  1.63639e-06|  0.09%|      return pyspiel.PlayerId.CHANCE\n",
      "   319|         0|            0|            0|  0.00%|    else:\n",
      "   320|   2321396|      3.33431|  1.43634e-06|  0.54%|      return self._cur_player\n",
      "   321|         0|            0|            0|  0.00%|\n",
      "   322|    274574|     0.831386|  3.02791e-06|  0.13%|  def _legal_actions(self, player):\n",
      "   323|         0|            0|            0|  0.00%|    \"\"\"Returns a list of legal actions, sorted in ascending order.\"\"\"\n",
      "   324|    274574|     0.796335|  2.90025e-06|  0.13%|    assert player >= 0\n",
      "   325|    274574|     0.735284|  2.67791e-06|  0.12%|    legal_actions = []\n",
      "   326|    274574|     0.748774|  2.72704e-06|  0.12%|    bidder = self.bidders[player]\n",
      "   327|         0|            0|            0|  0.00%|\n",
      "   328|    274574|      1.76323|  6.42169e-06|  0.28%|    if len(bidder.submitted_demand) > 1 and bidder.submitted_demand[-1].sum() == 0:\n",
      "(call)|    149790|      1.19713|  7.99206e-06|  0.19%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py:46 _sum\n",
      "   329|         0|            0|            0|  0.00%|      # Don't need to recalculate - can only bid 0\n",
      "   330|         0|            0|            0|  0.00%|      return [0]\n",
      "   331|         0|            0|            0|  0.00%|\n",
      "   332|    274574|      1.04777|  3.81599e-06|  0.17%|    price = np.array(self.clock_prices[-1])\n",
      "   333|    274574|     0.782339|  2.84928e-06|  0.13%|    budget = bidder.bidder.budget\n",
      "   334|    274574|     0.663582|  2.41677e-06|  0.11%|    hard_budget_on = True # TODO: Make param\n",
      "   335|    274574|     0.610396|  2.22307e-06|  0.10%|    positive_profit_on = False # TODO: Make param\n",
      "   336|         0|            0|            0|  0.00%|\n",
      "   337|    274574|      1.85049|  6.73949e-06|  0.30%|    prices = self.auction_params.all_bids @ price\n",
      "   338|    274574|      1.80473|  6.57284e-06|  0.29%|    profits = bidder.bidder.get_profits(price)\n",
      "(call)|    274574|      3.68559|   1.3423e-05|  0.59%|# /apps/open_spiel/open_spiel/python/games/clock_auction_bidders.py:25 get_profits\n",
      "   339|         0|            0|            0|  0.00%|\n",
      "   340|         0|            0|            0|  0.00%|    # Note we assume all drops go through. A more sophisticated bidder might think differently (e.g., try to fulfill budget in expectation)\n",
      "   341|         0|            0|            0|  0.00%|    # Consider e.g. if you drop a product you might get stuck! So you can wind up over your budget if your drop fails\n",
      "   342|         0|            0|            0|  0.00%|    # Also consider that if you drop a product and get stuck, you only pay SoR on that product\n",
      "   343|         0|            0|            0|  0.00%|\n",
      "   344|    274574|      0.86007|  3.13238e-06|  0.14%|    legal_actions = self.legal_action_mask.copy()\n",
      "   345|         0|            0|            0|  0.00%|\n",
      "   346|    274574|     0.764322|  2.78367e-06|  0.12%|    if self.auction_params.activity_policy == ActivityPolicy.ON:\n",
      "   347|    274574|      3.45086|  1.25681e-05|  0.56%|      legal_actions[np.where(bidder.activity < self.auction_params.all_bids_activity)[0]] = 0\n",
      "(call)|    274574|      5.25995|  1.91568e-05|  0.85%|# <__array_function__ internals>:177 where\n",
      "   348|         0|            0|            0|  0.00%|\n",
      "   349|    274574|     0.586132|   2.1347e-06|  0.09%|    if hard_budget_on:\n",
      "   350|    274574|        1.621|  5.90369e-06|  0.26%|      legal_actions[prices > budget] = 0\n",
      "   351|         0|            0|            0|  0.00%|\n",
      "   352|    274574|     0.576862|  2.10093e-06|  0.09%|    if positive_profit_on:\n",
      "   353|         0|            0|            0|  0.00%|      legal_actions[profits < 0] = 0\n",
      "   354|         0|            0|            0|  0.00%|\n",
      "   355|         0|            0|            0|  0.00%|    # TODO: Shouldn't I be using LegalProfits here? (i.e., accounting for the fact that it needs to be a legal bundle?)\n",
      "   356|    274574|      2.92256|   1.0644e-05|  0.47%|    if not (profits[legal_actions] > 0).any():\n",
      "(call)|    274574|      2.25922|  8.22808e-06|  0.36%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py:54 _any\n",
      "   357|         0|            0|            0|  0.00%|      # If you have no way to make a profit ever going forwards, just drop out. Helps minimize game size\n",
      "   358|         0|            0|            0|  0.00%|      return [0]\n",
      "   359|         0|            0|            0|  0.00%|    else:\n",
      "   360|    274574|     0.590041|  2.14893e-06|  0.10%|      if bidder.bidder.drop_out_heuristic:\n",
      "   361|         0|            0|            0|  0.00%|        # At least one bid leads to positive profit. Dropping out is never the right thing to do in this case. It will always be action 0\n",
      "   362|    274574|     0.622467|  2.26703e-06|  0.10%|        legal_actions[0] = 0\n",
      "   363|         0|            0|            0|  0.00%|\n",
      "   364|    274574|      2.00623|  7.30669e-06|  0.32%|    if legal_actions.sum() == 0:\n",
      "(call)|    274574|      1.92045|  6.99429e-06|  0.31%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py:46 _sum\n",
      "   365|         0|            0|            0|  0.00%|      print(self)\n",
      "   366|         0|            0|            0|  0.00%|      raise ValueError(\"No legal actions!\")\n",
      "   367|         0|            0|            0|  0.00%|\n",
      "   368|    274574|     0.877213|  3.19482e-06|  0.14%|    return legal_actions.nonzero()[0]\n",
      "   369|         0|            0|            0|  0.00%|\n",
      "   370|    174720|     0.448683|  2.56801e-06|  0.07%|  def _apply_action(self, action):\n",
      "   371|         0|            0|            0|  0.00%|    \"\"\"Applies the specified action to the state.\n",
      "   372|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   373|    174720|       1.7739|  1.01528e-05|  0.29%|    if not self.is_chance_node():\n",
      "(call)|    174720|      1.07714|  6.16497e-06|  0.17%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:309 current_player\n",
      "   374|         0|            0|            0|  0.00%|      # Colelct the bid in submitted demand\n",
      "   375|     99840|     0.242538|  2.42927e-06|  0.04%|      assert not self._game_over\n",
      "   376|     99840|     0.237528|  2.37908e-06|  0.04%|      bidder = self.bidders[self._cur_player]\n",
      "   377|     99840|     0.262239|   2.6266e-06|  0.04%|      assert len(bidder.processed_demand) == len(bidder.submitted_demand)\n",
      "   378|     99840|     0.226855|  2.27218e-06|  0.04%|      assert self.round == len(bidder.submitted_demand)\n",
      "   379|     99840|      0.34381|  3.44361e-06|  0.06%|      bid = self.auction_params.all_bids[action]\n",
      "   380|         0|            0|            0|  0.00%|\n",
      "   381|     99840|     0.295041|  2.95514e-06|  0.05%|      if self.auction_params.activity_policy == ActivityPolicy.ON:\n",
      "   382|     99840|     0.279659|  2.80107e-06|  0.05%|        bid_activity_cost = self.auction_params.all_bids_activity[action]\n",
      "   383|     99840|     0.236073|  2.36451e-06|  0.04%|        if bidder.activity < bid_activity_cost:\n",
      "   384|         0|            0|            0|  0.00%|          raise ValueError(f\"Bidder {self._cur_player} is not active enough ({bidder.activity}) to bid on {bid} with cost of {bid_activity_cost}\")\n",
      "   385|         0|            0|            0|  0.00%|\n",
      "   386|     99840|     0.445846|  4.46561e-06|  0.07%|      bidder.submitted_demand.append(np.array(bid))\n",
      "   387|         0|            0|            0|  0.00%|\n",
      "   388|         0|            0|            0|  0.00%|      # Collect bids until we've got from all players: only then can we process the demand\n",
      "   389|     99840|     0.489666|   4.9045e-06|  0.08%|      if self._cur_player == self.num_players() - 1:\n",
      "   390|     49920|     0.350281|  7.01684e-06|  0.06%|        self._handle_bids()\n",
      "(call)|     49920|      8.28232|  0.000165912|  1.33%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:423 _handle_bids\n",
      "   391|         0|            0|            0|  0.00%|      else:\n",
      "   392|     49920|     0.132413|  2.65251e-06|  0.02%|        self._cur_player += 1 # Advance player and collect more bids\n",
      "   393|         0|            0|            0|  0.00%|\n",
      "   394|         0|            0|            0|  0.00%|    else: # CHANCE NODE\n",
      "   395|     74880|      0.18401|   2.4574e-06|  0.03%|      if self.round == 1:\n",
      "   396|     49916|     0.221732|  4.44211e-06|  0.04%|        if len(self.bidders) < self.num_players(): # Chance node assigns a value and budget to a player\n",
      "   397|     99832|     0.236201|  2.36599e-06|  0.04%|          self.bidders.append(\n",
      "   398|     99832|     0.511111|  5.11971e-06|  0.08%|            BidderState(\n",
      "(call)|     49916|     0.684134|  1.37057e-05|  0.11%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:2 __init__\n",
      "   399|     49916|     0.206819|  4.14335e-06|  0.03%|              submitted_demand=[np.zeros(self.auction_params.num_products, dtype=int)], # Start with this dummy entry so we can always index by round\n",
      "   400|     49916|     0.161711|  3.23967e-06|  0.03%|              processed_demand=[np.zeros(self.auction_params.num_products, dtype=int)],\n",
      "   401|     49916|     0.127358|  2.55145e-06|  0.02%|              bidder=self.auction_params.player_types[len(self.bidders)][action]['bidder'],\n",
      "   402|     49916|      0.11292|   2.2622e-06|  0.02%|              activity=self.auction_params.max_activity,\n",
      "   403|     49916|     0.103456|   2.0726e-06|  0.02%|              type_index=action,\n",
      "   404|         0|            0|            0|  0.00%|            )\n",
      "   405|         0|            0|            0|  0.00%|          )\n",
      "   406|     49916|     0.230596|  4.61968e-06|  0.04%|        if len(self.bidders) == self.num_players(): # All of the assignments have been made\n",
      "   407|     24958|    0.0616724|  2.47105e-06|  0.01%|          self._is_chance = False\n",
      "   408|         0|            0|            0|  0.00%|      else:\n",
      "   409|         0|            0|            0|  0.00%|        # Tie breaking\n",
      "   410|     24964|     0.190026|  7.61201e-06|  0.03%|        self.processing_queue = permute_array(self.processing_queue, action)\n",
      "(call)|     24964|      2.20924|  8.84971e-05|  0.36%|# /apps/open_spiel/open_spiel/python/examples/ubc_utils.py:206 permute_array\n",
      "   411|     24964|      0.24199|  9.69356e-06|  0.04%|        self._process_bids()\n",
      "(call)|     24964|      25.1068|   0.00100572|  4.04%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:581 _process_bids\n",
      "   412|         0|            0|            0|  0.00%|\n",
      "   413|     24964|    0.0548296|  2.19635e-06|  0.01%|  def _generate_processing_queue(self):\n",
      "   414|         0|            0|            0|  0.00%|    # Generate a processing queue, in a fixed order, where each element is a bid of the form (player, product, delta)\n",
      "   415|     24964|    0.0695927|  2.78772e-06|  0.01%|    self.processing_queue = []\n",
      "   416|     74892|     0.172002|  2.29667e-06|  0.03%|    for player_id, bidder in enumerate(self.bidders):\n",
      "   417|    199712|     0.379839|  1.90193e-06|  0.06%|      for product_id in range(self.auction_params.num_products):\n",
      "   418|    149784|     0.295898|   1.9755e-06|  0.05%|        current = bidder.processed_demand[-1][product_id]\n",
      "   419|    149784|     0.285206|  1.90411e-06|  0.05%|        requested = bidder.submitted_demand[-1][product_id]\n",
      "   420|    149784|     0.254545|  1.69942e-06|  0.04%|        if current != requested:\n",
      "   421|    125455|     0.255013|   2.0327e-06|  0.04%|          self.processing_queue.append((player_id, product_id, requested - current))\n",
      "   422|         0|            0|            0|  0.00%|\n",
      "   423|     49920|     0.112461|  2.25282e-06|  0.02%|  def _handle_bids(self):\n",
      "   424|         0|            0|            0|  0.00%|    # Demand Processing\n",
      "   425|     49920|     0.128185|  2.56781e-06|  0.02%|    if self.round == 1 or self.auction_params.undersell_policy == UndersellPolicy.UNDERSELL_ALLOWED:\n",
      "   426|         0|            0|            0|  0.00%|      # Just copy it straight over\n",
      "   427|     74868|     0.140607|  1.87807e-06|  0.02%|      for bidder in self.bidders:\n",
      "   428|     49912|    0.0910733|  1.82468e-06|  0.01%|        bid = bidder.submitted_demand[-1]\n",
      "   429|     49912|     0.103325|  2.07015e-06|  0.02%|        bidder.processed_demand.append(np.asarray(bid))\n",
      "   430|     24956|     0.175293|  7.02408e-06|  0.03%|      self._post_process()\n",
      "(call)|     24956|      5.46992|  0.000219182|  0.88%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:512 _post_process\n",
      "   431|     24964|    0.0563414|  2.25691e-06|  0.01%|    elif self.auction_params.undersell_policy == UndersellPolicy.UNDERSELL:\n",
      "   432|     24964|     0.179015|  7.17093e-06|  0.03%|      self._generate_processing_queue()\n",
      "(call)|     24964|      1.76692|  7.07789e-05|  0.28%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:413 _generate_processing_queue\n",
      "   433|     24964|    0.0591755|  2.37043e-06|  0.01%|      self._is_chance = True\n",
      "   434|         0|            0|            0|  0.00%|    else:\n",
      "   435|         0|            0|            0|  0.00%|      raise ValueError(\"Unknown undersell policy\")\n",
      "   436|         0|            0|            0|  0.00%|\n",
      "   437|         0|            0|            0|  0.00%|  def _action_to_string(self, player, action):\n",
      "   438|         0|            0|            0|  0.00%|    \"\"\"Action -> string.\"\"\"\n",
      "   439|         0|            0|            0|  0.00%|    if player == pyspiel.PlayerId.CHANCE:\n",
      "   440|         0|            0|            0|  0.00%|      if len(self.bidders) < self.num_players():\n",
      "   441|         0|            0|            0|  0.00%|        return f'Assign player {len(self.bidders)} type {self.auction_params.player_types[len(self.bidders)][action][\"bidder\"]}'\n",
      "   442|         0|            0|            0|  0.00%|      else:\n",
      "   443|         0|            0|            0|  0.00%|        return f'Tie-break action {action}'\n",
      "   444|         0|            0|            0|  0.00%|    else:\n",
      "   445|         0|            0|            0|  0.00%|      bid = self.auction_params.all_bids[action]\n",
      "   446|         0|            0|            0|  0.00%|      activity = self.auction_params.all_bids_activity[action]\n",
      "   447|         0|            0|            0|  0.00%|      price = bid @ self.clock_prices[-1]\n",
      "   448|         0|            0|            0|  0.00%|      return f'Bid for {bid} licenses @ ${price:.2f} with activity {activity}'\n",
      "   449|         0|            0|            0|  0.00%|\n",
      "   450|   1248012|      2.14046|   1.7151e-06|  0.34%|  def is_terminal(self):\n",
      "   451|         0|            0|            0|  0.00%|    \"\"\"Returns True if the game is over.\"\"\"\n",
      "   452|   1248012|       2.2171|  1.77651e-06|  0.36%|    return self._game_over\n",
      "   453|         0|            0|            0|  0.00%|\n",
      "   454|     49908|     0.128704|  2.57882e-06|  0.02%|  def returns(self):\n",
      "   455|         0|            0|            0|  0.00%|    \"\"\"Total reward for each player over the course of the game so far.\"\"\"\n",
      "   456|     49908|     0.129732|  2.59943e-06|  0.02%|    assert self._game_over\n",
      "   457|     49908|     0.118791|   2.3802e-06|  0.02%|    assert self._auction_finished\n",
      "   458|         0|            0|            0|  0.00%|\n",
      "   459|     49908|     0.356773|  7.14862e-06|  0.06%|    self._final_payments = np.zeros(self.num_players())\n",
      "   460|     49908|     0.364084|  7.29511e-06|  0.06%|    returns = np.zeros_like(self._final_payments)\n",
      "(call)|     49908|      3.88148|  7.77727e-05|  0.62%|# <__array_function__ internals>:177 zeros_like\n",
      "   461|     49908|     0.139084|  2.78681e-06|  0.02%|    final_prices = self.posted_prices[-1]\n",
      "   462|         0|            0|            0|  0.00%|\n",
      "   463|    149724|     0.382771|  2.55651e-06|  0.06%|    for player_id, bidder in enumerate(self.bidders):\n",
      "   464|     99816|     0.214916|  2.15312e-06|  0.03%|      final_bid = bidder.processed_demand[-1]\n",
      "   465|     99816|      0.65959|  6.60806e-06|  0.11%|      payment = final_bid @ final_prices\n",
      "   466|     99816|     0.243889|  2.44338e-06|  0.04%|      self._final_payments[player_id] = payment\n",
      "   467|     99816|     0.598512|  5.99615e-06|  0.10%|      value = bidder.bidder.value_for_package(final_bid)\n",
      "(call)|     99816|      4.73034|  4.73906e-05|  0.76%|# /apps/open_spiel/open_spiel/python/games/clock_auction_bidders.py:62 value_for_package\n",
      "   468|     99816|     0.785275|  7.86722e-06|  0.13%|      returns[player_id] = value - payment\n",
      "   469|         0|            0|            0|  0.00%|\n",
      "   470|     49908|    0.0840075|  1.68325e-06|  0.01%|    return returns\n",
      "   471|         0|            0|            0|  0.00%|\n",
      "   472|         0|            0|            0|  0.00%|  def __str__(self):\n",
      "   473|         0|            0|            0|  0.00%|    with np.printoptions(precision=3):\n",
      "   474|         0|            0|            0|  0.00%|\n",
      "   475|         0|            0|            0|  0.00%|      \"\"\"String for debug purposes. No particular semantics are required.\"\"\"\n",
      "   476|         0|            0|            0|  0.00%|      result = f'Round: {self.round}\\n'\n",
      "   477|         0|            0|            0|  0.00%|\n",
      "   478|         0|            0|            0|  0.00%|      # Player types\n",
      "   479|         0|            0|            0|  0.00%|      for player_id, bidder in enumerate(self.bidders):\n",
      "   480|         0|            0|            0|  0.00%|        result += f'Player {player_id}: {bidder.bidder}\\n'\n",
      "   481|         0|            0|            0|  0.00%|\n",
      "   482|         0|            0|            0|  0.00%|      if self.round > 1:\n",
      "   483|         0|            0|            0|  0.00%|        result += f'Price: {self.posted_prices[-1]}\\n'\n",
      "   484|         0|            0|            0|  0.00%|\n",
      "   485|         0|            0|            0|  0.00%|        result += 'Processed Demand:\\n'\n",
      "   486|         0|            0|            0|  0.00%|        for player_id, player in enumerate(self.bidders):\n",
      "   487|         0|            0|            0|  0.00%|          if len(player.processed_demand) > 0:\n",
      "   488|         0|            0|            0|  0.00%|            result += f'{player.processed_demand[-1]}\\n'\n",
      "   489|         0|            0|            0|  0.00%|\n",
      "   490|         0|            0|            0|  0.00%|        result += f'Aggregate demand: {self.aggregate_demand[-1]}\\n'\n",
      "   491|         0|            0|            0|  0.00%|\n",
      "   492|         0|            0|            0|  0.00%|      if self._auction_finished:\n",
      "   493|         0|            0|            0|  0.00%|        for player_id, player in enumerate(self.bidders):\n",
      "   494|         0|            0|            0|  0.00%|          result += f'Final bids player {player_id}: {player.processed_demand[-1]}\\n'\n",
      "   495|         0|            0|            0|  0.00%|\n",
      "   496|         0|            0|            0|  0.00%|    return result\n",
      "   497|         0|            0|            0|  0.00%|\n",
      "   498|     74880|     0.169361|  2.26176e-06|  0.03%|  def chance_outcomes(self):\n",
      "   499|         0|            0|            0|  0.00%|    \"\"\"Returns the possible chance outcomes and their probabilities.\"\"\"\n",
      "   500|     74880|     0.171523|  2.29064e-06|  0.03%|    assert self._is_chance\n",
      "   501|     74880|     0.335526|  4.48085e-06|  0.05%|    if len(self.bidders) < self.num_players(): # Chance node is assigning a type\n",
      "   502|     49916|     0.115802|  2.31993e-06|  0.02%|      player_index = len(self.bidders)\n",
      "   503|     49916|     0.113446|  2.27273e-06|  0.02%|      player_types = self.auction_params.player_types[player_index]\n",
      "   504|    249580|      0.62483|  2.50352e-06|  0.10%|      probs = [t['prob'] for t in player_types]\n",
      "(call)|     49916|     0.321948|   6.4498e-06|  0.05%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:504 <listcomp>\n",
      "   505|         0|            0|            0|  0.00%|    else: # Chance node is breaking a tie\n",
      "   506|     24964|    0.0829682|  3.32352e-06|  0.01%|      chance_outcomes_required = math.factorial(len(self.processing_queue))\n",
      "   507|     24964|     0.054286|  2.17457e-06|  0.01%|      if chance_outcomes_required > 1_000:\n",
      "   508|         0|            0|            0|  0.00%|        return dict(upper=chance_outcomes_required - 1) # This breaks the openspiel API, but otherwise generating the list gets too big and blows up memory. See UBC Chance Event Sampler that knows how to decode this. Also, it's WAYYYY faster than actually making the lists\n",
      "   509|     24964|    0.0805519|  3.22672e-06|  0.01%|      probs = [1 / chance_outcomes_required] * chance_outcomes_required\n",
      "   510|     74880|     0.528666|  7.06017e-06|  0.09%|    return list(enumerate(probs))\n",
      "   511|         0|            0|            0|  0.00%|\n",
      "   512|     49920|     0.140725|    2.819e-06|  0.02%|  def _post_process(self):\n",
      "   513|         0|            0|            0|  0.00%|    # Calculate aggregate demand\n",
      "   514|     49920|     0.287565|  5.76051e-06|  0.05%|    aggregate_demand = np.zeros(self.auction_params.num_products, dtype=int)\n",
      "   515|    149760|     0.383838|  2.56302e-06|  0.06%|    for bidder in self.bidders:\n",
      "   516|     99840|     0.337587|  3.38128e-06|  0.05%|      bid = bidder.processed_demand[-1].copy()\n",
      "   517|         0|            0|            0|  0.00%|\n",
      "   518|         0|            0|            0|  0.00%|      # Lower activity based on processed demand (TODO: May want to revisit this for grace period)\n",
      "   519|     99840|     0.917911|  9.19382e-06|  0.15%|      bidder.activity = bid @ self.auction_params.activity\n",
      "   520|     99840|     0.505654|  5.06465e-06|  0.08%|      aggregate_demand += bid\n",
      "   521|         0|            0|            0|  0.00%|\n",
      "   522|         0|            0|            0|  0.00%|    # Calculate excess demand\n",
      "   523|     49920|     0.213363|  4.27409e-06|  0.03%|    excess_demand = aggregate_demand > self.auction_params.licenses\n",
      "   524|         0|            0|            0|  0.00%|\n",
      "   525|     49920|     0.137979|  2.76401e-06|  0.02%|    self.aggregate_demand.append(aggregate_demand)\n",
      "   526|         0|            0|            0|  0.00%|\n",
      "   527|     49920|     0.445231|  8.91889e-06|  0.07%|    if excess_demand.any():\n",
      "(call)|     49920|     0.533451|  1.06861e-05|  0.09%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py:54 _any\n",
      "   528|         0|            0|            0|  0.00%|      # Normal case: Increment price for overdemanded items, leave other items alone\n",
      "   529|     24966|      0.11147|  4.46488e-06|  0.02%|      next_price = np.zeros(self.auction_params.num_products)\n",
      "   530|     24966|     0.176054|  7.05174e-06|  0.03%|      next_clock = np.zeros_like(next_price)\n",
      "(call)|     24966|      1.96655|   7.8769e-05|  0.32%|# <__array_function__ internals>:177 zeros_like\n",
      "   531|     99864|     0.221444|  2.21746e-06|  0.04%|      for j in range(self.auction_params.num_products):\n",
      "   532|     74898|     0.149571|    1.997e-06|  0.02%|        if excess_demand[j]:\n",
      "   533|     26517|    0.0767632|  2.89487e-06|  0.01%|          self.price_increments[j] += 1\n",
      "   534|     74898|      0.20503|  2.73745e-06|  0.03%|        next_price[j] = self.clock_prices[-1][j] if excess_demand[j] else self.sor_prices[-1][j]\n",
      "   535|     74898|     0.202548|  2.70432e-06|  0.03%|        next_clock[j] = next_price[j] * (1 + self.auction_params.increment)\n",
      "   536|     24966|    0.0503151|  2.01535e-06|  0.01%|      self.posted_prices.append(next_price)\n",
      "   537|     24966|    0.0498228|  1.99563e-06|  0.01%|      self.sor_prices.append(next_price)\n",
      "   538|     24966|    0.0486488|   1.9486e-06|  0.01%|      self.clock_prices.append(next_clock)\n",
      "   539|         0|            0|            0|  0.00%|    else:\n",
      "   540|         0|            0|            0|  0.00%|      # Demand <= supply for each item. We are finished\n",
      "   541|     24954|    0.0624626|  2.50311e-06|  0.01%|      self._auction_finished = True\n",
      "   542|     24954|    0.0558765|  2.23918e-06|  0.01%|      self._game_over = True\n",
      "   543|     24954|     0.092664|  3.71339e-06|  0.01%|      self.posted_prices.append(np.array(self.posted_prices[-1]))\n",
      "   544|         0|            0|            0|  0.00%|\n",
      "   545|     49920|     0.103597|  2.07526e-06|  0.02%|    if not self._auction_finished:\n",
      "   546|     24966|    0.0526483|   2.1088e-06|  0.01%|      self.round += 1\n",
      "   547|     24966|    0.0525866|  2.10633e-06|  0.01%|      if self.round > self.auction_params.max_round:\n",
      "   548|         0|            0|            0|  0.00%|        # An alternative: set game_over = True (auction_finished will still be false) and simply track this. Maybe give large negative rewards. But right now this seems more obvious as a way of triggering\n",
      "   549|         0|            0|            0|  0.00%|        raise ValueError(\"Auction went on too long\")\n",
      "   550|         0|            0|            0|  0.00%|\n",
      "   551|     49920|     0.104683|  2.09701e-06|  0.02%|    self._cur_player = 0\n",
      "   552|     49920|     0.115138|  2.30645e-06|  0.02%|    self._is_chance = False\n",
      "   553|         0|            0|            0|  0.00%|\n",
      "   554|    408971|      0.95487|  2.33481e-06|  0.15%|  def _apply_bid(self, change_request, bid, points, current_agg):\n",
      "   555|    408971|     0.955907|  2.33735e-06|  0.15%|    (player_id, product_id, delta) = change_request\n",
      "   556|    408971|     0.818942|  2.00244e-06|  0.13%|    changed = False\n",
      "   557|         0|            0|            0|  0.00%|\n",
      "   558|         0|            0|            0|  0.00%|    # Process drop\n",
      "   559|    408971|     0.893265|  2.18418e-06|  0.14%|    if delta < 0:\n",
      "   560|    209897|     0.521058|  2.48244e-06|  0.08%|      drop_room = current_agg[product_id] - self.auction_params.licenses[product_id]\n",
      "   561|    209897|     0.437494|  2.08432e-06|  0.07%|      if drop_room > 0:\n",
      "   562|     68069|     0.189726|  2.78726e-06|  0.03%|        amount = min(drop_room, -delta)\n",
      "   563|     68069|     0.161031|   2.3657e-06|  0.03%|        bid[product_id] -= amount\n",
      "   564|     68069|     0.134146|  1.97074e-06|  0.02%|        delta += amount\n",
      "   565|     68069|     0.153185|  2.25044e-06|  0.02%|        assert bid[product_id] >= 0\n",
      "   566|     68069|     0.121234|  1.78105e-06|  0.02%|        changed = True\n",
      "   567|     68069|     0.167471|  2.46031e-06|  0.03%|        points[player_id] += amount * self.auction_params.activity[product_id]\n",
      "   568|     68069|     0.143455|  2.10749e-06|  0.02%|        current_agg[product_id] -= amount\n",
      "   569|         0|            0|            0|  0.00%|\n",
      "   570|         0|            0|            0|  0.00%|    # Process pickup\n",
      "   571|    461397|      1.16137|  2.51707e-06|  0.19%|    while delta > 0 and (self.auction_params.activity_policy == ActivityPolicy.OFF or points[player_id] >= self.auction_params.activity[product_id]):\n",
      "   572|     52426|     0.141427|  2.69764e-06|  0.02%|      bid[product_id] += 1\n",
      "   573|     52426|     0.130089|  2.48138e-06|  0.02%|      assert bid[product_id] <= self.auction_params.licenses[product_id]\n",
      "   574|     52426|     0.124374|  2.37237e-06|  0.02%|      current_agg[product_id] += 1\n",
      "   575|     52426|     0.104681|  1.99674e-06|  0.02%|      changed = True\n",
      "   576|     52426|     0.130649|  2.49207e-06|  0.02%|      points[player_id] -= self.auction_params.activity[product_id]\n",
      "   577|     52426|     0.118463|  2.25961e-06|  0.02%|      delta -= 1\n",
      "   578|         0|            0|            0|  0.00%|\n",
      "   579|    408971|     0.822539|  2.01124e-06|  0.13%|    return changed, delta\n",
      "   580|         0|            0|            0|  0.00%|\n",
      "   581|     24964|     0.098819|  3.95846e-06|  0.02%|  def _process_bids(self):\n",
      "   582|         0|            0|            0|  0.00%|    # Copy over the current aggregate demand\n",
      "   583|         0|            0|            0|  0.00%|    # TODO: For now points is a zero vector, but possible grace period implementations would change that\n",
      "   584|    124820|     0.374301|  2.99872e-06|  0.06%|    points = [bidder.activity for bidder in self.bidders]\n",
      "(call)|     24964|     0.173571|  6.95285e-06|  0.03%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:584 <listcomp>\n",
      "   585|     24964|    0.0902815|  3.61647e-06|  0.01%|    bids = []\n",
      "   586|     74892|     0.263777|   3.5221e-06|  0.04%|    for player_id, bidder in enumerate(self.bidders):\n",
      "   587|     49928|     0.173133|  3.46766e-06|  0.03%|      last_round_holdings = bidder.processed_demand[-1]\n",
      "   588|    199712|     0.655847|  3.28396e-06|  0.11%|      for j in range(self.auction_params.num_products):\n",
      "   589|    149784|     0.604224|  4.03397e-06|  0.10%|        points[player_id] -= last_round_holdings[j] * self.auction_params.activity[j]\n",
      "   590|     49928|     0.218868|  4.38367e-06|  0.04%|      bids.append(last_round_holdings.copy())\n",
      "   591|         0|            0|            0|  0.00%|\n",
      "   592|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   593|         0|            0|            0|  0.00%|    for (player_id, product_id, delta) in self.processing_queue:\n",
      "   594|         0|            0|            0|  0.00%|      - try to process it\n",
      "   595|         0|            0|            0|  0.00%|      - if it fails, add it to the unfinished queue\n",
      "   596|         0|            0|            0|  0.00%|\n",
      "   597|         0|            0|            0|  0.00%|      - try:\n",
      "   598|         0|            0|            0|  0.00%|        - process each bid in the unfinished queue; restart from beginning of unfinished queue if even partially successful\n",
      "   599|         0|            0|            0|  0.00%|      - until nothing changes\n",
      "   600|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   601|     24964|    0.0950861|  3.80893e-06|  0.02%|    current_agg = self.aggregate_demand[-1].copy()\n",
      "   602|     24964|    0.0959113|  3.84198e-06|  0.02%|    starting_agg_demand = current_agg.copy()\n",
      "   603|     24964|    0.0853825|  3.42022e-06|  0.01%|    self.unfinished_queue = []\n",
      "   604|         0|            0|            0|  0.00%|\n",
      "   605|    150419|     0.347258|   2.3086e-06|  0.06%|    for change_request in self.processing_queue:\n",
      "   606|    125455|     0.308199|  2.45665e-06|  0.05%|      (player_id, product_id, delta) = change_request\n",
      "   607|    125455|     0.911096|  7.26233e-06|  0.15%|      _, new_delta = self._apply_bid(change_request, bids[player_id], points, current_agg)\n",
      "(call)|    125455|      3.20986|  2.55857e-05|  0.52%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:554 _apply_bid\n",
      "   608|         0|            0|            0|  0.00%|\n",
      "   609|    125455|     0.322642|  2.57177e-06|  0.05%|      if new_delta != 0: # The bid is not fully finished\n",
      "   610|     79389|     0.199101|  2.50792e-06|  0.03%|        self.unfinished_queue.append((player_id, product_id, new_delta))\n",
      "   611|         0|            0|            0|  0.00%|\n",
      "   612|    125455|     0.282329|  2.25044e-06|  0.05%|      unfinished_processing = True\n",
      "   613|         0|            0|            0|  0.00%|\n",
      "   614|         0|            0|            0|  0.00%|      # Try to process the unfinished queue\n",
      "   615|    295224|     0.627259|  2.12469e-06|  0.10%|      while unfinished_processing:\n",
      "   616|    169769|     0.363123|  2.13892e-06|  0.06%|        unfinished_processing = False\n",
      "   617|    408971|     0.986784|  2.41285e-06|  0.16%|        for idx, unfinished_change_request in enumerate(list(self.unfinished_queue)):\n",
      "   618|    283516|     0.613489|  2.16386e-06|  0.10%|          (player_id, product_id, delta) = unfinished_change_request\n",
      "   619|    283516|       1.9157|  6.75695e-06|  0.31%|          unfinished_change, new_delta = self._apply_bid(unfinished_change_request, bids[player_id], points, current_agg)\n",
      "(call)|    283516|      5.17551|  1.82548e-05|  0.83%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:554 _apply_bid\n",
      "   620|    283516|      0.67529|  2.38184e-06|  0.11%|          if new_delta == 0:\n",
      "   621|     31469|     0.075799|  2.40869e-06|  0.01%|            del self.unfinished_queue[idx]\n",
      "   622|         0|            0|            0|  0.00%|          else:\n",
      "   623|    252047|     0.567364|  2.25103e-06|  0.09%|            self.unfinished_queue[idx] = (player_id, product_id, new_delta)\n",
      "   624|    283516|     0.591255|  2.08544e-06|  0.10%|          if unfinished_change:\n",
      "   625|     44314|    0.0926919|  2.09171e-06|  0.01%|            unfinished_processing = True\n",
      "   626|     44314|     0.098887|  2.23151e-06|  0.02%|            break\n",
      "   627|         0|            0|            0|  0.00%|\n",
      "   628|         0|            0|            0|  0.00%|    # Finally, copy over submitted -> processed\n",
      "   629|     74892|     0.173294|  2.31392e-06|  0.03%|    for player_id, bidder in enumerate(self.bidders):\n",
      "   630|    199712|     0.451925|  2.26288e-06|  0.07%|      for product_id in range(self.auction_params.num_products):\n",
      "   631|    149784|     0.366495|  2.44682e-06|  0.06%|        assert bids[player_id][product_id] >= 0\n",
      "   632|    149784|     0.368932|  2.46309e-06|  0.06%|        assert bids[player_id][product_id] <= self.auction_params.licenses[product_id]\n",
      "   633|    149784|     0.444545|  2.96791e-06|  0.07%|        assert bids[player_id][product_id] <= max(bidder.submitted_demand[-1][product_id], bidder.processed_demand[-1][product_id]) # Either what you asked for, or what you used to have\n",
      "   634|         0|            0|            0|  0.00%|\n",
      "   635|     49928|     0.127258|  2.54882e-06|  0.02%|      bidder.processed_demand.append(np.asarray(bids[player_id]))\n",
      "   636|     49928|     0.118484|  2.37311e-06|  0.02%|      assert len(bidder.processed_demand) == len(bidder.submitted_demand)\n",
      "   637|         0|            0|            0|  0.00%|\n",
      "   638|     99856|     0.219926|  2.20243e-06|  0.04%|    for j in range(self.auction_params.num_products):\n",
      "   639|     74892|     0.186245|  2.48685e-06|  0.03%|      if (current_agg[j] < self.auction_params.licenses[j]) and (current_agg[j] < starting_agg_demand[j]):\n",
      "   640|         0|            0|            0|  0.00%|        raise ValueError(\"Aggregate demand fell for underdemanded product {}\".format(j))\n",
      "   641|         0|            0|            0|  0.00%|\n",
      "   642|     24964|     0.199207|  7.97977e-06|  0.03%|    self._post_process()\n",
      "(call)|     24964|      2.33126|  9.33847e-05|  0.38%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:512 _post_process\n",
      "   643|         0|            0|            0|  0.00%|\n",
      "   644|         0|            0|            0|  0.00%|\n",
      "   645|         0|            0|            0|  0.00%|  def get_final_payments(self):\n",
      "   646|         0|            0|            0|  0.00%|    assert self._game_over\n",
      "   647|         0|            0|            0|  0.00%|    return self._final_payments\n",
      "   648|         0|            0|            0|  0.00%|\n",
      "   649|         0|            0|            0|  0.00%|  @property\n",
      "   650|         0|            0|            0|  0.00%|  def revenue(self):\n",
      "   651|         0|            0|            0|  0.00%|    assert self._game_over\n",
      "   652|         0|            0|            0|  0.00%|    return sum(self.get_final_payments())\n",
      "   653|         0|            0|            0|  0.00%|\n",
      "   654|         0|            0|            0|  0.00%|  # METRICS\n",
      "   655|         0|            0|            0|  0.00%|  @property\n",
      "   656|         0|            0|            0|  0.00%|  def revenue_potential(self):\n",
      "   657|         0|            0|            0|  0.00%|    if self.round == 1:\n",
      "   658|         0|            0|            0|  0.00%|      return 0\n",
      "   659|         0|            0|            0|  0.00%|\n",
      "   660|         0|            0|            0|  0.00%|    # TODO: Assumes undersell is turned on, or this is completely wrong\n",
      "   661|         0|            0|            0|  0.00%|    guaranteed_to_sell = np.minimum(self.auction_params.licenses, self.aggregate_demand[-1])\n",
      "   662|         0|            0|            0|  0.00%|    return guaranteed_to_sell @ self.posted_prices[-1]\n",
      "   663|         0|            0|            0|  0.00%|\n",
      "   664|         0|            0|            0|  0.00%|  @property\n",
      "   665|         0|            0|            0|  0.00%|  def revenue_potential_normalized(self):\n",
      "   666|         0|            0|            0|  0.00%|    return self.revenue_potential / self.auction_params.max_total_spend\n",
      "   667|         0|            0|            0|  0.00%|\n",
      "   668|         0|            0|            0|  0.00%|  @property\n",
      "   669|         0|            0|            0|  0.00%|  def pricing_potential(self):\n",
      "   670|         0|            0|            0|  0.00%|      if self.round == 1:\n",
      "   671|         0|            0|            0|  0.00%|        return np.zeros(self.num_players())\n",
      "   672|         0|            0|            0|  0.00%|\n",
      "   673|         0|            0|            0|  0.00%|      # Define pricing as opponent payments - oppnonent cost of bundle at starting prices\n",
      "   674|         0|            0|            0|  0.00%|      increased_cost = np.zeros(self.num_players())\n",
      "   675|         0|            0|            0|  0.00%|      allocation = [bidder.processed_demand[-1] for bidder in self.bidders]\n",
      "   676|         0|            0|            0|  0.00%|      for player_id, bidder in enumerate(self.bidders):\n",
      "   677|         0|            0|            0|  0.00%|        increased_cost[player_id] = (self.posted_prices[-1] - self.auction_params.opening_prices) @ allocation[player_id]\n",
      "   678|         0|            0|            0|  0.00%|      pricing = np.zeros_like(increased_cost)\n",
      "   679|         0|            0|            0|  0.00%|      for player_id in range(self.num_players()):\n",
      "   680|         0|            0|            0|  0.00%|        for other_player_id in players_not_me(player_id,  self.num_players()):\n",
      "   681|         0|            0|            0|  0.00%|            pricing[player_id] += increased_cost[other_player_id]\n",
      "   682|         0|            0|            0|  0.00%|      return pricing\n",
      "   683|         0|            0|            0|  0.00%|\n",
      "   684|         0|            0|            0|  0.00%|  @property\n",
      "   685|         0|            0|            0|  0.00%|  def pricing_potential_normalized(self):\n",
      "   686|         0|            0|            0|  0.00%|    return self.pricing_potential / self.auction_params.max_opponent_spends\n",
      "   687|         0|            0|            0|  0.00%|\n",
      "   688|         0|            0|            0|  0.00%|  @property\n",
      "   689|         0|            0|            0|  0.00%|  def auction_length_potential(self):\n",
      "   690|         0|            0|            0|  0.00%|    return self.round\n",
      "   691|         0|            0|            0|  0.00%|\n",
      "   692|         0|            0|            0|  0.00%|  @property\n",
      "   693|         0|            0|            0|  0.00%|  def auction_length_potential_normalized(self):\n",
      "   694|         0|            0|            0|  0.00%|    return self.round / self.auction_params.max_round\n",
      "   695|         0|            0|            0|  0.00%|\n",
      "   696|         0|            0|            0|  0.00%|  @property\n",
      "   697|         0|            0|            0|  0.00%|  def welfare_potential(self):\n",
      "   698|         0|            0|            0|  0.00%|    # Calculate on-demand welfare of current solution\n",
      "   699|         0|            0|            0|  0.00%|    return self.get_welfare()\n",
      "   700|         0|            0|            0|  0.00%|\n",
      "   701|         0|            0|            0|  0.00%|  @property\n",
      "   702|         0|            0|            0|  0.00%|  def welfare_potential_normalized(self):\n",
      "   703|         0|            0|            0|  0.00%|    # TODO: This is a real bound but too expensive to compute. No reason we couldn't use something much looser.\n",
      "   704|         0|            0|            0|  0.00%|    # For example, you could find the type of each player that values the full bundle the most and then sum over the best-types for each player\n",
      "   705|         0|            0|            0|  0.00%|    return self.get_welfare() / self.efficent_allocation()[0]\n",
      "   706|         0|            0|            0|  0.00%|\n",
      "   707|         0|            0|            0|  0.00%|  def get_allocation(self):\n",
      "   708|         0|            0|            0|  0.00%|    assert self._game_over\n",
      "   709|         0|            0|            0|  0.00%|    return [bidder.processed_demand[-1] for bidder in self.bidders]\n",
      "   710|         0|            0|            0|  0.00%|\n",
      "   711|         0|            0|            0|  0.00%|  def get_welfare(self):\n",
      "   712|         0|            0|            0|  0.00%|    welfare = 0\n",
      "   713|         0|            0|            0|  0.00%|    allocation = [bidder.processed_demand[-1] for bidder in self.bidders]\n",
      "   714|         0|            0|            0|  0.00%|    for bidder, package in zip(self.bidders, allocation):\n",
      "   715|         0|            0|            0|  0.00%|      welfare += bidder.bidder.value_for_package(package)\n",
      "   716|         0|            0|            0|  0.00%|    return float(welfare)\n",
      "   717|         0|            0|            0|  0.00%|\n",
      "   718|         0|            0|            0|  0.00%|  def get_welfare_sparse_normalized(self):\n",
      "   719|         0|            0|            0|  0.00%|    if self._game_over:\n",
      "   720|         0|            0|            0|  0.00%|      max_welfare, alloc = self.efficent_allocation()\n",
      "   721|         0|            0|            0|  0.00%|      return self.get_welfare() / max_welfare\n",
      "   722|         0|            0|            0|  0.00%|    else:\n",
      "   723|         0|            0|            0|  0.00%|      return 0\n",
      "   724|         0|            0|            0|  0.00%|\n",
      "   725|         0|            0|            0|  0.00%|  def efficient_allocation(self):\n",
      "   726|         0|            0|            0|  0.00%|    num_actions = len(self.auction_params.all_bids)\n",
      "   727|         0|            0|            0|  0.00%|    num_players = self.num_players()\n",
      "   728|         0|            0|            0|  0.00%|    n_vars = num_players * num_actions\n",
      "   729|         0|            0|            0|  0.00%|    var_id_to_player_bundle = dict() # VarId -> (player, bundle)\n",
      "   730|         0|            0|            0|  0.00%|\n",
      "   731|         0|            0|            0|  0.00%|    values = []\n",
      "   732|         0|            0|            0|  0.00%|    q = 0\n",
      "   733|         0|            0|            0|  0.00%|    for player, bidder in enumerate(self.bidders):\n",
      "   734|         0|            0|            0|  0.00%|        v = bidder.bidder.get_values()\n",
      "   735|         0|            0|            0|  0.00%|        values += v\n",
      "   736|         0|            0|            0|  0.00%|        for val, bundle in zip(v, self.auction_params.all_bids):\n",
      "   737|         0|            0|            0|  0.00%|          var_id_to_player_bundle[q] = (player, bundle)\n",
      "   738|         0|            0|            0|  0.00%|          q += 1\n",
      "   739|         0|            0|            0|  0.00%|\n",
      "   740|         0|            0|            0|  0.00%|    problem = LpProblem(f\"EfficientAllocation\", LpMaximize)\n",
      "   741|         0|            0|            0|  0.00%|    bundle_variables = LpVariable.dicts(\"X\", np.arange(n_vars), cat=LpBinary)\n",
      "   742|         0|            0|            0|  0.00%|\n",
      "   743|         0|            0|            0|  0.00%|    # OBJECTIVE\n",
      "   744|         0|            0|            0|  0.00%|    problem += lpDot(values, bundle_variables.values())\n",
      "   745|         0|            0|            0|  0.00%|\n",
      "   746|         0|            0|            0|  0.00%|    # Constraint: Only 1 bundle per bidder\n",
      "   747|         0|            0|            0|  0.00%|    for i in range(num_players):\n",
      "   748|         0|            0|            0|  0.00%|        problem += lpSum(list(bundle_variables.values())[i * num_actions: (i+1) * num_actions]) == 1, f\"1-per-bidder-{i}\"\n",
      "   749|         0|            0|            0|  0.00%|\n",
      "   750|         0|            0|            0|  0.00%|    # Constraint: Can't overallocate any items\n",
      "   751|         0|            0|            0|  0.00%|    supply = self.auction_params.licenses\n",
      "   752|         0|            0|            0|  0.00%|    for i in range(self.auction_params.num_products):\n",
      "   753|         0|            0|            0|  0.00%|        product_amounts = [bundle[i] for (player, bundle) in var_id_to_player_bundle.values()]\n",
      "   754|         0|            0|            0|  0.00%|        problem += lpDot(bundle_variables.values(), product_amounts) <= supply[i], f\"supply-{i}\"\n",
      "   755|         0|            0|            0|  0.00%|\n",
      "   756|         0|            0|            0|  0.00%|    allocation = []\n",
      "   757|         0|            0|            0|  0.00%|    try:\n",
      "   758|         0|            0|            0|  0.00%|        problem.writeLP(f'efficient_allocation_{random_string(10)}.lp')\n",
      "   759|         0|            0|            0|  0.00%|        obj = pulp_solve(problem, save_if_failed=True)\n",
      "   760|         0|            0|            0|  0.00%|        for var_id in range(n_vars):\n",
      "   761|         0|            0|            0|  0.00%|            # print(var_id, bundle_variables[var_id], value(bundle_variables[var_id]), var_id_to_player_bundle[var_id])\n",
      "   762|         0|            0|            0|  0.00%|            if value(bundle_variables[var_id]) > .99: # Rounding stupidness\n",
      "   763|         0|            0|            0|  0.00%|                allocation.append(var_id_to_player_bundle[var_id][1])\n",
      "   764|         0|            0|            0|  0.00%|    except ValueError as e:\n",
      "   765|         0|            0|            0|  0.00%|        # if MIP is infeasible, drop out - TODO: Should this ever happen?\n",
      "   766|         0|            0|            0|  0.00%|        feasible_result = False\n",
      "   767|         0|            0|            0|  0.00%|        logging.warning(f'Failed to solve MIP; dropping out')\n",
      "   768|         0|            0|            0|  0.00%|\n",
      "   769|         0|            0|            0|  0.00%|    return obj, allocation\n",
      "   770|         0|            0|            0|  0.00%|\n",
      "   771|         0|            0|            0|  0.00%|\n",
      "   772|         0|            0|            0|  0.00%|class ClockAuctionObserver:\n",
      "   773|         0|            0|            0|  0.00%|  \"\"\"Observer, conforming to the PyObserver interface (see observation.py).\"\"\"\n",
      "   774|         0|            0|            0|  0.00%|\n",
      "   775|         0|            0|            0|  0.00%|  def __init__(self, iig_obs_type, params):\n",
      "   776|         0|            0|            0|  0.00%|    auction_params = params.get('auction_params')\n",
      "   777|         0|            0|            0|  0.00%|    self.normalize = params.get('normalize', True) # Raw features? Or normalized ones\n",
      "   778|         0|            0|            0|  0.00%|    if not isinstance(auction_params, AuctionParams):\n",
      "   779|         0|            0|            0|  0.00%|      raise ValueError(\"params must be an AuctionParams object\")\n",
      "   780|         0|            0|            0|  0.00%|    self.auction_params = auction_params\n",
      "   781|         0|            0|            0|  0.00%|\n",
      "   782|         0|            0|            0|  0.00%|    num_players = len(auction_params.player_types)\n",
      "   783|         0|            0|            0|  0.00%|    num_products = auction_params.num_products\n",
      "   784|         0|            0|            0|  0.00%|\n",
      "   785|         0|            0|            0|  0.00%|    # self.round_buffer = 100 if iig_obs_type.perfect_recall else auction_params.agent_memory\n",
      "   786|         0|            0|            0|  0.00%|    self.round_buffer = auction_params.agent_memory # TODO: We are abusing the API here a bit. Only offers binary perfect recall or not, but we want to interpolate.\n",
      "   787|         0|            0|            0|  0.00%|    length = self.round_buffer * num_products\n",
      "   788|         0|            0|            0|  0.00%|    shape = (self.round_buffer, num_products)\n",
      "   789|         0|            0|            0|  0.00%|\n",
      "   790|         0|            0|            0|  0.00%|    \"\"\"Initializes an empty observation tensor.\"\"\"\n",
      "   791|         0|            0|            0|  0.00%|    # Determine which observation pieces we want to include.\n",
      "   792|         0|            0|            0|  0.00%|    # NOTE: It should be possible to use the params to exclude some of these if we want a smaller input to the NN (or to have the NN reassamble the tensor from specific pieces).\n",
      "   793|         0|            0|            0|  0.00%|\n",
      "   794|         0|            0|            0|  0.00%|    pieces = [(\"player\", num_players, (num_players,))]\n",
      "   795|         0|            0|            0|  0.00%|    if iig_obs_type.private_info == pyspiel.PrivateInfoType.SINGLE_PLAYER:\n",
      "   796|         0|            0|            0|  0.00%|      # 1-hot type encoding\n",
      "   797|         0|            0|            0|  0.00%|      max_num_types = max([len(p) for p in auction_params.player_types.values()])\n",
      "   798|         0|            0|            0|  0.00%|      pieces.append((\"bidder_type\", max_num_types, (max_num_types,)))\n",
      "   799|         0|            0|            0|  0.00%|      pieces.append((\"activity\", 1, (1,)))\n",
      "   800|         0|            0|            0|  0.00%|      pieces.append((\"sor_exposure\", 1, (1,)))\n",
      "   801|         0|            0|            0|  0.00%|\n",
      "   802|         0|            0|            0|  0.00%|      pieces.append((\"submitted_demand_history\", length, shape))\n",
      "   803|         0|            0|            0|  0.00%|      pieces.append((\"processed_demand_history\", length, shape))\n",
      "   804|         0|            0|            0|  0.00%|      num_bundles = len(auction_params.all_bids)\n",
      "   805|         0|            0|            0|  0.00%|      pieces.append((\"sor_profits\", num_bundles, (num_bundles,)))\n",
      "   806|         0|            0|            0|  0.00%|      pieces.append((\"clock_profits\", num_bundles, (num_bundles,)))\n",
      "   807|         0|            0|            0|  0.00%|\n",
      "   808|         0|            0|            0|  0.00%|    if iig_obs_type.public_info:\n",
      "   809|         0|            0|            0|  0.00%|      # 1-hot round encoding\n",
      "   810|         0|            0|            0|  0.00%|      pieces.append((\"round\", self.auction_params.max_round, (self.auction_params.max_round,))) # Always remember what round you are in, regardless of anything else\n",
      "   811|         0|            0|            0|  0.00%|      pieces.append((\"agg_demand_history\", length, shape))\n",
      "   812|         0|            0|            0|  0.00%|      pieces.append((\"posted_price_history\", self.round_buffer * num_products, (self.round_buffer, num_products)))\n",
      "   813|         0|            0|            0|  0.00%|      pieces.append((\"clock_prices\", num_products, (num_products,)))\n",
      "   814|         0|            0|            0|  0.00%|      pieces.append((\"price_increments\", num_products, (num_products,)))\n",
      "   815|         0|            0|            0|  0.00%|\n",
      "   816|         0|            0|            0|  0.00%|      if self.auction_params.reveal_type_round != -1:\n",
      "   817|         0|            0|            0|  0.00%|        for i in range(num_players):\n",
      "   818|         0|            0|            0|  0.00%|          player_types = len(self.auction_params.player_types[i])\n",
      "   819|         0|            0|            0|  0.00%|          # Could be smaller if I excluded my own types, but this is simpler\n",
      "   820|         0|            0|            0|  0.00%|          pieces.append((f\"revealed_types_{i}\", player_types, (player_types,)))\n",
      "   821|         0|            0|            0|  0.00%|\n",
      "   822|         0|            0|            0|  0.00%|    # Build the single flat tensor.\n",
      "   823|         0|            0|            0|  0.00%|    total_size = sum(size for name, size, shape in pieces)\n",
      "   824|         0|            0|            0|  0.00%|    self.tensor = np.zeros(total_size, np.float32)\n",
      "   825|         0|            0|            0|  0.00%|\n",
      "   826|         0|            0|            0|  0.00%|    # Build the named & reshaped views of the bits of the flat tensor.\n",
      "   827|         0|            0|            0|  0.00%|    self.dict = {}\n",
      "   828|         0|            0|            0|  0.00%|    index = 0\n",
      "   829|         0|            0|            0|  0.00%|    for name, size, shape in pieces:\n",
      "   830|         0|            0|            0|  0.00%|      self.dict[name] = self.tensor[index:index + size].reshape(shape)\n",
      "   831|         0|            0|            0|  0.00%|      index += size\n",
      "   832|         0|            0|            0|  0.00%|\n",
      "   833|    648964|      2.73961|  4.22152e-06|  0.44%|  def set_from(self, state, player):\n",
      "   834|         0|            0|            0|  0.00%|    \"\"\"Updates `tensor` and `dict` to reflect `state` from PoV of `player`.\"\"\"\n",
      "   835|         0|            0|            0|  0.00%|\n",
      "   836|         0|            0|            0|  0.00%|    # BE VERY VERY CAREFUL NOT TO OVERRIDE THE DICT ENTRIES FROM POINTING INTO THE TENSOR\n",
      "   837|         0|            0|            0|  0.00%|    # Very subtle e.g., self.dict[\"sor_profits\"][:] = profits vs self.dict[\"sor_profits\"] = profits\n",
      "   838|    648964|      3.68438|  5.67733e-06|  0.59%|    self.tensor.fill(0)\n",
      "   839|         0|            0|            0|  0.00%|\n",
      "   840|    648964|      3.09866|  4.77477e-06|  0.50%|    length = min(state.round, self.round_buffer)\n",
      "   841|    648964|      2.84282|  4.38055e-06|  0.46%|    start_ind = max(1, state.round - self.round_buffer)\n",
      "   842|    648964|      2.61748|  4.03332e-06|  0.42%|    end_ind = start_ind + length\n",
      "   843|         0|            0|            0|  0.00%|\n",
      "   844|    648964|      2.52714|  3.89412e-06|  0.41%|    if \"player\" in self.dict:\n",
      "   845|    648964|      2.79549|  4.30763e-06|  0.45%|      self.dict[\"player\"][player] = 1\n",
      "   846|    648964|      2.38462|   3.6745e-06|  0.38%|    if \"round\" in self.dict:\n",
      "   847|    648964|      2.53496|  3.90616e-06|  0.41%|      self.dict[\"round\"][state.round - 1] = 1\n",
      "   848|         0|            0|            0|  0.00%|\n",
      "   849|         0|            0|            0|  0.00%|    # These require the bidder to be initialized\n",
      "   850|    648964|      2.47599|   3.8153e-06|  0.40%|    if len(state.bidders) > player:\n",
      "   851|    648964|      2.33036|  3.59089e-06|  0.38%|      if \"bidder_type\" in self.dict:\n",
      "   852|    648964|      2.56003|  3.94479e-06|  0.41%|        self.dict[\"bidder_type\"][state.bidders[player].type_index] = 1\n",
      "   853|    648964|      2.31958|  3.57428e-06|  0.37%|      if \"activity\" in self.dict:\n",
      "   854|    648964|      2.41854|  3.72677e-06|  0.39%|        activity = state.bidders[player].activity\n",
      "   855|    648964|      2.35485|  3.62863e-06|  0.38%|        if self.normalize:\n",
      "   856|    648964|      2.62448|   4.0441e-06|  0.42%|          activity /= self.auction_params.max_activity\n",
      "   857|    648964|      2.76434|  4.25963e-06|  0.45%|        self.dict[\"activity\"][0] = activity\n",
      "   858|    648964|      2.33974|  3.60535e-06|  0.38%|      if \"sor_profits\" in self.dict:\n",
      "   859|    648964|      3.28677|  5.06464e-06|  0.53%|        price = np.array(state.sor_prices[-1])\n",
      "   860|    648964|      5.53275|   8.5255e-06|  0.89%|        profits = state.bidders[player].bidder.get_profits(price)\n",
      "(call)|    648964|      11.1328|  1.71547e-05|  1.79%|# /apps/open_spiel/open_spiel/python/games/clock_auction_bidders.py:25 get_profits\n",
      "   861|    648964|      2.38224|  3.67084e-06|  0.38%|        if self.normalize:\n",
      "   862|    648964|      5.07001|  7.81247e-06|  0.82%|          profits = profits / self.auction_params.max_budget\n",
      "   863|    648964|       3.3634|  5.18272e-06|  0.54%|        self.dict[\"sor_profits\"][:] = profits\n",
      "   864|    648964|      2.31072|  3.56063e-06|  0.37%|      if \"clock_profits\" in self.dict:\n",
      "   865|    648964|      2.84065|  4.37721e-06|  0.46%|        price = np.array(state.clock_prices[-1])\n",
      "   866|    648964|      5.20375|  8.01855e-06|  0.84%|        profits = state.bidders[player].bidder.get_profits(price)\n",
      "(call)|    648964|      8.64633|  1.33233e-05|  1.39%|# /apps/open_spiel/open_spiel/python/games/clock_auction_bidders.py:25 get_profits\n",
      "   867|    648964|      2.29383|   3.5346e-06|  0.37%|        if self.normalize:\n",
      "   868|    648964|      4.09416|  6.30876e-06|  0.66%|          profits = profits / self.auction_params.max_budget\n",
      "   869|    648964|      2.93529|  4.52304e-06|  0.47%|        self.dict[\"clock_profits\"][:] = profits\n",
      "   870|         0|            0|            0|  0.00%|\n",
      "   871|    648964|      2.29193|  3.53168e-06|  0.37%|    if state.round > 1:\n",
      "   872|    399396|      1.37252|   3.4365e-06|  0.22%|      if \"agg_demand_history\" in self.dict:\n",
      "   873|    399396|      1.65759|  4.15024e-06|  0.27%|        if self.auction_params.information_policy == InformationPolicy.SHOW_DEMAND:\n",
      "   874|    399396|      2.54292|  6.36691e-06|  0.41%|          agg_demand_history = np.array(state.aggregate_demand)[start_ind:end_ind]\n",
      "   875|         0|            0|            0|  0.00%|        elif self.auction_params.information_policy == InformationPolicy.HIDE_DEMAND:\n",
      "   876|         0|            0|            0|  0.00%|          agg_demand_history = np.array(state.aggregate_demand)[start_ind:end_ind]\n",
      "   877|         0|            0|            0|  0.00%|          over_demanded = agg_demand_history > self.auction_params.licenses\n",
      "   878|         0|            0|            0|  0.00%|          at_demand = agg_demand_history == self.auction_params.licenses\n",
      "   879|         0|            0|            0|  0.00%|          under_demanded = agg_demand_history < self.auction_params.licenses\n",
      "   880|         0|            0|            0|  0.00%|          agg_demand_history[over_demanded] = InformationPolicyConstants.OVER_DEMAND\n",
      "   881|         0|            0|            0|  0.00%|          agg_demand_history[at_demand] = InformationPolicyConstants.AT_SUPPLY\n",
      "   882|         0|            0|            0|  0.00%|          agg_demand_history[under_demanded] = InformationPolicyConstants.UNDER_DEMAND\n",
      "   883|         0|            0|            0|  0.00%|        else:\n",
      "   884|         0|            0|            0|  0.00%|          raise ValueError(\"Unknown information policy\")\n",
      "   885|    399396|      1.41637|  3.54628e-06|  0.23%|        if self.normalize:\n",
      "   886|    399396|      2.97529|  7.44948e-06|  0.48%|          agg_demand_history = agg_demand_history / self.auction_params.licenses\n",
      "   887|    399396|      2.20502|  5.52088e-06|  0.36%|        self.dict[\"agg_demand_history\"][:min(length, len(agg_demand_history))] = agg_demand_history\n",
      "   888|    399396|      1.42521|  3.56842e-06|  0.23%|      if \"submitted_demand_history\" in self.dict:\n",
      "   889|    399396|      2.30457|  5.77013e-06|  0.37%|        submitted_demand_history = np.array(state.bidders[player].submitted_demand)[start_ind:end_ind]\n",
      "   890|    399396|      1.40996|  3.53024e-06|  0.23%|        if self.normalize:\n",
      "   891|    399396|      2.44631|  6.12503e-06|  0.39%|          submitted_demand_history = submitted_demand_history / self.auction_params.licenses\n",
      "   892|    399396|      1.96677|  4.92435e-06|  0.32%|        self.dict[\"submitted_demand_history\"][:min(length, len(submitted_demand_history))] = submitted_demand_history\n",
      "   893|    399396|      1.36079|  3.40712e-06|  0.22%|      if \"processed_demand_history\" in self.dict:\n",
      "   894|    399396|      2.01883|  5.05471e-06|  0.33%|        processed_demand_history = np.array(state.bidders[player].processed_demand)[start_ind:end_ind]\n",
      "   895|    399396|      1.35386|  3.38978e-06|  0.22%|        if self.normalize:\n",
      "   896|    399396|      2.24534|  5.62185e-06|  0.36%|          processed_demand_history = processed_demand_history / self.auction_params.licenses\n",
      "   897|    399396|      1.87692|  4.69939e-06|  0.30%|        self.dict[\"processed_demand_history\"][:min(length, len(processed_demand_history))] = processed_demand_history\n",
      "   898|    399396|      1.31912|  3.30279e-06|  0.21%|      if \"sor_exposure\" in self.dict:\n",
      "   899|    399396|      2.78439|   6.9715e-06|  0.45%|        sor_exposure = state.bidders[player].processed_demand[-1] @ state.sor_prices[-1]\n",
      "   900|    399396|       1.3186|  3.30149e-06|  0.21%|        if self.normalize:\n",
      "   901|    399396|      1.66473|  4.16811e-06|  0.27%|          sor_exposure = sor_exposure / self.auction_params.max_budget # TODO: Why not use a player specific bound here?\n",
      "   902|    399396|      1.44745|   3.6241e-06|  0.23%|        self.dict[\"sor_exposure\"][0] = sor_exposure\n",
      "   903|         0|            0|            0|  0.00%|\n",
      "   904|    648964|      2.09016|  3.22077e-06|  0.34%|    if \"posted_price_history\" in self.dict:\n",
      "   905|    648964|      3.57905|  5.51502e-06|  0.58%|      posted_prices = np.array(state.posted_prices)[start_ind:end_ind]\n",
      "   906|    648964|      2.15238|  3.31664e-06|  0.35%|      if self.normalize:\n",
      "   907|    648964|      4.41994|  6.81077e-06|  0.71%|        posted_prices = posted_prices / self.auction_params.max_budget\n",
      "   908|    648964|      3.33414|  5.13763e-06|  0.54%|      self.dict[\"posted_price_history\"][:min(length, len(posted_prices))] = posted_prices\n",
      "   909|         0|            0|            0|  0.00%|\n",
      "   910|    648964|      2.13048|   3.2829e-06|  0.34%|    if \"clock_prices\" in self.dict:\n",
      "   911|    648964|      2.59035|  3.99152e-06|  0.42%|      clock_prices = np.array(state.clock_prices[-1])\n",
      "   912|    648964|      2.07276|  3.19395e-06|  0.33%|      if self.normalize:\n",
      "   913|    648964|      3.64947|  5.62353e-06|  0.59%|        clock_prices = clock_prices / self.auction_params.max_budget\n",
      "   914|    648964|      2.67773|  4.12616e-06|  0.43%|      self.dict['clock_prices'][:] = clock_prices\n",
      "   915|         0|            0|            0|  0.00%|\n",
      "   916|    648964|         2.09|  3.22052e-06|  0.34%|    if \"price_increments\" in self.dict:\n",
      "   917|    648964|        2.412|  3.71669e-06|  0.39%|      price_increments = np.array(state.price_increments)\n",
      "   918|    648964|       2.0533|  3.16397e-06|  0.33%|      if self.normalize:\n",
      "   919|    648964|      3.93854|  6.06897e-06|  0.63%|        price_increments = price_increments / self.auction_params.max_round\n",
      "   920|    648964|        2.565|  3.95245e-06|  0.41%|      self.dict['price_increments'][:] = price_increments\n",
      "   921|         0|            0|            0|  0.00%|\n",
      "   922|   1946892|      6.20093|  3.18504e-06|  1.00%|    for i in range(len(self.auction_params.player_types)):\n",
      "   923|   1297928|      5.02683|  3.87297e-06|  0.81%|      if f\"revealed_types_{i}\" in self.dict and state.round >= self.auction_params.reveal_type_round:\n",
      "   924|    798792|       2.8037|  3.50992e-06|  0.45%|        self.dict[f'revealed_types_{i}'][state.bidders[i].type_index] = 1\n",
      "   925|         0|            0|            0|  0.00%|\n",
      "   926|    648964|      6.94217|  1.06973e-05|  1.12%|    if np.isnan(self.tensor).any():\n",
      "(call)|    648964|      6.21893|  9.58286e-06|  1.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py:54 _any\n",
      "   927|         0|            0|            0|  0.00%|      raise ValueError(f\"NaN in observation {self.dict}\")\n",
      "   928|         0|            0|            0|  0.00%|\n",
      "   929|         0|            0|            0|  0.00%|  def string_from(self, state, player):\n",
      "   930|         0|            0|            0|  0.00%|    \"\"\"Observation of `state` from the PoV of `player`, as a string.\"\"\"\n",
      "   931|         0|            0|            0|  0.00%|    pieces = []\n",
      "   932|         0|            0|            0|  0.00%|    if \"player\" in self.dict:\n",
      "   933|         0|            0|            0|  0.00%|      pieces.append(f\"p{player}\")\n",
      "   934|         0|            0|            0|  0.00%|    if \"bidder_type\" in self.dict:\n",
      "   935|         0|            0|            0|  0.00%|      pieces.append(f\"t{state.bidders[player].type_index}\")\n",
      "   936|         0|            0|            0|  0.00%|    if \"activity\" in self.dict:\n",
      "   937|         0|            0|            0|  0.00%|      pieces.append(f\"a{state.bidders[player].activity}\")\n",
      "   938|         0|            0|            0|  0.00%|    if \"round\" in self.dict:\n",
      "   939|         0|            0|            0|  0.00%|      pieces.append(f\"r{state.round}\")\n",
      "   940|         0|            0|            0|  0.00%|    if \"agg_demand_history\" in self.dict:\n",
      "   941|         0|            0|            0|  0.00%|      pieces.append(f\"agg{state.aggregate_demand}\")\n",
      "   942|         0|            0|            0|  0.00%|    if \"submitted_demand_history\" in self.dict:\n",
      "   943|         0|            0|            0|  0.00%|      pieces.append(f\"sub{state.bidders[player].submitted_demand}\")\n",
      "   944|         0|            0|            0|  0.00%|    if \"processed_demand_history\" in self.dict:\n",
      "   945|         0|            0|            0|  0.00%|      pieces.append(f\"proc{state.bidders[player].processed_demand}\")\n",
      "   946|         0|            0|            0|  0.00%|    if \"posted_price_history\" in self.dict:\n",
      "   947|         0|            0|            0|  0.00%|      pieces.append(f\"posted{state.posted_prices}\")\n",
      "   948|         0|            0|            0|  0.00%|    if \"clock_prices\" in self.dict:\n",
      "   949|         0|            0|            0|  0.00%|      pieces.append(f\"clock{state.clock_prices[-1]}\")\n",
      "   950|         0|            0|            0|  0.00%|    if \"sor_exposure\" in self.dict and state.round > 1:\n",
      "   951|         0|            0|            0|  0.00%|      pieces.append(f\"sor_exposure{state.bidders[player].processed_demand[-1] @ state.sor_prices[-1]}\")\n",
      "   952|         0|            0|            0|  0.00%|    if \"price_increments\" in self.dict and state.round > 1:\n",
      "   953|         0|            0|            0|  0.00%|      pieces.append(f\"increments{state.price_increments}\")\n",
      "   954|         0|            0|            0|  0.00%|\n",
      "   955|         0|            0|            0|  0.00%|    return \" \".join(str(p) for p in pieces)\n",
      "   956|         0|            0|            0|  0.00%|\n",
      "   957|         0|            0|            0|  0.00%|# Register the game with the OpenSpiel library\n",
      "   958|         0|            0|            0|  0.00%|pyspiel.register_game(_GAME_TYPE, ClockAuctionGame)\n",
      "File: /apps/open_spiel/open_spiel/python/rl_environment.py\n",
      "File duration: 116.182s (18.71%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|# Copyright 2019 DeepMind Technologies Limited\n",
      "     2|         0|            0|            0|  0.00%|#\n",
      "     3|         0|            0|            0|  0.00%|# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "     4|         0|            0|            0|  0.00%|# you may not use this file except in compliance with the License.\n",
      "     5|         0|            0|            0|  0.00%|# You may obtain a copy of the License at\n",
      "     6|         0|            0|            0|  0.00%|#\n",
      "     7|         0|            0|            0|  0.00%|#      http://www.apache.org/licenses/LICENSE-2.0\n",
      "     8|         0|            0|            0|  0.00%|#\n",
      "     9|         0|            0|            0|  0.00%|# Unless required by applicable law or agreed to in writing, software\n",
      "    10|         0|            0|            0|  0.00%|# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "    11|         0|            0|            0|  0.00%|# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "    12|         0|            0|            0|  0.00%|# See the License for the specific language governing permissions and\n",
      "    13|         0|            0|            0|  0.00%|# limitations under the License.\n",
      "    14|         0|            0|            0|  0.00%|\n",
      "    15|         0|            0|            0|  0.00%|\"\"\"Reinforcement Learning (RL) Environment for Open Spiel.\n",
      "    16|         0|            0|            0|  0.00%|\n",
      "    17|         0|            0|            0|  0.00%|This module wraps Open Spiel Python interface providing an RL-friendly API. It\n",
      "    18|         0|            0|            0|  0.00%|covers both turn-based and simultaneous move games. Interactions between agents\n",
      "    19|         0|            0|            0|  0.00%|and the underlying game occur mostly through the `reset` and `step` methods,\n",
      "    20|         0|            0|            0|  0.00%|which return a `TimeStep` structure (see its docstrings for more info).\n",
      "    21|         0|            0|            0|  0.00%|\n",
      "    22|         0|            0|            0|  0.00%|The following example illustrates the interaction dynamics. Consider a 2-player\n",
      "    23|         0|            0|            0|  0.00%|Kuhn Poker (turn-based game). Agents have access to the `observations` (a dict)\n",
      "    24|         0|            0|            0|  0.00%|field from `TimeSpec`, containing the following members:\n",
      "    25|         0|            0|            0|  0.00%| * `info_state`: list containing the game information state for each player. The\n",
      "    26|         0|            0|            0|  0.00%|   size of the list always correspond to the number of players. E.g.:\n",
      "    27|         0|            0|            0|  0.00%|   [[0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]].\n",
      "    28|         0|            0|            0|  0.00%| * `legal_actions`: list containing legal action ID lists (one for each player).\n",
      "    29|         0|            0|            0|  0.00%|   E.g.: [[0, 1], [0]], which corresponds to actions 0 and 1 being valid for\n",
      "    30|         0|            0|            0|  0.00%|   player 0 (the 1st player) and action 0 being valid for player 1 (2nd player).\n",
      "    31|         0|            0|            0|  0.00%| * `current_player`: zero-based integer representing the player to make a move.\n",
      "    32|         0|            0|            0|  0.00%|\n",
      "    33|         0|            0|            0|  0.00%|At each `step` call, the environment expects a singleton list with the action\n",
      "    34|         0|            0|            0|  0.00%|(as it's a turn-based game), e.g.: [1]. This (zero-based) action must correspond\n",
      "    35|         0|            0|            0|  0.00%|to the player specified at `current_player`. The game (which is at decision\n",
      "    36|         0|            0|            0|  0.00%|node) will process the action and take as many steps necessary to cover chance\n",
      "    37|         0|            0|            0|  0.00%|nodes, halting at a new decision or final node. Finally, a new `TimeStep`is\n",
      "    38|         0|            0|            0|  0.00%|returned to the agent.\n",
      "    39|         0|            0|            0|  0.00%|\n",
      "    40|         0|            0|            0|  0.00%|Simultaneous-move games follow analogous dynamics. The only differences is the\n",
      "    41|         0|            0|            0|  0.00%|environment expects a list of actions, one per player. Note the `current_player`\n",
      "    42|         0|            0|            0|  0.00%|field is \"irrelevant\" here, admitting a constant value defined in spiel.h, which\n",
      "    43|         0|            0|            0|  0.00%|defaults to -2 (module level constant `SIMULTANEOUS_PLAYER_ID`).\n",
      "    44|         0|            0|            0|  0.00%|\n",
      "    45|         0|            0|            0|  0.00%|See open_spiel/python/examples/rl_example.py for example usages.\n",
      "    46|         0|            0|            0|  0.00%|\"\"\"\n",
      "    47|         0|            0|            0|  0.00%|\n",
      "    48|         0|            0|            0|  0.00%|import collections\n",
      "    49|         0|            0|            0|  0.00%|\n",
      "    50|         0|            0|            0|  0.00%|import enum\n",
      "    51|         0|            0|            0|  0.00%|from absl import logging\n",
      "    52|         0|            0|            0|  0.00%|import numpy as np\n",
      "    53|         0|            0|            0|  0.00%|from open_spiel.python.observation import make_observation\n",
      "    54|         0|            0|            0|  0.00%|from copy import deepcopy\n",
      "    55|         0|            0|            0|  0.00%|\n",
      "    56|         0|            0|            0|  0.00%|import pyspiel\n",
      "    57|         0|            0|            0|  0.00%|\n",
      "    58|         0|            0|            0|  0.00%|SIMULTANEOUS_PLAYER_ID = pyspiel.PlayerId.SIMULTANEOUS\n",
      "    59|         0|            0|            0|  0.00%|\n",
      "    60|    648964|      1.16336|  1.79265e-06|  0.19%|def fastcopy(d):\n",
      "    61|         0|            0|            0|  0.00%|  \"\"\"assumes d is a dict of np arrays\"\"\"\n",
      "    62|  11681352|      26.5576|  2.27351e-06|  4.28%|  return {k: v.copy() for k, v in d.items()}\n",
      "(call)|    648964|      22.5679|  3.47752e-05|  3.63%|# /apps/open_spiel/open_spiel/python/rl_environment.py:62 <dictcomp>\n",
      "    63|         0|            0|            0|  0.00%|\n",
      "    64|         0|            0|            0|  0.00%|\n",
      "    65|         0|            0|            0|  0.00%|class TimeStep(\n",
      "    66|         0|            0|            0|  0.00%|    collections.namedtuple(\n",
      "    67|         0|            0|            0|  0.00%|        \"TimeStep\", [\"observations\", \"rewards\", \"discounts\", \"step_type\"])):\n",
      "    68|         0|            0|            0|  0.00%|  \"\"\"Returned with every call to `step` and `reset`.\n",
      "    69|         0|            0|            0|  0.00%|\n",
      "    70|         0|            0|            0|  0.00%|  A `TimeStep` contains the data emitted by a game at each step of interaction.\n",
      "    71|         0|            0|            0|  0.00%|  A `TimeStep` holds an `observation` (list of dicts, one per player),\n",
      "    72|         0|            0|            0|  0.00%|  associated lists of `rewards`, `discounts` and a `step_type`.\n",
      "    73|         0|            0|            0|  0.00%|\n",
      "    74|         0|            0|            0|  0.00%|  The first `TimeStep` in a sequence will have `StepType.FIRST`. The final\n",
      "    75|         0|            0|            0|  0.00%|  `TimeStep` will have `StepType.LAST`. All other `TimeStep`s in a sequence will\n",
      "    76|         0|            0|            0|  0.00%|  have `StepType.MID.\n",
      "    77|         0|            0|            0|  0.00%|\n",
      "    78|         0|            0|            0|  0.00%|  Attributes:\n",
      "    79|         0|            0|            0|  0.00%|    observations: a list of dicts containing observations per player.\n",
      "    80|         0|            0|            0|  0.00%|    rewards: A list of scalars (one per player), or `None` if `step_type` is\n",
      "    81|         0|            0|            0|  0.00%|      `StepType.FIRST`, i.e. at the start of a sequence.\n",
      "    82|         0|            0|            0|  0.00%|    discounts: A list of discount values in the range `[0, 1]` (one per player),\n",
      "    83|         0|            0|            0|  0.00%|      or `None` if `step_type` is `StepType.FIRST`.\n",
      "    84|         0|            0|            0|  0.00%|    step_type: A `StepType` enum value.\n",
      "    85|         0|            0|            0|  0.00%|  \"\"\"\n",
      "    86|         0|            0|            0|  0.00%|  __slots__ = ()\n",
      "    87|         0|            0|            0|  0.00%|\n",
      "    88|         0|            0|            0|  0.00%|  def first(self):\n",
      "    89|         0|            0|            0|  0.00%|    return self.step_type == StepType.FIRST\n",
      "    90|         0|            0|            0|  0.00%|\n",
      "    91|         0|            0|            0|  0.00%|  def mid(self):\n",
      "    92|         0|            0|            0|  0.00%|    return self.step_type == StepType.MID\n",
      "    93|         0|            0|            0|  0.00%|\n",
      "    94|     99840|      0.10997|  1.10146e-06|  0.02%|  def last(self):\n",
      "    95|     99840|     0.179083|   1.7937e-06|  0.03%|    return self.step_type == StepType.LAST\n",
      "    96|         0|            0|            0|  0.00%|\n",
      "    97|         0|            0|            0|  0.00%|  def is_simultaneous_move(self):\n",
      "    98|         0|            0|            0|  0.00%|    return self.observations[\"current_player\"] == SIMULTANEOUS_PLAYER_ID\n",
      "    99|         0|            0|            0|  0.00%|\n",
      "   100|         0|            0|            0|  0.00%|  def current_player(self):\n",
      "   101|         0|            0|            0|  0.00%|    return self.observations[\"current_player\"]\n",
      "   102|         0|            0|            0|  0.00%|\n",
      "   103|         0|            0|            0|  0.00%|\n",
      "   104|         0|            0|            0|  0.00%|class StepType(enum.Enum):\n",
      "   105|         0|            0|            0|  0.00%|  \"\"\"Defines the status of a `TimeStep` within a sequence.\"\"\"\n",
      "   106|         0|            0|            0|  0.00%|\n",
      "   107|         0|            0|            0|  0.00%|  FIRST = 0  # Denotes the first `TimeStep` in a sequence.\n",
      "   108|         0|            0|            0|  0.00%|  MID = 1  # Denotes any `TimeStep` in a sequence that is not FIRST or LAST.\n",
      "   109|         0|            0|            0|  0.00%|  LAST = 2  # Denotes the last `TimeStep` in a sequence.\n",
      "   110|         0|            0|            0|  0.00%|\n",
      "   111|         0|            0|            0|  0.00%|  def first(self):\n",
      "   112|         0|            0|            0|  0.00%|    return self is StepType.FIRST\n",
      "   113|         0|            0|            0|  0.00%|\n",
      "   114|         0|            0|            0|  0.00%|  def mid(self):\n",
      "   115|         0|            0|            0|  0.00%|    return self is StepType.MID\n",
      "   116|         0|            0|            0|  0.00%|\n",
      "   117|         0|            0|            0|  0.00%|  def last(self):\n",
      "   118|         0|            0|            0|  0.00%|    return self is StepType.LAST\n",
      "   119|         0|            0|            0|  0.00%|\n",
      "   120|         0|            0|            0|  0.00%|\n",
      "   121|         0|            0|            0|  0.00%|# Global pyspiel members\n",
      "   122|         0|            0|            0|  0.00%|def registered_games():\n",
      "   123|         0|            0|            0|  0.00%|  return pyspiel.registered_games()\n",
      "   124|         0|            0|            0|  0.00%|\n",
      "   125|         0|            0|            0|  0.00%|\n",
      "   126|         0|            0|            0|  0.00%|class ChanceEventSampler(object):\n",
      "   127|         0|            0|            0|  0.00%|  \"\"\"Default sampler for external chance events.\"\"\"\n",
      "   128|         0|            0|            0|  0.00%|\n",
      "   129|         0|            0|            0|  0.00%|  def __init__(self, seed=None):\n",
      "   130|         0|            0|            0|  0.00%|    self.seed(seed)\n",
      "   131|         0|            0|            0|  0.00%|\n",
      "   132|         0|            0|            0|  0.00%|  def seed(self, seed=None):\n",
      "   133|         0|            0|            0|  0.00%|    self._rng = np.random.RandomState(seed)\n",
      "   134|         0|            0|            0|  0.00%|\n",
      "   135|         0|            0|            0|  0.00%|  def __call__(self, state):\n",
      "   136|         0|            0|            0|  0.00%|    \"\"\"Sample a chance event in the given state.\"\"\"\n",
      "   137|         0|            0|            0|  0.00%|    actions, probs = zip(*state.chance_outcomes())\n",
      "   138|         0|            0|            0|  0.00%|    return self._rng.choice(actions, p=probs)\n",
      "   139|         0|            0|            0|  0.00%|\n",
      "   140|         0|            0|            0|  0.00%|\n",
      "   141|         0|            0|            0|  0.00%|class ObservationType(enum.Enum):\n",
      "   142|         0|            0|            0|  0.00%|  \"\"\"Defines what kind of observation to use.\"\"\"\n",
      "   143|         0|            0|            0|  0.00%|  OBSERVATION = 0  # Use observation_tensor\n",
      "   144|         0|            0|            0|  0.00%|  INFORMATION_STATE = 1  # Use information_state_tensor\n",
      "   145|         0|            0|            0|  0.00%|\n",
      "   146|         0|            0|            0|  0.00%|\n",
      "   147|         0|            0|            0|  0.00%|class Environment(object):\n",
      "   148|         0|            0|            0|  0.00%|  \"\"\"Open Spiel reinforcement learning environment class.\"\"\"\n",
      "   149|         0|            0|            0|  0.00%|\n",
      "   150|         0|            0|            0|  0.00%|  def __init__(self,\n",
      "   151|         0|            0|            0|  0.00%|               game,\n",
      "   152|         0|            0|            0|  0.00%|               discount=1.0,\n",
      "   153|         0|            0|            0|  0.00%|               chance_event_sampler=None,\n",
      "   154|         0|            0|            0|  0.00%|               history_prefix=None,\n",
      "   155|         0|            0|            0|  0.00%|               observation_type=None,\n",
      "   156|         0|            0|            0|  0.00%|               include_full_state=False,\n",
      "   157|         0|            0|            0|  0.00%|               mfg_distribution=None,\n",
      "   158|         0|            0|            0|  0.00%|               mfg_population=None,\n",
      "   159|         0|            0|            0|  0.00%|               enable_legality_check=False,\n",
      "   160|         0|            0|            0|  0.00%|               use_observer_api=False,\n",
      "   161|         0|            0|            0|  0.00%|               observer_params=None,\n",
      "   162|         0|            0|            0|  0.00%|               **kwargs):\n",
      "   163|         0|            0|            0|  0.00%|    \"\"\"Constructor.\n",
      "   164|         0|            0|            0|  0.00%|\n",
      "   165|         0|            0|            0|  0.00%|    Args:\n",
      "   166|         0|            0|            0|  0.00%|      game: [string, pyspiel.Game] Open Spiel game name or game instance.\n",
      "   167|         0|            0|            0|  0.00%|      discount: float, discount used in non-initial steps. Defaults to 1.0.\n",
      "   168|         0|            0|            0|  0.00%|      chance_event_sampler: optional object with `sample_external_events` method\n",
      "   169|         0|            0|            0|  0.00%|        to sample chance events.\n",
      "   170|         0|            0|            0|  0.00%|      observation_type: what kind of observation to use. If not specified, will\n",
      "   171|         0|            0|            0|  0.00%|        default to INFORMATION_STATE unless the game doesn't provide it.\n",
      "   172|         0|            0|            0|  0.00%|      include_full_state: whether or not to include the full serialized\n",
      "   173|         0|            0|            0|  0.00%|        OpenSpiel state in the observations (sometimes useful for debugging).\n",
      "   174|         0|            0|            0|  0.00%|      mfg_distribution: the distribution over states if the game is a mean field\n",
      "   175|         0|            0|            0|  0.00%|        game.\n",
      "   176|         0|            0|            0|  0.00%|      mfg_population: The Mean Field Game population to consider.\n",
      "   177|         0|            0|            0|  0.00%|      enable_legality_check: Check the legality of the move before stepping.\n",
      "   178|         0|            0|            0|  0.00%|      **kwargs: dict, additional settings passed to the Open Spiel game.\n",
      "   179|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   180|         0|            0|            0|  0.00%|    self._chance_event_sampler = chance_event_sampler or ChanceEventSampler()\n",
      "   181|         0|            0|            0|  0.00%|    if history_prefix is None:\n",
      "   182|         0|            0|            0|  0.00%|      history_prefix = []\n",
      "   183|         0|            0|            0|  0.00%|    self._history_prefix = history_prefix\n",
      "   184|         0|            0|            0|  0.00%|    self._include_full_state = include_full_state\n",
      "   185|         0|            0|            0|  0.00%|    self._mfg_distribution = mfg_distribution\n",
      "   186|         0|            0|            0|  0.00%|    self._mfg_population = mfg_population\n",
      "   187|         0|            0|            0|  0.00%|    self._enable_legality_check = enable_legality_check\n",
      "   188|         0|            0|            0|  0.00%|\n",
      "   189|         0|            0|            0|  0.00%|    if isinstance(game, str):\n",
      "   190|         0|            0|            0|  0.00%|      if kwargs:\n",
      "   191|         0|            0|            0|  0.00%|        game_settings = {key: val for (key, val) in kwargs.items()}\n",
      "   192|         0|            0|            0|  0.00%|        logging.info(\"Using game settings: %s\", game_settings)\n",
      "   193|         0|            0|            0|  0.00%|        self._game = pyspiel.load_game(game, game_settings)\n",
      "   194|         0|            0|            0|  0.00%|      else:\n",
      "   195|         0|            0|            0|  0.00%|        logging.info(\"Using game string: %s\", game)\n",
      "   196|         0|            0|            0|  0.00%|        self._game = pyspiel.load_game(game)\n",
      "   197|         0|            0|            0|  0.00%|    else:  # pyspiel.Game or API-compatible object.\n",
      "   198|         0|            0|            0|  0.00%|      logging.info(\"Using game instance: %s\", game.get_type().short_name)\n",
      "   199|         0|            0|            0|  0.00%|      self._game = game\n",
      "   200|         0|            0|            0|  0.00%|\n",
      "   201|         0|            0|            0|  0.00%|    self._num_players = self._game.num_players()\n",
      "   202|         0|            0|            0|  0.00%|    self._state = None\n",
      "   203|         0|            0|            0|  0.00%|    self._should_reset = True\n",
      "   204|         0|            0|            0|  0.00%|\n",
      "   205|         0|            0|            0|  0.00%|    # Discount returned at non-initial steps.\n",
      "   206|         0|            0|            0|  0.00%|    self._discounts = [discount] * self._num_players\n",
      "   207|         0|            0|            0|  0.00%|\n",
      "   208|         0|            0|            0|  0.00%|    # Determine what observation type to use.\n",
      "   209|         0|            0|            0|  0.00%|    if observation_type is None:\n",
      "   210|         0|            0|            0|  0.00%|      if self._game.get_type().provides_information_state_tensor:\n",
      "   211|         0|            0|            0|  0.00%|        observation_type = ObservationType.INFORMATION_STATE\n",
      "   212|         0|            0|            0|  0.00%|      else:\n",
      "   213|         0|            0|            0|  0.00%|        observation_type = ObservationType.OBSERVATION\n",
      "   214|         0|            0|            0|  0.00%|\n",
      "   215|         0|            0|            0|  0.00%|    # Check the requested observation type is supported.\n",
      "   216|         0|            0|            0|  0.00%|    if observation_type == ObservationType.OBSERVATION:\n",
      "   217|         0|            0|            0|  0.00%|      if not self._game.get_type().provides_observation_tensor:\n",
      "   218|         0|            0|            0|  0.00%|        raise ValueError(f\"observation_tensor not supported by {game}\")\n",
      "   219|         0|            0|            0|  0.00%|    elif observation_type == ObservationType.INFORMATION_STATE:\n",
      "   220|         0|            0|            0|  0.00%|      if not self._game.get_type().provides_information_state_tensor:\n",
      "   221|         0|            0|            0|  0.00%|        raise ValueError(f\"information_state_tensor not supported by {game}\")\n",
      "   222|         0|            0|            0|  0.00%|    self._use_observation = (observation_type == ObservationType.OBSERVATION)\n",
      "   223|         0|            0|            0|  0.00%|\n",
      "   224|         0|            0|            0|  0.00%|    if self._game.get_type().dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD:\n",
      "   225|         0|            0|            0|  0.00%|      assert mfg_distribution is not None\n",
      "   226|         0|            0|            0|  0.00%|      assert mfg_population is not None\n",
      "   227|         0|            0|            0|  0.00%|      assert 0 <= mfg_population < self._num_players\n",
      "   228|         0|            0|            0|  0.00%|\n",
      "   229|         0|            0|            0|  0.00%|    # MODIFIED\n",
      "   230|         0|            0|            0|  0.00%|    self.observer = None\n",
      "   231|         0|            0|            0|  0.00%|    if use_observer_api:\n",
      "   232|         0|            0|            0|  0.00%|      self.observer = make_observation(self._game, params=observer_params)\n",
      "   233|         0|            0|            0|  0.00%|\n",
      "   234|         0|            0|            0|  0.00%|  def seed(self, seed=None):\n",
      "   235|         0|            0|            0|  0.00%|    self._chance_event_sampler.seed(self._game)\n",
      "   236|         0|            0|            0|  0.00%|\n",
      "   237|    299524|     0.806348|   2.6921e-06|  0.13%|  def get_time_step(self):\n",
      "   238|         0|            0|            0|  0.00%|    \"\"\"Returns a `TimeStep` without updating the environment.\n",
      "   239|         0|            0|            0|  0.00%|\n",
      "   240|         0|            0|            0|  0.00%|    Returns:\n",
      "   241|         0|            0|            0|  0.00%|      A `TimeStep` namedtuple containing:\n",
      "   242|         0|            0|            0|  0.00%|        observation: list of dicts containing one observations per player, each\n",
      "   243|         0|            0|            0|  0.00%|          corresponding to `observation_spec()`.\n",
      "   244|         0|            0|            0|  0.00%|        reward: list of rewards at this timestep, or None if step_type is\n",
      "   245|         0|            0|            0|  0.00%|          `StepType.FIRST`.\n",
      "   246|         0|            0|            0|  0.00%|        discount: list of discounts in the range [0, 1], or None if step_type is\n",
      "   247|         0|            0|            0|  0.00%|          `StepType.FIRST`.\n",
      "   248|         0|            0|            0|  0.00%|        step_type: A `StepType` value.\n",
      "   249|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   250|    299524|     0.879195|  2.93531e-06|  0.14%|    observations = {\n",
      "   251|    299524|     0.808697|  2.69994e-06|  0.13%|        \"info_state\": [],\n",
      "   252|    299524|     0.760224|  2.53811e-06|  0.12%|        \"legal_actions\": [],\n",
      "   253|    299524|     0.750346|  2.50513e-06|  0.12%|        \"current_player\": [],\n",
      "   254|    299524|     0.766965|  2.56061e-06|  0.12%|        \"serialized_state\": [],\n",
      "   255|         0|            0|            0|  0.00%|    }\n",
      "   256|    299524|     0.782897|   2.6138e-06|  0.13%|    if self.observer is not None:\n",
      "   257|    299524|     0.777969|  2.59735e-06|  0.13%|      observations['info_dict'] = []\n",
      "   258|    299524|     0.757038|  2.52747e-06|  0.12%|    rewards = []\n",
      "   259|    299524|      2.14852|  7.17311e-06|  0.35%|    step_type = StepType.LAST if self._state.is_terminal() else StepType.MID\n",
      "(call)|    299524|     0.957859|  3.19794e-06|  0.15%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:450 is_terminal\n",
      "   260|    299524|     0.938874|  3.13456e-06|  0.15%|    self._should_reset = step_type == StepType.LAST\n",
      "   261|         0|            0|            0|  0.00%|\n",
      "   262|    299524|       5.3812|  1.79658e-05|  0.87%|    cur_rewards = self._state.rewards()\n",
      "(call)|    299524|     0.969246|  3.23596e-06|  0.16%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:450 is_terminal\n",
      "(call)|    249616|      1.52033|  6.09066e-06|  0.24%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:309 current_player\n",
      "(call)|     49908|       12.818|  0.000256832|  2.06%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:454 returns\n",
      "   263|    898572|      3.77995|  4.20662e-06|  0.61%|    for player_id in range(self.num_players):\n",
      "(call)|    299524|      1.93714|   6.4674e-06|  0.31%|# /apps/open_spiel/open_spiel/python/rl_environment.py:463 num_players\n",
      "   264|    599048|      1.47847|  2.46803e-06|  0.24%|      rewards.append(cur_rewards[player_id])\n",
      "   265|    599048|      1.35901|  2.26861e-06|  0.22%|      if self.observer is None:\n",
      "   266|         0|            0|            0|  0.00%|        info_state = self._state.observation_tensor(player_id) if self._use_observation else self._state.information_state_tensor(player_id)\n",
      "   267|         0|            0|            0|  0.00%|      else:\n",
      "   268|    599048|      6.32472|  1.05579e-05|  1.02%|        self.observer.set_from(self._state, player=player_id)\n",
      "(call)|    599048|       202.98|  0.000338837| 32.68%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:833 set_from\n",
      "   269|    599048|      2.43034|    4.057e-06|  0.39%|        info_state = np.array(self.observer.tensor)\n",
      "   270|    599048|      4.14943|  6.92671e-06|  0.67%|        observations['info_dict'].append(fastcopy(self.observer.dict))\n",
      "(call)|    599048|      25.5745|  4.26919e-05|  4.12%|# /apps/open_spiel/open_spiel/python/rl_environment.py:60 fastcopy\n",
      "   271|         0|            0|            0|  0.00%|\n",
      "   272|    599048|      1.48239|  2.47458e-06|  0.24%|      observations[\"info_state\"].append(info_state)\n",
      "   273|    599048|      17.1961|  2.87058e-05|  2.77%|      observations[\"legal_actions\"].append(self._state.legal_actions(player_id))\n",
      "(call)|    599048|       2.2406|  3.74028e-06|  0.36%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:450 is_terminal\n",
      "(call)|   1248080|       7.4541|  5.97245e-06|  1.20%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:309 current_player\n",
      "(call)|    249616|       37.319|  0.000149506|  6.01%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:322 _legal_actions\n",
      "   274|    299524|      1.91113|  6.38055e-06|  0.31%|    observations[\"current_player\"] = self._state.current_player()\n",
      "(call)|    299524|      1.75434|  5.85709e-06|  0.28%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:309 current_player\n",
      "   275|    299524|     0.681759|  2.27614e-06|  0.11%|    discounts = self._discounts\n",
      "   276|    299524|     0.809836|  2.70374e-06|  0.13%|    if step_type == StepType.LAST:\n",
      "   277|         0|            0|            0|  0.00%|      # When the game is in a terminal state set the discount to 0.\n",
      "   278|    249540|     0.663047|  2.65708e-06|  0.11%|      discounts = [0. for _ in discounts]\n",
      "(call)|     49908|     0.343455|  6.88176e-06|  0.06%|# /apps/open_spiel/open_spiel/python/rl_environment.py:278 <listcomp>\n",
      "   279|         0|            0|            0|  0.00%|\n",
      "   280|    299524|     0.652632|   2.1789e-06|  0.11%|    if self._include_full_state:\n",
      "   281|         0|            0|            0|  0.00%|      observations[\"serialized_state\"] = pyspiel.serialize_game_and_state(\n",
      "   282|         0|            0|            0|  0.00%|          self._game, self._state)\n",
      "   283|         0|            0|            0|  0.00%|\n",
      "   284|    599048|      2.87016|   4.7912e-06|  0.46%|    return TimeStep(\n",
      "(call)|    299524|      1.35484|  4.52332e-06|  0.22%|# <string>:1 __new__\n",
      "   285|    299524|     0.579234|  1.93385e-06|  0.09%|        observations=observations,\n",
      "   286|    299524|     0.570098|  1.90335e-06|  0.09%|        rewards=rewards,\n",
      "   287|    299524|       0.5629|  1.87931e-06|  0.09%|        discounts=discounts,\n",
      "   288|    299524|     0.559076|  1.86655e-06|  0.09%|        step_type=step_type)\n",
      "   289|         0|            0|            0|  0.00%|\n",
      "   290|         0|            0|            0|  0.00%|  def _check_legality(self, actions):\n",
      "   291|         0|            0|            0|  0.00%|    if self.is_turn_based:\n",
      "   292|         0|            0|            0|  0.00%|      legal_actions = self._state.legal_actions()\n",
      "   293|         0|            0|            0|  0.00%|      if actions[0] not in legal_actions:\n",
      "   294|         0|            0|            0|  0.00%|        raise RuntimeError(f\"step() called on illegal action {actions[0]}\")\n",
      "   295|         0|            0|            0|  0.00%|    else:\n",
      "   296|         0|            0|            0|  0.00%|      for p in range(len(actions)):\n",
      "   297|         0|            0|            0|  0.00%|        legal_actions = self._state.legal_actions(p)\n",
      "   298|         0|            0|            0|  0.00%|        if legal_actions and actions[p] not in legal_actions:\n",
      "   299|         0|            0|            0|  0.00%|          raise RuntimeError(f\"step() by player {p} called on illegal \" +\n",
      "   300|         0|            0|            0|  0.00%|                             f\"action: {actions[p]}\")\n",
      "   301|         0|            0|            0|  0.00%|\n",
      "   302|     99840|     0.164886|   1.6515e-06|  0.03%|  def step(self, actions):\n",
      "   303|         0|            0|            0|  0.00%|    \"\"\"Updates the environment according to `actions` and returns a `TimeStep`.\n",
      "   304|         0|            0|            0|  0.00%|\n",
      "   305|         0|            0|            0|  0.00%|    If the environment returned a `TimeStep` with `StepType.LAST` at the\n",
      "   306|         0|            0|            0|  0.00%|    previous step, this call to `step` will start a new sequence and `actions`\n",
      "   307|         0|            0|            0|  0.00%|    will be ignored.\n",
      "   308|         0|            0|            0|  0.00%|\n",
      "   309|         0|            0|            0|  0.00%|    This method will also start a new sequence if called after the environment\n",
      "   310|         0|            0|            0|  0.00%|    has been constructed and `reset` has not been called. Again, in this case\n",
      "   311|         0|            0|            0|  0.00%|    `actions` will be ignored.\n",
      "   312|         0|            0|            0|  0.00%|\n",
      "   313|         0|            0|            0|  0.00%|    Args:\n",
      "   314|         0|            0|            0|  0.00%|      actions: a list containing one action per player, following specifications\n",
      "   315|         0|            0|            0|  0.00%|        defined in `action_spec()`.\n",
      "   316|         0|            0|            0|  0.00%|\n",
      "   317|         0|            0|            0|  0.00%|    Returns:\n",
      "   318|         0|            0|            0|  0.00%|      A `TimeStep` namedtuple containing:\n",
      "   319|         0|            0|            0|  0.00%|        observation: list of dicts containing one observations per player, each\n",
      "   320|         0|            0|            0|  0.00%|          corresponding to `observation_spec()`.\n",
      "   321|         0|            0|            0|  0.00%|        reward: list of rewards at this timestep, or None if step_type is\n",
      "   322|         0|            0|            0|  0.00%|          `StepType.FIRST`.\n",
      "   323|         0|            0|            0|  0.00%|        discount: list of discounts in the range [0, 1], or None if step_type is\n",
      "   324|         0|            0|            0|  0.00%|          `StepType.FIRST`.\n",
      "   325|         0|            0|            0|  0.00%|        step_type: A `StepType` value.\n",
      "   326|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   327|     99840|     0.618798|  6.19789e-06|  0.10%|    assert len(actions) == self.num_actions_per_step, (\n",
      "(call)|     99840|      3.17237|  3.17745e-05|  0.51%|# /apps/open_spiel/open_spiel/python/rl_environment.py:467 num_actions_per_step\n",
      "   328|         0|            0|            0|  0.00%|        \"Invalid number of actions! Expected {}\".format(\n",
      "   329|         0|            0|            0|  0.00%|            self.num_actions_per_step))\n",
      "   330|     99840|     0.186466|  1.86764e-06|  0.03%|    if self._should_reset:\n",
      "   331|         0|            0|            0|  0.00%|      return self.reset()\n",
      "   332|         0|            0|            0|  0.00%|\n",
      "   333|     99840|     0.163202|  1.63464e-06|  0.03%|    if self._enable_legality_check:\n",
      "   334|         0|            0|            0|  0.00%|      self._check_legality(actions)\n",
      "   335|         0|            0|            0|  0.00%|\n",
      "   336|     99840|       0.5125|  5.13322e-06|  0.08%|    if self.is_turn_based:\n",
      "(call)|     99840|      1.39722|  1.39945e-05|  0.22%|# /apps/open_spiel/open_spiel/python/rl_environment.py:472 is_turn_based\n",
      "   337|     99840|      1.89611|  1.89915e-05|  0.31%|      self._state.apply_action(actions[0])\n",
      "(call)|     99840|     0.663009|  6.64072e-06|  0.11%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:309 current_player\n",
      "(call)|     99840|      13.7287|  0.000137507|  2.21%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:370 _apply_action\n",
      "   338|         0|            0|            0|  0.00%|    else:\n",
      "   339|         0|            0|            0|  0.00%|      self._state.apply_actions(actions)\n",
      "   340|     99840|     0.732523|  7.33697e-06|  0.12%|    self._sample_external_events()\n",
      "(call)|     99840|      40.7811|  0.000408465|  6.57%|# /apps/open_spiel/open_spiel/python/rl_environment.py:398 _sample_external_events\n",
      "   341|         0|            0|            0|  0.00%|\n",
      "   342|     99840|     0.743932|  7.45124e-06|  0.12%|    return self.get_time_step()\n",
      "(call)|     99840|      123.521|   0.00123719| 19.89%|# /apps/open_spiel/open_spiel/python/rl_environment.py:237 get_time_step\n",
      "   343|         0|            0|            0|  0.00%|\n",
      "   344|     24958|    0.0628228|  2.51714e-06|  0.01%|  def reset(self):\n",
      "   345|         0|            0|            0|  0.00%|    \"\"\"Starts a new sequence and returns the first `TimeStep` of this sequence.\n",
      "   346|         0|            0|            0|  0.00%|\n",
      "   347|         0|            0|            0|  0.00%|    Returns:\n",
      "   348|         0|            0|            0|  0.00%|      A `TimeStep` namedtuple containing:\n",
      "   349|         0|            0|            0|  0.00%|        observations: list of dicts containing one observations per player, each\n",
      "   350|         0|            0|            0|  0.00%|          corresponding to `observation_spec()`.\n",
      "   351|         0|            0|            0|  0.00%|        rewards: list of rewards at this timestep, or None if step_type is\n",
      "   352|         0|            0|            0|  0.00%|          `StepType.FIRST`.\n",
      "   353|         0|            0|            0|  0.00%|        discounts: list of discounts in the range [0, 1], or None if step_type\n",
      "   354|         0|            0|            0|  0.00%|          is `StepType.FIRST`.\n",
      "   355|         0|            0|            0|  0.00%|        step_type: A `StepType` value.\n",
      "   356|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   357|     24958|    0.0735924|  2.94865e-06|  0.01%|    self._should_reset = False\n",
      "   358|     24958|     0.431973|   1.7308e-05|  0.07%|    if self._game.get_type().dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD and self._num_players > 1:\n",
      "   359|         0|            0|            0|  0.00%|      self._state = self._game.new_initial_state_for_population(self._mfg_population)\n",
      "   360|         0|            0|            0|  0.00%|    else:\n",
      "   361|     24958|     0.330685|  1.32496e-05|  0.05%|      self._state = self._game.new_initial_state()\n",
      "(call)|     24958|      2.45896|  9.85241e-05|  0.40%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:272 new_initial_state\n",
      "   362|     24958|    0.0762329|  3.05445e-06|  0.01%|    for action in self._history_prefix:\n",
      "   363|         0|            0|            0|  0.00%|      if self.is_turn_based:\n",
      "   364|         0|            0|            0|  0.00%|        self._state.apply_action(action)\n",
      "   365|         0|            0|            0|  0.00%|      else:\n",
      "   366|         0|            0|            0|  0.00%|        self._state.apply_actions(action)\n",
      "   367|     24958|     0.207735|  8.32339e-06|  0.03%|    self._sample_external_events()\n",
      "(call)|     24958|      17.3369|  0.000694643|  2.79%|# /apps/open_spiel/open_spiel/python/rl_environment.py:398 _sample_external_events\n",
      "   368|         0|            0|            0|  0.00%|\n",
      "   369|     24958|    0.0634577|  2.54258e-06|  0.01%|    observations = {\n",
      "   370|     24958|    0.0610733|  2.44704e-06|  0.01%|        \"info_state\": [],\n",
      "   371|     24958|    0.0555372|  2.22523e-06|  0.01%|        \"legal_actions\": [],\n",
      "   372|     24958|    0.0549064|  2.19995e-06|  0.01%|        \"current_player\": [],\n",
      "   373|     24958|    0.0542824|  2.17495e-06|  0.01%|        \"serialized_state\": []\n",
      "   374|         0|            0|            0|  0.00%|    }\n",
      "   375|     24958|    0.0572636|   2.2944e-06|  0.01%|    if self.observer is not None:\n",
      "   376|     24958|    0.0547521|  2.19377e-06|  0.01%|      observations['info_dict'] = []\n",
      "   377|     74874|     0.296881|  3.96507e-06|  0.05%|    for player_id in range(self.num_players):\n",
      "(call)|     24958|     0.148872|  5.96491e-06|  0.02%|# /apps/open_spiel/open_spiel/python/rl_environment.py:463 num_players\n",
      "   378|     49916|     0.113804|  2.27991e-06|  0.02%|      if self.observer is None:\n",
      "   379|         0|            0|            0|  0.00%|        info_state = self._state.observation_tensor(player_id) if self._use_observation else self._state.information_state_tensor(player_id)\n",
      "   380|         0|            0|            0|  0.00%|      else:\n",
      "   381|     49916|     0.517932|  1.03761e-05|  0.08%|        self.observer.set_from(self._state, player=player_id)\n",
      "(call)|     49916|      13.8782|  0.000278032|  2.23%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:833 set_from\n",
      "   382|     49916|     0.238794|  4.78391e-06|  0.04%|        info_state = np.array(self.observer.tensor)\n",
      "   383|     49916|     0.340902|  6.82952e-06|  0.05%|        observations['info_dict'].append(fastcopy(self.observer.dict))\n",
      "(call)|     49916|      2.14648|  4.30019e-05|  0.35%|# /apps/open_spiel/open_spiel/python/rl_environment.py:60 fastcopy\n",
      "   384|     49916|     0.116495|  2.33382e-06|  0.02%|      observations[\"info_state\"].append(info_state)\n",
      "   385|     49916|      1.60329|  3.21197e-05|  0.26%|      observations[\"legal_actions\"].append(self._state.legal_actions(player_id))\n",
      "(call)|     49916|     0.189854|  3.80346e-06|  0.03%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:450 is_terminal\n",
      "(call)|    124790|     0.747549|  5.99046e-06|  0.12%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:309 current_player\n",
      "(call)|     24958|      3.51542|  0.000140853|  0.57%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:322 _legal_actions\n",
      "   386|     24958|     0.151299|  6.06213e-06|  0.02%|    observations[\"current_player\"] = self._state.current_player()\n",
      "(call)|     24958|     0.142191|  5.69721e-06|  0.02%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:309 current_player\n",
      "   387|         0|            0|            0|  0.00%|\n",
      "   388|     24958|    0.0547965|  2.19555e-06|  0.01%|    if self._include_full_state:\n",
      "   389|         0|            0|            0|  0.00%|      observations[\"serialized_state\"] = pyspiel.serialize_game_and_state(\n",
      "   390|         0|            0|            0|  0.00%|          self._game, self._state)\n",
      "   391|         0|            0|            0|  0.00%|\n",
      "   392|     49916|     0.246892|  4.94614e-06|  0.04%|    return TimeStep(\n",
      "(call)|     24958|     0.115231|  4.61699e-06|  0.02%|# <string>:1 __new__\n",
      "   393|     24958|    0.0491376|  1.96881e-06|  0.01%|        observations=observations,\n",
      "   394|     24958|     0.048151|  1.92928e-06|  0.01%|        rewards=None,\n",
      "   395|     24958|    0.0479932|  1.92296e-06|  0.01%|        discounts=None,\n",
      "   396|     24958|    0.0657887|  2.63598e-06|  0.01%|        step_type=StepType.FIRST)\n",
      "   397|         0|            0|            0|  0.00%|\n",
      "   398|    124798|     0.348651|  2.79372e-06|  0.06%|  def _sample_external_events(self):\n",
      "   399|         0|            0|            0|  0.00%|    \"\"\"Sample chance events until we get to a decision node.\"\"\"\n",
      "   400|    324476|      3.17913|  9.79774e-06|  0.51%|    while self._state.is_chance_node() or (self._state.current_player()\n",
      "(call)|    324476|      1.89782|  5.84889e-06|  0.31%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:309 current_player\n",
      "   401|    124798|     0.304017|  2.43607e-06|  0.05%|                                           == pyspiel.PlayerId.MEAN_FIELD):\n",
      "   402|     74880|     0.697332|  9.31267e-06|  0.11%|      if self._state.is_chance_node():\n",
      "(call)|     74880|      0.44241|  5.90825e-06|  0.07%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:309 current_player\n",
      "   403|     74880|     0.702551|  9.38235e-06|  0.11%|        outcome = self._chance_event_sampler(self._state)\n",
      "(call)|     74880|      15.4754|  0.000206669|  2.49%|# /apps/open_spiel/open_spiel/python/examples/ubc_utils.py:156 __call__\n",
      "   404|     74880|      1.49575|  1.99753e-05|  0.24%|        self._state.apply_action(outcome)\n",
      "(call)|     74880|     0.495346|  6.61519e-06|  0.08%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:309 current_player\n",
      "(call)|     74880|      31.9851|  0.000427152|  5.15%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:370 _apply_action\n",
      "   405|     74880|     0.675673|  9.02341e-06|  0.11%|      if self._state.current_player() == pyspiel.PlayerId.MEAN_FIELD:\n",
      "(call)|     74880|     0.418831|  5.59336e-06|  0.07%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:309 current_player\n",
      "   406|         0|            0|            0|  0.00%|        dist_to_register = self._state.distribution_support()\n",
      "   407|         0|            0|            0|  0.00%|        dist = [\n",
      "   408|         0|            0|            0|  0.00%|            self._mfg_distribution.value_str(str_state, default_value=0.0)\n",
      "   409|         0|            0|            0|  0.00%|            for str_state in dist_to_register\n",
      "   410|         0|            0|            0|  0.00%|        ]\n",
      "   411|         0|            0|            0|  0.00%|        self._state.update_distribution(dist)\n",
      "   412|         0|            0|            0|  0.00%|\n",
      "   413|         0|            0|            0|  0.00%|  def observation_spec(self):\n",
      "   414|         0|            0|            0|  0.00%|    \"\"\"Defines the observation per player provided by the environment.\n",
      "   415|         0|            0|            0|  0.00%|\n",
      "   416|         0|            0|            0|  0.00%|    Each dict member will contain its expected structure and shape. E.g.: for\n",
      "   417|         0|            0|            0|  0.00%|    Kuhn Poker {\"info_state\": (6,), \"legal_actions\": (2,), \"current_player\": (),\n",
      "   418|         0|            0|            0|  0.00%|                \"serialized_state\": ()}\n",
      "   419|         0|            0|            0|  0.00%|\n",
      "   420|         0|            0|            0|  0.00%|    Returns:\n",
      "   421|         0|            0|            0|  0.00%|      A specification dict describing the observation fields and shapes.\n",
      "   422|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   423|         0|            0|            0|  0.00%|    return dict(\n",
      "   424|         0|            0|            0|  0.00%|        info_state=tuple([\n",
      "   425|         0|            0|            0|  0.00%|            self._game.observation_tensor_size() if self._use_observation else\n",
      "   426|         0|            0|            0|  0.00%|            self._game.information_state_tensor_size()\n",
      "   427|         0|            0|            0|  0.00%|        ]),\n",
      "   428|         0|            0|            0|  0.00%|        legal_actions=(self._game.num_distinct_actions(),),\n",
      "   429|         0|            0|            0|  0.00%|        current_player=(),\n",
      "   430|         0|            0|            0|  0.00%|        serialized_state=(),\n",
      "   431|         0|            0|            0|  0.00%|    )\n",
      "   432|         0|            0|            0|  0.00%|\n",
      "   433|         0|            0|            0|  0.00%|  def action_spec(self):\n",
      "   434|         0|            0|            0|  0.00%|    \"\"\"Defines per player action specifications.\n",
      "   435|         0|            0|            0|  0.00%|\n",
      "   436|         0|            0|            0|  0.00%|    Specifications include action boundaries and their data type.\n",
      "   437|         0|            0|            0|  0.00%|    E.g.: for Kuhn Poker {\"num_actions\": 2, \"min\": 0, \"max\":1, \"dtype\": int}\n",
      "   438|         0|            0|            0|  0.00%|\n",
      "   439|         0|            0|            0|  0.00%|    Returns:\n",
      "   440|         0|            0|            0|  0.00%|      A specification dict containing per player action properties.\n",
      "   441|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   442|         0|            0|            0|  0.00%|    return dict(\n",
      "   443|         0|            0|            0|  0.00%|        num_actions=self._game.num_distinct_actions(),\n",
      "   444|         0|            0|            0|  0.00%|        min=0,\n",
      "   445|         0|            0|            0|  0.00%|        max=self._game.num_distinct_actions() - 1,\n",
      "   446|         0|            0|            0|  0.00%|        dtype=int,\n",
      "   447|         0|            0|            0|  0.00%|    )\n",
      "   448|         0|            0|            0|  0.00%|\n",
      "   449|         0|            0|            0|  0.00%|  # Environment properties\n",
      "   450|         0|            0|            0|  0.00%|  @property\n",
      "   451|         0|            0|            0|  0.00%|  def use_observation(self):\n",
      "   452|         0|            0|            0|  0.00%|    \"\"\"Returns whether the environment is using the game's observation.\n",
      "   453|         0|            0|            0|  0.00%|\n",
      "   454|         0|            0|            0|  0.00%|    If false, it is using the game's information state.\n",
      "   455|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   456|         0|            0|            0|  0.00%|    return self._use_observation\n",
      "   457|         0|            0|            0|  0.00%|\n",
      "   458|         0|            0|            0|  0.00%|  # Game properties\n",
      "   459|         0|            0|            0|  0.00%|  @property\n",
      "   460|         0|            0|            0|  0.00%|  def name(self):\n",
      "   461|         0|            0|            0|  0.00%|    return self._game.get_type().short_name\n",
      "   462|         0|            0|            0|  0.00%|\n",
      "   463|    324482|     0.498065|  1.53495e-06|  0.08%|  @property\n",
      "   464|         0|            0|            0|  0.00%|  def num_players(self):\n",
      "   465|    324482|      1.58795|   4.8938e-06|  0.26%|    return self._game.num_players()\n",
      "   466|         0|            0|            0|  0.00%|\n",
      "   467|     99840|     0.142746|  1.42975e-06|  0.02%|  @property\n",
      "   468|         0|            0|            0|  0.00%|  def num_actions_per_step(self):\n",
      "   469|     99840|     0.569122|  5.70034e-06|  0.09%|    return 1 if self.is_turn_based else self.num_players\n",
      "(call)|     99840|       2.4605|  2.46444e-05|  0.40%|# /apps/open_spiel/open_spiel/python/rl_environment.py:472 is_turn_based\n",
      "   470|         0|            0|            0|  0.00%|\n",
      "   471|         0|            0|            0|  0.00%|  # New RL calls for more advanced use cases (e.g. search + RL).\n",
      "   472|    199680|     0.279591|  1.40019e-06|  0.05%|  @property\n",
      "   473|         0|            0|            0|  0.00%|  def is_turn_based(self):\n",
      "   474|    599040|      3.12633|   5.2189e-06|  0.50%|    return ((self._game.get_type().dynamics\n",
      "   475|    199680|     0.451793|  2.26259e-06|  0.07%|             == pyspiel.GameType.Dynamics.SEQUENTIAL) or\n",
      "   476|         0|            0|            0|  0.00%|            (self._game.get_type().dynamics\n",
      "   477|         0|            0|            0|  0.00%|             == pyspiel.GameType.Dynamics.MEAN_FIELD))\n",
      "   478|         0|            0|            0|  0.00%|\n",
      "   479|         0|            0|            0|  0.00%|  @property\n",
      "   480|         0|            0|            0|  0.00%|  def max_game_length(self):\n",
      "   481|         0|            0|            0|  0.00%|    return self._game.max_game_length()\n",
      "   482|         0|            0|            0|  0.00%|\n",
      "   483|         0|            0|            0|  0.00%|  @property\n",
      "   484|         0|            0|            0|  0.00%|  def is_chance_node(self):\n",
      "   485|         0|            0|            0|  0.00%|    return self._state.is_chance_node()\n",
      "   486|         0|            0|            0|  0.00%|\n",
      "   487|         0|            0|            0|  0.00%|  @property\n",
      "   488|         0|            0|            0|  0.00%|  def game(self):\n",
      "   489|         0|            0|            0|  0.00%|    return self._game\n",
      "   490|         0|            0|            0|  0.00%|\n",
      "   491|         0|            0|            0|  0.00%|  def set_state(self, new_state):\n",
      "   492|         0|            0|            0|  0.00%|    \"\"\"Updates the game state.\"\"\"\n",
      "   493|         0|            0|            0|  0.00%|    assert new_state.get_game() == self.game, (\n",
      "   494|         0|            0|            0|  0.00%|        \"State must have been created by the same game.\")\n",
      "   495|         0|            0|            0|  0.00%|    self._state = new_state\n",
      "   496|         0|            0|            0|  0.00%|\n",
      "   497|         0|            0|            0|  0.00%|  @property\n",
      "   498|         0|            0|            0|  0.00%|  def get_state(self):\n",
      "   499|         0|            0|            0|  0.00%|    return self._state\n",
      "   500|         0|            0|            0|  0.00%|\n",
      "   501|         0|            0|            0|  0.00%|  @property\n",
      "   502|         0|            0|            0|  0.00%|  def mfg_distribution(self):\n",
      "   503|         0|            0|            0|  0.00%|    return self._mfg_distribution\n",
      "   504|         0|            0|            0|  0.00%|\n",
      "   505|         0|            0|            0|  0.00%|  def update_mfg_distribution(self, mfg_distribution):\n",
      "   506|         0|            0|            0|  0.00%|    \"\"\"Updates the distribution over the states of the mean field game.\"\"\"\n",
      "   507|         0|            0|            0|  0.00%|    assert (\n",
      "   508|         0|            0|            0|  0.00%|        self._game.get_type().dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD)\n",
      "   509|         0|            0|            0|  0.00%|    self._mfg_distribution = mfg_distribution\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\n",
      "File duration: 29.3846s (4.73%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|from collections import OrderedDict, namedtuple\n",
      "     2|         0|            0|            0|  0.00%|import itertools\n",
      "     3|         0|            0|            0|  0.00%|import warnings\n",
      "     4|         0|            0|            0|  0.00%|import functools\n",
      "     5|         0|            0|            0|  0.00%|\n",
      "     6|         0|            0|            0|  0.00%|import torch\n",
      "     7|         0|            0|            0|  0.00%|from ..parameter import Parameter\n",
      "     8|         0|            0|            0|  0.00%|import torch.utils.hooks as hooks\n",
      "     9|         0|            0|            0|  0.00%|\n",
      "    10|         0|            0|            0|  0.00%|from torch import Tensor, device, dtype\n",
      "    11|         0|            0|            0|  0.00%|from typing import Union, Tuple, Any, Callable, Iterator, Set, Optional, overload, TypeVar, Mapping, Dict, List\n",
      "    12|         0|            0|            0|  0.00%|from ...utils.hooks import RemovableHandle\n",
      "    13|         0|            0|            0|  0.00%|\n",
      "    14|         0|            0|            0|  0.00%|_grad_t = Union[Tuple[Tensor, ...], Tensor]\n",
      "    15|         0|            0|            0|  0.00%|# See https://mypy.readthedocs.io/en/latest/generics.html#generic-methods-and-generic-self for the use\n",
      "    16|         0|            0|            0|  0.00%|# of `T` to annotate `self`. Many methods of `Module` return `self` and we want those return values to be\n",
      "    17|         0|            0|            0|  0.00%|# the type of the subclass, not the looser type of `Module`.\n",
      "    18|         0|            0|            0|  0.00%|T = TypeVar('T', bound='Module')\n",
      "    19|         0|            0|            0|  0.00%|\n",
      "    20|         0|            0|            0|  0.00%|class _IncompatibleKeys(namedtuple('IncompatibleKeys', ['missing_keys', 'unexpected_keys'])):\n",
      "    21|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "    22|         0|            0|            0|  0.00%|        if not self.missing_keys and not self.unexpected_keys:\n",
      "    23|         0|            0|            0|  0.00%|            return '<All keys matched successfully>'\n",
      "    24|         0|            0|            0|  0.00%|        return super(_IncompatibleKeys, self).__repr__()\n",
      "    25|         0|            0|            0|  0.00%|\n",
      "    26|         0|            0|            0|  0.00%|    __str__ = __repr__\n",
      "    27|         0|            0|            0|  0.00%|\n",
      "    28|         0|            0|            0|  0.00%|\n",
      "    29|         0|            0|            0|  0.00%|def _addindent(s_, numSpaces):\n",
      "    30|         0|            0|            0|  0.00%|    s = s_.split('\\n')\n",
      "    31|         0|            0|            0|  0.00%|    # don't do anything for single-line stuff\n",
      "    32|         0|            0|            0|  0.00%|    if len(s) == 1:\n",
      "    33|         0|            0|            0|  0.00%|        return s_\n",
      "    34|         0|            0|            0|  0.00%|    first = s.pop(0)\n",
      "    35|         0|            0|            0|  0.00%|    s = [(numSpaces * ' ') + line for line in s]\n",
      "    36|         0|            0|            0|  0.00%|    s = '\\n'.join(s)\n",
      "    37|         0|            0|            0|  0.00%|    s = first + '\\n' + s\n",
      "    38|         0|            0|            0|  0.00%|    return s\n",
      "    39|         0|            0|            0|  0.00%|\n",
      "    40|         0|            0|            0|  0.00%|\n",
      "    41|         0|            0|            0|  0.00%|r\"\"\"This tracks hooks common to all modules that are executed before/after\n",
      "    42|         0|            0|            0|  0.00%|calling forward and backward. This is global state used for debugging/profiling\n",
      "    43|         0|            0|            0|  0.00%|purposes\"\"\"\n",
      "    44|         0|            0|            0|  0.00%|_global_backward_hooks: Dict[int, Callable] = OrderedDict()\n",
      "    45|         0|            0|            0|  0.00%|_global_is_full_backward_hook: Optional[bool] = None\n",
      "    46|         0|            0|            0|  0.00%|_global_forward_pre_hooks: Dict[int, Callable] = OrderedDict()\n",
      "    47|         0|            0|            0|  0.00%|_global_forward_hooks: Dict[int, Callable] = OrderedDict()\n",
      "    48|         0|            0|            0|  0.00%|\n",
      "    49|         0|            0|            0|  0.00%|_EXTRA_STATE_KEY_SUFFIX = '_extra_state'\n",
      "    50|         0|            0|            0|  0.00%|\n",
      "    51|         0|            0|            0|  0.00%|\n",
      "    52|         0|            0|            0|  0.00%|def register_module_forward_pre_hook(hook: Callable[..., None]) -> RemovableHandle:\n",
      "    53|         0|            0|            0|  0.00%|    r\"\"\"Registers a forward pre-hook common to all modules.\n",
      "    54|         0|            0|            0|  0.00%|\n",
      "    55|         0|            0|            0|  0.00%|    .. warning ::\n",
      "    56|         0|            0|            0|  0.00%|\n",
      "    57|         0|            0|            0|  0.00%|        This adds global state to the `nn.module` module\n",
      "    58|         0|            0|            0|  0.00%|        and it is only intended for debugging/profiling purposes.\n",
      "    59|         0|            0|            0|  0.00%|\n",
      "    60|         0|            0|            0|  0.00%|    The hook will be called every time before :func:`forward` is invoked.\n",
      "    61|         0|            0|            0|  0.00%|    It should have the following signature::\n",
      "    62|         0|            0|            0|  0.00%|\n",
      "    63|         0|            0|            0|  0.00%|        hook(module, input) -> None or modified input\n",
      "    64|         0|            0|            0|  0.00%|\n",
      "    65|         0|            0|            0|  0.00%|    The input contains only the positional arguments given to the module.\n",
      "    66|         0|            0|            0|  0.00%|    Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "    67|         0|            0|            0|  0.00%|    The hook can modify the input. User can either return a tuple or a\n",
      "    68|         0|            0|            0|  0.00%|    single modified value in the hook. We will wrap the value into a tuple\n",
      "    69|         0|            0|            0|  0.00%|    if a single value is returned(unless that value is already a tuple).\n",
      "    70|         0|            0|            0|  0.00%|\n",
      "    71|         0|            0|            0|  0.00%|    This hook has precedence over the specific module hooks registered with\n",
      "    72|         0|            0|            0|  0.00%|    ``register_forward_pre_hook``.\n",
      "    73|         0|            0|            0|  0.00%|\n",
      "    74|         0|            0|            0|  0.00%|    Returns:\n",
      "    75|         0|            0|            0|  0.00%|        :class:`torch.utils.hooks.RemovableHandle`:\n",
      "    76|         0|            0|            0|  0.00%|            a handle that can be used to remove the added hook by calling\n",
      "    77|         0|            0|            0|  0.00%|            ``handle.remove()``\n",
      "    78|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    79|         0|            0|            0|  0.00%|    handle = hooks.RemovableHandle(_global_forward_pre_hooks)\n",
      "    80|         0|            0|            0|  0.00%|    _global_forward_pre_hooks[handle.id] = hook\n",
      "    81|         0|            0|            0|  0.00%|    return handle\n",
      "    82|         0|            0|            0|  0.00%|\n",
      "    83|         0|            0|            0|  0.00%|\n",
      "    84|         0|            0|            0|  0.00%|def register_module_forward_hook(hook: Callable[..., None]) -> RemovableHandle:\n",
      "    85|         0|            0|            0|  0.00%|    r\"\"\"Registers a global forward hook for all the modules\n",
      "    86|         0|            0|            0|  0.00%|\n",
      "    87|         0|            0|            0|  0.00%|    .. warning ::\n",
      "    88|         0|            0|            0|  0.00%|\n",
      "    89|         0|            0|            0|  0.00%|        This adds global state to the `nn.module` module\n",
      "    90|         0|            0|            0|  0.00%|        and it is only intended for debugging/profiling purposes.\n",
      "    91|         0|            0|            0|  0.00%|\n",
      "    92|         0|            0|            0|  0.00%|    The hook will be called every time after :func:`forward` has computed an output.\n",
      "    93|         0|            0|            0|  0.00%|    It should have the following signature::\n",
      "    94|         0|            0|            0|  0.00%|\n",
      "    95|         0|            0|            0|  0.00%|        hook(module, input, output) -> None or modified output\n",
      "    96|         0|            0|            0|  0.00%|\n",
      "    97|         0|            0|            0|  0.00%|    The input contains only the positional arguments given to the module.\n",
      "    98|         0|            0|            0|  0.00%|    Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "    99|         0|            0|            0|  0.00%|    The hook can modify the output. It can modify the input inplace but\n",
      "   100|         0|            0|            0|  0.00%|    it will not have effect on forward since this is called after\n",
      "   101|         0|            0|            0|  0.00%|    :func:`forward` is called.\n",
      "   102|         0|            0|            0|  0.00%|\n",
      "   103|         0|            0|            0|  0.00%|    Returns:\n",
      "   104|         0|            0|            0|  0.00%|        :class:`torch.utils.hooks.RemovableHandle`:\n",
      "   105|         0|            0|            0|  0.00%|            a handle that can be used to remove the added hook by calling\n",
      "   106|         0|            0|            0|  0.00%|            ``handle.remove()``\n",
      "   107|         0|            0|            0|  0.00%|\n",
      "   108|         0|            0|            0|  0.00%|    This hook will be executed before specific module hooks registered with\n",
      "   109|         0|            0|            0|  0.00%|    ``register_forward_hook``.\n",
      "   110|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   111|         0|            0|            0|  0.00%|    handle = hooks.RemovableHandle(_global_forward_hooks)\n",
      "   112|         0|            0|            0|  0.00%|    _global_forward_hooks[handle.id] = hook\n",
      "   113|         0|            0|            0|  0.00%|    return handle\n",
      "   114|         0|            0|            0|  0.00%|\n",
      "   115|         0|            0|            0|  0.00%|def register_module_backward_hook(\n",
      "   116|         0|            0|            0|  0.00%|    hook: Callable[['Module', _grad_t, _grad_t], Union[None, Tensor]]\n",
      "   117|         0|            0|            0|  0.00%|) -> RemovableHandle:\n",
      "   118|         0|            0|            0|  0.00%|    r\"\"\"Registers a backward hook common to all the modules.\n",
      "   119|         0|            0|            0|  0.00%|\n",
      "   120|         0|            0|            0|  0.00%|    This function is deprecated in favor of\n",
      "   121|         0|            0|            0|  0.00%|    :func:`torch.nn.modules.module.register_module_full_backward_hook`\n",
      "   122|         0|            0|            0|  0.00%|    and the behavior of this function will change in future versions.\n",
      "   123|         0|            0|            0|  0.00%|\n",
      "   124|         0|            0|            0|  0.00%|    Returns:\n",
      "   125|         0|            0|            0|  0.00%|        :class:`torch.utils.hooks.RemovableHandle`:\n",
      "   126|         0|            0|            0|  0.00%|            a handle that can be used to remove the added hook by calling\n",
      "   127|         0|            0|            0|  0.00%|            ``handle.remove()``\n",
      "   128|         0|            0|            0|  0.00%|\n",
      "   129|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   130|         0|            0|            0|  0.00%|    global _global_is_full_backward_hook\n",
      "   131|         0|            0|            0|  0.00%|    if _global_is_full_backward_hook is True:\n",
      "   132|         0|            0|            0|  0.00%|        raise RuntimeError(\"Cannot use both regular backward hooks and full backward hooks as a \"\n",
      "   133|         0|            0|            0|  0.00%|                           \"global Module hook. Please use only one of them.\")\n",
      "   134|         0|            0|            0|  0.00%|\n",
      "   135|         0|            0|            0|  0.00%|    _global_is_full_backward_hook = False\n",
      "   136|         0|            0|            0|  0.00%|\n",
      "   137|         0|            0|            0|  0.00%|    handle = hooks.RemovableHandle(_global_backward_hooks)\n",
      "   138|         0|            0|            0|  0.00%|    _global_backward_hooks[handle.id] = hook\n",
      "   139|         0|            0|            0|  0.00%|    return handle\n",
      "   140|         0|            0|            0|  0.00%|\n",
      "   141|         0|            0|            0|  0.00%|def register_module_full_backward_hook(\n",
      "   142|         0|            0|            0|  0.00%|    hook: Callable[['Module', _grad_t, _grad_t], Union[None, Tensor]]\n",
      "   143|         0|            0|            0|  0.00%|) -> RemovableHandle:\n",
      "   144|         0|            0|            0|  0.00%|    r\"\"\"Registers a backward hook common to all the modules.\n",
      "   145|         0|            0|            0|  0.00%|\n",
      "   146|         0|            0|            0|  0.00%|    .. warning ::\n",
      "   147|         0|            0|            0|  0.00%|        This adds global state to the `nn.module` module\n",
      "   148|         0|            0|            0|  0.00%|        and it is only intended for debugging/profiling purposes.\n",
      "   149|         0|            0|            0|  0.00%|\n",
      "   150|         0|            0|            0|  0.00%|    The hook will be called every time the gradients with respect to module\n",
      "   151|         0|            0|            0|  0.00%|    inputs are computed. The hook should have the following signature::\n",
      "   152|         0|            0|            0|  0.00%|\n",
      "   153|         0|            0|            0|  0.00%|        hook(module, grad_input, grad_output) -> Tensor or None\n",
      "   154|         0|            0|            0|  0.00%|\n",
      "   155|         0|            0|            0|  0.00%|    The :attr:`grad_input` and :attr:`grad_output` are tuples. The hook should\n",
      "   156|         0|            0|            0|  0.00%|    not modify its arguments, but it can optionally return a new gradient with\n",
      "   157|         0|            0|            0|  0.00%|    respect to the input that will be used in place of :attr:`grad_input` in\n",
      "   158|         0|            0|            0|  0.00%|    subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "   159|         0|            0|            0|  0.00%|    as positional arguments and all kwarg arguments will not appear in the hook. Entries\n",
      "   160|         0|            0|            0|  0.00%|    in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "   161|         0|            0|            0|  0.00%|    arguments.\n",
      "   162|         0|            0|            0|  0.00%|\n",
      "   163|         0|            0|            0|  0.00%|    For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "   164|         0|            0|            0|  0.00%|    receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "   165|         0|            0|            0|  0.00%|    of each Tensor returned by the Module's forward function.\n",
      "   166|         0|            0|            0|  0.00%|\n",
      "   167|         0|            0|            0|  0.00%|    Global hooks are called before hooks registered with `register_backward_hook`\n",
      "   168|         0|            0|            0|  0.00%|\n",
      "   169|         0|            0|            0|  0.00%|    Returns:\n",
      "   170|         0|            0|            0|  0.00%|        :class:`torch.utils.hooks.RemovableHandle`:\n",
      "   171|         0|            0|            0|  0.00%|            a handle that can be used to remove the added hook by calling\n",
      "   172|         0|            0|            0|  0.00%|            ``handle.remove()``\n",
      "   173|         0|            0|            0|  0.00%|\n",
      "   174|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   175|         0|            0|            0|  0.00%|    global _global_is_full_backward_hook\n",
      "   176|         0|            0|            0|  0.00%|    if _global_is_full_backward_hook is False:\n",
      "   177|         0|            0|            0|  0.00%|        raise RuntimeError(\"Cannot use both regular backward hooks and full backward hooks as a \"\n",
      "   178|         0|            0|            0|  0.00%|                           \"global Module hook. Please use only one of them.\")\n",
      "   179|         0|            0|            0|  0.00%|\n",
      "   180|         0|            0|            0|  0.00%|    _global_is_full_backward_hook = True\n",
      "   181|         0|            0|            0|  0.00%|\n",
      "   182|         0|            0|            0|  0.00%|    handle = hooks.RemovableHandle(_global_backward_hooks)\n",
      "   183|         0|            0|            0|  0.00%|    _global_backward_hooks[handle.id] = hook\n",
      "   184|         0|            0|            0|  0.00%|    return handle\n",
      "   185|         0|            0|            0|  0.00%|\n",
      "   186|         0|            0|            0|  0.00%|\n",
      "   187|         0|            0|            0|  0.00%|# Trick mypy into not applying contravariance rules to inputs by defining\n",
      "   188|         0|            0|            0|  0.00%|# forward as a value, rather than a function.  See also\n",
      "   189|         0|            0|            0|  0.00%|# https://github.com/python/mypy/issues/8795\n",
      "   190|         0|            0|            0|  0.00%|def _forward_unimplemented(self, *input: Any) -> None:\n",
      "   191|         0|            0|            0|  0.00%|    r\"\"\"Defines the computation performed at every call.\n",
      "   192|         0|            0|            0|  0.00%|\n",
      "   193|         0|            0|            0|  0.00%|    Should be overridden by all subclasses.\n",
      "   194|         0|            0|            0|  0.00%|\n",
      "   195|         0|            0|            0|  0.00%|    .. note::\n",
      "   196|         0|            0|            0|  0.00%|        Although the recipe for forward pass needs to be defined within\n",
      "   197|         0|            0|            0|  0.00%|        this function, one should call the :class:`Module` instance afterwards\n",
      "   198|         0|            0|            0|  0.00%|        instead of this since the former takes care of running the\n",
      "   199|         0|            0|            0|  0.00%|        registered hooks while the latter silently ignores them.\n",
      "   200|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   201|         0|            0|            0|  0.00%|    raise NotImplementedError(f\"Module [{type(self).__name__}] is missing the required \\\"forward\\\" function\")\n",
      "   202|         0|            0|            0|  0.00%|\n",
      "   203|         0|            0|            0|  0.00%|\n",
      "   204|         0|            0|            0|  0.00%|class Module:\n",
      "   205|         0|            0|            0|  0.00%|    r\"\"\"Base class for all neural network modules.\n",
      "   206|         0|            0|            0|  0.00%|\n",
      "   207|         0|            0|            0|  0.00%|    Your models should also subclass this class.\n",
      "   208|         0|            0|            0|  0.00%|\n",
      "   209|         0|            0|            0|  0.00%|    Modules can also contain other Modules, allowing to nest them in\n",
      "   210|         0|            0|            0|  0.00%|    a tree structure. You can assign the submodules as regular attributes::\n",
      "   211|         0|            0|            0|  0.00%|\n",
      "   212|         0|            0|            0|  0.00%|        import torch.nn as nn\n",
      "   213|         0|            0|            0|  0.00%|        import torch.nn.functional as F\n",
      "   214|         0|            0|            0|  0.00%|\n",
      "   215|         0|            0|            0|  0.00%|        class Model(nn.Module):\n",
      "   216|         0|            0|            0|  0.00%|            def __init__(self):\n",
      "   217|         0|            0|            0|  0.00%|                super().__init__()\n",
      "   218|         0|            0|            0|  0.00%|                self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "   219|         0|            0|            0|  0.00%|                self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "   220|         0|            0|            0|  0.00%|\n",
      "   221|         0|            0|            0|  0.00%|            def forward(self, x):\n",
      "   222|         0|            0|            0|  0.00%|                x = F.relu(self.conv1(x))\n",
      "   223|         0|            0|            0|  0.00%|                return F.relu(self.conv2(x))\n",
      "   224|         0|            0|            0|  0.00%|\n",
      "   225|         0|            0|            0|  0.00%|    Submodules assigned in this way will be registered, and will have their\n",
      "   226|         0|            0|            0|  0.00%|    parameters converted too when you call :meth:`to`, etc.\n",
      "   227|         0|            0|            0|  0.00%|\n",
      "   228|         0|            0|            0|  0.00%|    .. note::\n",
      "   229|         0|            0|            0|  0.00%|        As per the example above, an ``__init__()`` call to the parent class\n",
      "   230|         0|            0|            0|  0.00%|        must be made before assignment on the child.\n",
      "   231|         0|            0|            0|  0.00%|\n",
      "   232|         0|            0|            0|  0.00%|    :ivar training: Boolean represents whether this module is in training or\n",
      "   233|         0|            0|            0|  0.00%|                    evaluation mode.\n",
      "   234|         0|            0|            0|  0.00%|    :vartype training: bool\n",
      "   235|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   236|         0|            0|            0|  0.00%|\n",
      "   237|         0|            0|            0|  0.00%|    dump_patches: bool = False\n",
      "   238|         0|            0|            0|  0.00%|\n",
      "   239|         0|            0|            0|  0.00%|    _version: int = 1\n",
      "   240|         0|            0|            0|  0.00%|    r\"\"\"This allows better BC support for :meth:`load_state_dict`. In\n",
      "   241|         0|            0|            0|  0.00%|    :meth:`state_dict`, the version number will be saved as in the attribute\n",
      "   242|         0|            0|            0|  0.00%|    `_metadata` of the returned state dict, and thus pickled. `_metadata` is a\n",
      "   243|         0|            0|            0|  0.00%|    dictionary with keys that follow the naming convention of state dict. See\n",
      "   244|         0|            0|            0|  0.00%|    ``_load_from_state_dict`` on how to use this information in loading.\n",
      "   245|         0|            0|            0|  0.00%|\n",
      "   246|         0|            0|            0|  0.00%|    If new parameters/buffers are added/removed from a module, this number shall\n",
      "   247|         0|            0|            0|  0.00%|    be bumped, and the module's `_load_from_state_dict` method can compare the\n",
      "   248|         0|            0|            0|  0.00%|    version number and do appropriate changes if the state dict is from before\n",
      "   249|         0|            0|            0|  0.00%|    the change.\"\"\"\n",
      "   250|         0|            0|            0|  0.00%|\n",
      "   251|         0|            0|            0|  0.00%|    training: bool\n",
      "   252|         0|            0|            0|  0.00%|    _is_full_backward_hook: Optional[bool]\n",
      "   253|         0|            0|            0|  0.00%|\n",
      "   254|         0|            0|            0|  0.00%|    def __init__(self) -> None:\n",
      "   255|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   256|         0|            0|            0|  0.00%|        Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "   257|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   258|         0|            0|            0|  0.00%|        torch._C._log_api_usage_once(\"python.nn_module\")\n",
      "   259|         0|            0|            0|  0.00%|\n",
      "   260|         0|            0|            0|  0.00%|        self.training = True\n",
      "   261|         0|            0|            0|  0.00%|        self._parameters: Dict[str, Optional[Parameter]] = OrderedDict()\n",
      "   262|         0|            0|            0|  0.00%|        self._buffers: Dict[str, Optional[Tensor]] = OrderedDict()\n",
      "   263|         0|            0|            0|  0.00%|        self._non_persistent_buffers_set: Set[str] = set()\n",
      "   264|         0|            0|            0|  0.00%|        self._backward_hooks: Dict[int, Callable] = OrderedDict()\n",
      "   265|         0|            0|            0|  0.00%|        self._is_full_backward_hook = None\n",
      "   266|         0|            0|            0|  0.00%|        self._forward_hooks: Dict[int, Callable] = OrderedDict()\n",
      "   267|         0|            0|            0|  0.00%|        self._forward_pre_hooks: Dict[int, Callable] = OrderedDict()\n",
      "   268|         0|            0|            0|  0.00%|        self._state_dict_hooks: Dict[int, Callable] = OrderedDict()\n",
      "   269|         0|            0|            0|  0.00%|        self._load_state_dict_pre_hooks: Dict[int, Callable] = OrderedDict()\n",
      "   270|         0|            0|            0|  0.00%|        self._load_state_dict_post_hooks: Dict[int, Callable] = OrderedDict()\n",
      "   271|         0|            0|            0|  0.00%|        self._modules: Dict[str, Optional['Module']] = OrderedDict()\n",
      "   272|         0|            0|            0|  0.00%|\n",
      "   273|         0|            0|            0|  0.00%|    forward: Callable[..., Any] = _forward_unimplemented\n",
      "   274|         0|            0|            0|  0.00%|\n",
      "   275|         0|            0|            0|  0.00%|    def register_buffer(self, name: str, tensor: Optional[Tensor], persistent: bool = True) -> None:\n",
      "   276|         0|            0|            0|  0.00%|        r\"\"\"Adds a buffer to the module.\n",
      "   277|         0|            0|            0|  0.00%|\n",
      "   278|         0|            0|            0|  0.00%|        This is typically used to register a buffer that should not to be\n",
      "   279|         0|            0|            0|  0.00%|        considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "   280|         0|            0|            0|  0.00%|        is not a parameter, but is part of the module's state. Buffers, by\n",
      "   281|         0|            0|            0|  0.00%|        default, are persistent and will be saved alongside parameters. This\n",
      "   282|         0|            0|            0|  0.00%|        behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "   283|         0|            0|            0|  0.00%|        only difference between a persistent buffer and a non-persistent buffer\n",
      "   284|         0|            0|            0|  0.00%|        is that the latter will not be a part of this module's\n",
      "   285|         0|            0|            0|  0.00%|        :attr:`state_dict`.\n",
      "   286|         0|            0|            0|  0.00%|\n",
      "   287|         0|            0|            0|  0.00%|        Buffers can be accessed as attributes using given names.\n",
      "   288|         0|            0|            0|  0.00%|\n",
      "   289|         0|            0|            0|  0.00%|        Args:\n",
      "   290|         0|            0|            0|  0.00%|            name (string): name of the buffer. The buffer can be accessed\n",
      "   291|         0|            0|            0|  0.00%|                from this module using the given name\n",
      "   292|         0|            0|            0|  0.00%|            tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "   293|         0|            0|            0|  0.00%|                that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "   294|         0|            0|            0|  0.00%|                the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "   295|         0|            0|            0|  0.00%|            persistent (bool): whether the buffer is part of this module's\n",
      "   296|         0|            0|            0|  0.00%|                :attr:`state_dict`.\n",
      "   297|         0|            0|            0|  0.00%|\n",
      "   298|         0|            0|            0|  0.00%|        Example::\n",
      "   299|         0|            0|            0|  0.00%|\n",
      "   300|         0|            0|            0|  0.00%|            >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "   301|         0|            0|            0|  0.00%|\n",
      "   302|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   303|         0|            0|            0|  0.00%|        if persistent is False and isinstance(self, torch.jit.ScriptModule):\n",
      "   304|         0|            0|            0|  0.00%|            raise RuntimeError(\"ScriptModule does not support non-persistent buffers\")\n",
      "   305|         0|            0|            0|  0.00%|\n",
      "   306|         0|            0|            0|  0.00%|        if '_buffers' not in self.__dict__:\n",
      "   307|         0|            0|            0|  0.00%|            raise AttributeError(\n",
      "   308|         0|            0|            0|  0.00%|                \"cannot assign buffer before Module.__init__() call\")\n",
      "   309|         0|            0|            0|  0.00%|        elif not isinstance(name, torch._six.string_classes):\n",
      "   310|         0|            0|            0|  0.00%|            raise TypeError(\"buffer name should be a string. \"\n",
      "   311|         0|            0|            0|  0.00%|                            \"Got {}\".format(torch.typename(name)))\n",
      "   312|         0|            0|            0|  0.00%|        elif '.' in name:\n",
      "   313|         0|            0|            0|  0.00%|            raise KeyError(\"buffer name can't contain \\\".\\\"\")\n",
      "   314|         0|            0|            0|  0.00%|        elif name == '':\n",
      "   315|         0|            0|            0|  0.00%|            raise KeyError(\"buffer name can't be empty string \\\"\\\"\")\n",
      "   316|         0|            0|            0|  0.00%|        elif hasattr(self, name) and name not in self._buffers:\n",
      "   317|         0|            0|            0|  0.00%|            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
      "   318|         0|            0|            0|  0.00%|        elif tensor is not None and not isinstance(tensor, torch.Tensor):\n",
      "   319|         0|            0|            0|  0.00%|            raise TypeError(\"cannot assign '{}' object to buffer '{}' \"\n",
      "   320|         0|            0|            0|  0.00%|                            \"(torch Tensor or None required)\"\n",
      "   321|         0|            0|            0|  0.00%|                            .format(torch.typename(tensor), name))\n",
      "   322|         0|            0|            0|  0.00%|        else:\n",
      "   323|         0|            0|            0|  0.00%|            self._buffers[name] = tensor\n",
      "   324|         0|            0|            0|  0.00%|            if persistent:\n",
      "   325|         0|            0|            0|  0.00%|                self._non_persistent_buffers_set.discard(name)\n",
      "   326|         0|            0|            0|  0.00%|            else:\n",
      "   327|         0|            0|            0|  0.00%|                self._non_persistent_buffers_set.add(name)\n",
      "   328|         0|            0|            0|  0.00%|\n",
      "   329|         0|            0|            0|  0.00%|    def register_parameter(self, name: str, param: Optional[Parameter]) -> None:\n",
      "   330|         0|            0|            0|  0.00%|        r\"\"\"Adds a parameter to the module.\n",
      "   331|         0|            0|            0|  0.00%|\n",
      "   332|         0|            0|            0|  0.00%|        The parameter can be accessed as an attribute using given name.\n",
      "   333|         0|            0|            0|  0.00%|\n",
      "   334|         0|            0|            0|  0.00%|        Args:\n",
      "   335|         0|            0|            0|  0.00%|            name (string): name of the parameter. The parameter can be accessed\n",
      "   336|         0|            0|            0|  0.00%|                from this module using the given name\n",
      "   337|         0|            0|            0|  0.00%|            param (Parameter or None): parameter to be added to the module. If\n",
      "   338|         0|            0|            0|  0.00%|                ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "   339|         0|            0|            0|  0.00%|                are ignored. If ``None``, the parameter is **not** included in the\n",
      "   340|         0|            0|            0|  0.00%|                module's :attr:`state_dict`.\n",
      "   341|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   342|         0|            0|            0|  0.00%|        if '_parameters' not in self.__dict__:\n",
      "   343|         0|            0|            0|  0.00%|            raise AttributeError(\n",
      "   344|         0|            0|            0|  0.00%|                \"cannot assign parameter before Module.__init__() call\")\n",
      "   345|         0|            0|            0|  0.00%|\n",
      "   346|         0|            0|            0|  0.00%|        elif not isinstance(name, torch._six.string_classes):\n",
      "   347|         0|            0|            0|  0.00%|            raise TypeError(\"parameter name should be a string. \"\n",
      "   348|         0|            0|            0|  0.00%|                            \"Got {}\".format(torch.typename(name)))\n",
      "   349|         0|            0|            0|  0.00%|        elif '.' in name:\n",
      "   350|         0|            0|            0|  0.00%|            raise KeyError(\"parameter name can't contain \\\".\\\"\")\n",
      "   351|         0|            0|            0|  0.00%|        elif name == '':\n",
      "   352|         0|            0|            0|  0.00%|            raise KeyError(\"parameter name can't be empty string \\\"\\\"\")\n",
      "   353|         0|            0|            0|  0.00%|        elif hasattr(self, name) and name not in self._parameters:\n",
      "   354|         0|            0|            0|  0.00%|            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
      "   355|         0|            0|            0|  0.00%|\n",
      "   356|         0|            0|            0|  0.00%|        if param is None:\n",
      "   357|         0|            0|            0|  0.00%|            self._parameters[name] = None\n",
      "   358|         0|            0|            0|  0.00%|        elif not isinstance(param, Parameter):\n",
      "   359|         0|            0|            0|  0.00%|            raise TypeError(\"cannot assign '{}' object to parameter '{}' \"\n",
      "   360|         0|            0|            0|  0.00%|                            \"(torch.nn.Parameter or None required)\"\n",
      "   361|         0|            0|            0|  0.00%|                            .format(torch.typename(param), name))\n",
      "   362|         0|            0|            0|  0.00%|        elif param.grad_fn:\n",
      "   363|         0|            0|            0|  0.00%|            raise ValueError(\n",
      "   364|         0|            0|            0|  0.00%|                \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \"\n",
      "   365|         0|            0|            0|  0.00%|                \"parameters must be created explicitly. To express '{0}' \"\n",
      "   366|         0|            0|            0|  0.00%|                \"as a function of another Tensor, compute the value in \"\n",
      "   367|         0|            0|            0|  0.00%|                \"the forward() method.\".format(name))\n",
      "   368|         0|            0|            0|  0.00%|        else:\n",
      "   369|         0|            0|            0|  0.00%|            self._parameters[name] = param\n",
      "   370|         0|            0|            0|  0.00%|\n",
      "   371|         0|            0|            0|  0.00%|    def add_module(self, name: str, module: Optional['Module']) -> None:\n",
      "   372|         0|            0|            0|  0.00%|        r\"\"\"Adds a child module to the current module.\n",
      "   373|         0|            0|            0|  0.00%|\n",
      "   374|         0|            0|            0|  0.00%|        The module can be accessed as an attribute using the given name.\n",
      "   375|         0|            0|            0|  0.00%|\n",
      "   376|         0|            0|            0|  0.00%|        Args:\n",
      "   377|         0|            0|            0|  0.00%|            name (string): name of the child module. The child module can be\n",
      "   378|         0|            0|            0|  0.00%|                accessed from this module using the given name\n",
      "   379|         0|            0|            0|  0.00%|            module (Module): child module to be added to the module.\n",
      "   380|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   381|         0|            0|            0|  0.00%|        if not isinstance(module, Module) and module is not None:\n",
      "   382|         0|            0|            0|  0.00%|            raise TypeError(\"{} is not a Module subclass\".format(\n",
      "   383|         0|            0|            0|  0.00%|                torch.typename(module)))\n",
      "   384|         0|            0|            0|  0.00%|        elif not isinstance(name, torch._six.string_classes):\n",
      "   385|         0|            0|            0|  0.00%|            raise TypeError(\"module name should be a string. Got {}\".format(\n",
      "   386|         0|            0|            0|  0.00%|                torch.typename(name)))\n",
      "   387|         0|            0|            0|  0.00%|        elif hasattr(self, name) and name not in self._modules:\n",
      "   388|         0|            0|            0|  0.00%|            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
      "   389|         0|            0|            0|  0.00%|        elif '.' in name:\n",
      "   390|         0|            0|            0|  0.00%|            raise KeyError(\"module name can't contain \\\".\\\", got: {}\".format(name))\n",
      "   391|         0|            0|            0|  0.00%|        elif name == '':\n",
      "   392|         0|            0|            0|  0.00%|            raise KeyError(\"module name can't be empty string \\\"\\\"\")\n",
      "   393|         0|            0|            0|  0.00%|        self._modules[name] = module\n",
      "   394|         0|            0|            0|  0.00%|\n",
      "   395|         0|            0|            0|  0.00%|    def register_module(self, name: str, module: Optional['Module']) -> None:\n",
      "   396|         0|            0|            0|  0.00%|        r\"\"\"Alias for :func:`add_module`.\"\"\"\n",
      "   397|         0|            0|            0|  0.00%|        self.add_module(name, module)\n",
      "   398|         0|            0|            0|  0.00%|\n",
      "   399|         0|            0|            0|  0.00%|    def get_submodule(self, target: str) -> \"Module\":\n",
      "   400|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   401|         0|            0|            0|  0.00%|        Returns the submodule given by ``target`` if it exists,\n",
      "   402|         0|            0|            0|  0.00%|        otherwise throws an error.\n",
      "   403|         0|            0|            0|  0.00%|\n",
      "   404|         0|            0|            0|  0.00%|        For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "   405|         0|            0|            0|  0.00%|        looks like this:\n",
      "   406|         0|            0|            0|  0.00%|\n",
      "   407|         0|            0|            0|  0.00%|        .. code-block:: text\n",
      "   408|         0|            0|            0|  0.00%|\n",
      "   409|         0|            0|            0|  0.00%|            A(\n",
      "   410|         0|            0|            0|  0.00%|                (net_b): Module(\n",
      "   411|         0|            0|            0|  0.00%|                    (net_c): Module(\n",
      "   412|         0|            0|            0|  0.00%|                        (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "   413|         0|            0|            0|  0.00%|                    )\n",
      "   414|         0|            0|            0|  0.00%|                    (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "   415|         0|            0|            0|  0.00%|                )\n",
      "   416|         0|            0|            0|  0.00%|            )\n",
      "   417|         0|            0|            0|  0.00%|\n",
      "   418|         0|            0|            0|  0.00%|        (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "   419|         0|            0|            0|  0.00%|        submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "   420|         0|            0|            0|  0.00%|        and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "   421|         0|            0|            0|  0.00%|\n",
      "   422|         0|            0|            0|  0.00%|        To check whether or not we have the ``linear`` submodule, we\n",
      "   423|         0|            0|            0|  0.00%|        would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "   424|         0|            0|            0|  0.00%|        we have the ``conv`` submodule, we would call\n",
      "   425|         0|            0|            0|  0.00%|        ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "   426|         0|            0|            0|  0.00%|\n",
      "   427|         0|            0|            0|  0.00%|        The runtime of ``get_submodule`` is bounded by the degree\n",
      "   428|         0|            0|            0|  0.00%|        of module nesting in ``target``. A query against\n",
      "   429|         0|            0|            0|  0.00%|        ``named_modules`` achieves the same result, but it is O(N) in\n",
      "   430|         0|            0|            0|  0.00%|        the number of transitive modules. So, for a simple check to see\n",
      "   431|         0|            0|            0|  0.00%|        if some submodule exists, ``get_submodule`` should always be\n",
      "   432|         0|            0|            0|  0.00%|        used.\n",
      "   433|         0|            0|            0|  0.00%|\n",
      "   434|         0|            0|            0|  0.00%|        Args:\n",
      "   435|         0|            0|            0|  0.00%|            target: The fully-qualified string name of the submodule\n",
      "   436|         0|            0|            0|  0.00%|                to look for. (See above example for how to specify a\n",
      "   437|         0|            0|            0|  0.00%|                fully-qualified string.)\n",
      "   438|         0|            0|            0|  0.00%|\n",
      "   439|         0|            0|            0|  0.00%|        Returns:\n",
      "   440|         0|            0|            0|  0.00%|            torch.nn.Module: The submodule referenced by ``target``\n",
      "   441|         0|            0|            0|  0.00%|\n",
      "   442|         0|            0|            0|  0.00%|        Raises:\n",
      "   443|         0|            0|            0|  0.00%|            AttributeError: If the target string references an invalid\n",
      "   444|         0|            0|            0|  0.00%|                path or resolves to something that is not an\n",
      "   445|         0|            0|            0|  0.00%|                ``nn.Module``\n",
      "   446|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   447|         0|            0|            0|  0.00%|        if target == \"\":\n",
      "   448|         0|            0|            0|  0.00%|            return self\n",
      "   449|         0|            0|            0|  0.00%|\n",
      "   450|         0|            0|            0|  0.00%|        atoms: List[str] = target.split(\".\")\n",
      "   451|         0|            0|            0|  0.00%|        mod: torch.nn.Module = self\n",
      "   452|         0|            0|            0|  0.00%|\n",
      "   453|         0|            0|            0|  0.00%|        for item in atoms:\n",
      "   454|         0|            0|            0|  0.00%|\n",
      "   455|         0|            0|            0|  0.00%|            if not hasattr(mod, item):\n",
      "   456|         0|            0|            0|  0.00%|                raise AttributeError(mod._get_name() + \" has no \"\n",
      "   457|         0|            0|            0|  0.00%|                                     \"attribute `\" + item + \"`\")\n",
      "   458|         0|            0|            0|  0.00%|\n",
      "   459|         0|            0|            0|  0.00%|            mod = getattr(mod, item)\n",
      "   460|         0|            0|            0|  0.00%|\n",
      "   461|         0|            0|            0|  0.00%|            if not isinstance(mod, torch.nn.Module):\n",
      "   462|         0|            0|            0|  0.00%|                raise AttributeError(\"`\" + item + \"` is not \"\n",
      "   463|         0|            0|            0|  0.00%|                                     \"an nn.Module\")\n",
      "   464|         0|            0|            0|  0.00%|\n",
      "   465|         0|            0|            0|  0.00%|        return mod\n",
      "   466|         0|            0|            0|  0.00%|\n",
      "   467|         0|            0|            0|  0.00%|    def get_parameter(self, target: str) -> \"Parameter\":\n",
      "   468|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   469|         0|            0|            0|  0.00%|        Returns the parameter given by ``target`` if it exists,\n",
      "   470|         0|            0|            0|  0.00%|        otherwise throws an error.\n",
      "   471|         0|            0|            0|  0.00%|\n",
      "   472|         0|            0|            0|  0.00%|        See the docstring for ``get_submodule`` for a more detailed\n",
      "   473|         0|            0|            0|  0.00%|        explanation of this method's functionality as well as how to\n",
      "   474|         0|            0|            0|  0.00%|        correctly specify ``target``.\n",
      "   475|         0|            0|            0|  0.00%|\n",
      "   476|         0|            0|            0|  0.00%|        Args:\n",
      "   477|         0|            0|            0|  0.00%|            target: The fully-qualified string name of the Parameter\n",
      "   478|         0|            0|            0|  0.00%|                to look for. (See ``get_submodule`` for how to specify a\n",
      "   479|         0|            0|            0|  0.00%|                fully-qualified string.)\n",
      "   480|         0|            0|            0|  0.00%|\n",
      "   481|         0|            0|            0|  0.00%|        Returns:\n",
      "   482|         0|            0|            0|  0.00%|            torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "   483|         0|            0|            0|  0.00%|\n",
      "   484|         0|            0|            0|  0.00%|        Raises:\n",
      "   485|         0|            0|            0|  0.00%|            AttributeError: If the target string references an invalid\n",
      "   486|         0|            0|            0|  0.00%|                path or resolves to something that is not an\n",
      "   487|         0|            0|            0|  0.00%|                ``nn.Parameter``\n",
      "   488|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   489|         0|            0|            0|  0.00%|        module_path, _, param_name = target.rpartition(\".\")\n",
      "   490|         0|            0|            0|  0.00%|\n",
      "   491|         0|            0|            0|  0.00%|        mod: torch.nn.Module = self.get_submodule(module_path)\n",
      "   492|         0|            0|            0|  0.00%|\n",
      "   493|         0|            0|            0|  0.00%|        if not hasattr(mod, param_name):\n",
      "   494|         0|            0|            0|  0.00%|            raise AttributeError(mod._get_name() + \" has no attribute `\"\n",
      "   495|         0|            0|            0|  0.00%|                                 + param_name + \"`\")\n",
      "   496|         0|            0|            0|  0.00%|\n",
      "   497|         0|            0|            0|  0.00%|        param: torch.nn.Parameter = getattr(mod, param_name)\n",
      "   498|         0|            0|            0|  0.00%|\n",
      "   499|         0|            0|            0|  0.00%|        if not isinstance(param, torch.nn.Parameter):\n",
      "   500|         0|            0|            0|  0.00%|            raise AttributeError(\"`\" + param_name + \"` is not an \"\n",
      "   501|         0|            0|            0|  0.00%|                                 \"nn.Parameter\")\n",
      "   502|         0|            0|            0|  0.00%|\n",
      "   503|         0|            0|            0|  0.00%|        return param\n",
      "   504|         0|            0|            0|  0.00%|\n",
      "   505|         0|            0|            0|  0.00%|    def get_buffer(self, target: str) -> \"Tensor\":\n",
      "   506|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   507|         0|            0|            0|  0.00%|        Returns the buffer given by ``target`` if it exists,\n",
      "   508|         0|            0|            0|  0.00%|        otherwise throws an error.\n",
      "   509|         0|            0|            0|  0.00%|\n",
      "   510|         0|            0|            0|  0.00%|        See the docstring for ``get_submodule`` for a more detailed\n",
      "   511|         0|            0|            0|  0.00%|        explanation of this method's functionality as well as how to\n",
      "   512|         0|            0|            0|  0.00%|        correctly specify ``target``.\n",
      "   513|         0|            0|            0|  0.00%|\n",
      "   514|         0|            0|            0|  0.00%|        Args:\n",
      "   515|         0|            0|            0|  0.00%|            target: The fully-qualified string name of the buffer\n",
      "   516|         0|            0|            0|  0.00%|                to look for. (See ``get_submodule`` for how to specify a\n",
      "   517|         0|            0|            0|  0.00%|                fully-qualified string.)\n",
      "   518|         0|            0|            0|  0.00%|\n",
      "   519|         0|            0|            0|  0.00%|        Returns:\n",
      "   520|         0|            0|            0|  0.00%|            torch.Tensor: The buffer referenced by ``target``\n",
      "   521|         0|            0|            0|  0.00%|\n",
      "   522|         0|            0|            0|  0.00%|        Raises:\n",
      "   523|         0|            0|            0|  0.00%|            AttributeError: If the target string references an invalid\n",
      "   524|         0|            0|            0|  0.00%|                path or resolves to something that is not a\n",
      "   525|         0|            0|            0|  0.00%|                buffer\n",
      "   526|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   527|         0|            0|            0|  0.00%|        module_path, _, buffer_name = target.rpartition(\".\")\n",
      "   528|         0|            0|            0|  0.00%|\n",
      "   529|         0|            0|            0|  0.00%|        mod: torch.nn.Module = self.get_submodule(module_path)\n",
      "   530|         0|            0|            0|  0.00%|\n",
      "   531|         0|            0|            0|  0.00%|        if not hasattr(mod, buffer_name):\n",
      "   532|         0|            0|            0|  0.00%|            raise AttributeError(mod._get_name() + \" has no attribute `\"\n",
      "   533|         0|            0|            0|  0.00%|                                 + buffer_name + \"`\")\n",
      "   534|         0|            0|            0|  0.00%|\n",
      "   535|         0|            0|            0|  0.00%|        buffer: torch.Tensor = getattr(mod, buffer_name)\n",
      "   536|         0|            0|            0|  0.00%|\n",
      "   537|         0|            0|            0|  0.00%|        if buffer_name not in mod._buffers:\n",
      "   538|         0|            0|            0|  0.00%|            raise AttributeError(\"`\" + buffer_name + \"` is not a buffer\")\n",
      "   539|         0|            0|            0|  0.00%|\n",
      "   540|         0|            0|            0|  0.00%|        return buffer\n",
      "   541|         0|            0|            0|  0.00%|\n",
      "   542|         0|            0|            0|  0.00%|    def get_extra_state(self) -> Any:\n",
      "   543|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   544|         0|            0|            0|  0.00%|        Returns any extra state to include in the module's state_dict.\n",
      "   545|         0|            0|            0|  0.00%|        Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "   546|         0|            0|            0|  0.00%|        if you need to store extra state. This function is called when building the\n",
      "   547|         0|            0|            0|  0.00%|        module's `state_dict()`.\n",
      "   548|         0|            0|            0|  0.00%|\n",
      "   549|         0|            0|            0|  0.00%|        Note that extra state should be pickleable to ensure working serialization\n",
      "   550|         0|            0|            0|  0.00%|        of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "   551|         0|            0|            0|  0.00%|        for serializing Tensors; other objects may break backwards compatibility if\n",
      "   552|         0|            0|            0|  0.00%|        their serialized pickled form changes.\n",
      "   553|         0|            0|            0|  0.00%|\n",
      "   554|         0|            0|            0|  0.00%|        Returns:\n",
      "   555|         0|            0|            0|  0.00%|            object: Any extra state to store in the module's state_dict\n",
      "   556|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   557|         0|            0|            0|  0.00%|        raise RuntimeError(\n",
      "   558|         0|            0|            0|  0.00%|            \"Reached a code path in Module.get_extra_state() that should never be called. \"\n",
      "   559|         0|            0|            0|  0.00%|            \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\n",
      "   560|         0|            0|            0|  0.00%|            \"to report this bug.\")\n",
      "   561|         0|            0|            0|  0.00%|\n",
      "   562|         0|            0|            0|  0.00%|    def set_extra_state(self, state: Any):\n",
      "   563|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   564|         0|            0|            0|  0.00%|        This function is called from :func:`load_state_dict` to handle any extra state\n",
      "   565|         0|            0|            0|  0.00%|        found within the `state_dict`. Implement this function and a corresponding\n",
      "   566|         0|            0|            0|  0.00%|        :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "   567|         0|            0|            0|  0.00%|        `state_dict`.\n",
      "   568|         0|            0|            0|  0.00%|\n",
      "   569|         0|            0|            0|  0.00%|        Args:\n",
      "   570|         0|            0|            0|  0.00%|            state (dict): Extra state from the `state_dict`\n",
      "   571|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   572|         0|            0|            0|  0.00%|        raise RuntimeError(\n",
      "   573|         0|            0|            0|  0.00%|            \"Reached a code path in Module.set_extra_state() that should never be called. \"\n",
      "   574|         0|            0|            0|  0.00%|            \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\n",
      "   575|         0|            0|            0|  0.00%|            \"to report this bug.\")\n",
      "   576|         0|            0|            0|  0.00%|\n",
      "   577|         0|            0|            0|  0.00%|    def _apply(self, fn):\n",
      "   578|         0|            0|            0|  0.00%|        for module in self.children():\n",
      "   579|         0|            0|            0|  0.00%|            module._apply(fn)\n",
      "   580|         0|            0|            0|  0.00%|\n",
      "   581|         0|            0|            0|  0.00%|        def compute_should_use_set_data(tensor, tensor_applied):\n",
      "   582|         0|            0|            0|  0.00%|            if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "   583|         0|            0|            0|  0.00%|                # If the new tensor has compatible tensor type as the existing tensor,\n",
      "   584|         0|            0|            0|  0.00%|                # the current behavior is to change the tensor in-place using `.data =`,\n",
      "   585|         0|            0|            0|  0.00%|                # and the future behavior is to overwrite the existing tensor. However,\n",
      "   586|         0|            0|            0|  0.00%|                # changing the current behavior is a BC-breaking change, and we want it\n",
      "   587|         0|            0|            0|  0.00%|                # to happen in future releases. So for now we introduce the\n",
      "   588|         0|            0|            0|  0.00%|                # `torch.__future__.get_overwrite_module_params_on_conversion()`\n",
      "   589|         0|            0|            0|  0.00%|                # global flag to let the user control whether they want the future\n",
      "   590|         0|            0|            0|  0.00%|                # behavior of overwriting the existing tensor or not.\n",
      "   591|         0|            0|            0|  0.00%|                return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      "   592|         0|            0|            0|  0.00%|            else:\n",
      "   593|         0|            0|            0|  0.00%|                return False\n",
      "   594|         0|            0|            0|  0.00%|\n",
      "   595|         0|            0|            0|  0.00%|        for key, param in self._parameters.items():\n",
      "   596|         0|            0|            0|  0.00%|            if param is None:\n",
      "   597|         0|            0|            0|  0.00%|                continue\n",
      "   598|         0|            0|            0|  0.00%|            # Tensors stored in modules are graph leaves, and we don't want to\n",
      "   599|         0|            0|            0|  0.00%|            # track autograd history of `param_applied`, so we have to use\n",
      "   600|         0|            0|            0|  0.00%|            # `with torch.no_grad():`\n",
      "   601|         0|            0|            0|  0.00%|            with torch.no_grad():\n",
      "   602|         0|            0|            0|  0.00%|                param_applied = fn(param)\n",
      "   603|         0|            0|            0|  0.00%|            should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      "   604|         0|            0|            0|  0.00%|            if should_use_set_data:\n",
      "   605|         0|            0|            0|  0.00%|                param.data = param_applied\n",
      "   606|         0|            0|            0|  0.00%|                out_param = param\n",
      "   607|         0|            0|            0|  0.00%|            else:\n",
      "   608|         0|            0|            0|  0.00%|                assert isinstance(param, Parameter)\n",
      "   609|         0|            0|            0|  0.00%|                assert param.is_leaf\n",
      "   610|         0|            0|            0|  0.00%|                out_param = Parameter(param_applied, param.requires_grad)\n",
      "   611|         0|            0|            0|  0.00%|                self._parameters[key] = out_param\n",
      "   612|         0|            0|            0|  0.00%|\n",
      "   613|         0|            0|            0|  0.00%|            if param.grad is not None:\n",
      "   614|         0|            0|            0|  0.00%|                with torch.no_grad():\n",
      "   615|         0|            0|            0|  0.00%|                    grad_applied = fn(param.grad)\n",
      "   616|         0|            0|            0|  0.00%|                should_use_set_data = compute_should_use_set_data(param.grad, grad_applied)\n",
      "   617|         0|            0|            0|  0.00%|                if should_use_set_data:\n",
      "   618|         0|            0|            0|  0.00%|                    out_param.grad.data = grad_applied\n",
      "   619|         0|            0|            0|  0.00%|                else:\n",
      "   620|         0|            0|            0|  0.00%|                    assert param.grad.is_leaf\n",
      "   621|         0|            0|            0|  0.00%|                    out_param.grad = grad_applied.requires_grad_(param.grad.requires_grad)\n",
      "   622|         0|            0|            0|  0.00%|\n",
      "   623|         0|            0|            0|  0.00%|        for key, buf in self._buffers.items():\n",
      "   624|         0|            0|            0|  0.00%|            if buf is not None:\n",
      "   625|         0|            0|            0|  0.00%|                self._buffers[key] = fn(buf)\n",
      "   626|         0|            0|            0|  0.00%|\n",
      "   627|         0|            0|            0|  0.00%|        return self\n",
      "   628|         0|            0|            0|  0.00%|\n",
      "   629|         0|            0|            0|  0.00%|    def apply(self: T, fn: Callable[['Module'], None]) -> T:\n",
      "   630|         0|            0|            0|  0.00%|        r\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "   631|         0|            0|            0|  0.00%|        as well as self. Typical use includes initializing the parameters of a model\n",
      "   632|         0|            0|            0|  0.00%|        (see also :ref:`nn-init-doc`).\n",
      "   633|         0|            0|            0|  0.00%|\n",
      "   634|         0|            0|            0|  0.00%|        Args:\n",
      "   635|         0|            0|            0|  0.00%|            fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "   636|         0|            0|            0|  0.00%|\n",
      "   637|         0|            0|            0|  0.00%|        Returns:\n",
      "   638|         0|            0|            0|  0.00%|            Module: self\n",
      "   639|         0|            0|            0|  0.00%|\n",
      "   640|         0|            0|            0|  0.00%|        Example::\n",
      "   641|         0|            0|            0|  0.00%|\n",
      "   642|         0|            0|            0|  0.00%|            >>> @torch.no_grad()\n",
      "   643|         0|            0|            0|  0.00%|            >>> def init_weights(m):\n",
      "   644|         0|            0|            0|  0.00%|            >>>     print(m)\n",
      "   645|         0|            0|            0|  0.00%|            >>>     if type(m) == nn.Linear:\n",
      "   646|         0|            0|            0|  0.00%|            >>>         m.weight.fill_(1.0)\n",
      "   647|         0|            0|            0|  0.00%|            >>>         print(m.weight)\n",
      "   648|         0|            0|            0|  0.00%|            >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "   649|         0|            0|            0|  0.00%|            >>> net.apply(init_weights)\n",
      "   650|         0|            0|            0|  0.00%|            Linear(in_features=2, out_features=2, bias=True)\n",
      "   651|         0|            0|            0|  0.00%|            Parameter containing:\n",
      "   652|         0|            0|            0|  0.00%|            tensor([[ 1.,  1.],\n",
      "   653|         0|            0|            0|  0.00%|                    [ 1.,  1.]])\n",
      "   654|         0|            0|            0|  0.00%|            Linear(in_features=2, out_features=2, bias=True)\n",
      "   655|         0|            0|            0|  0.00%|            Parameter containing:\n",
      "   656|         0|            0|            0|  0.00%|            tensor([[ 1.,  1.],\n",
      "   657|         0|            0|            0|  0.00%|                    [ 1.,  1.]])\n",
      "   658|         0|            0|            0|  0.00%|            Sequential(\n",
      "   659|         0|            0|            0|  0.00%|              (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "   660|         0|            0|            0|  0.00%|              (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "   661|         0|            0|            0|  0.00%|            )\n",
      "   662|         0|            0|            0|  0.00%|            Sequential(\n",
      "   663|         0|            0|            0|  0.00%|              (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "   664|         0|            0|            0|  0.00%|              (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "   665|         0|            0|            0|  0.00%|            )\n",
      "   666|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   667|         0|            0|            0|  0.00%|        for module in self.children():\n",
      "   668|         0|            0|            0|  0.00%|            module.apply(fn)\n",
      "   669|         0|            0|            0|  0.00%|        fn(self)\n",
      "   670|         0|            0|            0|  0.00%|        return self\n",
      "   671|         0|            0|            0|  0.00%|\n",
      "   672|         0|            0|            0|  0.00%|    def cuda(self: T, device: Optional[Union[int, device]] = None) -> T:\n",
      "   673|         0|            0|            0|  0.00%|        r\"\"\"Moves all model parameters and buffers to the GPU.\n",
      "   674|         0|            0|            0|  0.00%|\n",
      "   675|         0|            0|            0|  0.00%|        This also makes associated parameters and buffers different objects. So\n",
      "   676|         0|            0|            0|  0.00%|        it should be called before constructing optimizer if the module will\n",
      "   677|         0|            0|            0|  0.00%|        live on GPU while being optimized.\n",
      "   678|         0|            0|            0|  0.00%|\n",
      "   679|         0|            0|            0|  0.00%|        .. note::\n",
      "   680|         0|            0|            0|  0.00%|            This method modifies the module in-place.\n",
      "   681|         0|            0|            0|  0.00%|\n",
      "   682|         0|            0|            0|  0.00%|        Args:\n",
      "   683|         0|            0|            0|  0.00%|            device (int, optional): if specified, all parameters will be\n",
      "   684|         0|            0|            0|  0.00%|                copied to that device\n",
      "   685|         0|            0|            0|  0.00%|\n",
      "   686|         0|            0|            0|  0.00%|        Returns:\n",
      "   687|         0|            0|            0|  0.00%|            Module: self\n",
      "   688|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   689|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.cuda(device))\n",
      "   690|         0|            0|            0|  0.00%|\n",
      "   691|         0|            0|            0|  0.00%|    def ipu(self: T, device: Optional[Union[int, device]] = None) -> T:\n",
      "   692|         0|            0|            0|  0.00%|        r\"\"\"Moves all model parameters and buffers to the IPU.\n",
      "   693|         0|            0|            0|  0.00%|\n",
      "   694|         0|            0|            0|  0.00%|        This also makes associated parameters and buffers different objects. So\n",
      "   695|         0|            0|            0|  0.00%|        it should be called before constructing optimizer if the module will\n",
      "   696|         0|            0|            0|  0.00%|        live on IPU while being optimized.\n",
      "   697|         0|            0|            0|  0.00%|\n",
      "   698|         0|            0|            0|  0.00%|        .. note::\n",
      "   699|         0|            0|            0|  0.00%|            This method modifies the module in-place.\n",
      "   700|         0|            0|            0|  0.00%|\n",
      "   701|         0|            0|            0|  0.00%|        Arguments:\n",
      "   702|         0|            0|            0|  0.00%|            device (int, optional): if specified, all parameters will be\n",
      "   703|         0|            0|            0|  0.00%|                copied to that device\n",
      "   704|         0|            0|            0|  0.00%|\n",
      "   705|         0|            0|            0|  0.00%|        Returns:\n",
      "   706|         0|            0|            0|  0.00%|            Module: self\n",
      "   707|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   708|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.ipu(device))\n",
      "   709|         0|            0|            0|  0.00%|\n",
      "   710|         0|            0|            0|  0.00%|    def xpu(self: T, device: Optional[Union[int, device]] = None) -> T:\n",
      "   711|         0|            0|            0|  0.00%|        r\"\"\"Moves all model parameters and buffers to the XPU.\n",
      "   712|         0|            0|            0|  0.00%|\n",
      "   713|         0|            0|            0|  0.00%|        This also makes associated parameters and buffers different objects. So\n",
      "   714|         0|            0|            0|  0.00%|        it should be called before constructing optimizer if the module will\n",
      "   715|         0|            0|            0|  0.00%|        live on XPU while being optimized.\n",
      "   716|         0|            0|            0|  0.00%|\n",
      "   717|         0|            0|            0|  0.00%|        .. note::\n",
      "   718|         0|            0|            0|  0.00%|            This method modifies the module in-place.\n",
      "   719|         0|            0|            0|  0.00%|\n",
      "   720|         0|            0|            0|  0.00%|        Arguments:\n",
      "   721|         0|            0|            0|  0.00%|            device (int, optional): if specified, all parameters will be\n",
      "   722|         0|            0|            0|  0.00%|                copied to that device\n",
      "   723|         0|            0|            0|  0.00%|\n",
      "   724|         0|            0|            0|  0.00%|        Returns:\n",
      "   725|         0|            0|            0|  0.00%|            Module: self\n",
      "   726|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   727|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.xpu(device))\n",
      "   728|         0|            0|            0|  0.00%|\n",
      "   729|         0|            0|            0|  0.00%|    def cpu(self: T) -> T:\n",
      "   730|         0|            0|            0|  0.00%|        r\"\"\"Moves all model parameters and buffers to the CPU.\n",
      "   731|         0|            0|            0|  0.00%|\n",
      "   732|         0|            0|            0|  0.00%|        .. note::\n",
      "   733|         0|            0|            0|  0.00%|            This method modifies the module in-place.\n",
      "   734|         0|            0|            0|  0.00%|\n",
      "   735|         0|            0|            0|  0.00%|        Returns:\n",
      "   736|         0|            0|            0|  0.00%|            Module: self\n",
      "   737|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   738|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.cpu())\n",
      "   739|         0|            0|            0|  0.00%|\n",
      "   740|         0|            0|            0|  0.00%|    def type(self: T, dst_type: Union[dtype, str]) -> T:\n",
      "   741|         0|            0|            0|  0.00%|        r\"\"\"Casts all parameters and buffers to :attr:`dst_type`.\n",
      "   742|         0|            0|            0|  0.00%|\n",
      "   743|         0|            0|            0|  0.00%|        .. note::\n",
      "   744|         0|            0|            0|  0.00%|            This method modifies the module in-place.\n",
      "   745|         0|            0|            0|  0.00%|\n",
      "   746|         0|            0|            0|  0.00%|        Args:\n",
      "   747|         0|            0|            0|  0.00%|            dst_type (type or string): the desired type\n",
      "   748|         0|            0|            0|  0.00%|\n",
      "   749|         0|            0|            0|  0.00%|        Returns:\n",
      "   750|         0|            0|            0|  0.00%|            Module: self\n",
      "   751|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   752|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.type(dst_type))\n",
      "   753|         0|            0|            0|  0.00%|\n",
      "   754|         0|            0|            0|  0.00%|    def float(self: T) -> T:\n",
      "   755|         0|            0|            0|  0.00%|        r\"\"\"Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "   756|         0|            0|            0|  0.00%|\n",
      "   757|         0|            0|            0|  0.00%|        .. note::\n",
      "   758|         0|            0|            0|  0.00%|            This method modifies the module in-place.\n",
      "   759|         0|            0|            0|  0.00%|\n",
      "   760|         0|            0|            0|  0.00%|        Returns:\n",
      "   761|         0|            0|            0|  0.00%|            Module: self\n",
      "   762|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   763|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.float() if t.is_floating_point() else t)\n",
      "   764|         0|            0|            0|  0.00%|\n",
      "   765|         0|            0|            0|  0.00%|    def double(self: T) -> T:\n",
      "   766|         0|            0|            0|  0.00%|        r\"\"\"Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "   767|         0|            0|            0|  0.00%|\n",
      "   768|         0|            0|            0|  0.00%|        .. note::\n",
      "   769|         0|            0|            0|  0.00%|            This method modifies the module in-place.\n",
      "   770|         0|            0|            0|  0.00%|\n",
      "   771|         0|            0|            0|  0.00%|        Returns:\n",
      "   772|         0|            0|            0|  0.00%|            Module: self\n",
      "   773|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   774|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.double() if t.is_floating_point() else t)\n",
      "   775|         0|            0|            0|  0.00%|\n",
      "   776|         0|            0|            0|  0.00%|    def half(self: T) -> T:\n",
      "   777|         0|            0|            0|  0.00%|        r\"\"\"Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "   778|         0|            0|            0|  0.00%|\n",
      "   779|         0|            0|            0|  0.00%|        .. note::\n",
      "   780|         0|            0|            0|  0.00%|            This method modifies the module in-place.\n",
      "   781|         0|            0|            0|  0.00%|\n",
      "   782|         0|            0|            0|  0.00%|        Returns:\n",
      "   783|         0|            0|            0|  0.00%|            Module: self\n",
      "   784|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   785|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.half() if t.is_floating_point() else t)\n",
      "   786|         0|            0|            0|  0.00%|\n",
      "   787|         0|            0|            0|  0.00%|    def bfloat16(self: T) -> T:\n",
      "   788|         0|            0|            0|  0.00%|        r\"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "   789|         0|            0|            0|  0.00%|\n",
      "   790|         0|            0|            0|  0.00%|        .. note::\n",
      "   791|         0|            0|            0|  0.00%|            This method modifies the module in-place.\n",
      "   792|         0|            0|            0|  0.00%|\n",
      "   793|         0|            0|            0|  0.00%|        Returns:\n",
      "   794|         0|            0|            0|  0.00%|            Module: self\n",
      "   795|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   796|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.bfloat16() if t.is_floating_point() else t)\n",
      "   797|         0|            0|            0|  0.00%|\n",
      "   798|         0|            0|            0|  0.00%|    def to_empty(self: T, *, device: Union[str, device]) -> T:\n",
      "   799|         0|            0|            0|  0.00%|        r\"\"\"Moves the parameters and buffers to the specified device without copying storage.\n",
      "   800|         0|            0|            0|  0.00%|\n",
      "   801|         0|            0|            0|  0.00%|        Args:\n",
      "   802|         0|            0|            0|  0.00%|            device (:class:`torch.device`): The desired device of the parameters\n",
      "   803|         0|            0|            0|  0.00%|                and buffers in this module.\n",
      "   804|         0|            0|            0|  0.00%|\n",
      "   805|         0|            0|            0|  0.00%|        Returns:\n",
      "   806|         0|            0|            0|  0.00%|            Module: self\n",
      "   807|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   808|         0|            0|            0|  0.00%|        return self._apply(lambda t: torch.empty_like(t, device=device))\n",
      "   809|         0|            0|            0|  0.00%|\n",
      "   810|         0|            0|            0|  0.00%|    @overload\n",
      "   811|         0|            0|            0|  0.00%|    def to(self: T, device: Optional[Union[int, device]] = ..., dtype: Optional[Union[dtype, str]] = ...,\n",
      "   812|         0|            0|            0|  0.00%|           non_blocking: bool = ...) -> T:\n",
      "   813|         0|            0|            0|  0.00%|        ...\n",
      "   814|         0|            0|            0|  0.00%|\n",
      "   815|         0|            0|            0|  0.00%|    @overload\n",
      "   816|         0|            0|            0|  0.00%|    def to(self: T, dtype: Union[dtype, str], non_blocking: bool = ...) -> T:\n",
      "   817|         0|            0|            0|  0.00%|        ...\n",
      "   818|         0|            0|            0|  0.00%|\n",
      "   819|         0|            0|            0|  0.00%|    @overload\n",
      "   820|         0|            0|            0|  0.00%|    def to(self: T, tensor: Tensor, non_blocking: bool = ...) -> T:\n",
      "   821|         0|            0|            0|  0.00%|        ...\n",
      "   822|         0|            0|            0|  0.00%|\n",
      "   823|         0|            0|            0|  0.00%|    def to(self, *args, **kwargs):\n",
      "   824|         0|            0|            0|  0.00%|        r\"\"\"Moves and/or casts the parameters and buffers.\n",
      "   825|         0|            0|            0|  0.00%|\n",
      "   826|         0|            0|            0|  0.00%|        This can be called as\n",
      "   827|         0|            0|            0|  0.00%|\n",
      "   828|         0|            0|            0|  0.00%|        .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "   829|         0|            0|            0|  0.00%|           :noindex:\n",
      "   830|         0|            0|            0|  0.00%|\n",
      "   831|         0|            0|            0|  0.00%|        .. function:: to(dtype, non_blocking=False)\n",
      "   832|         0|            0|            0|  0.00%|           :noindex:\n",
      "   833|         0|            0|            0|  0.00%|\n",
      "   834|         0|            0|            0|  0.00%|        .. function:: to(tensor, non_blocking=False)\n",
      "   835|         0|            0|            0|  0.00%|           :noindex:\n",
      "   836|         0|            0|            0|  0.00%|\n",
      "   837|         0|            0|            0|  0.00%|        .. function:: to(memory_format=torch.channels_last)\n",
      "   838|         0|            0|            0|  0.00%|           :noindex:\n",
      "   839|         0|            0|            0|  0.00%|\n",
      "   840|         0|            0|            0|  0.00%|        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "   841|         0|            0|            0|  0.00%|        floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "   842|         0|            0|            0|  0.00%|        only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "   843|         0|            0|            0|  0.00%|        (if given). The integral parameters and buffers will be moved\n",
      "   844|         0|            0|            0|  0.00%|        :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "   845|         0|            0|            0|  0.00%|        :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "   846|         0|            0|            0|  0.00%|        with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "   847|         0|            0|            0|  0.00%|        pinned memory to CUDA devices.\n",
      "   848|         0|            0|            0|  0.00%|\n",
      "   849|         0|            0|            0|  0.00%|        See below for examples.\n",
      "   850|         0|            0|            0|  0.00%|\n",
      "   851|         0|            0|            0|  0.00%|        .. note::\n",
      "   852|         0|            0|            0|  0.00%|            This method modifies the module in-place.\n",
      "   853|         0|            0|            0|  0.00%|\n",
      "   854|         0|            0|            0|  0.00%|        Args:\n",
      "   855|         0|            0|            0|  0.00%|            device (:class:`torch.device`): the desired device of the parameters\n",
      "   856|         0|            0|            0|  0.00%|                and buffers in this module\n",
      "   857|         0|            0|            0|  0.00%|            dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "   858|         0|            0|            0|  0.00%|                the parameters and buffers in this module\n",
      "   859|         0|            0|            0|  0.00%|            tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "   860|         0|            0|            0|  0.00%|                dtype and device for all parameters and buffers in this module\n",
      "   861|         0|            0|            0|  0.00%|            memory_format (:class:`torch.memory_format`): the desired memory\n",
      "   862|         0|            0|            0|  0.00%|                format for 4D parameters and buffers in this module (keyword\n",
      "   863|         0|            0|            0|  0.00%|                only argument)\n",
      "   864|         0|            0|            0|  0.00%|\n",
      "   865|         0|            0|            0|  0.00%|        Returns:\n",
      "   866|         0|            0|            0|  0.00%|            Module: self\n",
      "   867|         0|            0|            0|  0.00%|\n",
      "   868|         0|            0|            0|  0.00%|        Examples::\n",
      "   869|         0|            0|            0|  0.00%|\n",
      "   870|         0|            0|            0|  0.00%|            >>> linear = nn.Linear(2, 2)\n",
      "   871|         0|            0|            0|  0.00%|            >>> linear.weight\n",
      "   872|         0|            0|            0|  0.00%|            Parameter containing:\n",
      "   873|         0|            0|            0|  0.00%|            tensor([[ 0.1913, -0.3420],\n",
      "   874|         0|            0|            0|  0.00%|                    [-0.5113, -0.2325]])\n",
      "   875|         0|            0|            0|  0.00%|            >>> linear.to(torch.double)\n",
      "   876|         0|            0|            0|  0.00%|            Linear(in_features=2, out_features=2, bias=True)\n",
      "   877|         0|            0|            0|  0.00%|            >>> linear.weight\n",
      "   878|         0|            0|            0|  0.00%|            Parameter containing:\n",
      "   879|         0|            0|            0|  0.00%|            tensor([[ 0.1913, -0.3420],\n",
      "   880|         0|            0|            0|  0.00%|                    [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "   881|         0|            0|            0|  0.00%|            >>> gpu1 = torch.device(\"cuda:1\")\n",
      "   882|         0|            0|            0|  0.00%|            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "   883|         0|            0|            0|  0.00%|            Linear(in_features=2, out_features=2, bias=True)\n",
      "   884|         0|            0|            0|  0.00%|            >>> linear.weight\n",
      "   885|         0|            0|            0|  0.00%|            Parameter containing:\n",
      "   886|         0|            0|            0|  0.00%|            tensor([[ 0.1914, -0.3420],\n",
      "   887|         0|            0|            0|  0.00%|                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "   888|         0|            0|            0|  0.00%|            >>> cpu = torch.device(\"cpu\")\n",
      "   889|         0|            0|            0|  0.00%|            >>> linear.to(cpu)\n",
      "   890|         0|            0|            0|  0.00%|            Linear(in_features=2, out_features=2, bias=True)\n",
      "   891|         0|            0|            0|  0.00%|            >>> linear.weight\n",
      "   892|         0|            0|            0|  0.00%|            Parameter containing:\n",
      "   893|         0|            0|            0|  0.00%|            tensor([[ 0.1914, -0.3420],\n",
      "   894|         0|            0|            0|  0.00%|                    [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "   895|         0|            0|            0|  0.00%|\n",
      "   896|         0|            0|            0|  0.00%|            >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "   897|         0|            0|            0|  0.00%|            >>> linear.weight\n",
      "   898|         0|            0|            0|  0.00%|            Parameter containing:\n",
      "   899|         0|            0|            0|  0.00%|            tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "   900|         0|            0|            0|  0.00%|                    [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "   901|         0|            0|            0|  0.00%|            >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "   902|         0|            0|            0|  0.00%|            tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "   903|         0|            0|            0|  0.00%|                    [0.6122+0.j, 0.1150+0.j],\n",
      "   904|         0|            0|            0|  0.00%|                    [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "   905|         0|            0|            0|  0.00%|\n",
      "   906|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   907|         0|            0|            0|  0.00%|\n",
      "   908|         0|            0|            0|  0.00%|        device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)\n",
      "   909|         0|            0|            0|  0.00%|\n",
      "   910|         0|            0|            0|  0.00%|        if dtype is not None:\n",
      "   911|         0|            0|            0|  0.00%|            if not (dtype.is_floating_point or dtype.is_complex):\n",
      "   912|         0|            0|            0|  0.00%|                raise TypeError('nn.Module.to only accepts floating point or complex '\n",
      "   913|         0|            0|            0|  0.00%|                                'dtypes, but got desired dtype={}'.format(dtype))\n",
      "   914|         0|            0|            0|  0.00%|            if dtype.is_complex:\n",
      "   915|         0|            0|            0|  0.00%|                warnings.warn(\n",
      "   916|         0|            0|            0|  0.00%|                    \"Complex modules are a new feature under active development whose design may change, \"\n",
      "   917|         0|            0|            0|  0.00%|                    \"and some modules might not work as expected when using complex tensors as parameters or buffers. \"\n",
      "   918|         0|            0|            0|  0.00%|                    \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\n",
      "   919|         0|            0|            0|  0.00%|                    \"if a complex module does not work as expected.\")\n",
      "   920|         0|            0|            0|  0.00%|\n",
      "   921|         0|            0|            0|  0.00%|        def convert(t):\n",
      "   922|         0|            0|            0|  0.00%|            if convert_to_format is not None and t.dim() in (4, 5):\n",
      "   923|         0|            0|            0|  0.00%|                return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "   924|         0|            0|            0|  0.00%|                            non_blocking, memory_format=convert_to_format)\n",
      "   925|         0|            0|            0|  0.00%|            return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "   926|         0|            0|            0|  0.00%|\n",
      "   927|         0|            0|            0|  0.00%|        return self._apply(convert)\n",
      "   928|         0|            0|            0|  0.00%|\n",
      "   929|         0|            0|            0|  0.00%|    def register_backward_hook(\n",
      "   930|         0|            0|            0|  0.00%|        self, hook: Callable[['Module', _grad_t, _grad_t], Union[None, Tensor]]\n",
      "   931|         0|            0|            0|  0.00%|    ) -> RemovableHandle:\n",
      "   932|         0|            0|            0|  0.00%|        r\"\"\"Registers a backward hook on the module.\n",
      "   933|         0|            0|            0|  0.00%|\n",
      "   934|         0|            0|            0|  0.00%|        This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "   935|         0|            0|            0|  0.00%|        the behavior of this function will change in future versions.\n",
      "   936|         0|            0|            0|  0.00%|\n",
      "   937|         0|            0|            0|  0.00%|        Returns:\n",
      "   938|         0|            0|            0|  0.00%|            :class:`torch.utils.hooks.RemovableHandle`:\n",
      "   939|         0|            0|            0|  0.00%|                a handle that can be used to remove the added hook by calling\n",
      "   940|         0|            0|            0|  0.00%|                ``handle.remove()``\n",
      "   941|         0|            0|            0|  0.00%|\n",
      "   942|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   943|         0|            0|            0|  0.00%|        if self._is_full_backward_hook is True:\n",
      "   944|         0|            0|            0|  0.00%|            raise RuntimeError(\"Cannot use both regular backward hooks and full backward hooks on a \"\n",
      "   945|         0|            0|            0|  0.00%|                               \"single Module. Please use only one of them.\")\n",
      "   946|         0|            0|            0|  0.00%|\n",
      "   947|         0|            0|            0|  0.00%|        self._is_full_backward_hook = False\n",
      "   948|         0|            0|            0|  0.00%|\n",
      "   949|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._backward_hooks)\n",
      "   950|         0|            0|            0|  0.00%|        self._backward_hooks[handle.id] = hook\n",
      "   951|         0|            0|            0|  0.00%|        return handle\n",
      "   952|         0|            0|            0|  0.00%|\n",
      "   953|         0|            0|            0|  0.00%|    def register_full_backward_hook(\n",
      "   954|         0|            0|            0|  0.00%|        self, hook: Callable[['Module', _grad_t, _grad_t], Union[None, Tensor]]\n",
      "   955|         0|            0|            0|  0.00%|    ) -> RemovableHandle:\n",
      "   956|         0|            0|            0|  0.00%|        r\"\"\"Registers a backward hook on the module.\n",
      "   957|         0|            0|            0|  0.00%|\n",
      "   958|         0|            0|            0|  0.00%|        The hook will be called every time the gradients with respect to module\n",
      "   959|         0|            0|            0|  0.00%|        inputs are computed. The hook should have the following signature::\n",
      "   960|         0|            0|            0|  0.00%|\n",
      "   961|         0|            0|            0|  0.00%|            hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "   962|         0|            0|            0|  0.00%|\n",
      "   963|         0|            0|            0|  0.00%|        The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "   964|         0|            0|            0|  0.00%|        with respect to the inputs and outputs respectively. The hook should\n",
      "   965|         0|            0|            0|  0.00%|        not modify its arguments, but it can optionally return a new gradient with\n",
      "   966|         0|            0|            0|  0.00%|        respect to the input that will be used in place of :attr:`grad_input` in\n",
      "   967|         0|            0|            0|  0.00%|        subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "   968|         0|            0|            0|  0.00%|        as positional arguments and all kwarg arguments are ignored. Entries\n",
      "   969|         0|            0|            0|  0.00%|        in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "   970|         0|            0|            0|  0.00%|        arguments.\n",
      "   971|         0|            0|            0|  0.00%|\n",
      "   972|         0|            0|            0|  0.00%|        For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "   973|         0|            0|            0|  0.00%|        receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "   974|         0|            0|            0|  0.00%|        of each Tensor returned by the Module's forward function.\n",
      "   975|         0|            0|            0|  0.00%|\n",
      "   976|         0|            0|            0|  0.00%|        .. warning ::\n",
      "   977|         0|            0|            0|  0.00%|            Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "   978|         0|            0|            0|  0.00%|            will raise an error.\n",
      "   979|         0|            0|            0|  0.00%|\n",
      "   980|         0|            0|            0|  0.00%|        Returns:\n",
      "   981|         0|            0|            0|  0.00%|            :class:`torch.utils.hooks.RemovableHandle`:\n",
      "   982|         0|            0|            0|  0.00%|                a handle that can be used to remove the added hook by calling\n",
      "   983|         0|            0|            0|  0.00%|                ``handle.remove()``\n",
      "   984|         0|            0|            0|  0.00%|\n",
      "   985|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   986|         0|            0|            0|  0.00%|        if self._is_full_backward_hook is False:\n",
      "   987|         0|            0|            0|  0.00%|            raise RuntimeError(\"Cannot use both regular backward hooks and full backward hooks on a \"\n",
      "   988|         0|            0|            0|  0.00%|                               \"single Module. Please use only one of them.\")\n",
      "   989|         0|            0|            0|  0.00%|\n",
      "   990|         0|            0|            0|  0.00%|        self._is_full_backward_hook = True\n",
      "   991|         0|            0|            0|  0.00%|\n",
      "   992|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._backward_hooks)\n",
      "   993|         0|            0|            0|  0.00%|        self._backward_hooks[handle.id] = hook\n",
      "   994|         0|            0|            0|  0.00%|        return handle\n",
      "   995|         0|            0|            0|  0.00%|\n",
      "   996|         0|            0|            0|  0.00%|    def _get_backward_hooks(self):\n",
      "   997|         0|            0|            0|  0.00%|        r\"\"\"Returns the backward hooks for use in the call function.\n",
      "   998|         0|            0|            0|  0.00%|        It returns two lists, one with the full backward hooks and one with the non-full\n",
      "   999|         0|            0|            0|  0.00%|        backward hooks.\n",
      "  1000|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1001|         0|            0|            0|  0.00%|        full_backward_hooks: List[Callable] = []\n",
      "  1002|         0|            0|            0|  0.00%|        if (_global_is_full_backward_hook is True):\n",
      "  1003|         0|            0|            0|  0.00%|            full_backward_hooks += _global_backward_hooks.values()\n",
      "  1004|         0|            0|            0|  0.00%|        if (self._is_full_backward_hook is True):\n",
      "  1005|         0|            0|            0|  0.00%|            full_backward_hooks += self._backward_hooks.values()\n",
      "  1006|         0|            0|            0|  0.00%|\n",
      "  1007|         0|            0|            0|  0.00%|        non_full_backward_hooks: List[Callable] = []\n",
      "  1008|         0|            0|            0|  0.00%|        if (_global_is_full_backward_hook is False):\n",
      "  1009|         0|            0|            0|  0.00%|            non_full_backward_hooks += _global_backward_hooks.values()\n",
      "  1010|         0|            0|            0|  0.00%|        if (self._is_full_backward_hook is False):\n",
      "  1011|         0|            0|            0|  0.00%|            non_full_backward_hooks += self._backward_hooks.values()\n",
      "  1012|         0|            0|            0|  0.00%|\n",
      "  1013|         0|            0|            0|  0.00%|        return full_backward_hooks, non_full_backward_hooks\n",
      "  1014|         0|            0|            0|  0.00%|\n",
      "  1015|         0|            0|            0|  0.00%|    def _maybe_warn_non_full_backward_hook(self, inputs, result, grad_fn):\n",
      "  1016|         0|            0|            0|  0.00%|        if not isinstance(result, torch.Tensor):\n",
      "  1017|         0|            0|            0|  0.00%|            if not (isinstance(result, tuple) and all([isinstance(r, torch.Tensor) for r in result])):\n",
      "  1018|         0|            0|            0|  0.00%|                warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "  1019|         0|            0|            0|  0.00%|                              \"single Tensor or a tuple of Tensors is deprecated and will be removed \"\n",
      "  1020|         0|            0|            0|  0.00%|                              \"in future versions. This hook will be missing some of the grad_output. \"\n",
      "  1021|         0|            0|            0|  0.00%|                              \"Please use register_full_backward_hook to get the documented behavior.\")\n",
      "  1022|         0|            0|            0|  0.00%|                return\n",
      "  1023|         0|            0|            0|  0.00%|        else:\n",
      "  1024|         0|            0|            0|  0.00%|            result = (result,)\n",
      "  1025|         0|            0|            0|  0.00%|\n",
      "  1026|         0|            0|            0|  0.00%|        if not isinstance(inputs, torch.Tensor):\n",
      "  1027|         0|            0|            0|  0.00%|            if not (isinstance(inputs, tuple) and all([isinstance(i, torch.Tensor) for i in inputs])):\n",
      "  1028|         0|            0|            0|  0.00%|                warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
      "  1029|         0|            0|            0|  0.00%|                              \"single Tensor or a tuple of Tensors is deprecated and will be removed \"\n",
      "  1030|         0|            0|            0|  0.00%|                              \"in future versions. This hook will be missing some of the grad_input. \"\n",
      "  1031|         0|            0|            0|  0.00%|                              \"Please use register_full_backward_hook to get the documented behavior.\")\n",
      "  1032|         0|            0|            0|  0.00%|                return\n",
      "  1033|         0|            0|            0|  0.00%|        else:\n",
      "  1034|         0|            0|            0|  0.00%|            inputs = (inputs,)\n",
      "  1035|         0|            0|            0|  0.00%|\n",
      "  1036|         0|            0|            0|  0.00%|        # At this point we are sure that inputs and result are tuple of Tensors\n",
      "  1037|         0|            0|            0|  0.00%|        out_grad_fn = {r.grad_fn for r in result if r.grad_fn is not None}\n",
      "  1038|         0|            0|            0|  0.00%|        if len(out_grad_fn) == 0 or (len(out_grad_fn) == 1 and grad_fn not in out_grad_fn):\n",
      "  1039|         0|            0|            0|  0.00%|            warnings.warn(\"Using a non-full backward hook when outputs are nested in python data structure \"\n",
      "  1040|         0|            0|            0|  0.00%|                          \"is deprecated and will be removed in future versions. This hook will be missing \"\n",
      "  1041|         0|            0|            0|  0.00%|                          \"some grad_output.\")\n",
      "  1042|         0|            0|            0|  0.00%|        elif len(out_grad_fn) > 1:\n",
      "  1043|         0|            0|            0|  0.00%|            warnings.warn(\"Using a non-full backward hook when outputs are generated by different autograd Nodes \"\n",
      "  1044|         0|            0|            0|  0.00%|                          \"is deprecated and will be removed in future versions. This hook will be missing \"\n",
      "  1045|         0|            0|            0|  0.00%|                          \"some grad_output. Please use register_full_backward_hook to get the documented behavior.\")\n",
      "  1046|         0|            0|            0|  0.00%|        else:\n",
      "  1047|         0|            0|            0|  0.00%|            # At this point the grad_ouput part of the hook will most likely be correct\n",
      "  1048|         0|            0|            0|  0.00%|            inputs_grad_fn = {i.grad_fn for i in inputs if i.grad_fn is not None}\n",
      "  1049|         0|            0|            0|  0.00%|\n",
      "  1050|         0|            0|            0|  0.00%|            next_functions = {n[0] for n in grad_fn.next_functions}\n",
      "  1051|         0|            0|            0|  0.00%|\n",
      "  1052|         0|            0|            0|  0.00%|            if inputs_grad_fn != next_functions:\n",
      "  1053|         0|            0|            0|  0.00%|                warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "  1054|         0|            0|            0|  0.00%|                              \"is deprecated and will be removed in future versions. This hook will be missing \"\n",
      "  1055|         0|            0|            0|  0.00%|                              \"some grad_input. Please use register_full_backward_hook to get the documented \"\n",
      "  1056|         0|            0|            0|  0.00%|                              \"behavior.\")\n",
      "  1057|         0|            0|            0|  0.00%|\n",
      "  1058|         0|            0|            0|  0.00%|    def register_forward_pre_hook(self, hook: Callable[..., None]) -> RemovableHandle:\n",
      "  1059|         0|            0|            0|  0.00%|        r\"\"\"Registers a forward pre-hook on the module.\n",
      "  1060|         0|            0|            0|  0.00%|\n",
      "  1061|         0|            0|            0|  0.00%|        The hook will be called every time before :func:`forward` is invoked.\n",
      "  1062|         0|            0|            0|  0.00%|        It should have the following signature::\n",
      "  1063|         0|            0|            0|  0.00%|\n",
      "  1064|         0|            0|            0|  0.00%|            hook(module, input) -> None or modified input\n",
      "  1065|         0|            0|            0|  0.00%|\n",
      "  1066|         0|            0|            0|  0.00%|        The input contains only the positional arguments given to the module.\n",
      "  1067|         0|            0|            0|  0.00%|        Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "  1068|         0|            0|            0|  0.00%|        The hook can modify the input. User can either return a tuple or a\n",
      "  1069|         0|            0|            0|  0.00%|        single modified value in the hook. We will wrap the value into a tuple\n",
      "  1070|         0|            0|            0|  0.00%|        if a single value is returned(unless that value is already a tuple).\n",
      "  1071|         0|            0|            0|  0.00%|\n",
      "  1072|         0|            0|            0|  0.00%|        Returns:\n",
      "  1073|         0|            0|            0|  0.00%|            :class:`torch.utils.hooks.RemovableHandle`:\n",
      "  1074|         0|            0|            0|  0.00%|                a handle that can be used to remove the added hook by calling\n",
      "  1075|         0|            0|            0|  0.00%|                ``handle.remove()``\n",
      "  1076|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1077|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._forward_pre_hooks)\n",
      "  1078|         0|            0|            0|  0.00%|        self._forward_pre_hooks[handle.id] = hook\n",
      "  1079|         0|            0|            0|  0.00%|        return handle\n",
      "  1080|         0|            0|            0|  0.00%|\n",
      "  1081|         0|            0|            0|  0.00%|    def register_forward_hook(self, hook: Callable[..., None]) -> RemovableHandle:\n",
      "  1082|         0|            0|            0|  0.00%|        r\"\"\"Registers a forward hook on the module.\n",
      "  1083|         0|            0|            0|  0.00%|\n",
      "  1084|         0|            0|            0|  0.00%|        The hook will be called every time after :func:`forward` has computed an output.\n",
      "  1085|         0|            0|            0|  0.00%|        It should have the following signature::\n",
      "  1086|         0|            0|            0|  0.00%|\n",
      "  1087|         0|            0|            0|  0.00%|            hook(module, input, output) -> None or modified output\n",
      "  1088|         0|            0|            0|  0.00%|\n",
      "  1089|         0|            0|            0|  0.00%|        The input contains only the positional arguments given to the module.\n",
      "  1090|         0|            0|            0|  0.00%|        Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "  1091|         0|            0|            0|  0.00%|        The hook can modify the output. It can modify the input inplace but\n",
      "  1092|         0|            0|            0|  0.00%|        it will not have effect on forward since this is called after\n",
      "  1093|         0|            0|            0|  0.00%|        :func:`forward` is called.\n",
      "  1094|         0|            0|            0|  0.00%|\n",
      "  1095|         0|            0|            0|  0.00%|        Returns:\n",
      "  1096|         0|            0|            0|  0.00%|            :class:`torch.utils.hooks.RemovableHandle`:\n",
      "  1097|         0|            0|            0|  0.00%|                a handle that can be used to remove the added hook by calling\n",
      "  1098|         0|            0|            0|  0.00%|                ``handle.remove()``\n",
      "  1099|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1100|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._forward_hooks)\n",
      "  1101|         0|            0|            0|  0.00%|        self._forward_hooks[handle.id] = hook\n",
      "  1102|         0|            0|            0|  0.00%|        return handle\n",
      "  1103|         0|            0|            0|  0.00%|\n",
      "  1104|         0|            0|            0|  0.00%|    def _slow_forward(self, *input, **kwargs):\n",
      "  1105|         0|            0|            0|  0.00%|        tracing_state = torch._C._get_tracing_state()\n",
      "  1106|         0|            0|            0|  0.00%|        if not tracing_state or isinstance(self.forward, torch._C.ScriptMethod):\n",
      "  1107|         0|            0|            0|  0.00%|            return self.forward(*input, **kwargs)\n",
      "  1108|         0|            0|            0|  0.00%|        recording_scopes = torch.jit._trace._trace_module_map is not None\n",
      "  1109|         0|            0|            0|  0.00%|        if recording_scopes:\n",
      "  1110|         0|            0|            0|  0.00%|            # type ignore was added because at this point one knows that\n",
      "  1111|         0|            0|            0|  0.00%|            # torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\n",
      "  1112|         0|            0|            0|  0.00%|            name = torch.jit._trace._trace_module_map[self] if self in torch.jit._trace._trace_module_map else None  # type: ignore[index, operator] # noqa: B950\n",
      "  1113|         0|            0|            0|  0.00%|            if name:\n",
      "  1114|         0|            0|            0|  0.00%|                tracing_state.push_scope(name)\n",
      "  1115|         0|            0|            0|  0.00%|            else:\n",
      "  1116|         0|            0|            0|  0.00%|                recording_scopes = False\n",
      "  1117|         0|            0|            0|  0.00%|        try:\n",
      "  1118|         0|            0|            0|  0.00%|            result = self.forward(*input, **kwargs)\n",
      "  1119|         0|            0|            0|  0.00%|        finally:\n",
      "  1120|         0|            0|            0|  0.00%|            if recording_scopes:\n",
      "  1121|         0|            0|            0|  0.00%|                tracing_state.pop_scope()\n",
      "  1122|         0|            0|            0|  0.00%|        return result\n",
      "  1123|         0|            0|            0|  0.00%|\n",
      "  1124|    381420|      1.37843|  3.61394e-06|  0.22%|    def _call_impl(self, *input, **kwargs):\n",
      "  1125|    381420|      1.81944|  4.77017e-06|  0.29%|        forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)\n",
      "  1126|         0|            0|            0|  0.00%|        # If we don't have any hooks, we want to skip the rest of the logic in\n",
      "  1127|         0|            0|            0|  0.00%|        # this function, and just call forward.\n",
      "  1128|   1144260|      3.76446|  3.28987e-06|  0.61%|        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
      "  1129|    762840|      2.38776|  3.13009e-06|  0.38%|                or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "  1130|    381420|      3.09459|  8.11335e-06|  0.50%|            return forward_call(*input, **kwargs)\n",
      "(call)|    190710|      10.1252|  5.30924e-05|  1.63%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:113 forward\n",
      "(call)|     63180|      1.64428|  2.60253e-05|  0.26%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:97 forward\n",
      "(call)|     63570|      27.2165|  0.000428135|  4.38%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/container.py:137 forward\n",
      "(call)|     63960|     0.710802|  1.11132e-05|  0.11%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:353 forward\n",
      "  1131|         0|            0|            0|  0.00%|        # Do not call functions when jit is used\n",
      "  1132|         0|            0|            0|  0.00%|        full_backward_hooks, non_full_backward_hooks = [], []\n",
      "  1133|         0|            0|            0|  0.00%|        if self._backward_hooks or _global_backward_hooks:\n",
      "  1134|         0|            0|            0|  0.00%|            full_backward_hooks, non_full_backward_hooks = self._get_backward_hooks()\n",
      "  1135|         0|            0|            0|  0.00%|        if _global_forward_pre_hooks or self._forward_pre_hooks:\n",
      "  1136|         0|            0|            0|  0.00%|            for hook in (*_global_forward_pre_hooks.values(), *self._forward_pre_hooks.values()):\n",
      "  1137|         0|            0|            0|  0.00%|                result = hook(self, input)\n",
      "  1138|         0|            0|            0|  0.00%|                if result is not None:\n",
      "  1139|         0|            0|            0|  0.00%|                    if not isinstance(result, tuple):\n",
      "  1140|         0|            0|            0|  0.00%|                        result = (result,)\n",
      "  1141|         0|            0|            0|  0.00%|                    input = result\n",
      "  1142|         0|            0|            0|  0.00%|\n",
      "  1143|         0|            0|            0|  0.00%|        bw_hook = None\n",
      "  1144|         0|            0|            0|  0.00%|        if full_backward_hooks:\n",
      "  1145|         0|            0|            0|  0.00%|            bw_hook = hooks.BackwardHook(self, full_backward_hooks)\n",
      "  1146|         0|            0|            0|  0.00%|            input = bw_hook.setup_input_hook(input)\n",
      "  1147|         0|            0|            0|  0.00%|\n",
      "  1148|         0|            0|            0|  0.00%|        result = forward_call(*input, **kwargs)\n",
      "  1149|         0|            0|            0|  0.00%|        if _global_forward_hooks or self._forward_hooks:\n",
      "  1150|         0|            0|            0|  0.00%|            for hook in (*_global_forward_hooks.values(), *self._forward_hooks.values()):\n",
      "  1151|         0|            0|            0|  0.00%|                hook_result = hook(self, input, result)\n",
      "  1152|         0|            0|            0|  0.00%|                if hook_result is not None:\n",
      "  1153|         0|            0|            0|  0.00%|                    result = hook_result\n",
      "  1154|         0|            0|            0|  0.00%|\n",
      "  1155|         0|            0|            0|  0.00%|        if bw_hook:\n",
      "  1156|         0|            0|            0|  0.00%|            result = bw_hook.setup_output_hook(result)\n",
      "  1157|         0|            0|            0|  0.00%|\n",
      "  1158|         0|            0|            0|  0.00%|        # Handle the non-full backward hooks\n",
      "  1159|         0|            0|            0|  0.00%|        if non_full_backward_hooks:\n",
      "  1160|         0|            0|            0|  0.00%|            var = result\n",
      "  1161|         0|            0|            0|  0.00%|            while not isinstance(var, torch.Tensor):\n",
      "  1162|         0|            0|            0|  0.00%|                if isinstance(var, dict):\n",
      "  1163|         0|            0|            0|  0.00%|                    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))\n",
      "  1164|         0|            0|            0|  0.00%|                else:\n",
      "  1165|         0|            0|            0|  0.00%|                    var = var[0]\n",
      "  1166|         0|            0|            0|  0.00%|            grad_fn = var.grad_fn\n",
      "  1167|         0|            0|            0|  0.00%|            if grad_fn is not None:\n",
      "  1168|         0|            0|            0|  0.00%|                for hook in non_full_backward_hooks:\n",
      "  1169|         0|            0|            0|  0.00%|                    wrapper = functools.partial(hook, self)\n",
      "  1170|         0|            0|            0|  0.00%|                    functools.update_wrapper(wrapper, hook)\n",
      "  1171|         0|            0|            0|  0.00%|                    grad_fn.register_hook(wrapper)\n",
      "  1172|         0|            0|            0|  0.00%|                self._maybe_warn_non_full_backward_hook(input, result, grad_fn)\n",
      "  1173|         0|            0|            0|  0.00%|\n",
      "  1174|         0|            0|            0|  0.00%|        return result\n",
      "  1175|         0|            0|            0|  0.00%|\n",
      "  1176|         0|            0|            0|  0.00%|    __call__ : Callable[..., Any] = _call_impl\n",
      "  1177|         0|            0|            0|  0.00%|\n",
      "  1178|         0|            0|            0|  0.00%|    def __setstate__(self, state):\n",
      "  1179|         0|            0|            0|  0.00%|        self.__dict__.update(state)\n",
      "  1180|         0|            0|            0|  0.00%|        # Support loading old checkpoints that don't have the following attrs:\n",
      "  1181|         0|            0|            0|  0.00%|        if '_forward_pre_hooks' not in self.__dict__:\n",
      "  1182|         0|            0|            0|  0.00%|            self._forward_pre_hooks = OrderedDict()\n",
      "  1183|         0|            0|            0|  0.00%|        if '_state_dict_hooks' not in self.__dict__:\n",
      "  1184|         0|            0|            0|  0.00%|            self._state_dict_hooks = OrderedDict()\n",
      "  1185|         0|            0|            0|  0.00%|        if '_load_state_dict_pre_hooks' not in self.__dict__:\n",
      "  1186|         0|            0|            0|  0.00%|            self._load_state_dict_pre_hooks = OrderedDict()\n",
      "  1187|         0|            0|            0|  0.00%|        if '_load_state_dict_post_hooks' not in self.__dict__:\n",
      "  1188|         0|            0|            0|  0.00%|            self._load_state_dict_post_hooks = OrderedDict()\n",
      "  1189|         0|            0|            0|  0.00%|        if '_non_persistent_buffers_set' not in self.__dict__:\n",
      "  1190|         0|            0|            0|  0.00%|            self._non_persistent_buffers_set = set()\n",
      "  1191|         0|            0|            0|  0.00%|        if '_is_full_backward_hook' not in self.__dict__:\n",
      "  1192|         0|            0|            0|  0.00%|            self._is_full_backward_hook = None\n",
      "  1193|         0|            0|            0|  0.00%|\n",
      "  1194|    508560|     0.967488|  1.90241e-06|  0.16%|    def __getattr__(self, name: str) -> Union[Tensor, 'Module']:\n",
      "  1195|    508560|      1.18015|  2.32057e-06|  0.19%|        if '_parameters' in self.__dict__:\n",
      "  1196|    508560|      1.11677|  2.19594e-06|  0.18%|            _parameters = self.__dict__['_parameters']\n",
      "  1197|    508560|      1.01992|   2.0055e-06|  0.16%|            if name in _parameters:\n",
      "  1198|    381420|     0.734465|  1.92561e-06|  0.12%|                return _parameters[name]\n",
      "  1199|    127140|      0.25713|  2.02242e-06|  0.04%|        if '_buffers' in self.__dict__:\n",
      "  1200|    127140|     0.258883|   2.0362e-06|  0.04%|            _buffers = self.__dict__['_buffers']\n",
      "  1201|    127140|     0.243508|  1.91527e-06|  0.04%|            if name in _buffers:\n",
      "  1202|     31590|    0.0657198|   2.0804e-06|  0.01%|                return _buffers[name]\n",
      "  1203|     95550|     0.178945|  1.87279e-06|  0.03%|        if '_modules' in self.__dict__:\n",
      "  1204|     95550|     0.174343|  1.82462e-06|  0.03%|            modules = self.__dict__['_modules']\n",
      "  1205|     95550|     0.168502|  1.76349e-06|  0.03%|            if name in modules:\n",
      "  1206|     95550|     0.161309|  1.68821e-06|  0.03%|                return modules[name]\n",
      "  1207|         0|            0|            0|  0.00%|        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
      "  1208|         0|            0|            0|  0.00%|            type(self).__name__, name))\n",
      "  1209|         0|            0|            0|  0.00%|\n",
      "  1210|     51090|     0.128144|  2.50821e-06|  0.02%|    def __setattr__(self, name: str, value: Union[Tensor, 'Module']) -> None:\n",
      "  1211|     51090|     0.131277|  2.56952e-06|  0.02%|        def remove_from(*dicts_or_sets):\n",
      "  1212|         0|            0|            0|  0.00%|            for d in dicts_or_sets:\n",
      "  1213|         0|            0|            0|  0.00%|                if name in d:\n",
      "  1214|         0|            0|            0|  0.00%|                    if isinstance(d, dict):\n",
      "  1215|         0|            0|            0|  0.00%|                        del d[name]\n",
      "  1216|         0|            0|            0|  0.00%|                    else:\n",
      "  1217|         0|            0|            0|  0.00%|                        d.discard(name)\n",
      "  1218|         0|            0|            0|  0.00%|\n",
      "  1219|     51090|     0.131323|  2.57042e-06|  0.02%|        params = self.__dict__.get('_parameters')\n",
      "  1220|     51090|     0.336539|  6.58718e-06|  0.05%|        if isinstance(value, Parameter):\n",
      "(call)|     51090|     0.381078|  7.45896e-06|  0.06%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/parameter.py:9 __instancecheck__\n",
      "  1221|         0|            0|            0|  0.00%|            if params is None:\n",
      "  1222|         0|            0|            0|  0.00%|                raise AttributeError(\n",
      "  1223|         0|            0|            0|  0.00%|                    \"cannot assign parameters before Module.__init__() call\")\n",
      "  1224|         0|            0|            0|  0.00%|            remove_from(self.__dict__, self._buffers, self._modules, self._non_persistent_buffers_set)\n",
      "  1225|         0|            0|            0|  0.00%|            self.register_parameter(name, value)\n",
      "  1226|     51090|     0.124079|  2.42864e-06|  0.02%|        elif params is not None and name in params:\n",
      "  1227|         0|            0|            0|  0.00%|            if value is not None:\n",
      "  1228|         0|            0|            0|  0.00%|                raise TypeError(\"cannot assign '{}' as parameter '{}' \"\n",
      "  1229|         0|            0|            0|  0.00%|                                \"(torch.nn.Parameter or None expected)\"\n",
      "  1230|         0|            0|            0|  0.00%|                                .format(torch.typename(value), name))\n",
      "  1231|         0|            0|            0|  0.00%|            self.register_parameter(name, value)\n",
      "  1232|         0|            0|            0|  0.00%|        else:\n",
      "  1233|     51090|     0.126044|  2.46709e-06|  0.02%|            modules = self.__dict__.get('_modules')\n",
      "  1234|     51090|     0.113965|  2.23068e-06|  0.02%|            if isinstance(value, Module):\n",
      "  1235|         0|            0|            0|  0.00%|                if modules is None:\n",
      "  1236|         0|            0|            0|  0.00%|                    raise AttributeError(\n",
      "  1237|         0|            0|            0|  0.00%|                        \"cannot assign module before Module.__init__() call\")\n",
      "  1238|         0|            0|            0|  0.00%|                remove_from(self.__dict__, self._parameters, self._buffers, self._non_persistent_buffers_set)\n",
      "  1239|         0|            0|            0|  0.00%|                modules[name] = value\n",
      "  1240|     51090|     0.108518|  2.12405e-06|  0.02%|            elif modules is not None and name in modules:\n",
      "  1241|         0|            0|            0|  0.00%|                if value is not None:\n",
      "  1242|         0|            0|            0|  0.00%|                    raise TypeError(\"cannot assign '{}' as child module '{}' \"\n",
      "  1243|         0|            0|            0|  0.00%|                                    \"(torch.nn.Module or None expected)\"\n",
      "  1244|         0|            0|            0|  0.00%|                                    .format(torch.typename(value), name))\n",
      "  1245|         0|            0|            0|  0.00%|                modules[name] = value\n",
      "  1246|         0|            0|            0|  0.00%|            else:\n",
      "  1247|     51090|     0.110806|  2.16883e-06|  0.02%|                buffers = self.__dict__.get('_buffers')\n",
      "  1248|     51090|     0.100349|  1.96416e-06|  0.02%|                if buffers is not None and name in buffers:\n",
      "  1249|         0|            0|            0|  0.00%|                    if value is not None and not isinstance(value, torch.Tensor):\n",
      "  1250|         0|            0|            0|  0.00%|                        raise TypeError(\"cannot assign '{}' as buffer '{}' \"\n",
      "  1251|         0|            0|            0|  0.00%|                                        \"(torch.Tensor or None expected)\"\n",
      "  1252|         0|            0|            0|  0.00%|                                        .format(torch.typename(value), name))\n",
      "  1253|         0|            0|            0|  0.00%|                    buffers[name] = value\n",
      "  1254|         0|            0|            0|  0.00%|                else:\n",
      "  1255|     51090|     0.128986|  2.52468e-06|  0.02%|                    object.__setattr__(self, name, value)\n",
      "  1256|         0|            0|            0|  0.00%|\n",
      "  1257|         0|            0|            0|  0.00%|    def __delattr__(self, name):\n",
      "  1258|         0|            0|            0|  0.00%|        if name in self._parameters:\n",
      "  1259|         0|            0|            0|  0.00%|            del self._parameters[name]\n",
      "  1260|         0|            0|            0|  0.00%|        elif name in self._buffers:\n",
      "  1261|         0|            0|            0|  0.00%|            del self._buffers[name]\n",
      "  1262|         0|            0|            0|  0.00%|            self._non_persistent_buffers_set.discard(name)\n",
      "  1263|         0|            0|            0|  0.00%|        elif name in self._modules:\n",
      "  1264|         0|            0|            0|  0.00%|            del self._modules[name]\n",
      "  1265|         0|            0|            0|  0.00%|        else:\n",
      "  1266|         0|            0|            0|  0.00%|            object.__delattr__(self, name)\n",
      "  1267|         0|            0|            0|  0.00%|\n",
      "  1268|         0|            0|            0|  0.00%|    def _register_state_dict_hook(self, hook):\n",
      "  1269|         0|            0|            0|  0.00%|        r\"\"\"These hooks will be called with arguments: `self`, `state_dict`,\n",
      "  1270|         0|            0|            0|  0.00%|        `prefix`, `local_metadata`, after the `state_dict` of `self` is set.\n",
      "  1271|         0|            0|            0|  0.00%|        Note that only parameters and buffers of `self` or its children are\n",
      "  1272|         0|            0|            0|  0.00%|        guaranteed to exist in `state_dict`. The hooks may modify `state_dict`\n",
      "  1273|         0|            0|            0|  0.00%|        inplace or return a new one.\n",
      "  1274|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1275|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._state_dict_hooks)\n",
      "  1276|         0|            0|            0|  0.00%|        self._state_dict_hooks[handle.id] = hook\n",
      "  1277|         0|            0|            0|  0.00%|        return handle\n",
      "  1278|         0|            0|            0|  0.00%|\n",
      "  1279|        26|  5.34058e-05|  2.05407e-06|  0.00%|    def _save_to_state_dict(self, destination, prefix, keep_vars):\n",
      "  1280|         0|            0|            0|  0.00%|        r\"\"\"Saves module state to `destination` dictionary, containing a state\n",
      "  1281|         0|            0|            0|  0.00%|        of the module, but not its descendants. This is called on every\n",
      "  1282|         0|            0|            0|  0.00%|        submodule in :meth:`~torch.nn.Module.state_dict`.\n",
      "  1283|         0|            0|            0|  0.00%|\n",
      "  1284|         0|            0|            0|  0.00%|        In rare cases, subclasses can achieve class-specific behavior by\n",
      "  1285|         0|            0|            0|  0.00%|        overriding this method with custom logic.\n",
      "  1286|         0|            0|            0|  0.00%|\n",
      "  1287|         0|            0|            0|  0.00%|        Args:\n",
      "  1288|         0|            0|            0|  0.00%|            destination (dict): a dict where state will be stored\n",
      "  1289|         0|            0|            0|  0.00%|            prefix (str): the prefix for parameters and buffers used in this\n",
      "  1290|         0|            0|            0|  0.00%|                module\n",
      "  1291|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1292|        50|  0.000114918|  2.29836e-06|  0.00%|        for name, param in self._parameters.items():\n",
      "  1293|        24|   4.3869e-05|  1.82788e-06|  0.00%|            if param is not None:\n",
      "  1294|        24|  9.20296e-05|  3.83457e-06|  0.00%|                destination[prefix + name] = param if keep_vars else param.detach()\n",
      "  1295|        28|   6.4373e-05|  2.29904e-06|  0.00%|        for name, buf in self._buffers.items():\n",
      "  1296|         2|  5.96046e-06|  2.98023e-06|  0.00%|            if buf is not None and name not in self._non_persistent_buffers_set:\n",
      "  1297|         2|  1.81198e-05|  9.05991e-06|  0.00%|                destination[prefix + name] = buf if keep_vars else buf.detach()\n",
      "  1298|        26|  5.05447e-05|  1.94403e-06|  0.00%|        extra_state_key = prefix + _EXTRA_STATE_KEY_SUFFIX\n",
      "  1299|        26|  6.05583e-05|  2.32917e-06|  0.00%|        if getattr(self.__class__, \"get_extra_state\", Module.get_extra_state) is not Module.get_extra_state:\n",
      "  1300|         0|            0|            0|  0.00%|            destination[extra_state_key] = self.get_extra_state()\n",
      "  1301|         0|            0|            0|  0.00%|\n",
      "  1302|         0|            0|            0|  0.00%|    # The user can pass an optional arbitrary mappable object to `state_dict`, in which case `state_dict` returns\n",
      "  1303|         0|            0|            0|  0.00%|    # back that same object. But if they pass nothing, an `OrederedDict` is created and returned.\n",
      "  1304|         0|            0|            0|  0.00%|    T_destination = TypeVar('T_destination', bound=Dict[str, Any])\n",
      "  1305|         0|            0|            0|  0.00%|\n",
      "  1306|         0|            0|            0|  0.00%|    @overload\n",
      "  1307|         0|            0|            0|  0.00%|    def state_dict(self, *, destination: T_destination, prefix: str = ..., keep_vars: bool = ...) -> T_destination:\n",
      "  1308|         0|            0|            0|  0.00%|        ...\n",
      "  1309|         0|            0|            0|  0.00%|\n",
      "  1310|         0|            0|            0|  0.00%|    @overload\n",
      "  1311|         0|            0|            0|  0.00%|    def state_dict(self, *, prefix: str = ..., keep_vars: bool = ...) -> Dict[str, Any]:\n",
      "  1312|         0|            0|            0|  0.00%|        ...\n",
      "  1313|         0|            0|            0|  0.00%|\n",
      "  1314|         0|            0|            0|  0.00%|    # TODO: Change `*args` to `*` and remove the copprespinding warning in docs when BC allows.\n",
      "  1315|         0|            0|            0|  0.00%|    # Also remove the logic for arg parsing together.\n",
      "  1316|        26|  6.31809e-05|  2.43004e-06|  0.00%|    def state_dict(self, *args, destination=None, prefix='', keep_vars=False):\n",
      "  1317|         0|            0|            0|  0.00%|        r\"\"\"Returns a dictionary containing a whole state of the module.\n",
      "  1318|         0|            0|            0|  0.00%|\n",
      "  1319|         0|            0|            0|  0.00%|        Both parameters and persistent buffers (e.g. running averages) are\n",
      "  1320|         0|            0|            0|  0.00%|        included. Keys are corresponding parameter and buffer names.\n",
      "  1321|         0|            0|            0|  0.00%|        Parameters and buffers set to ``None`` are not included.\n",
      "  1322|         0|            0|            0|  0.00%|\n",
      "  1323|         0|            0|            0|  0.00%|        .. warning::\n",
      "  1324|         0|            0|            0|  0.00%|            Currently ``state_dict()`` also accepts positional arguments for\n",
      "  1325|         0|            0|            0|  0.00%|            ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      "  1326|         0|            0|            0|  0.00%|            this is being deprecated and keyword arguments will be enforced in\n",
      "  1327|         0|            0|            0|  0.00%|            future releases.\n",
      "  1328|         0|            0|            0|  0.00%|\n",
      "  1329|         0|            0|            0|  0.00%|        .. warning::\n",
      "  1330|         0|            0|            0|  0.00%|            Please avoid the use of argument ``destination`` as it is not\n",
      "  1331|         0|            0|            0|  0.00%|            designed for end-users.\n",
      "  1332|         0|            0|            0|  0.00%|\n",
      "  1333|         0|            0|            0|  0.00%|        Args:\n",
      "  1334|         0|            0|            0|  0.00%|            destination (dict, optional): If provided, the state of module will\n",
      "  1335|         0|            0|            0|  0.00%|                be updated into the dict and the same object is returned.\n",
      "  1336|         0|            0|            0|  0.00%|                Otherwise, an ``OrderedDict`` will be created and returned.\n",
      "  1337|         0|            0|            0|  0.00%|                Default: ``None``.\n",
      "  1338|         0|            0|            0|  0.00%|            prefix (str, optional): a prefix added to parameter and buffer\n",
      "  1339|         0|            0|            0|  0.00%|                names to compose the keys in state_dict. Default: ``''``.\n",
      "  1340|         0|            0|            0|  0.00%|            keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      "  1341|         0|            0|            0|  0.00%|                returned in the state dict are detached from autograd. If it's\n",
      "  1342|         0|            0|            0|  0.00%|                set to ``True``, detaching will not be performed.\n",
      "  1343|         0|            0|            0|  0.00%|                Default: ``False``.\n",
      "  1344|         0|            0|            0|  0.00%|\n",
      "  1345|         0|            0|            0|  0.00%|        Returns:\n",
      "  1346|         0|            0|            0|  0.00%|            dict:\n",
      "  1347|         0|            0|            0|  0.00%|                a dictionary containing a whole state of the module\n",
      "  1348|         0|            0|            0|  0.00%|\n",
      "  1349|         0|            0|            0|  0.00%|        Example::\n",
      "  1350|         0|            0|            0|  0.00%|\n",
      "  1351|         0|            0|            0|  0.00%|            >>> module.state_dict().keys()\n",
      "  1352|         0|            0|            0|  0.00%|            ['bias', 'weight']\n",
      "  1353|         0|            0|            0|  0.00%|\n",
      "  1354|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1355|         0|            0|            0|  0.00%|\n",
      "  1356|         0|            0|            0|  0.00%|        # TODO: Remove `args` and the parsing logic when BC allows.\n",
      "  1357|        26|  6.31809e-05|  2.43004e-06|  0.00%|        if len(args) > 0:\n",
      "  1358|         0|            0|            0|  0.00%|            if destination is None:\n",
      "  1359|         0|            0|            0|  0.00%|                destination = args[0]\n",
      "  1360|         0|            0|            0|  0.00%|            if len(args) > 1 and prefix == '':\n",
      "  1361|         0|            0|            0|  0.00%|                prefix = args[1]\n",
      "  1362|         0|            0|            0|  0.00%|            if len(args) > 2 and keep_vars is False:\n",
      "  1363|         0|            0|            0|  0.00%|                keep_vars = args[2]\n",
      "  1364|         0|            0|            0|  0.00%|            # DeprecationWarning is ignored by default\n",
      "  1365|         0|            0|            0|  0.00%|            warnings.warn(\n",
      "  1366|         0|            0|            0|  0.00%|                \"Positional args are being deprecated, use kwargs instead. Refer to \"\n",
      "  1367|         0|            0|            0|  0.00%|                \"https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict\"\n",
      "  1368|         0|            0|            0|  0.00%|                \" for details.\")\n",
      "  1369|         0|            0|            0|  0.00%|\n",
      "  1370|        26|  7.34329e-05|  2.82434e-06|  0.00%|        if destination is None:\n",
      "  1371|         2|  5.72205e-06|  2.86102e-06|  0.00%|            destination = OrderedDict()\n",
      "  1372|         2|   6.4373e-06|  3.21865e-06|  0.00%|            destination._metadata = OrderedDict()\n",
      "  1373|         0|            0|            0|  0.00%|\n",
      "  1374|        26|  7.05719e-05|   2.7143e-06|  0.00%|        local_metadata = dict(version=self._version)\n",
      "  1375|        26|  5.96046e-05|  2.29249e-06|  0.00%|        if hasattr(destination, \"_metadata\"):\n",
      "  1376|        26|  7.05719e-05|   2.7143e-06|  0.00%|            destination._metadata[prefix[:-1]] = local_metadata\n",
      "  1377|         0|            0|            0|  0.00%|\n",
      "  1378|        26|  0.000167608|  6.44647e-06|  0.00%|        self._save_to_state_dict(destination, prefix, keep_vars)\n",
      "(call)|        26|  0.000503778|  1.93761e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1279 _save_to_state_dict\n",
      "  1379|        50|  0.000115633|  2.31266e-06|  0.00%|        for name, module in self._modules.items():\n",
      "  1380|        24|  4.79221e-05|  1.99676e-06|  0.00%|            if module is not None:\n",
      "  1381|        24|   0.00016427|   6.8446e-06|  0.00%|                module.state_dict(destination=destination, prefix=prefix + name + '.', keep_vars=keep_vars)\n",
      "(call)|        24|   0.00132465|  5.51939e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1316 state_dict\n",
      "  1382|        26|  6.10352e-05|  2.34751e-06|  0.00%|        for hook in self._state_dict_hooks.values():\n",
      "  1383|         0|            0|            0|  0.00%|            hook_result = hook(self, destination, prefix, local_metadata)\n",
      "  1384|         0|            0|            0|  0.00%|            if hook_result is not None:\n",
      "  1385|         0|            0|            0|  0.00%|                destination = hook_result\n",
      "  1386|        26|  5.24521e-05|  2.01739e-06|  0.00%|        return destination\n",
      "  1387|         0|            0|            0|  0.00%|\n",
      "  1388|         0|            0|            0|  0.00%|    def _register_load_state_dict_pre_hook(self, hook, with_module=False):\n",
      "  1389|         0|            0|            0|  0.00%|        r\"\"\"These hooks will be called with arguments: `state_dict`, `prefix`,\n",
      "  1390|         0|            0|            0|  0.00%|        `local_metadata`, `strict`, `missing_keys`, `unexpected_keys`,\n",
      "  1391|         0|            0|            0|  0.00%|        `error_msgs`, before loading `state_dict` into `self`. These arguments\n",
      "  1392|         0|            0|            0|  0.00%|        are exactly the same as those of `_load_from_state_dict`.\n",
      "  1393|         0|            0|            0|  0.00%|\n",
      "  1394|         0|            0|            0|  0.00%|        If ``with_module`` is ``True``, then the first argument to the hook is\n",
      "  1395|         0|            0|            0|  0.00%|        an instance of the module.\n",
      "  1396|         0|            0|            0|  0.00%|\n",
      "  1397|         0|            0|            0|  0.00%|        Arguments:\n",
      "  1398|         0|            0|            0|  0.00%|            hook (Callable): Callable hook that will be invoked before\n",
      "  1399|         0|            0|            0|  0.00%|                loading the state dict.\n",
      "  1400|         0|            0|            0|  0.00%|            with_module (bool, optional): Whether or not to pass the module\n",
      "  1401|         0|            0|            0|  0.00%|                instance to the hook as the first parameter.\n",
      "  1402|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1403|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._load_state_dict_pre_hooks)\n",
      "  1404|         0|            0|            0|  0.00%|        if with_module:\n",
      "  1405|         0|            0|            0|  0.00%|            hook = functools.partial(hook, self)\n",
      "  1406|         0|            0|            0|  0.00%|        self._load_state_dict_pre_hooks[handle.id] = hook\n",
      "  1407|         0|            0|            0|  0.00%|        return handle\n",
      "  1408|         0|            0|            0|  0.00%|\n",
      "  1409|         0|            0|            0|  0.00%|    def register_load_state_dict_post_hook(self, hook):\n",
      "  1410|         0|            0|            0|  0.00%|        r\"\"\"Registers a post hook to be run after module's ``load_state_dict``\n",
      "  1411|         0|            0|            0|  0.00%|        is called.\n",
      "  1412|         0|            0|            0|  0.00%|\n",
      "  1413|         0|            0|            0|  0.00%|        It should have the following signature::\n",
      "  1414|         0|            0|            0|  0.00%|            hook(module, incompatible_keys) -> None\n",
      "  1415|         0|            0|            0|  0.00%|\n",
      "  1416|         0|            0|            0|  0.00%|        The ``module`` argument is the current module that this hook is registered\n",
      "  1417|         0|            0|            0|  0.00%|        on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      "  1418|         0|            0|            0|  0.00%|        of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      "  1419|         0|            0|            0|  0.00%|        is a ``list`` of ``str`` containing the missing keys and\n",
      "  1420|         0|            0|            0|  0.00%|        ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      "  1421|         0|            0|            0|  0.00%|\n",
      "  1422|         0|            0|            0|  0.00%|        The given incompatible_keys can be modified inplace if needed.\n",
      "  1423|         0|            0|            0|  0.00%|\n",
      "  1424|         0|            0|            0|  0.00%|        Note that the checks performed when calling :func:`load_state_dict` with\n",
      "  1425|         0|            0|            0|  0.00%|        ``strict=True`` are affected by modifications the hook makes to\n",
      "  1426|         0|            0|            0|  0.00%|        ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      "  1427|         0|            0|            0|  0.00%|        set of keys will result in an error being thrown when ``strict=True``, and\n",
      "  1428|         0|            0|            0|  0.00%|        clearning out both missing and unexpected keys will avoid an error.\n",
      "  1429|         0|            0|            0|  0.00%|\n",
      "  1430|         0|            0|            0|  0.00%|        Returns:\n",
      "  1431|         0|            0|            0|  0.00%|            :class:`torch.utils.hooks.RemovableHandle`:\n",
      "  1432|         0|            0|            0|  0.00%|                a handle that can be used to remove the added hook by calling\n",
      "  1433|         0|            0|            0|  0.00%|                ``handle.remove()``\n",
      "  1434|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1435|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._load_state_dict_post_hooks)\n",
      "  1436|         0|            0|            0|  0.00%|        self._load_state_dict_post_hooks[handle.id] = hook\n",
      "  1437|         0|            0|            0|  0.00%|        return handle\n",
      "  1438|         0|            0|            0|  0.00%|\n",
      "  1439|         0|            0|            0|  0.00%|\n",
      "  1440|         0|            0|            0|  0.00%|    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
      "  1441|         0|            0|            0|  0.00%|                              missing_keys, unexpected_keys, error_msgs):\n",
      "  1442|         0|            0|            0|  0.00%|        r\"\"\"Copies parameters and buffers from :attr:`state_dict` into only\n",
      "  1443|         0|            0|            0|  0.00%|        this module, but not its descendants. This is called on every submodule\n",
      "  1444|         0|            0|            0|  0.00%|        in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n",
      "  1445|         0|            0|            0|  0.00%|        module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\n",
      "  1446|         0|            0|            0|  0.00%|        For state dicts without metadata, :attr:`local_metadata` is empty.\n",
      "  1447|         0|            0|            0|  0.00%|        Subclasses can achieve class-specific backward compatible loading using\n",
      "  1448|         0|            0|            0|  0.00%|        the version number at `local_metadata.get(\"version\", None)`.\n",
      "  1449|         0|            0|            0|  0.00%|\n",
      "  1450|         0|            0|            0|  0.00%|        .. note::\n",
      "  1451|         0|            0|            0|  0.00%|            :attr:`state_dict` is not the same object as the input\n",
      "  1452|         0|            0|            0|  0.00%|            :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n",
      "  1453|         0|            0|            0|  0.00%|            it can be modified.\n",
      "  1454|         0|            0|            0|  0.00%|\n",
      "  1455|         0|            0|            0|  0.00%|        Args:\n",
      "  1456|         0|            0|            0|  0.00%|            state_dict (dict): a dict containing parameters and\n",
      "  1457|         0|            0|            0|  0.00%|                persistent buffers.\n",
      "  1458|         0|            0|            0|  0.00%|            prefix (str): the prefix for parameters and buffers used in this\n",
      "  1459|         0|            0|            0|  0.00%|                module\n",
      "  1460|         0|            0|            0|  0.00%|            local_metadata (dict): a dict containing the metadata for this module.\n",
      "  1461|         0|            0|            0|  0.00%|                See\n",
      "  1462|         0|            0|            0|  0.00%|            strict (bool): whether to strictly enforce that the keys in\n",
      "  1463|         0|            0|            0|  0.00%|                :attr:`state_dict` with :attr:`prefix` match the names of\n",
      "  1464|         0|            0|            0|  0.00%|                parameters and buffers in this module\n",
      "  1465|         0|            0|            0|  0.00%|            missing_keys (list of str): if ``strict=True``, add missing keys to\n",
      "  1466|         0|            0|            0|  0.00%|                this list\n",
      "  1467|         0|            0|            0|  0.00%|            unexpected_keys (list of str): if ``strict=True``, add unexpected\n",
      "  1468|         0|            0|            0|  0.00%|                keys to this list\n",
      "  1469|         0|            0|            0|  0.00%|            error_msgs (list of str): error messages should be added to this\n",
      "  1470|         0|            0|            0|  0.00%|                list, and will be reported together in\n",
      "  1471|         0|            0|            0|  0.00%|                :meth:`~torch.nn.Module.load_state_dict`\n",
      "  1472|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1473|         0|            0|            0|  0.00%|        for hook in self._load_state_dict_pre_hooks.values():\n",
      "  1474|         0|            0|            0|  0.00%|            hook(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\n",
      "  1475|         0|            0|            0|  0.00%|\n",
      "  1476|         0|            0|            0|  0.00%|        persistent_buffers = {k: v for k, v in self._buffers.items() if k not in self._non_persistent_buffers_set}\n",
      "  1477|         0|            0|            0|  0.00%|        local_name_params = itertools.chain(self._parameters.items(), persistent_buffers.items())\n",
      "  1478|         0|            0|            0|  0.00%|        local_state = {k: v for k, v in local_name_params if v is not None}\n",
      "  1479|         0|            0|            0|  0.00%|\n",
      "  1480|         0|            0|            0|  0.00%|        for name, param in local_state.items():\n",
      "  1481|         0|            0|            0|  0.00%|            key = prefix + name\n",
      "  1482|         0|            0|            0|  0.00%|            if key in state_dict:\n",
      "  1483|         0|            0|            0|  0.00%|                input_param = state_dict[key]\n",
      "  1484|         0|            0|            0|  0.00%|                if not torch.overrides.is_tensor_like(input_param):\n",
      "  1485|         0|            0|            0|  0.00%|                    error_msgs.append('While copying the parameter named \"{}\", '\n",
      "  1486|         0|            0|            0|  0.00%|                                      'expected torch.Tensor or Tensor-like object from checkpoint but '\n",
      "  1487|         0|            0|            0|  0.00%|                                      'received {}'\n",
      "  1488|         0|            0|            0|  0.00%|                                      .format(key, type(input_param)))\n",
      "  1489|         0|            0|            0|  0.00%|                    continue\n",
      "  1490|         0|            0|            0|  0.00%|\n",
      "  1491|         0|            0|            0|  0.00%|                # This is used to avoid copying uninitialized parameters into\n",
      "  1492|         0|            0|            0|  0.00%|                # non-lazy modules, since they dont have the hook to do the checks\n",
      "  1493|         0|            0|            0|  0.00%|                # in such case, it will error when accessing the .shape attribute.\n",
      "  1494|         0|            0|            0|  0.00%|                is_param_lazy = torch.nn.parameter.is_lazy(param)\n",
      "  1495|         0|            0|            0|  0.00%|                # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n",
      "  1496|         0|            0|            0|  0.00%|                if not is_param_lazy and len(param.shape) == 0 and len(input_param.shape) == 1:\n",
      "  1497|         0|            0|            0|  0.00%|                    input_param = input_param[0]\n",
      "  1498|         0|            0|            0|  0.00%|\n",
      "  1499|         0|            0|            0|  0.00%|                if not is_param_lazy and input_param.shape != param.shape:\n",
      "  1500|         0|            0|            0|  0.00%|                    # local shape should match the one in checkpoint\n",
      "  1501|         0|            0|            0|  0.00%|                    error_msgs.append('size mismatch for {}: copying a param with shape {} from checkpoint, '\n",
      "  1502|         0|            0|            0|  0.00%|                                      'the shape in current model is {}.'\n",
      "  1503|         0|            0|            0|  0.00%|                                      .format(key, input_param.shape, param.shape))\n",
      "  1504|         0|            0|            0|  0.00%|                    continue\n",
      "  1505|         0|            0|            0|  0.00%|                try:\n",
      "  1506|         0|            0|            0|  0.00%|                    with torch.no_grad():\n",
      "  1507|         0|            0|            0|  0.00%|                        param.copy_(input_param)\n",
      "  1508|         0|            0|            0|  0.00%|                except Exception as ex:\n",
      "  1509|         0|            0|            0|  0.00%|                    error_msgs.append('While copying the parameter named \"{}\", '\n",
      "  1510|         0|            0|            0|  0.00%|                                      'whose dimensions in the model are {} and '\n",
      "  1511|         0|            0|            0|  0.00%|                                      'whose dimensions in the checkpoint are {}, '\n",
      "  1512|         0|            0|            0|  0.00%|                                      'an exception occurred : {}.'\n",
      "  1513|         0|            0|            0|  0.00%|                                      .format(key, param.size(), input_param.size(), ex.args))\n",
      "  1514|         0|            0|            0|  0.00%|            elif strict:\n",
      "  1515|         0|            0|            0|  0.00%|                missing_keys.append(key)\n",
      "  1516|         0|            0|            0|  0.00%|\n",
      "  1517|         0|            0|            0|  0.00%|        extra_state_key = prefix + _EXTRA_STATE_KEY_SUFFIX\n",
      "  1518|         0|            0|            0|  0.00%|        if getattr(self.__class__, \"set_extra_state\", Module.set_extra_state) is not Module.set_extra_state:\n",
      "  1519|         0|            0|            0|  0.00%|            if extra_state_key in state_dict:\n",
      "  1520|         0|            0|            0|  0.00%|                self.set_extra_state(state_dict[extra_state_key])\n",
      "  1521|         0|            0|            0|  0.00%|            elif strict:\n",
      "  1522|         0|            0|            0|  0.00%|                missing_keys.append(extra_state_key)\n",
      "  1523|         0|            0|            0|  0.00%|        elif strict and (extra_state_key in state_dict):\n",
      "  1524|         0|            0|            0|  0.00%|            unexpected_keys.append(extra_state_key)\n",
      "  1525|         0|            0|            0|  0.00%|\n",
      "  1526|         0|            0|            0|  0.00%|        if strict:\n",
      "  1527|         0|            0|            0|  0.00%|            for key in state_dict.keys():\n",
      "  1528|         0|            0|            0|  0.00%|                if key.startswith(prefix) and key != extra_state_key:\n",
      "  1529|         0|            0|            0|  0.00%|                    input_name = key[len(prefix):]\n",
      "  1530|         0|            0|            0|  0.00%|                    input_name = input_name.split('.', 1)[0]  # get the name of param/buffer/child\n",
      "  1531|         0|            0|            0|  0.00%|                    if input_name not in self._modules and input_name not in local_state:\n",
      "  1532|         0|            0|            0|  0.00%|                        unexpected_keys.append(key)\n",
      "  1533|         0|            0|            0|  0.00%|\n",
      "  1534|         0|            0|            0|  0.00%|    def load_state_dict(self, state_dict: Mapping[str, Any],\n",
      "  1535|         0|            0|            0|  0.00%|                        strict: bool = True):\n",
      "  1536|         0|            0|            0|  0.00%|        r\"\"\"Copies parameters and buffers from :attr:`state_dict` into\n",
      "  1537|         0|            0|            0|  0.00%|        this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "  1538|         0|            0|            0|  0.00%|        the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "  1539|         0|            0|            0|  0.00%|        by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "  1540|         0|            0|            0|  0.00%|\n",
      "  1541|         0|            0|            0|  0.00%|        Args:\n",
      "  1542|         0|            0|            0|  0.00%|            state_dict (dict): a dict containing parameters and\n",
      "  1543|         0|            0|            0|  0.00%|                persistent buffers.\n",
      "  1544|         0|            0|            0|  0.00%|            strict (bool, optional): whether to strictly enforce that the keys\n",
      "  1545|         0|            0|            0|  0.00%|                in :attr:`state_dict` match the keys returned by this module's\n",
      "  1546|         0|            0|            0|  0.00%|                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "  1547|         0|            0|            0|  0.00%|\n",
      "  1548|         0|            0|            0|  0.00%|        Returns:\n",
      "  1549|         0|            0|            0|  0.00%|            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "  1550|         0|            0|            0|  0.00%|                * **missing_keys** is a list of str containing the missing keys\n",
      "  1551|         0|            0|            0|  0.00%|                * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "  1552|         0|            0|            0|  0.00%|\n",
      "  1553|         0|            0|            0|  0.00%|        Note:\n",
      "  1554|         0|            0|            0|  0.00%|            If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "  1555|         0|            0|            0|  0.00%|            exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "  1556|         0|            0|            0|  0.00%|            ``RuntimeError``.\n",
      "  1557|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1558|         0|            0|            0|  0.00%|        if not isinstance(state_dict, Mapping):\n",
      "  1559|         0|            0|            0|  0.00%|            raise TypeError(\"Expected state_dict to be dict-like, got {}.\".format(type(state_dict)))\n",
      "  1560|         0|            0|            0|  0.00%|\n",
      "  1561|         0|            0|            0|  0.00%|        missing_keys: List[str] = []\n",
      "  1562|         0|            0|            0|  0.00%|        unexpected_keys: List[str] = []\n",
      "  1563|         0|            0|            0|  0.00%|        error_msgs: List[str] = []\n",
      "  1564|         0|            0|            0|  0.00%|\n",
      "  1565|         0|            0|            0|  0.00%|        # copy state_dict so _load_from_state_dict can modify it\n",
      "  1566|         0|            0|            0|  0.00%|        metadata = getattr(state_dict, '_metadata', None)\n",
      "  1567|         0|            0|            0|  0.00%|        state_dict = OrderedDict(state_dict)\n",
      "  1568|         0|            0|            0|  0.00%|        if metadata is not None:\n",
      "  1569|         0|            0|            0|  0.00%|            # mypy isn't aware that \"_metadata\" exists in state_dict\n",
      "  1570|         0|            0|            0|  0.00%|            state_dict._metadata = metadata  # type: ignore[attr-defined]\n",
      "  1571|         0|            0|            0|  0.00%|\n",
      "  1572|         0|            0|            0|  0.00%|        def load(module, prefix=''):\n",
      "  1573|         0|            0|            0|  0.00%|            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
      "  1574|         0|            0|            0|  0.00%|            module._load_from_state_dict(\n",
      "  1575|         0|            0|            0|  0.00%|                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
      "  1576|         0|            0|            0|  0.00%|            for name, child in module._modules.items():\n",
      "  1577|         0|            0|            0|  0.00%|                if child is not None:\n",
      "  1578|         0|            0|            0|  0.00%|                    load(child, prefix + name + '.')\n",
      "  1579|         0|            0|            0|  0.00%|\n",
      "  1580|         0|            0|            0|  0.00%|            # Note that the hook can modify missing_keys and unexpected_keys.\n",
      "  1581|         0|            0|            0|  0.00%|            incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "  1582|         0|            0|            0|  0.00%|            for hook in module._load_state_dict_post_hooks.values():\n",
      "  1583|         0|            0|            0|  0.00%|                out = hook(module, incompatible_keys)\n",
      "  1584|         0|            0|            0|  0.00%|                assert out is None, (\n",
      "  1585|         0|            0|            0|  0.00%|                    \"Hooks registered with ``register_load_state_dict_post_hook`` are not\"\n",
      "  1586|         0|            0|            0|  0.00%|                    \"expected to return new values, if incompatible_keys need to be modified,\"\n",
      "  1587|         0|            0|            0|  0.00%|                    \"it should be done inplace.\"\n",
      "  1588|         0|            0|            0|  0.00%|                )\n",
      "  1589|         0|            0|            0|  0.00%|\n",
      "  1590|         0|            0|            0|  0.00%|        load(self)\n",
      "  1591|         0|            0|            0|  0.00%|        del load\n",
      "  1592|         0|            0|            0|  0.00%|\n",
      "  1593|         0|            0|            0|  0.00%|        if strict:\n",
      "  1594|         0|            0|            0|  0.00%|            if len(unexpected_keys) > 0:\n",
      "  1595|         0|            0|            0|  0.00%|                error_msgs.insert(\n",
      "  1596|         0|            0|            0|  0.00%|                    0, 'Unexpected key(s) in state_dict: {}. '.format(\n",
      "  1597|         0|            0|            0|  0.00%|                        ', '.join('\"{}\"'.format(k) for k in unexpected_keys)))\n",
      "  1598|         0|            0|            0|  0.00%|            if len(missing_keys) > 0:\n",
      "  1599|         0|            0|            0|  0.00%|                error_msgs.insert(\n",
      "  1600|         0|            0|            0|  0.00%|                    0, 'Missing key(s) in state_dict: {}. '.format(\n",
      "  1601|         0|            0|            0|  0.00%|                        ', '.join('\"{}\"'.format(k) for k in missing_keys)))\n",
      "  1602|         0|            0|            0|  0.00%|\n",
      "  1603|         0|            0|            0|  0.00%|        if len(error_msgs) > 0:\n",
      "  1604|         0|            0|            0|  0.00%|            raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "  1605|         0|            0|            0|  0.00%|                               self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
      "  1606|         0|            0|            0|  0.00%|        return _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "  1607|         0|            0|            0|  0.00%|\n",
      "  1608|      6240|    0.0203526|  3.26163e-06|  0.00%|    def _named_members(self, get_members_fn, prefix='', recurse=True):\n",
      "  1609|         0|            0|            0|  0.00%|        r\"\"\"Helper method for yielding various names + members of modules.\"\"\"\n",
      "  1610|      6240|    0.0266843|  4.27633e-06|  0.00%|        memo = set()\n",
      "  1611|      6240|    0.0263956|  4.23006e-06|  0.00%|        modules = self.named_modules(prefix=prefix) if recurse else [(prefix, self)]\n",
      "  1612|     93600|     0.539039|  5.75897e-06|  0.09%|        for module_prefix, module in modules:\n",
      "(call)|     93600|      4.53085|  4.84065e-05|  0.73%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1775 named_modules\n",
      "  1613|     87360|     0.475429|  5.44219e-06|  0.08%|            members = get_members_fn(module)\n",
      "(call)|     87360|     0.272485|  3.11911e-06|  0.04%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1666 <lambda>\n",
      "  1614|    162240|      0.34037|  2.09794e-06|  0.05%|            for k, v in members:\n",
      "  1615|     74880|     0.398759|  5.32531e-06|  0.06%|                if v is None or v in memo:\n",
      "(call)|     74880|     0.332917|    4.446e-06|  0.05%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:731 __hash__\n",
      "  1616|         0|            0|            0|  0.00%|                    continue\n",
      "  1617|     74880|     0.398477|  5.32155e-06|  0.06%|                memo.add(v)\n",
      "(call)|     74880|     0.322784|  4.31069e-06|  0.05%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:731 __hash__\n",
      "  1618|     74880|     0.165941|   2.2161e-06|  0.03%|                name = module_prefix + ('.' if module_prefix else '') + k\n",
      "  1619|    149760|     0.262988|  1.75607e-06|  0.04%|                yield name, v\n",
      "  1620|         0|            0|            0|  0.00%|\n",
      "  1621|      6240|    0.0140803|  2.25646e-06|  0.00%|    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n",
      "  1622|         0|            0|            0|  0.00%|        r\"\"\"Returns an iterator over module parameters.\n",
      "  1623|         0|            0|            0|  0.00%|\n",
      "  1624|         0|            0|            0|  0.00%|        This is typically passed to an optimizer.\n",
      "  1625|         0|            0|            0|  0.00%|\n",
      "  1626|         0|            0|            0|  0.00%|        Args:\n",
      "  1627|         0|            0|            0|  0.00%|            recurse (bool): if True, then yields parameters of this module\n",
      "  1628|         0|            0|            0|  0.00%|                and all submodules. Otherwise, yields only parameters that\n",
      "  1629|         0|            0|            0|  0.00%|                are direct members of this module.\n",
      "  1630|         0|            0|            0|  0.00%|\n",
      "  1631|         0|            0|            0|  0.00%|        Yields:\n",
      "  1632|         0|            0|            0|  0.00%|            Parameter: module parameter\n",
      "  1633|         0|            0|            0|  0.00%|\n",
      "  1634|         0|            0|            0|  0.00%|        Example::\n",
      "  1635|         0|            0|            0|  0.00%|\n",
      "  1636|         0|            0|            0|  0.00%|            >>> for param in model.parameters():\n",
      "  1637|         0|            0|            0|  0.00%|            >>>     print(type(param), param.size())\n",
      "  1638|         0|            0|            0|  0.00%|            <class 'torch.Tensor'> (20L,)\n",
      "  1639|         0|            0|            0|  0.00%|            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "  1640|         0|            0|            0|  0.00%|\n",
      "  1641|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1642|     81120|     0.422234|  5.20506e-06|  0.07%|        for name, param in self.named_parameters(recurse=recurse):\n",
      "(call)|     81120|      8.87103|  0.000109357|  1.43%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1645 named_parameters\n",
      "  1643|    149760|     0.219616|  1.46645e-06|  0.04%|            yield param\n",
      "  1644|         0|            0|            0|  0.00%|\n",
      "  1645|      6240|    0.0149534|  2.39637e-06|  0.00%|    def named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, Parameter]]:\n",
      "  1646|         0|            0|            0|  0.00%|        r\"\"\"Returns an iterator over module parameters, yielding both the\n",
      "  1647|         0|            0|            0|  0.00%|        name of the parameter as well as the parameter itself.\n",
      "  1648|         0|            0|            0|  0.00%|\n",
      "  1649|         0|            0|            0|  0.00%|        Args:\n",
      "  1650|         0|            0|            0|  0.00%|            prefix (str): prefix to prepend to all parameter names.\n",
      "  1651|         0|            0|            0|  0.00%|            recurse (bool): if True, then yields parameters of this module\n",
      "  1652|         0|            0|            0|  0.00%|                and all submodules. Otherwise, yields only parameters that\n",
      "  1653|         0|            0|            0|  0.00%|                are direct members of this module.\n",
      "  1654|         0|            0|            0|  0.00%|\n",
      "  1655|         0|            0|            0|  0.00%|        Yields:\n",
      "  1656|         0|            0|            0|  0.00%|            (string, Parameter): Tuple containing the name and parameter\n",
      "  1657|         0|            0|            0|  0.00%|\n",
      "  1658|         0|            0|            0|  0.00%|        Example::\n",
      "  1659|         0|            0|            0|  0.00%|\n",
      "  1660|         0|            0|            0|  0.00%|            >>> for name, param in self.named_parameters():\n",
      "  1661|         0|            0|            0|  0.00%|            >>>    if name in ['bias']:\n",
      "  1662|         0|            0|            0|  0.00%|            >>>        print(param.size())\n",
      "  1663|         0|            0|            0|  0.00%|\n",
      "  1664|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1665|     12480|     0.033555|   2.6887e-06|  0.01%|        gen = self._named_members(\n",
      "  1666|    180960|     0.288288|   1.5931e-06|  0.05%|            lambda module: module._parameters.items(),\n",
      "  1667|      6240|    0.0142417|  2.28232e-06|  0.00%|            prefix=prefix, recurse=recurse)\n",
      "  1668|     81120|     0.448949|  5.53438e-06|  0.07%|        for elem in gen:\n",
      "(call)|     81120|      8.11347|  0.000100018|  1.31%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1608 _named_members\n",
      "  1669|    149760|     0.230056|  1.53616e-06|  0.04%|            yield elem\n",
      "  1670|         0|            0|            0|  0.00%|\n",
      "  1671|         0|            0|            0|  0.00%|    def buffers(self, recurse: bool = True) -> Iterator[Tensor]:\n",
      "  1672|         0|            0|            0|  0.00%|        r\"\"\"Returns an iterator over module buffers.\n",
      "  1673|         0|            0|            0|  0.00%|\n",
      "  1674|         0|            0|            0|  0.00%|        Args:\n",
      "  1675|         0|            0|            0|  0.00%|            recurse (bool): if True, then yields buffers of this module\n",
      "  1676|         0|            0|            0|  0.00%|                and all submodules. Otherwise, yields only buffers that\n",
      "  1677|         0|            0|            0|  0.00%|                are direct members of this module.\n",
      "  1678|         0|            0|            0|  0.00%|\n",
      "  1679|         0|            0|            0|  0.00%|        Yields:\n",
      "  1680|         0|            0|            0|  0.00%|            torch.Tensor: module buffer\n",
      "  1681|         0|            0|            0|  0.00%|\n",
      "  1682|         0|            0|            0|  0.00%|        Example::\n",
      "  1683|         0|            0|            0|  0.00%|\n",
      "  1684|         0|            0|            0|  0.00%|            >>> for buf in model.buffers():\n",
      "  1685|         0|            0|            0|  0.00%|            >>>     print(type(buf), buf.size())\n",
      "  1686|         0|            0|            0|  0.00%|            <class 'torch.Tensor'> (20L,)\n",
      "  1687|         0|            0|            0|  0.00%|            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "  1688|         0|            0|            0|  0.00%|\n",
      "  1689|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1690|         0|            0|            0|  0.00%|        for _, buf in self.named_buffers(recurse=recurse):\n",
      "  1691|         0|            0|            0|  0.00%|            yield buf\n",
      "  1692|         0|            0|            0|  0.00%|\n",
      "  1693|         0|            0|            0|  0.00%|    def named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, Tensor]]:\n",
      "  1694|         0|            0|            0|  0.00%|        r\"\"\"Returns an iterator over module buffers, yielding both the\n",
      "  1695|         0|            0|            0|  0.00%|        name of the buffer as well as the buffer itself.\n",
      "  1696|         0|            0|            0|  0.00%|\n",
      "  1697|         0|            0|            0|  0.00%|        Args:\n",
      "  1698|         0|            0|            0|  0.00%|            prefix (str): prefix to prepend to all buffer names.\n",
      "  1699|         0|            0|            0|  0.00%|            recurse (bool): if True, then yields buffers of this module\n",
      "  1700|         0|            0|            0|  0.00%|                and all submodules. Otherwise, yields only buffers that\n",
      "  1701|         0|            0|            0|  0.00%|                are direct members of this module.\n",
      "  1702|         0|            0|            0|  0.00%|\n",
      "  1703|         0|            0|            0|  0.00%|        Yields:\n",
      "  1704|         0|            0|            0|  0.00%|            (string, torch.Tensor): Tuple containing the name and buffer\n",
      "  1705|         0|            0|            0|  0.00%|\n",
      "  1706|         0|            0|            0|  0.00%|        Example::\n",
      "  1707|         0|            0|            0|  0.00%|\n",
      "  1708|         0|            0|            0|  0.00%|            >>> for name, buf in self.named_buffers():\n",
      "  1709|         0|            0|            0|  0.00%|            >>>    if name in ['running_var']:\n",
      "  1710|         0|            0|            0|  0.00%|            >>>        print(buf.size())\n",
      "  1711|         0|            0|            0|  0.00%|\n",
      "  1712|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1713|         0|            0|            0|  0.00%|        gen = self._named_members(\n",
      "  1714|         0|            0|            0|  0.00%|            lambda module: module._buffers.items(),\n",
      "  1715|         0|            0|            0|  0.00%|            prefix=prefix, recurse=recurse)\n",
      "  1716|         0|            0|            0|  0.00%|        for elem in gen:\n",
      "  1717|         0|            0|            0|  0.00%|            yield elem\n",
      "  1718|         0|            0|            0|  0.00%|\n",
      "  1719|         0|            0|            0|  0.00%|    def children(self) -> Iterator['Module']:\n",
      "  1720|         0|            0|            0|  0.00%|        r\"\"\"Returns an iterator over immediate children modules.\n",
      "  1721|         0|            0|            0|  0.00%|\n",
      "  1722|         0|            0|            0|  0.00%|        Yields:\n",
      "  1723|         0|            0|            0|  0.00%|            Module: a child module\n",
      "  1724|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1725|         0|            0|            0|  0.00%|        for name, module in self.named_children():\n",
      "  1726|         0|            0|            0|  0.00%|            yield module\n",
      "  1727|         0|            0|            0|  0.00%|\n",
      "  1728|         0|            0|            0|  0.00%|    def named_children(self) -> Iterator[Tuple[str, 'Module']]:\n",
      "  1729|         0|            0|            0|  0.00%|        r\"\"\"Returns an iterator over immediate children modules, yielding both\n",
      "  1730|         0|            0|            0|  0.00%|        the name of the module as well as the module itself.\n",
      "  1731|         0|            0|            0|  0.00%|\n",
      "  1732|         0|            0|            0|  0.00%|        Yields:\n",
      "  1733|         0|            0|            0|  0.00%|            (string, Module): Tuple containing a name and child module\n",
      "  1734|         0|            0|            0|  0.00%|\n",
      "  1735|         0|            0|            0|  0.00%|        Example::\n",
      "  1736|         0|            0|            0|  0.00%|\n",
      "  1737|         0|            0|            0|  0.00%|            >>> for name, module in model.named_children():\n",
      "  1738|         0|            0|            0|  0.00%|            >>>     if name in ['conv4', 'conv5']:\n",
      "  1739|         0|            0|            0|  0.00%|            >>>         print(module)\n",
      "  1740|         0|            0|            0|  0.00%|\n",
      "  1741|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1742|         0|            0|            0|  0.00%|        memo = set()\n",
      "  1743|         0|            0|            0|  0.00%|        for name, module in self._modules.items():\n",
      "  1744|         0|            0|            0|  0.00%|            if module is not None and module not in memo:\n",
      "  1745|         0|            0|            0|  0.00%|                memo.add(module)\n",
      "  1746|         0|            0|            0|  0.00%|                yield name, module\n",
      "  1747|         0|            0|            0|  0.00%|\n",
      "  1748|         0|            0|            0|  0.00%|    def modules(self) -> Iterator['Module']:\n",
      "  1749|         0|            0|            0|  0.00%|        r\"\"\"Returns an iterator over all modules in the network.\n",
      "  1750|         0|            0|            0|  0.00%|\n",
      "  1751|         0|            0|            0|  0.00%|        Yields:\n",
      "  1752|         0|            0|            0|  0.00%|            Module: a module in the network\n",
      "  1753|         0|            0|            0|  0.00%|\n",
      "  1754|         0|            0|            0|  0.00%|        Note:\n",
      "  1755|         0|            0|            0|  0.00%|            Duplicate modules are returned only once. In the following\n",
      "  1756|         0|            0|            0|  0.00%|            example, ``l`` will be returned only once.\n",
      "  1757|         0|            0|            0|  0.00%|\n",
      "  1758|         0|            0|            0|  0.00%|        Example::\n",
      "  1759|         0|            0|            0|  0.00%|\n",
      "  1760|         0|            0|            0|  0.00%|            >>> l = nn.Linear(2, 2)\n",
      "  1761|         0|            0|            0|  0.00%|            >>> net = nn.Sequential(l, l)\n",
      "  1762|         0|            0|            0|  0.00%|            >>> for idx, m in enumerate(net.modules()):\n",
      "  1763|         0|            0|            0|  0.00%|                    print(idx, '->', m)\n",
      "  1764|         0|            0|            0|  0.00%|\n",
      "  1765|         0|            0|            0|  0.00%|            0 -> Sequential(\n",
      "  1766|         0|            0|            0|  0.00%|              (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  1767|         0|            0|            0|  0.00%|              (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  1768|         0|            0|            0|  0.00%|            )\n",
      "  1769|         0|            0|            0|  0.00%|            1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "  1770|         0|            0|            0|  0.00%|\n",
      "  1771|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1772|         0|            0|            0|  0.00%|        for _, module in self.named_modules():\n",
      "  1773|         0|            0|            0|  0.00%|            yield module\n",
      "  1774|         0|            0|            0|  0.00%|\n",
      "  1775|     87360|     0.175365|  2.00738e-06|  0.03%|    def named_modules(self, memo: Optional[Set['Module']] = None, prefix: str = '', remove_duplicate: bool = True):\n",
      "  1776|         0|            0|            0|  0.00%|        r\"\"\"Returns an iterator over all modules in the network, yielding\n",
      "  1777|         0|            0|            0|  0.00%|        both the name of the module as well as the module itself.\n",
      "  1778|         0|            0|            0|  0.00%|\n",
      "  1779|         0|            0|            0|  0.00%|        Args:\n",
      "  1780|         0|            0|            0|  0.00%|            memo: a memo to store the set of modules already added to the result\n",
      "  1781|         0|            0|            0|  0.00%|            prefix: a prefix that will be added to the name of the module\n",
      "  1782|         0|            0|            0|  0.00%|            remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "  1783|         0|            0|            0|  0.00%|                or not\n",
      "  1784|         0|            0|            0|  0.00%|\n",
      "  1785|         0|            0|            0|  0.00%|        Yields:\n",
      "  1786|         0|            0|            0|  0.00%|            (string, Module): Tuple of name and module\n",
      "  1787|         0|            0|            0|  0.00%|\n",
      "  1788|         0|            0|            0|  0.00%|        Note:\n",
      "  1789|         0|            0|            0|  0.00%|            Duplicate modules are returned only once. In the following\n",
      "  1790|         0|            0|            0|  0.00%|            example, ``l`` will be returned only once.\n",
      "  1791|         0|            0|            0|  0.00%|\n",
      "  1792|         0|            0|            0|  0.00%|        Example::\n",
      "  1793|         0|            0|            0|  0.00%|\n",
      "  1794|         0|            0|            0|  0.00%|            >>> l = nn.Linear(2, 2)\n",
      "  1795|         0|            0|            0|  0.00%|            >>> net = nn.Sequential(l, l)\n",
      "  1796|         0|            0|            0|  0.00%|            >>> for idx, m in enumerate(net.named_modules()):\n",
      "  1797|         0|            0|            0|  0.00%|                    print(idx, '->', m)\n",
      "  1798|         0|            0|            0|  0.00%|\n",
      "  1799|         0|            0|            0|  0.00%|            0 -> ('', Sequential(\n",
      "  1800|         0|            0|            0|  0.00%|              (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  1801|         0|            0|            0|  0.00%|              (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  1802|         0|            0|            0|  0.00%|            ))\n",
      "  1803|         0|            0|            0|  0.00%|            1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "  1804|         0|            0|            0|  0.00%|\n",
      "  1805|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1806|         0|            0|            0|  0.00%|\n",
      "  1807|     87360|     0.199743|  2.28644e-06|  0.03%|        if memo is None:\n",
      "  1808|      6240|    0.0181851|  2.91429e-06|  0.00%|            memo = set()\n",
      "  1809|     87360|      0.20093|  2.30003e-06|  0.03%|        if self not in memo:\n",
      "  1810|     87360|     0.191493|  2.19199e-06|  0.03%|            if remove_duplicate:\n",
      "  1811|     87360|     0.208137|  2.38252e-06|  0.03%|                memo.add(self)\n",
      "  1812|    174720|     0.369442|  2.11448e-06|  0.06%|            yield prefix, self\n",
      "  1813|    168480|      0.39329|  2.33434e-06|  0.06%|            for name, module in self._modules.items():\n",
      "  1814|     81120|      0.15038|  1.85379e-06|  0.02%|                if module is None:\n",
      "  1815|         0|            0|            0|  0.00%|                    continue\n",
      "  1816|     81120|     0.172206|  2.12286e-06|  0.03%|                submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "  1817|    299520|      1.75967|  5.87498e-06|  0.28%|                for m in module.named_modules(memo, submodule_prefix, remove_duplicate):\n",
      "(call)|    299520|      3.59684|  1.20087e-05|  0.58%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1775 named_modules\n",
      "  1818|    436800|     0.692003|  1.58426e-06|  0.11%|                    yield m\n",
      "  1819|         0|            0|            0|  0.00%|\n",
      "  1820|         0|            0|            0|  0.00%|    def train(self: T, mode: bool = True) -> T:\n",
      "  1821|         0|            0|            0|  0.00%|        r\"\"\"Sets the module in training mode.\n",
      "  1822|         0|            0|            0|  0.00%|\n",
      "  1823|         0|            0|            0|  0.00%|        This has any effect only on certain modules. See documentations of\n",
      "  1824|         0|            0|            0|  0.00%|        particular modules for details of their behaviors in training/evaluation\n",
      "  1825|         0|            0|            0|  0.00%|        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "  1826|         0|            0|            0|  0.00%|        etc.\n",
      "  1827|         0|            0|            0|  0.00%|\n",
      "  1828|         0|            0|            0|  0.00%|        Args:\n",
      "  1829|         0|            0|            0|  0.00%|            mode (bool): whether to set training mode (``True``) or evaluation\n",
      "  1830|         0|            0|            0|  0.00%|                         mode (``False``). Default: ``True``.\n",
      "  1831|         0|            0|            0|  0.00%|\n",
      "  1832|         0|            0|            0|  0.00%|        Returns:\n",
      "  1833|         0|            0|            0|  0.00%|            Module: self\n",
      "  1834|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1835|         0|            0|            0|  0.00%|        if not isinstance(mode, bool):\n",
      "  1836|         0|            0|            0|  0.00%|            raise ValueError(\"training mode is expected to be boolean\")\n",
      "  1837|         0|            0|            0|  0.00%|        self.training = mode\n",
      "  1838|         0|            0|            0|  0.00%|        for module in self.children():\n",
      "  1839|         0|            0|            0|  0.00%|            module.train(mode)\n",
      "  1840|         0|            0|            0|  0.00%|        return self\n",
      "  1841|         0|            0|            0|  0.00%|\n",
      "  1842|         0|            0|            0|  0.00%|    def eval(self: T) -> T:\n",
      "  1843|         0|            0|            0|  0.00%|        r\"\"\"Sets the module in evaluation mode.\n",
      "  1844|         0|            0|            0|  0.00%|\n",
      "  1845|         0|            0|            0|  0.00%|        This has any effect only on certain modules. See documentations of\n",
      "  1846|         0|            0|            0|  0.00%|        particular modules for details of their behaviors in training/evaluation\n",
      "  1847|         0|            0|            0|  0.00%|        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "  1848|         0|            0|            0|  0.00%|        etc.\n",
      "  1849|         0|            0|            0|  0.00%|\n",
      "  1850|         0|            0|            0|  0.00%|        This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "  1851|         0|            0|            0|  0.00%|\n",
      "  1852|         0|            0|            0|  0.00%|        See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "  1853|         0|            0|            0|  0.00%|        `.eval()` and several similar mechanisms that may be confused with it.\n",
      "  1854|         0|            0|            0|  0.00%|\n",
      "  1855|         0|            0|            0|  0.00%|        Returns:\n",
      "  1856|         0|            0|            0|  0.00%|            Module: self\n",
      "  1857|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1858|         0|            0|            0|  0.00%|        return self.train(False)\n",
      "  1859|         0|            0|            0|  0.00%|\n",
      "  1860|         0|            0|            0|  0.00%|    def requires_grad_(self: T, requires_grad: bool = True) -> T:\n",
      "  1861|         0|            0|            0|  0.00%|        r\"\"\"Change if autograd should record operations on parameters in this\n",
      "  1862|         0|            0|            0|  0.00%|        module.\n",
      "  1863|         0|            0|            0|  0.00%|\n",
      "  1864|         0|            0|            0|  0.00%|        This method sets the parameters' :attr:`requires_grad` attributes\n",
      "  1865|         0|            0|            0|  0.00%|        in-place.\n",
      "  1866|         0|            0|            0|  0.00%|\n",
      "  1867|         0|            0|            0|  0.00%|        This method is helpful for freezing part of the module for finetuning\n",
      "  1868|         0|            0|            0|  0.00%|        or training parts of a model individually (e.g., GAN training).\n",
      "  1869|         0|            0|            0|  0.00%|\n",
      "  1870|         0|            0|            0|  0.00%|        See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "  1871|         0|            0|            0|  0.00%|        `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "  1872|         0|            0|            0|  0.00%|\n",
      "  1873|         0|            0|            0|  0.00%|        Args:\n",
      "  1874|         0|            0|            0|  0.00%|            requires_grad (bool): whether autograd should record operations on\n",
      "  1875|         0|            0|            0|  0.00%|                                  parameters in this module. Default: ``True``.\n",
      "  1876|         0|            0|            0|  0.00%|\n",
      "  1877|         0|            0|            0|  0.00%|        Returns:\n",
      "  1878|         0|            0|            0|  0.00%|            Module: self\n",
      "  1879|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1880|         0|            0|            0|  0.00%|        for p in self.parameters():\n",
      "  1881|         0|            0|            0|  0.00%|            p.requires_grad_(requires_grad)\n",
      "  1882|         0|            0|            0|  0.00%|        return self\n",
      "  1883|         0|            0|            0|  0.00%|\n",
      "  1884|         0|            0|            0|  0.00%|    def zero_grad(self, set_to_none: bool = False) -> None:\n",
      "  1885|         0|            0|            0|  0.00%|        r\"\"\"Sets gradients of all model parameters to zero. See similar function\n",
      "  1886|         0|            0|            0|  0.00%|        under :class:`torch.optim.Optimizer` for more context.\n",
      "  1887|         0|            0|            0|  0.00%|\n",
      "  1888|         0|            0|            0|  0.00%|        Args:\n",
      "  1889|         0|            0|            0|  0.00%|            set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "  1890|         0|            0|            0|  0.00%|                See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "  1891|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1892|         0|            0|            0|  0.00%|        if getattr(self, '_is_replica', False):\n",
      "  1893|         0|            0|            0|  0.00%|            warnings.warn(\n",
      "  1894|         0|            0|            0|  0.00%|                \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \"\n",
      "  1895|         0|            0|            0|  0.00%|                \"The parameters are copied (in a differentiable manner) from the original module. \"\n",
      "  1896|         0|            0|            0|  0.00%|                \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \"\n",
      "  1897|         0|            0|            0|  0.00%|                \"If you need gradients in your forward method, consider using autograd.grad instead.\")\n",
      "  1898|         0|            0|            0|  0.00%|\n",
      "  1899|         0|            0|            0|  0.00%|        for p in self.parameters():\n",
      "  1900|         0|            0|            0|  0.00%|            if p.grad is not None:\n",
      "  1901|         0|            0|            0|  0.00%|                if set_to_none:\n",
      "  1902|         0|            0|            0|  0.00%|                    p.grad = None\n",
      "  1903|         0|            0|            0|  0.00%|                else:\n",
      "  1904|         0|            0|            0|  0.00%|                    if p.grad.grad_fn is not None:\n",
      "  1905|         0|            0|            0|  0.00%|                        p.grad.detach_()\n",
      "  1906|         0|            0|            0|  0.00%|                    else:\n",
      "  1907|         0|            0|            0|  0.00%|                        p.grad.requires_grad_(False)\n",
      "  1908|         0|            0|            0|  0.00%|                    p.grad.zero_()\n",
      "  1909|         0|            0|            0|  0.00%|\n",
      "  1910|         0|            0|            0|  0.00%|    def share_memory(self: T) -> T:\n",
      "  1911|         0|            0|            0|  0.00%|        r\"\"\"See :meth:`torch.Tensor.share_memory_`\"\"\"\n",
      "  1912|         0|            0|            0|  0.00%|        return self._apply(lambda t: t.share_memory_())\n",
      "  1913|         0|            0|            0|  0.00%|\n",
      "  1914|         0|            0|            0|  0.00%|    def _get_name(self):\n",
      "  1915|         0|            0|            0|  0.00%|        return self.__class__.__name__\n",
      "  1916|         0|            0|            0|  0.00%|\n",
      "  1917|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "  1918|         0|            0|            0|  0.00%|        r\"\"\"Set the extra representation of the module\n",
      "  1919|         0|            0|            0|  0.00%|\n",
      "  1920|         0|            0|            0|  0.00%|        To print customized extra information, you should re-implement\n",
      "  1921|         0|            0|            0|  0.00%|        this method in your own modules. Both single-line and multi-line\n",
      "  1922|         0|            0|            0|  0.00%|        strings are acceptable.\n",
      "  1923|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1924|         0|            0|            0|  0.00%|        return ''\n",
      "  1925|         0|            0|            0|  0.00%|\n",
      "  1926|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "  1927|         0|            0|            0|  0.00%|        # We treat the extra repr like the sub-module, one item per line\n",
      "  1928|         0|            0|            0|  0.00%|        extra_lines = []\n",
      "  1929|         0|            0|            0|  0.00%|        extra_repr = self.extra_repr()\n",
      "  1930|         0|            0|            0|  0.00%|        # empty string will be split into list ['']\n",
      "  1931|         0|            0|            0|  0.00%|        if extra_repr:\n",
      "  1932|         0|            0|            0|  0.00%|            extra_lines = extra_repr.split('\\n')\n",
      "  1933|         0|            0|            0|  0.00%|        child_lines = []\n",
      "  1934|         0|            0|            0|  0.00%|        for key, module in self._modules.items():\n",
      "  1935|         0|            0|            0|  0.00%|            mod_str = repr(module)\n",
      "  1936|         0|            0|            0|  0.00%|            mod_str = _addindent(mod_str, 2)\n",
      "  1937|         0|            0|            0|  0.00%|            child_lines.append('(' + key + '): ' + mod_str)\n",
      "  1938|         0|            0|            0|  0.00%|        lines = extra_lines + child_lines\n",
      "  1939|         0|            0|            0|  0.00%|\n",
      "  1940|         0|            0|            0|  0.00%|        main_str = self._get_name() + '('\n",
      "  1941|         0|            0|            0|  0.00%|        if lines:\n",
      "  1942|         0|            0|            0|  0.00%|            # simple one-liner info, which most builtin Modules will use\n",
      "  1943|         0|            0|            0|  0.00%|            if len(extra_lines) == 1 and not child_lines:\n",
      "  1944|         0|            0|            0|  0.00%|                main_str += extra_lines[0]\n",
      "  1945|         0|            0|            0|  0.00%|            else:\n",
      "  1946|         0|            0|            0|  0.00%|                main_str += '\\n  ' + '\\n  '.join(lines) + '\\n'\n",
      "  1947|         0|            0|            0|  0.00%|\n",
      "  1948|         0|            0|            0|  0.00%|        main_str += ')'\n",
      "  1949|         0|            0|            0|  0.00%|        return main_str\n",
      "  1950|         0|            0|            0|  0.00%|\n",
      "  1951|         0|            0|            0|  0.00%|    def __dir__(self):\n",
      "  1952|         0|            0|            0|  0.00%|        module_attrs = dir(self.__class__)\n",
      "  1953|         0|            0|            0|  0.00%|        attrs = list(self.__dict__.keys())\n",
      "  1954|         0|            0|            0|  0.00%|        parameters = list(self._parameters.keys())\n",
      "  1955|         0|            0|            0|  0.00%|        modules = list(self._modules.keys())\n",
      "  1956|         0|            0|            0|  0.00%|        buffers = list(self._buffers.keys())\n",
      "  1957|         0|            0|            0|  0.00%|        keys = module_attrs + attrs + parameters + modules + buffers\n",
      "  1958|         0|            0|            0|  0.00%|\n",
      "  1959|         0|            0|            0|  0.00%|        # Eliminate attrs that are not legal Python variable names\n",
      "  1960|         0|            0|            0|  0.00%|        keys = [key for key in keys if not key[0].isdigit()]\n",
      "  1961|         0|            0|            0|  0.00%|\n",
      "  1962|         0|            0|            0|  0.00%|        return sorted(keys)\n",
      "  1963|         0|            0|            0|  0.00%|\n",
      "  1964|         0|            0|            0|  0.00%|    def _replicate_for_data_parallel(self):\n",
      "  1965|         0|            0|            0|  0.00%|        replica = self.__new__(type(self))\n",
      "  1966|         0|            0|            0|  0.00%|        replica.__dict__ = self.__dict__.copy()\n",
      "  1967|         0|            0|            0|  0.00%|\n",
      "  1968|         0|            0|            0|  0.00%|        # replicas do not have parameters themselves, the replicas reference the original\n",
      "  1969|         0|            0|            0|  0.00%|        # module.\n",
      "  1970|         0|            0|            0|  0.00%|        replica._parameters = OrderedDict()\n",
      "  1971|         0|            0|            0|  0.00%|        replica._buffers = replica._buffers.copy()\n",
      "  1972|         0|            0|            0|  0.00%|        replica._modules = replica._modules.copy()\n",
      "  1973|         0|            0|            0|  0.00%|        replica._is_replica = True  # type: ignore[assignment]\n",
      "  1974|         0|            0|            0|  0.00%|\n",
      "  1975|         0|            0|            0|  0.00%|        return replica\n",
      "File: /apps/open_spiel/open_spiel/python/games/clock_auction_bidders.py\n",
      "File duration: 25.6057s (4.12%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|import numpy as np\n",
      "     2|         0|            0|            0|  0.00%|\n",
      "     3|         0|            0|            0|  0.00%|class Bidder:\n",
      "     4|         0|            0|            0|  0.00%|\n",
      "     5|         0|            0|            0|  0.00%|  def __init__(self, values, budget, pricing_bonus, all_bids, drop_out_heuristic) -> None:\n",
      "     6|         0|            0|            0|  0.00%|    self.values = np.array(values)\n",
      "     7|         0|            0|            0|  0.00%|    self.budget = budget\n",
      "     8|         0|            0|            0|  0.00%|    self.pricing_bonus = pricing_bonus\n",
      "     9|         0|            0|            0|  0.00%|    self.all_bids = all_bids\n",
      "    10|         0|            0|            0|  0.00%|    self.bundle_values = None\n",
      "    11|         0|            0|            0|  0.00%|    self.drop_out_heuristic = drop_out_heuristic\n",
      "    12|         0|            0|            0|  0.00%|\n",
      "    13|         0|            0|            0|  0.00%|  def value_for_package(package, package_index=None):\n",
      "    14|         0|            0|            0|  0.00%|    raise NotImplementedError()\n",
      "    15|         0|            0|            0|  0.00%|\n",
      "    16|         0|            0|            0|  0.00%|  def get_budget(self):\n",
      "    17|         0|            0|            0|  0.00%|    return self.budget\n",
      "    18|         0|            0|            0|  0.00%|\n",
      "    19|         0|            0|            0|  0.00%|  def get_pricing_bonus(self):\n",
      "    20|         0|            0|            0|  0.00%|    return self.price_bonus\n",
      "    21|         0|            0|            0|  0.00%|\n",
      "    22|   1572502|      1.82381|  1.15982e-06|  0.29%|  def get_values(self):\n",
      "    23|   1572502|      2.38221|  1.51492e-06|  0.38%|    return self.bundle_values\n",
      "    24|         0|            0|            0|  0.00%|\n",
      "    25|   1572502|      2.58423|  1.64339e-06|  0.42%|  def get_profits(self, prices):\n",
      "    26|   1572502|      16.6745|  1.06038e-05|  2.68%|    return self.get_values() - (self.all_bids @ np.asarray(prices))\n",
      "(call)|   1572502|      4.20602|  2.67473e-06|  0.68%|# /apps/open_spiel/open_spiel/python/games/clock_auction_bidders.py:22 get_values\n",
      "    27|         0|            0|            0|  0.00%|\n",
      "    28|         0|            0|            0|  0.00%|class LinearBidder(Bidder):\n",
      "    29|         0|            0|            0|  0.00%|\n",
      "    30|         0|            0|            0|  0.00%|  def __init__(self, values, budget, pricing_bonus, all_bids, drop_out_heuristic) -> None:\n",
      "    31|         0|            0|            0|  0.00%|    super().__init__(values, budget, pricing_bonus, all_bids, drop_out_heuristic)\n",
      "    32|         0|            0|            0|  0.00%|    self.bundle_values = all_bids @ self.values\n",
      "    33|         0|            0|            0|  0.00%|\n",
      "    34|         0|            0|            0|  0.00%|  def value_for_package(self, package, package_index=None):\n",
      "    35|         0|            0|            0|  0.00%|    return np.array(package) @ self.values\n",
      "    36|         0|            0|            0|  0.00%|\n",
      "    37|         0|            0|            0|  0.00%|  def __str__(self) -> str:\n",
      "    38|         0|            0|            0|  0.00%|    return f'LinearValues: {self.values} Budget: {self.budget}'\n",
      "    39|         0|            0|            0|  0.00%|\n",
      "    40|         0|            0|            0|  0.00%|class MarginalValueBidder(Bidder):\n",
      "    41|         0|            0|            0|  0.00%|\n",
      "    42|         0|            0|            0|  0.00%|  def __init__(self, values, budget, pricing_bonus, all_bids, drop_out_heuristic) -> None:\n",
      "    43|         0|            0|            0|  0.00%|    super().__init__(values, budget, pricing_bonus, all_bids, drop_out_heuristic)\n",
      "    44|         0|            0|            0|  0.00%|    self.bundle_values = [self.value_for_package(bid) for bid in all_bids]\n",
      "    45|         0|            0|            0|  0.00%|\n",
      "    46|         0|            0|            0|  0.00%|  def value_for_package(self, package, package_index=None):\n",
      "    47|         0|            0|            0|  0.00%|    value = 0\n",
      "    48|         0|            0|            0|  0.00%|    for i, quantity in enumerate(package):\n",
      "    49|         0|            0|            0|  0.00%|      value += self.values[i][:quantity].sum()\n",
      "    50|         0|            0|            0|  0.00%|    return value\n",
      "    51|         0|            0|            0|  0.00%|\n",
      "    52|         0|            0|            0|  0.00%|  def __str__(self) -> str:\n",
      "    53|         0|            0|            0|  0.00%|    return f'MarginalValues: {self.values} Budget: {self.budget}'\n",
      "    54|         0|            0|            0|  0.00%|\n",
      "    55|         0|            0|            0|  0.00%|class EnumeratedValueBidder(Bidder):\n",
      "    56|         0|            0|            0|  0.00%|\n",
      "    57|         0|            0|            0|  0.00%|  def __init__(self, values, budget, pricing_bonus, all_bids, drop_out_heuristic, name) -> None:\n",
      "    58|         0|            0|            0|  0.00%|    super().__init__(values, budget, pricing_bonus, all_bids, drop_out_heuristic)\n",
      "    59|         0|            0|            0|  0.00%|    self.bundle_values = self.values\n",
      "    60|         0|            0|            0|  0.00%|    self.name = name\n",
      "    61|         0|            0|            0|  0.00%|\n",
      "    62|     99816|     0.160614|   1.6091e-06|  0.03%|  def value_for_package(self, package, package_index=None):\n",
      "    63|     99816|     0.163299|    1.636e-06|  0.03%|    if package_index is None:\n",
      "    64|     99816|      1.59612|  1.59906e-05|  0.26%|      package_index = np.where((self.all_bids == package).all(axis=1))[0][0]\n",
      "(call)|     99816|      0.96628|  9.68061e-06|  0.16%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py:60 _all\n",
      "(call)|     99816|      1.62308|  1.62607e-05|  0.26%|# <__array_function__ internals>:177 where\n",
      "    65|     99816|     0.220956|  2.21364e-06|  0.04%|    return self.values[package_index]\n",
      "    66|         0|            0|            0|  0.00%|\n",
      "    67|         0|            0|            0|  0.00%|  def __str__(self) -> str:\n",
      "    68|         0|            0|            0|  0.00%|    if self.name is not None:\n",
      "    69|         0|            0|            0|  0.00%|      return self.name\n",
      "    70|         0|            0|            0|  0.00%|    return f'EnumeratedValues: {self.values} Budget: {self.budget}'\n",
      "File: /apps/open_spiel/open_spiel/python/pytorch/ppo.py\n",
      "File duration: 21.6376s (3.48%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|# Note: code adapted (with permission) from https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py and https://github.com/vwxyzjn/ppo-implementation-details/blob/main/ppo_atari.py\n",
      "     2|         0|            0|            0|  0.00%|\n",
      "     3|         0|            0|            0|  0.00%|import time\n",
      "     4|         0|            0|            0|  0.00%|\n",
      "     5|         0|            0|            0|  0.00%|import numpy as np\n",
      "     6|         0|            0|            0|  0.00%|import torch\n",
      "     7|         0|            0|            0|  0.00%|import torch.nn as nn\n",
      "     8|         0|            0|            0|  0.00%|import torch.optim as optim\n",
      "     9|         0|            0|            0|  0.00%|from torch.distributions.categorical import Categorical\n",
      "    10|         0|            0|            0|  0.00%|\n",
      "    11|         0|            0|            0|  0.00%|from open_spiel.python.rl_agent import StepOutput\n",
      "    12|         0|            0|            0|  0.00%|\n",
      "    13|         0|            0|            0|  0.00%|INVALID_ACTION_PENALTY = -1e6\n",
      "    14|         0|            0|            0|  0.00%|def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
      "    15|         0|            0|            0|  0.00%|    torch.nn.init.orthogonal_(layer.weight, std)\n",
      "    16|         0|            0|            0|  0.00%|    torch.nn.init.constant_(layer.bias, bias_const)\n",
      "    17|         0|            0|            0|  0.00%|    return layer\n",
      "    18|         0|            0|            0|  0.00%|\n",
      "    19|         0|            0|            0|  0.00%|class CategoricalMasked(Categorical):\n",
      "    20|     31590|    0.0702374|  2.22341e-06|  0.01%|    def __init__(self, probs=None, logits=None, validate_args=None, masks=[], mask_value=None):\n",
      "    21|     31590|      0.54276|  1.71814e-05|  0.09%|        logits = torch.where(masks.bool(), logits, mask_value)\n",
      "    22|     31590|     0.314184|  9.94569e-06|  0.05%|        super(CategoricalMasked, self).__init__(probs, logits, validate_args)\n",
      "(call)|     31590|      8.27641|  0.000261995|  1.33%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/categorical.py:49 __init__\n",
      "    23|         0|            0|            0|  0.00%|\n",
      "    24|         0|            0|            0|  0.00%|def string_to_activation(activation_string):\n",
      "    25|         0|            0|            0|  0.00%|    activations = {\n",
      "    26|         0|            0|            0|  0.00%|        'relu': nn.ReLU,\n",
      "    27|         0|            0|            0|  0.00%|        'tanh': nn.Tanh,\n",
      "    28|         0|            0|            0|  0.00%|    }\n",
      "    29|         0|            0|            0|  0.00%|    try:\n",
      "    30|         0|            0|            0|  0.00%|        return activations[activation_string.lower()]\n",
      "    31|         0|            0|            0|  0.00%|    except KeyError:\n",
      "    32|         0|            0|            0|  0.00%|        raise ValueError(f'Invalid activation {activation_string}; valid activations are {list(activations.keys())}')\n",
      "    33|         0|            0|            0|  0.00%|\n",
      "    34|         0|            0|            0|  0.00%|def build_sequential_network(observation_shape, output_size, hidden_sizes, activation, final_std):\n",
      "    35|         0|            0|            0|  0.00%|    layers = []\n",
      "    36|         0|            0|            0|  0.00%|    layers.append(layer_init(nn.Linear(np.array(observation_shape).prod(), hidden_sizes[0])))\n",
      "    37|         0|            0|            0|  0.00%|    layers.append(activation())\n",
      "    38|         0|            0|            0|  0.00%|    for i in range(len(hidden_sizes) - 1):\n",
      "    39|         0|            0|            0|  0.00%|        layers.append(layer_init(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1])))\n",
      "    40|         0|            0|            0|  0.00%|        layers.append(activation())\n",
      "    41|         0|            0|            0|  0.00%|    layers.append(layer_init(nn.Linear(hidden_sizes[-1], output_size), std=final_std))\n",
      "    42|         0|            0|            0|  0.00%|    return nn.Sequential(*layers)\n",
      "    43|         0|            0|            0|  0.00%|\n",
      "    44|         0|            0|            0|  0.00%|class PPOAgent(nn.Module):\n",
      "    45|         0|            0|            0|  0.00%|    def __init__(self, num_actions, observation_shape, device, actor_hidden_sizes=None, actor_activation=nn.Tanh, critic_hidden_sizes=None, critic_activation=nn.Tanh):\n",
      "    46|         0|            0|            0|  0.00%|        super().__init__()\n",
      "    47|         0|            0|            0|  0.00%|\n",
      "    48|         0|            0|            0|  0.00%|        if actor_hidden_sizes is None:\n",
      "    49|         0|            0|            0|  0.00%|            actor_hidden_sizes = [64, 64]\n",
      "    50|         0|            0|            0|  0.00%|        if critic_hidden_sizes is None:\n",
      "    51|         0|            0|            0|  0.00%|            critic_hidden_sizes = [64, 64]\n",
      "    52|         0|            0|            0|  0.00%|\n",
      "    53|         0|            0|            0|  0.00%|        if isinstance(actor_activation, str):\n",
      "    54|         0|            0|            0|  0.00%|            actor_activation = string_to_activation(actor_activation)\n",
      "    55|         0|            0|            0|  0.00%|        if isinstance(critic_activation, str):\n",
      "    56|         0|            0|            0|  0.00%|            critic_activation = string_to_activation(critic_activation)\n",
      "    57|         0|            0|            0|  0.00%|\n",
      "    58|         0|            0|            0|  0.00%|        # Construct networks\n",
      "    59|         0|            0|            0|  0.00%|        self.critic = build_sequential_network(observation_shape, 1, critic_hidden_sizes, critic_activation, final_std=1.0)\n",
      "    60|         0|            0|            0|  0.00%|        self.actor = build_sequential_network(observation_shape, num_actions, actor_hidden_sizes, actor_activation, final_std=0.01)\n",
      "    61|         0|            0|            0|  0.00%|\n",
      "    62|         0|            0|            0|  0.00%|        self.device = device\n",
      "    63|         0|            0|            0|  0.00%|        self.num_actions = num_actions\n",
      "    64|         0|            0|            0|  0.00%|        self.register_buffer(\"mask_value\", torch.tensor(INVALID_ACTION_PENALTY))\n",
      "    65|         0|            0|            0|  0.00%|\n",
      "    66|       390|  0.000627041|   1.6078e-06|  0.00%|    def get_value(self, x):\n",
      "    67|       390|    0.0051055|   1.3091e-05|  0.00%|        return self.critic(x)\n",
      "(call)|       390|    0.0081284|  2.08421e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194 __getattr__\n",
      "(call)|       390|     0.175793|   0.00045075|  0.03%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1124 _call_impl\n",
      "    68|         0|            0|            0|  0.00%|\n",
      "    69|     31590|    0.0674403|  2.13486e-06|  0.01%|    def get_action_and_value(self, x, legal_actions_mask=None, action=None):\n",
      "    70|     31590|    0.0686846|  2.17425e-06|  0.01%|        if legal_actions_mask is None:\n",
      "    71|         0|            0|            0|  0.00%|            legal_actions_mask = torch.ones((len(x), self.num_actions)).bool()\n",
      "    72|         0|            0|            0|  0.00%|\n",
      "    73|     31590|     0.443584|  1.40419e-05|  0.07%|        logits = self.actor(x)\n",
      "(call)|     31590|     0.633812|  2.00637e-05|  0.10%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194 __getattr__\n",
      "(call)|     31590|      15.3901|  0.000487183|  2.48%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1124 _call_impl\n",
      "    74|     31590|     0.791299|   2.5049e-05|  0.13%|        if torch.isnan(logits).any():\n",
      "    75|         0|            0|            0|  0.00%|            raise ValueError(\"Training is messed up - logits are NaN\")\n",
      "    76|         0|            0|            0|  0.00%|\n",
      "    77|     31590|     0.527201|  1.66888e-05|  0.08%|        probs = CategoricalMasked(logits=logits, masks=legal_actions_mask, mask_value=self.mask_value)\n",
      "(call)|     31590|      0.56168|  1.77803e-05|  0.09%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194 __getattr__\n",
      "(call)|     31590|      9.20359|  0.000291345|  1.48%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:20 __init__\n",
      "    78|     31590|    0.0749505|   2.3726e-06|  0.01%|        if action is None:\n",
      "    79|     24960|     0.209838|  8.40698e-06|  0.03%|            action = probs.sample()\n",
      "(call)|     24960|      3.11782|  0.000124913|  0.50%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/categorical.py:112 sample\n",
      "    80|     31590|     0.893114|   2.8272e-05|  0.14%|        return action, probs.log_prob(action), probs.entropy(), self.critic(x), probs.probs\n",
      "(call)|     31590|      6.63379|  0.000209997|  1.07%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/categorical.py:119 log_prob\n",
      "(call)|     31590|      2.24455|  7.10526e-05|  0.36%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/categorical.py:127 entropy\n",
      "(call)|     31590|     0.773193|  2.44759e-05|  0.12%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194 __getattr__\n",
      "(call)|     31590|      13.7308|  0.000434656|  2.21%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1124 _call_impl\n",
      "    81|         0|            0|            0|  0.00%|\n",
      "    82|         0|            0|            0|  0.00%|\n",
      "    83|         0|            0|            0|  0.00%|class PPOAtariAgent(nn.Module):\n",
      "    84|         0|            0|            0|  0.00%|    def __init__(self, num_actions, observation_shape, device):\n",
      "    85|         0|            0|            0|  0.00%|        super(PPOAtariAgent, self).__init__()\n",
      "    86|         0|            0|            0|  0.00%|        # Note: this network is intended for atari games, taken from https://github.com/vwxyzjn/ppo-implementation-details/blob/main/ppo_atari.py\n",
      "    87|         0|            0|            0|  0.00%|        self.network = nn.Sequential(\n",
      "    88|         0|            0|            0|  0.00%|            layer_init(nn.Conv2d(4, 32, 8, stride=4)),\n",
      "    89|         0|            0|            0|  0.00%|            nn.ReLU(),\n",
      "    90|         0|            0|            0|  0.00%|            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
      "    91|         0|            0|            0|  0.00%|            nn.ReLU(),\n",
      "    92|         0|            0|            0|  0.00%|            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
      "    93|         0|            0|            0|  0.00%|            nn.ReLU(),\n",
      "    94|         0|            0|            0|  0.00%|            nn.Flatten(),\n",
      "    95|         0|            0|            0|  0.00%|            layer_init(nn.Linear(64 * 7 * 7, 512)),\n",
      "    96|         0|            0|            0|  0.00%|            nn.ReLU(),\n",
      "    97|         0|            0|            0|  0.00%|        )\n",
      "    98|         0|            0|            0|  0.00%|        self.actor = layer_init(nn.Linear(512, num_actions), std=0.01)\n",
      "    99|         0|            0|            0|  0.00%|        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
      "   100|         0|            0|            0|  0.00%|        self.num_actions = num_actions\n",
      "   101|         0|            0|            0|  0.00%|        self.device = device\n",
      "   102|         0|            0|            0|  0.00%|        self.register_buffer(\"mask_value\", torch.tensor(INVALID_ACTION_PENALTY))\n",
      "   103|         0|            0|            0|  0.00%|\n",
      "   104|         0|            0|            0|  0.00%|    def get_value(self, x):\n",
      "   105|         0|            0|            0|  0.00%|        return self.critic(self.network(x / 255.0))\n",
      "   106|         0|            0|            0|  0.00%|\n",
      "   107|         0|            0|            0|  0.00%|    def get_action_and_value(self, x, legal_actions_mask=None, action=None):\n",
      "   108|         0|            0|            0|  0.00%|        if legal_actions_mask is None:\n",
      "   109|         0|            0|            0|  0.00%|            legal_actions_mask = torch.ones((len(x), self.num_actions)).bool()\n",
      "   110|         0|            0|            0|  0.00%|\n",
      "   111|         0|            0|            0|  0.00%|        hidden = self.network(x / 255.0)\n",
      "   112|         0|            0|            0|  0.00%|        logits = self.actor(hidden)\n",
      "   113|         0|            0|            0|  0.00%|        probs = CategoricalMasked(logits=logits, masks=legal_actions_mask, mask_value=self.mask_value)\n",
      "   114|         0|            0|            0|  0.00%|\n",
      "   115|         0|            0|            0|  0.00%|        if action is None:\n",
      "   116|         0|            0|            0|  0.00%|            action = probs.sample()\n",
      "   117|         0|            0|            0|  0.00%|        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden), probs.probs\n",
      "   118|         0|            0|            0|  0.00%|\n",
      "   119|     24960|    0.0623097|  2.49638e-06|  0.01%|def legal_actions_to_mask(legal_actions_list, num_actions):\n",
      "   120|         0|            0|            0|  0.00%|    '''Convert a list of legal actions to a mask of size num actions with a 1 in a legal position'''\n",
      "   121|     24960|     0.268177|  1.07443e-05|  0.04%|    legal_actions_mask = torch.zeros((len(legal_actions_list), num_actions), dtype=torch.bool)\n",
      "   122|    124800|      0.28373|  2.27348e-06|  0.05%|    for i, legal_actions in enumerate(legal_actions_list):\n",
      "   123|     99840|      1.77977|  1.78262e-05|  0.29%|        legal_actions_mask[i, legal_actions] = 1\n",
      "   124|     24960|    0.0396698|  1.58933e-06|  0.01%|    return legal_actions_mask\n",
      "   125|         0|            0|            0|  0.00%|\n",
      "   126|         0|            0|            0|  0.00%|class PPO(nn.Module):\n",
      "   127|         0|            0|            0|  0.00%|    \"\"\"PPO Agent implementation in PyTorch.\n",
      "   128|         0|            0|            0|  0.00%|\n",
      "   129|         0|            0|            0|  0.00%|    See open_spiel/python/examples/ppo_example.py for an usage example.\n",
      "   130|         0|            0|            0|  0.00%|\n",
      "   131|         0|            0|            0|  0.00%|    Note that PPO runs multiple environments concurrently on each step (see\n",
      "   132|         0|            0|            0|  0.00%|    open_spiel/python/vector_env.py). In practice, this tends to improve PPO's\n",
      "   133|         0|            0|            0|  0.00%|    performance. The number of parallel environments is controlled by the\n",
      "   134|         0|            0|            0|  0.00%|    num_envs argument.\n",
      "   135|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   136|         0|            0|            0|  0.00%|    def __init__(\n",
      "   137|         0|            0|            0|  0.00%|        self,\n",
      "   138|         0|            0|            0|  0.00%|        input_shape,\n",
      "   139|         0|            0|            0|  0.00%|        num_actions,\n",
      "   140|         0|            0|            0|  0.00%|        num_players,\n",
      "   141|         0|            0|            0|  0.00%|        player_id=0,\n",
      "   142|         0|            0|            0|  0.00%|        num_envs=1,\n",
      "   143|         0|            0|            0|  0.00%|        steps_per_batch=128,\n",
      "   144|         0|            0|            0|  0.00%|        num_minibatches=4,\n",
      "   145|         0|            0|            0|  0.00%|        update_epochs=4,\n",
      "   146|         0|            0|            0|  0.00%|        learning_rate=2.5e-4,\n",
      "   147|         0|            0|            0|  0.00%|        num_annealing_updates=None,\n",
      "   148|         0|            0|            0|  0.00%|        gae=True,\n",
      "   149|         0|            0|            0|  0.00%|        gamma=0.99,\n",
      "   150|         0|            0|            0|  0.00%|        gae_lambda=0.95,\n",
      "   151|         0|            0|            0|  0.00%|        normalize_advantages=True,\n",
      "   152|         0|            0|            0|  0.00%|        clip_coef=0.2,\n",
      "   153|         0|            0|            0|  0.00%|        clip_vloss=True,\n",
      "   154|         0|            0|            0|  0.00%|        entropy_coef=0.01,\n",
      "   155|         0|            0|            0|  0.00%|        value_coef=0.5,\n",
      "   156|         0|            0|            0|  0.00%|        max_grad_norm=0.5,\n",
      "   157|         0|            0|            0|  0.00%|        target_kl=None,\n",
      "   158|         0|            0|            0|  0.00%|        device='cpu',\n",
      "   159|         0|            0|            0|  0.00%|        writer=None, # Tensorboard SummaryWriter\n",
      "   160|         0|            0|            0|  0.00%|        use_wandb=True,\n",
      "   161|         0|            0|            0|  0.00%|        agent_fn=PPOAtariAgent,\n",
      "   162|         0|            0|            0|  0.00%|        agent_fn_kwargs=None,\n",
      "   163|         0|            0|            0|  0.00%|        ):\n",
      "   164|         0|            0|            0|  0.00%|        super().__init__()\n",
      "   165|         0|            0|            0|  0.00%|\n",
      "   166|         0|            0|            0|  0.00%|        if agent_fn_kwargs is None:\n",
      "   167|         0|            0|            0|  0.00%|            agent_fn_kwargs = {}\n",
      "   168|         0|            0|            0|  0.00%|\n",
      "   169|         0|            0|            0|  0.00%|        if isinstance(agent_fn, str):\n",
      "   170|         0|            0|            0|  0.00%|            if agent_fn == 'PPOAgent':\n",
      "   171|         0|            0|            0|  0.00%|                agent_fn = PPOAgent\n",
      "   172|         0|            0|            0|  0.00%|            else:\n",
      "   173|         0|            0|            0|  0.00%|                raise ValueError(f\"Unknown agent_fn {agent_fn}\")\n",
      "   174|         0|            0|            0|  0.00%|\n",
      "   175|         0|            0|            0|  0.00%|        self.input_shape = input_shape\n",
      "   176|         0|            0|            0|  0.00%|        self.num_actions = num_actions\n",
      "   177|         0|            0|            0|  0.00%|        self.num_players = num_players\n",
      "   178|         0|            0|            0|  0.00%|        self.player_id = player_id\n",
      "   179|         0|            0|            0|  0.00%|        self.device = device\n",
      "   180|         0|            0|            0|  0.00%|\n",
      "   181|         0|            0|            0|  0.00%|        # Training settings\n",
      "   182|         0|            0|            0|  0.00%|        self.num_envs = num_envs\n",
      "   183|         0|            0|            0|  0.00%|        self.steps_per_batch = steps_per_batch\n",
      "   184|         0|            0|            0|  0.00%|        self.batch_size = self.num_envs * self.steps_per_batch\n",
      "   185|         0|            0|            0|  0.00%|        self.num_minibatches = num_minibatches\n",
      "   186|         0|            0|            0|  0.00%|        self.minibatch_size = self.batch_size // self.num_minibatches\n",
      "   187|         0|            0|            0|  0.00%|        self.update_epochs = update_epochs\n",
      "   188|         0|            0|            0|  0.00%|        self.learning_rate = learning_rate\n",
      "   189|         0|            0|            0|  0.00%|        self.num_annealing_updates = num_annealing_updates\n",
      "   190|         0|            0|            0|  0.00%|\n",
      "   191|         0|            0|            0|  0.00%|        # Loss function\n",
      "   192|         0|            0|            0|  0.00%|        self.gae = gae\n",
      "   193|         0|            0|            0|  0.00%|        self.gamma = gamma\n",
      "   194|         0|            0|            0|  0.00%|        self.gae_lambda = gae_lambda\n",
      "   195|         0|            0|            0|  0.00%|        self.normalize_advantages = normalize_advantages\n",
      "   196|         0|            0|            0|  0.00%|        self.clip_coef = clip_coef\n",
      "   197|         0|            0|            0|  0.00%|        self.clip_vloss = clip_vloss\n",
      "   198|         0|            0|            0|  0.00%|        self.entropy_coef = entropy_coef\n",
      "   199|         0|            0|            0|  0.00%|        self.value_coef = value_coef\n",
      "   200|         0|            0|            0|  0.00%|        self.max_grad_norm = max_grad_norm\n",
      "   201|         0|            0|            0|  0.00%|        self.target_kl = target_kl\n",
      "   202|         0|            0|            0|  0.00%|\n",
      "   203|         0|            0|            0|  0.00%|        # Logging\n",
      "   204|         0|            0|            0|  0.00%|        self.writer = writer\n",
      "   205|         0|            0|            0|  0.00%|        self.use_wandb = use_wandb\n",
      "   206|         0|            0|            0|  0.00%|        self.watch_output = None\n",
      "   207|         0|            0|            0|  0.00%|\n",
      "   208|         0|            0|            0|  0.00%|        # Initialize networks\n",
      "   209|         0|            0|            0|  0.00%|        self.network = agent_fn(self.num_actions, self.input_shape, device, **agent_fn_kwargs).to(device)\n",
      "   210|         0|            0|            0|  0.00%|        self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate, eps=1e-5)\n",
      "   211|         0|            0|            0|  0.00%|\n",
      "   212|         0|            0|            0|  0.00%|        # Initialize training buffers\n",
      "   213|         0|            0|            0|  0.00%|        self.legal_actions_mask = torch.zeros((self.steps_per_batch, self.num_envs, self.num_actions), dtype=torch.bool).to(device)\n",
      "   214|         0|            0|            0|  0.00%|        self.obs = torch.zeros((self.steps_per_batch, self.num_envs) + self.input_shape).to(device)\n",
      "   215|         0|            0|            0|  0.00%|        self.actions = torch.zeros((self.steps_per_batch, self.num_envs)).to(device)\n",
      "   216|         0|            0|            0|  0.00%|        self.probs = torch.zeros((self.steps_per_batch, self.num_envs, self.num_actions)).to(device)\n",
      "   217|         0|            0|            0|  0.00%|        self.logprobs = torch.zeros((self.steps_per_batch, self.num_envs)).to(device)\n",
      "   218|         0|            0|            0|  0.00%|        self.rewards = torch.zeros((self.steps_per_batch, self.num_envs)).to(device)\n",
      "   219|         0|            0|            0|  0.00%|        self.dones = torch.zeros((self.steps_per_batch, self.num_envs)).to(device)\n",
      "   220|         0|            0|            0|  0.00%|        self.values = torch.zeros((self.steps_per_batch, self.num_envs)).to(device)\n",
      "   221|         0|            0|            0|  0.00%|\n",
      "   222|         0|            0|            0|  0.00%|        # Initialize counters\n",
      "   223|         0|            0|            0|  0.00%|        self.cur_batch_idx = 0\n",
      "   224|         0|            0|            0|  0.00%|        self.total_steps_done = 0\n",
      "   225|         0|            0|            0|  0.00%|        self.updates_done = 0\n",
      "   226|         0|            0|            0|  0.00%|        self.start_time = time.time()\n",
      "   227|         0|            0|            0|  0.00%|\n",
      "   228|         0|            0|            0|  0.00%|        self.max_policy_diff = 9999\n",
      "   229|         0|            0|            0|  0.00%|\n",
      "   230|         0|            0|            0|  0.00%|        self._savers = [\n",
      "   231|         0|            0|            0|  0.00%|            (\"ppo_network\", self.network),\n",
      "   232|         0|            0|            0|  0.00%|        ]\n",
      "   233|         0|            0|            0|  0.00%|\n",
      "   234|       390|  0.000935316|  2.39825e-06|  0.00%|    def get_value(self, x):\n",
      "   235|       390|   0.00479412|  1.22926e-05|  0.00%|        return self.network.get_value(x)\n",
      "(call)|       390|   0.00907969|  2.32813e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194 __getattr__\n",
      "(call)|       390|     0.189654|  0.000486291|  0.03%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:66 get_value\n",
      "   236|         0|            0|            0|  0.00%|\n",
      "   237|     31590|    0.0843232|   2.6693e-06|  0.01%|    def get_action_and_value(self, x, legal_actions_mask=None, action=None):\n",
      "   238|     31590|     0.482816|  1.52838e-05|  0.08%|        return self.network.get_action_and_value(x, legal_actions_mask, action)\n",
      "(call)|     31590|     0.721497|  2.28394e-05|  0.12%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194 __getattr__\n",
      "(call)|     31590|      55.3654|   0.00175262|  8.91%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:69 get_action_and_value\n",
      "   239|         0|            0|            0|  0.00%|\n",
      "   240|     24960|     0.123551|  4.94997e-06|  0.02%|    def step(self, time_step, is_evaluation=False):\n",
      "   241|     24960|    0.0995779|   3.9895e-06|  0.02%|        if is_evaluation:\n",
      "   242|         0|            0|            0|  0.00%|            singular_env = False\n",
      "   243|         0|            0|            0|  0.00%|            if not isinstance(time_step, list):\n",
      "   244|         0|            0|            0|  0.00%|                time_step = [time_step]\n",
      "   245|         0|            0|            0|  0.00%|                singular_env = True\n",
      "   246|         0|            0|            0|  0.00%|\n",
      "   247|         0|            0|            0|  0.00%|            with torch.no_grad():\n",
      "   248|         0|            0|            0|  0.00%|                legal_actions_mask = legal_actions_to_mask(\n",
      "   249|         0|            0|            0|  0.00%|                    [ts.observations['legal_actions'][self.player_id] for ts in time_step], self.num_actions\n",
      "   250|         0|            0|            0|  0.00%|                ).to(self.device)\n",
      "   251|         0|            0|            0|  0.00%|                obs = torch.Tensor(np.array([ts.observations['info_state'][self.player_id] for ts in time_step])).to(self.device)\n",
      "   252|         0|            0|            0|  0.00%|                action, log_prob, entropy, value, probs = self.get_action_and_value(obs, legal_actions_mask=legal_actions_mask)\n",
      "   253|         0|            0|            0|  0.00%|                probs = probs.cpu().numpy()\n",
      "   254|         0|            0|            0|  0.00%|                if singular_env:\n",
      "   255|         0|            0|            0|  0.00%|                    return StepOutput(action=action[0].item(), probs=probs[0])\n",
      "   256|         0|            0|            0|  0.00%|                else:\n",
      "   257|         0|            0|            0|  0.00%|                    return [StepOutput(action=a.item(), probs=p) for (a, p) in zip(action, probs)]\n",
      "   258|         0|            0|            0|  0.00%|        else:\n",
      "   259|     24960|     0.374577|  1.50071e-05|  0.06%|            with torch.no_grad():\n",
      "(call)|     24960|     0.414237|   1.6596e-05|  0.07%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:126 __init__\n",
      "(call)|     24960|     0.445627|  1.78537e-05|  0.07%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:131 __enter__\n",
      "   260|         0|            0|            0|  0.00%|                # act\n",
      "   261|    174720|      1.29212|  7.39536e-06|  0.21%|                obs = torch.Tensor(np.array([ts.observations['info_state'][self.player_id] for ts in time_step])).to(self.device)\n",
      "(call)|     24960|     0.282064|  1.13006e-05|  0.05%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:261 <listcomp>\n",
      "   262|     74880|     0.500387|  6.68252e-06|  0.08%|                legal_actions_mask = legal_actions_to_mask(\n",
      "(call)|     24960|      2.43366|  9.75023e-05|  0.39%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:119 legal_actions_to_mask\n",
      "   263|    174720|     0.479079|  2.74198e-06|  0.08%|                    [ts.observations['legal_actions'][self.player_id] for ts in time_step], self.num_actions\n",
      "(call)|     24960|     0.260243|  1.04264e-05|  0.04%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:263 <listcomp>\n",
      "   264|     24960|    0.0975363|  3.90771e-06|  0.02%|                ).to(self.device)\n",
      "   265|     24960|     0.303304|  1.21516e-05|  0.05%|                action, logprob, _, value, probs = self.get_action_and_value(obs, legal_actions_mask=legal_actions_mask)\n",
      "(call)|     24960|       43.844|   0.00175657|  7.06%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:237 get_action_and_value\n",
      "   266|         0|            0|            0|  0.00%|\n",
      "   267|         0|            0|            0|  0.00%|                # store\n",
      "   268|     24960|     0.312179|  1.25072e-05|  0.05%|                self.legal_actions_mask[self.cur_batch_idx] = legal_actions_mask\n",
      "   269|     24960|     0.208527|  8.35444e-06|  0.03%|                self.obs[self.cur_batch_idx] = obs\n",
      "   270|     24960|     0.189996|  7.61204e-06|  0.03%|                self.actions[self.cur_batch_idx] = action\n",
      "   271|     24960|     0.173959|  6.96952e-06|  0.03%|                self.probs[self.cur_batch_idx] = probs\n",
      "   272|     24960|     0.170941|  6.84859e-06|  0.03%|                self.logprobs[self.cur_batch_idx] = logprob\n",
      "   273|     24960|     0.318874|  1.27754e-05|  0.05%|                self.values[self.cur_batch_idx] = value.flatten()\n",
      "   274|         0|            0|            0|  0.00%|\n",
      "   275|    174720|      1.51402|  8.66539e-06|  0.24%|                agent_output = [StepOutput(action=a.item(), probs=p) for (a, p) in zip(action, probs)]\n",
      "(call)|     49920|     0.799465|  1.60149e-05|  0.13%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:713 __iter__\n",
      "(call)|     99840|     0.382184|  3.82797e-06|  0.06%|# <string>_0:1 __new__\n",
      "(call)|     24960|      1.29702|   5.1964e-05|  0.21%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:275 <listcomp>\n",
      "   276|     24960|     0.254542|   1.0198e-05|  0.04%|                return agent_output\n",
      "(call)|     24960|     0.464675|  1.86168e-05|  0.07%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:135 __exit__\n",
      "   277|         0|            0|            0|  0.00%|\n",
      "   278|         0|            0|            0|  0.00%|\n",
      "   279|     24960|    0.0497599|  1.99358e-06|  0.01%|    def post_step(self, reward, done):\n",
      "   280|     24960|     0.680484|   2.7263e-05|  0.11%|        self.rewards[self.cur_batch_idx] = torch.tensor(reward).to(self.device).view(-1)\n",
      "   281|     24960|     0.428445|  1.71653e-05|  0.07%|        self.dones[self.cur_batch_idx] = torch.tensor(done).to(self.device).view(-1)\n",
      "   282|         0|            0|            0|  0.00%|\n",
      "   283|     24960|     0.209896|   8.4093e-06|  0.03%|        self.total_steps_done += self.num_envs\n",
      "(call)|     24960|      1.01292|  4.05817e-05|  0.16%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1210 __setattr__\n",
      "   284|     24960|     0.164934|  6.60792e-06|  0.03%|        self.cur_batch_idx += 1\n",
      "(call)|     24960|     0.853982|   3.4214e-05|  0.14%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1210 __setattr__\n",
      "   285|         0|            0|            0|  0.00%|\n",
      "   286|         0|            0|            0|  0.00%|\n",
      "   287|       390|   0.00432444|  1.10883e-05|  0.00%|    def learn(self, time_step):\n",
      "   288|       390|   0.00378323|  9.70058e-06|  0.00%|        if self.use_wandb and self.watch_output is None:\n",
      "   289|         0|            0|            0|  0.00%|            pass # Honestly, this hasn't been super useful and takes up a lot of log bandwidth, but uncomment to watch the weights in wandb\n",
      "   290|         0|            0|            0|  0.00%|            # import wandb\n",
      "   291|         0|            0|            0|  0.00%|            # wandb.watch(self, log_freq=5)\n",
      "   292|         0|            0|            0|  0.00%|\n",
      "   293|      2730|    0.0252528|  9.25012e-06|  0.00%|        next_obs = torch.Tensor(np.array([ts.observations['info_state'][self.player_id] for ts in time_step])).to(self.device)\n",
      "(call)|       390|   0.00522661|  1.34016e-05|  0.00%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:293 <listcomp>\n",
      "   294|         0|            0|            0|  0.00%|\n",
      "   295|         0|            0|            0|  0.00%|        # Annealing the rate if instructed to do so.\n",
      "   296|       390|   0.00434089|  1.11305e-05|  0.00%|        if self.num_annealing_updates is not None:\n",
      "   297|       390|   0.00415492|  1.06536e-05|  0.00%|            frac = 1.0 - (self.updates_done) / self.num_annealing_updates\n",
      "   298|       390|   0.00389385|  9.98424e-06|  0.00%|            lrnow = frac * self.learning_rate\n",
      "   299|       390|   0.00403953|  1.03578e-05|  0.00%|            self.optimizer.param_groups[0][\"lr\"] = lrnow\n",
      "   300|         0|            0|            0|  0.00%|\n",
      "   301|         0|            0|            0|  0.00%|        # bootstrap value if not done\n",
      "   302|       390|   0.00837445|   2.1473e-05|  0.00%|        with torch.no_grad():\n",
      "(call)|       390|   0.00689054|   1.7668e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:126 __init__\n",
      "(call)|       390|   0.00715661|  1.83503e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:131 __enter__\n",
      "   303|       390|   0.00959682|  2.46072e-05|  0.00%|            next_value = self.get_value(next_obs).reshape(1, -1)\n",
      "(call)|       390|     0.204463|  0.000524264|  0.03%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:234 get_value\n",
      "   304|       390|   0.00405884|  1.04073e-05|  0.00%|            if self.gae:\n",
      "   305|         0|            0|            0|  0.00%|                advantages = torch.zeros_like(self.rewards).to(self.device)\n",
      "   306|         0|            0|            0|  0.00%|                lastgaelam = 0\n",
      "   307|         0|            0|            0|  0.00%|                for t in reversed(range(self.steps_per_batch)):\n",
      "   308|         0|            0|            0|  0.00%|                    nextvalues = next_value if t == self.steps_per_batch - 1 else self.values[t + 1]\n",
      "   309|         0|            0|            0|  0.00%|                    nextnonterminal = 1.0 - self.dones[t]\n",
      "   310|         0|            0|            0|  0.00%|                    delta = self.rewards[t] + self.gamma * nextvalues * nextnonterminal - self.values[t]\n",
      "   311|         0|            0|            0|  0.00%|                    advantages[t] = lastgaelam = delta + self.gamma * self.gae_lambda * nextnonterminal * lastgaelam\n",
      "   312|         0|            0|            0|  0.00%|                returns = advantages + self.values\n",
      "   313|         0|            0|            0|  0.00%|            else:\n",
      "   314|       390|   0.00843763|   2.1635e-05|  0.00%|                returns = torch.zeros_like(self.rewards).to(self.device)\n",
      "   315|     25350|     0.218184|  8.60687e-06|  0.04%|                for t in reversed(range(self.steps_per_batch)):\n",
      "   316|     24960|     0.258129|  1.03417e-05|  0.04%|                    next_return = next_value if t == self.steps_per_batch - 1 else returns[t + 1]\n",
      "   317|     24960|     0.394547|  1.58072e-05|  0.06%|                    nextnonterminal = 1.0 - self.dones[t]\n",
      "(call)|     24960|     0.530536|  2.12554e-05|  0.09%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:26 wrapped\n",
      "   318|     24960|     0.656427|  2.62992e-05|  0.11%|                    returns[t] = self.rewards[t] + self.gamma * nextnonterminal * next_return\n",
      "   319|       390|    0.0157597|  4.04095e-05|  0.00%|                advantages = returns - self.values\n",
      "(call)|       390|   0.00738907|  1.89463e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:135 __exit__\n",
      "   320|         0|            0|            0|  0.00%|\n",
      "   321|         0|            0|            0|  0.00%|        # flatten the batch\n",
      "   322|       390|   0.00579524|  1.48596e-05|  0.00%|        b_legal_actions_mask = self.legal_actions_mask.reshape((-1, self.num_actions))\n",
      "   323|       390|   0.00438023|  1.12313e-05|  0.00%|        b_obs = self.obs.reshape((-1,) + self.input_shape)\n",
      "   324|       390|   0.00411439|  1.05497e-05|  0.00%|        b_logprobs = self.logprobs.reshape(-1)\n",
      "   325|       390|   0.00395322|  1.01365e-05|  0.00%|        b_actions = self.actions.reshape(-1)\n",
      "   326|       390|   0.00396562|  1.01682e-05|  0.00%|        b_probs = self.probs.reshape((-1, self.num_actions))\n",
      "   327|       390|   0.00395703|  1.01462e-05|  0.00%|        b_advantages = advantages.reshape(-1)\n",
      "   328|       390|   0.00384784|  9.86625e-06|  0.00%|        b_returns = returns.reshape(-1)\n",
      "   329|       390|   0.00381255|  9.77577e-06|  0.00%|        b_values = self.values.reshape(-1)\n",
      "   330|         0|            0|            0|  0.00%|\n",
      "   331|         0|            0|            0|  0.00%|        # Optimizing the policy and value network\n",
      "   332|       390|   0.00586152|  1.50295e-05|  0.00%|        b_inds = np.arange(self.batch_size)\n",
      "   333|       390|   0.00293875|  7.53525e-06|  0.00%|        clipfracs = []\n",
      "   334|      1950|    0.0116851|  5.99238e-06|  0.00%|        for epoch in range(self.update_epochs):\n",
      "   335|      1560|    0.0369942|  2.37142e-05|  0.01%|            np.random.shuffle(b_inds)\n",
      "   336|      7800|    0.0486891|   6.2422e-06|  0.01%|            for start in range(0, self.batch_size, self.minibatch_size):\n",
      "   337|      6240|    0.0346425|  5.55168e-06|  0.01%|                end = start + self.minibatch_size\n",
      "   338|      6240|    0.0435672|  6.98192e-06|  0.01%|                mb_inds = b_inds[start:end]\n",
      "   339|         0|            0|            0|  0.00%|\n",
      "   340|      6240|     0.528928|  8.47641e-05|  0.09%|                _, newlogprob, entropy, newvalue, _ = self.get_action_and_value(b_obs[mb_inds], legal_actions_mask=b_legal_actions_mask[mb_inds], action=b_actions.long()[mb_inds])\n",
      "(call)|      6240|      11.9116|   0.00190891|  1.92%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:237 get_action_and_value\n",
      "   341|      6240|     0.244985|  3.92605e-05|  0.04%|                logratio = newlogprob - b_logprobs[mb_inds]\n",
      "   342|      6240|     0.100256|  1.60666e-05|  0.02%|                ratio = logratio.exp()\n",
      "   343|         0|            0|            0|  0.00%|\n",
      "   344|      6240|     0.126297|  2.02399e-05|  0.02%|                with torch.no_grad():\n",
      "(call)|      6240|     0.128481|  2.05899e-05|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:126 __init__\n",
      "(call)|      6240|      0.12248|  1.96282e-05|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:131 __enter__\n",
      "   345|         0|            0|            0|  0.00%|                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
      "   346|      6240|     0.200952|  3.22039e-05|  0.03%|                    old_approx_kl = (-logratio).mean()\n",
      "   347|      6240|     0.181322|   2.9058e-05|  0.03%|                    approx_kl = ((ratio - 1) - logratio).mean()\n",
      "   348|      6240|     0.346848|  5.55845e-05|  0.06%|                    clipfracs += [((ratio - 1.0).abs() > self.clip_coef).float().mean().item()]\n",
      "(call)|      6240|      0.12384|  1.98462e-05|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:135 __exit__\n",
      "   349|         0|            0|            0|  0.00%|\n",
      "   350|      6240|     0.154968|  2.48345e-05|  0.02%|                mb_advantages = b_advantages[mb_inds]\n",
      "   351|      6240|    0.0407767|  6.53473e-06|  0.01%|                if self.normalize_advantages:\n",
      "   352|      6240|     0.316309|  5.06906e-05|  0.05%|                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
      "   353|         0|            0|            0|  0.00%|\n",
      "   354|         0|            0|            0|  0.00%|                # Policy loss\n",
      "   355|      6240|     0.126539|  2.02787e-05|  0.02%|                pg_loss1 = -mb_advantages * ratio\n",
      "   356|      6240|     0.150076|  2.40507e-05|  0.02%|                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - self.clip_coef, 1 + self.clip_coef)\n",
      "   357|      6240|     0.166616|  2.67014e-05|  0.03%|                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
      "   358|         0|            0|            0|  0.00%|\n",
      "   359|         0|            0|            0|  0.00%|                # Value loss\n",
      "   360|      6240|    0.0798452|  1.27957e-05|  0.01%|                newvalue = newvalue.view(-1)\n",
      "   361|      6240|    0.0375476|  6.01724e-06|  0.01%|                if self.clip_vloss:\n",
      "   362|      6240|     0.250304|  4.01128e-05|  0.04%|                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
      "(call)|      6240|     0.137204|  2.19879e-05|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:26 wrapped\n",
      "   363|     12480|     0.250248|  2.00519e-05|  0.04%|                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
      "   364|      6240|     0.123442|  1.97824e-05|  0.02%|                        newvalue - b_values[mb_inds],\n",
      "   365|      6240|    0.0355008|  5.68923e-06|  0.01%|                        -self.clip_coef,\n",
      "   366|      6240|    0.0308771|  4.94826e-06|  0.00%|                        self.clip_coef,\n",
      "   367|         0|            0|            0|  0.00%|                    )\n",
      "   368|      6240|     0.168283|  2.69685e-05|  0.03%|                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
      "(call)|      6240|    0.0897348|  1.43806e-05|  0.01%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:26 wrapped\n",
      "   369|      6240|     0.072412|  1.16045e-05|  0.01%|                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
      "   370|      6240|     0.174811|  2.80146e-05|  0.03%|                    v_loss = 0.5 * v_loss_max.mean()\n",
      "   371|         0|            0|            0|  0.00%|                else:\n",
      "   372|         0|            0|            0|  0.00%|                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
      "   373|         0|            0|            0|  0.00%|\n",
      "   374|      6240|    0.0923479|  1.47993e-05|  0.01%|                entropy_loss = entropy.mean()\n",
      "   375|      6240|     0.277772|  4.45147e-05|  0.04%|                loss = pg_loss - self.entropy_coef * entropy_loss + v_loss * self.value_coef\n",
      "   376|         0|            0|            0|  0.00%|\n",
      "   377|      6240|    0.0947897|  1.51907e-05|  0.02%|                self.optimizer.zero_grad()\n",
      "(call)|      6240|       4.2261|   0.00067726|  0.68%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:216 zero_grad\n",
      "   378|      6240|     0.117341|  1.88047e-05|  0.02%|                loss.backward()\n",
      "(call)|      6240|      5.76886|  0.000924496|  0.93%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:340 backward\n",
      "   379|      6240|     0.145366|  2.32959e-05|  0.02%|                nn.utils.clip_grad_norm_(self.parameters(), self.max_grad_norm)\n",
      "(call)|      6240|      18.4941|    0.0029638|  2.98%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py:9 clip_grad_norm_\n",
      "   380|      6240|     0.103887|  1.66485e-05|  0.02%|                self.optimizer.step()\n",
      "(call)|      6240|      15.9495|   0.00255602|  2.57%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:108 wrapper\n",
      "   381|         0|            0|            0|  0.00%|\n",
      "   382|      1560|   0.00868678|  5.56845e-06|  0.00%|            if self.target_kl is not None:\n",
      "   383|         0|            0|            0|  0.00%|                if approx_kl > self.target_kl:\n",
      "   384|         0|            0|            0|  0.00%|                    break\n",
      "   385|         0|            0|            0|  0.00%|\n",
      "   386|       390|   0.00417733|  1.07111e-05|  0.00%|        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
      "   387|       390|   0.00551343|   1.4137e-05|  0.00%|        var_y = np.var(y_true)\n",
      "(call)|       390|    0.0914493|  0.000234485|  0.01%|# <__array_function__ internals>:177 var\n",
      "   388|       390|    0.0098412|  2.52339e-05|  0.00%|        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
      "(call)|       390|    0.0635643|  0.000162985|  0.01%|# <__array_function__ internals>:177 var\n",
      "   389|         0|            0|            0|  0.00%|\n",
      "   390|         0|            0|            0|  0.00%|        # compute L_\\infty change in probabilities\n",
      "   391|         0|            0|            0|  0.00%|        # TODO: split into minibatches?\n",
      "   392|       390|    0.0165994|  4.25626e-05|  0.00%|        _, _, _, _, new_probs = self.get_action_and_value(b_obs, legal_actions_mask=b_legal_actions_mask, action=b_actions.long())\n",
      "(call)|       390|     0.898444|    0.0023037|  0.14%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:237 get_action_and_value\n",
      "   393|       390|    0.0327353|  8.39368e-05|  0.01%|        self.max_policy_diff = (new_probs - b_probs).abs().max().item()\n",
      "(call)|       390|    0.0227168|  5.82481e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1210 __setattr__\n",
      "   394|         0|            0|            0|  0.00%|\n",
      "   395|         0|            0|            0|  0.00%|        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
      "   396|       390|   0.00235891|   6.0485e-06|  0.00%|        if self.writer is not None:\n",
      "   397|         0|            0|            0|  0.00%|            self.writer.add_scalar(\"charts/learning_rate\", self.optimizer.param_groups[0][\"lr\"], self.total_steps_done)\n",
      "   398|         0|            0|            0|  0.00%|            self.writer.add_scalar(\"losses/value_loss\", v_loss.item(), self.total_steps_done)\n",
      "   399|         0|            0|            0|  0.00%|            self.writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), self.total_steps_done)\n",
      "   400|         0|            0|            0|  0.00%|            self.writer.add_scalar(\"losses/entropy\", entropy_loss.item(), self.total_steps_done)\n",
      "   401|         0|            0|            0|  0.00%|            self.writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), self.total_steps_done)\n",
      "   402|         0|            0|            0|  0.00%|            self.writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), self.total_steps_done)\n",
      "   403|         0|            0|            0|  0.00%|            self.writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), self.total_steps_done)\n",
      "   404|         0|            0|            0|  0.00%|            self.writer.add_scalar(\"losses/explained_variance\", explained_var, self.total_steps_done)\n",
      "   405|         0|            0|            0|  0.00%|            self.writer.add_scalar(\"charts/SPS\", int(self.total_steps_done / (time.time() - self.start_time)), self.total_steps_done)\n",
      "   406|         0|            0|            0|  0.00%|\n",
      "   407|       390|   0.00233078|  5.97636e-06|  0.00%|        if self.use_wandb:\n",
      "   408|         0|            0|            0|  0.00%|            import wandb\n",
      "   409|         0|            0|            0|  0.00%|            prefix = f'network_{self.player_id}'\n",
      "   410|         0|            0|            0|  0.00%|            wandb.log({\n",
      "   411|         0|            0|            0|  0.00%|                f\"{prefix}/learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n",
      "   412|         0|            0|            0|  0.00%|                f\"{prefix}/value_loss\": v_loss.item(),\n",
      "   413|         0|            0|            0|  0.00%|                f\"{prefix}/policy_loss\": pg_loss.item(),\n",
      "   414|         0|            0|            0|  0.00%|                f\"{prefix}/entropy\": entropy_loss.item(),\n",
      "   415|         0|            0|            0|  0.00%|                f\"{prefix}/old_approx_kl\": old_approx_kl.item(),\n",
      "   416|         0|            0|            0|  0.00%|                f\"{prefix}/approx_kl\": approx_kl.item(),\n",
      "   417|         0|            0|            0|  0.00%|                f\"{prefix}/clipfrac\": np.mean(clipfracs),\n",
      "   418|         0|            0|            0|  0.00%|                f\"{prefix}/explained_variance\": explained_var,\n",
      "   419|         0|            0|            0|  0.00%|                f\"{prefix}/SPS\": int(self.total_steps_done / (time.time() - self.start_time)), # TODO: This is a bit silly if you do anything like an inline eval\n",
      "   420|         0|            0|            0|  0.00%|                f\"{prefix}/total_steps_done\": self.total_steps_done,\n",
      "   421|         0|            0|            0|  0.00%|                f\"{prefix}/updates_done\": self.updates_done,\n",
      "   422|         0|            0|            0|  0.00%|                f\"{prefix}/max_policy_diff\": self.max_policy_diff,\n",
      "   423|         0|            0|            0|  0.00%|            }, commit=False)\n",
      "   424|         0|            0|            0|  0.00%|\n",
      "   425|         0|            0|            0|  0.00%|        # Update counters\n",
      "   426|       390|   0.00509119|  1.30543e-05|  0.00%|        self.updates_done += 1\n",
      "(call)|       390|    0.0160143|  4.10624e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1210 __setattr__\n",
      "   427|       390|   0.00484705|  1.24283e-05|  0.00%|        self.cur_batch_idx = 0\n",
      "(call)|       390|     0.015475|  3.96796e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1210 __setattr__\n",
      "   428|         0|            0|            0|  0.00%|\n",
      "   429|         2|  4.76837e-06|  2.38419e-06|  0.00%|    def save(self):\n",
      "   430|         2|  5.72205e-06|  2.86102e-06|  0.00%|        restore_dict = dict()\n",
      "   431|         4|  1.16825e-05|  2.92063e-06|  0.00%|        for name, model in self._savers:\n",
      "   432|         2|  1.54972e-05|   7.7486e-06|  0.00%|            restore_dict[name] = model.state_dict()\n",
      "(call)|         2|    0.0015254|  0.000762701|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1316 state_dict\n",
      "   433|         2|  4.29153e-06|  2.14577e-06|  0.00%|        return restore_dict\n",
      "   434|         0|            0|            0|  0.00%|\n",
      "   435|         0|            0|            0|  0.00%|    def restore(self, restore_dict):\n",
      "   436|         0|            0|            0|  0.00%|        for name, model in self._savers:\n",
      "   437|         0|            0|            0|  0.00%|            model.load_state_dict(restore_dict[name])\n",
      "   438|         0|            0|            0|  0.00%|\n",
      "   439|       390|    0.0010004|  2.56514e-06|  0.00%|    def get_max_policy_diff(self):\n",
      "   440|       390|  0.000948668|  2.43248e-06|  0.00%|        return self.max_policy_diff\n",
      "   441|         0|            0|            0|  0.00%|\n",
      "File: /apps/open_spiel/open_spiel/python/env_decorator.py\n",
      "File duration: 15.8868s (2.56%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|from builtins import classmethod\n",
      "     2|         0|            0|            0|  0.00%|from open_spiel.python.rl_environment import Environment\n",
      "     3|         0|            0|            0|  0.00%|import torch\n",
      "     4|         0|            0|            0|  0.00%|from collections import defaultdict\n",
      "     5|         0|            0|            0|  0.00%|import numpy as np\n",
      "     6|         0|            0|            0|  0.00%|import collections\n",
      "     7|         0|            0|            0|  0.00%|from typing import Callable\n",
      "     8|         0|            0|            0|  0.00%|\n",
      "     9|         0|            0|            0|  0.00%|class EnvDecorator(object):\n",
      "    10|         0|            0|            0|  0.00%|\n",
      "    11|         0|            0|            0|  0.00%|    def __init__(self, env: Environment) -> None:\n",
      "    12|         0|            0|            0|  0.00%|        self._env = env\n",
      "    13|         0|            0|            0|  0.00%|        self.env_attributes = [attribute for attribute in self._env.__dict__.keys()]\n",
      "    14|         0|            0|            0|  0.00%|        self.env_methods = [m for m in dir(self._env) if not m.startswith('_') and m not in self.env_attributes]\n",
      "    15|         0|            0|            0|  0.00%|\n",
      "    16|         0|            0|            0|  0.00%|    def __getattr__(self, func):\n",
      "    17|         0|            0|            0|  0.00%|        if func in self.env_methods:\n",
      "    18|         0|            0|            0|  0.00%|            def method(*args):\n",
      "    19|         0|            0|            0|  0.00%|                return getattr(self._env, func)(*args)\n",
      "    20|         0|            0|            0|  0.00%|            return method\n",
      "    21|         0|            0|            0|  0.00%|        elif func in self.env_attributes:\n",
      "    22|         0|            0|            0|  0.00%|            return getattr(self._env, func)\n",
      "    23|         0|            0|            0|  0.00%|        else:\n",
      "    24|         0|            0|            0|  0.00%|            # For nesting decorators\n",
      "    25|         0|            0|            0|  0.00%|            if isinstance(self._env, EnvDecorator):\n",
      "    26|         0|            0|            0|  0.00%|                return self._env.__getattr__(func)\n",
      "    27|         0|            0|            0|  0.00%|            raise AttributeError(func)\n",
      "    28|         0|            0|            0|  0.00%|\n",
      "    29|     99840|     0.160694|  1.60952e-06|  0.03%|    def step(self, step_outputs):\n",
      "    30|     99840|     0.654188|  6.55237e-06|  0.11%|        _ = self._env.step(step_outputs)\n",
      "(call)|     99840|      188.282|   0.00188583| 30.32%|# /apps/open_spiel/open_spiel/python/rl_environment.py:302 step\n",
      "    31|     99840|     0.673319|  6.74398e-06|  0.11%|        return self.get_time_step()\n",
      "(call)|     99840|      129.165|   0.00129372| 20.80%|# /apps/open_spiel/open_spiel/python/env_decorator.py:48 get_time_step\n",
      "    32|         0|            0|            0|  0.00%|\n",
      "    33|     24958|    0.0373662|  1.49716e-06|  0.01%|    def reset(self):\n",
      "    34|     24958|     0.182461|  7.31071e-06|  0.03%|        _ = self._env.reset()\n",
      "(call)|     24958|      46.1562|   0.00184935|  7.43%|# /apps/open_spiel/open_spiel/python/rl_environment.py:344 reset\n",
      "    35|     24958|      0.16357|  6.55383e-06|  0.03%|        return self.get_time_step()\n",
      "(call)|     24958|      28.0272|   0.00112298|  4.51%|# /apps/open_spiel/open_spiel/python/env_decorator.py:48 get_time_step\n",
      "    36|         0|            0|            0|  0.00%|\n",
      "    37|         0|            0|            0|  0.00%|    @property\n",
      "    38|         0|            0|            0|  0.00%|    def env(self) -> Environment:\n",
      "    39|         0|            0|            0|  0.00%|        return self._env\n",
      "    40|         0|            0|            0|  0.00%|\n",
      "    41|         0|            0|            0|  0.00%|\n",
      "    42|         0|            0|            0|  0.00%|class NormalizingEnvDecorator(EnvDecorator):\n",
      "    43|         0|            0|            0|  0.00%|\n",
      "    44|         0|            0|            0|  0.00%|    def __init__(self, env: Environment, reward_normalizer: torch.tensor = None) -> None:\n",
      "    45|         0|            0|            0|  0.00%|        super().__init__(env)\n",
      "    46|         0|            0|            0|  0.00%|        self.reward_normalizer = reward_normalizer\n",
      "    47|         0|            0|            0|  0.00%|\n",
      "    48|    199684|     0.320497|  1.60502e-06|  0.05%|    def get_time_step(self):\n",
      "    49|    199684|      1.46126|  7.31788e-06|  0.24%|        time_step = self._env.get_time_step()\n",
      "(call)|    199684|      236.977|   0.00118676| 38.16%|# /apps/open_spiel/open_spiel/python/rl_environment.py:237 get_time_step\n",
      "    50|         0|            0|            0|  0.00%|\n",
      "    51|    199684|     0.458999|  2.29863e-06|  0.07%|        if self.reward_normalizer is not None:\n",
      "    52|    199684|      8.52245|  4.26797e-05|  1.37%|            time_step.rewards[:] = (torch.tensor(time_step.rewards) / self.reward_normalizer).tolist()\n",
      "    53|         0|            0|            0|  0.00%|\n",
      "    54|    199684|       2.9153|  1.45996e-05|  0.47%|        if np.isnan(time_step.rewards).any():\n",
      "(call)|    199684|        1.999|  1.00108e-05|  0.32%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py:54 _any\n",
      "    55|         0|            0|            0|  0.00%|            raise ValueError(\"Nan reward after normalization!\")\n",
      "    56|    199684|     0.336684|  1.68608e-06|  0.05%|        return time_step\n",
      "    57|         0|            0|            0|  0.00%|\n",
      "    58|         0|            0|            0|  0.00%|\n",
      "    59|         0|            0|            0|  0.00%|class PotentialShapingEnvDecorator(EnvDecorator):\n",
      "    60|         0|            0|            0|  0.00%|\n",
      "    61|         0|            0|            0|  0.00%|    def __init__(self, env: Environment, potential_function: Callable, n_players: int, scale_coef: float = 1.) -> None:\n",
      "    62|         0|            0|            0|  0.00%|        super().__init__(env)\n",
      "    63|         0|            0|            0|  0.00%|        self.potential_function = potential_function\n",
      "    64|         0|            0|            0|  0.00%|        self.n_players = n_players\n",
      "    65|         0|            0|            0|  0.00%|        self.last_potential = np.zeros(self.n_players)\n",
      "    66|         0|            0|            0|  0.00%|        self.scale_coef = scale_coef\n",
      "    67|         0|            0|            0|  0.00%|        # TODO: Store potentials for wandb and decide how to log them\n",
      "    68|         0|            0|            0|  0.00%|        # TODO: Are these way too small to be useful?\n",
      "    69|         0|            0|            0|  0.00%|\n",
      "    70|         0|            0|            0|  0.00%|    def step(self, step_outputs):\n",
      "    71|         0|            0|            0|  0.00%|        state = self._env._state\n",
      "    72|         0|            0|            0|  0.00%|        current_player = state.current_player()\n",
      "    73|         0|            0|            0|  0.00%|        if current_player == self.n_players - 1:\n",
      "    74|         0|            0|            0|  0.00%|            # Update potentials once\n",
      "    75|         0|            0|            0|  0.00%|            current_potential = self.potential_function(state)\n",
      "    76|         0|            0|            0|  0.00%|            self.last_potential = current_potential\n",
      "    77|         0|            0|            0|  0.00%|        _ = self._env.step(step_outputs)\n",
      "    78|         0|            0|            0|  0.00%|        return self.get_time_step()\n",
      "    79|         0|            0|            0|  0.00%|\n",
      "    80|         0|            0|            0|  0.00%|    def get_time_step(self):\n",
      "    81|         0|            0|            0|  0.00%|        time_step = self._env.get_time_step()\n",
      "    82|         0|            0|            0|  0.00%|        state = self._env._state\n",
      "    83|         0|            0|            0|  0.00%|        if time_step.last():\n",
      "    84|         0|            0|            0|  0.00%|            # There's no reason to add the current potential just to subtract it - it won't make a difference. So let's just undo all the potentials right now\n",
      "    85|         0|            0|            0|  0.00%|            shaped_reward = -self.last_potential\n",
      "    86|         0|            0|            0|  0.00%|        else:\n",
      "    87|         0|            0|            0|  0.00%|            current_potential = self.potential_function(state)\n",
      "    88|         0|            0|            0|  0.00%|            shaped_reward = current_potential - self.last_potential\n",
      "    89|         0|            0|            0|  0.00%|\n",
      "    90|         0|            0|            0|  0.00%|        new_rewards = torch.tensor(time_step.rewards) + (shaped_reward * self.scale_coef)\n",
      "    91|         0|            0|            0|  0.00%|\n",
      "    92|         0|            0|            0|  0.00%|        time_step.rewards[:] = new_rewards\n",
      "    93|         0|            0|            0|  0.00%|        return time_step\n",
      "    94|         0|            0|            0|  0.00%|\n",
      "    95|         0|            0|            0|  0.00%|    def reset(self):\n",
      "    96|         0|            0|            0|  0.00%|        _ = self._env.reset()\n",
      "    97|         0|            0|            0|  0.00%|        self.last_potential = np.zeros(self.n_players)\n",
      "    98|         0|            0|            0|  0.00%|        return self.get_time_step()\n",
      "    99|         0|            0|            0|  0.00%|\n",
      "   100|         0|            0|            0|  0.00%|class RewardShapingEnvDecorator(EnvDecorator):\n",
      "   101|         0|            0|            0|  0.00%|\n",
      "   102|         0|            0|            0|  0.00%|    def __init__(self, env: Environment, reward_function: Callable, schedule_function: Callable) -> None:\n",
      "   103|         0|            0|            0|  0.00%|        super().__init__(env)\n",
      "   104|         0|            0|            0|  0.00%|        self.reward_function = reward_function\n",
      "   105|         0|            0|            0|  0.00%|        self.schedule_function = schedule_function\n",
      "   106|         0|            0|            0|  0.00%|        self.t = 0\n",
      "   107|         0|            0|            0|  0.00%|\n",
      "   108|         0|            0|            0|  0.00%|    def get_time_step(self):\n",
      "   109|         0|            0|            0|  0.00%|        time_step = self._env.get_time_step()\n",
      "   110|         0|            0|            0|  0.00%|        state = self._env._state\n",
      "   111|         0|            0|            0|  0.00%|        lam = self.schedule_function(self.t)\n",
      "   112|         0|            0|            0|  0.00%|        new_rewards = lam * self.reward_function(state) + (1 - lam) * torch.tensor(time_step.rewards)\n",
      "   113|         0|            0|            0|  0.00%|        time_step.rewards[:] = new_rewards\n",
      "   114|         0|            0|            0|  0.00%|        return time_step\n",
      "   115|         0|            0|            0|  0.00%|\n",
      "   116|         0|            0|            0|  0.00%|    def step(self, step_outputs):\n",
      "   117|         0|            0|            0|  0.00%|        _ = self._env.step(step_outputs)\n",
      "   118|         0|            0|            0|  0.00%|        self.t += 1\n",
      "   119|         0|            0|            0|  0.00%|        return self.get_time_step()\n",
      "   120|         0|            0|            0|  0.00%|\n",
      "   121|         0|            0|            0|  0.00%|class TrapEnvDecorator(EnvDecorator):\n",
      "   122|         0|            0|            0|  0.00%|    '''Sets \"trap\" actions, which mirror the real actions, but cause you a loss of value after horizon turns (or all explode at the end)'''\n",
      "   123|         0|            0|            0|  0.00%|\n",
      "   124|         0|            0|            0|  0.00%|    def __init__(self, env: Environment, trap_value: float, trap_delay: int) -> None:\n",
      "   125|         0|            0|            0|  0.00%|        super().__init__(env)\n",
      "   126|         0|            0|            0|  0.00%|        self.trap_value = trap_value\n",
      "   127|         0|            0|            0|  0.00%|        self.trap_delay = trap_delay\n",
      "   128|         0|            0|            0|  0.00%|        self.traps = defaultdict(lambda: defaultdict(float)) # Player, round, trap\n",
      "   129|         0|            0|            0|  0.00%|        state = self._env._state\n",
      "   130|         0|            0|            0|  0.00%|        self.num_actions = env._game.num_distinct_actions()\n",
      "   131|         0|            0|            0|  0.00%|        self.num_players = env._game.num_players()\n",
      "   132|         0|            0|            0|  0.00%|        self.clear_traps()\n",
      "   133|         0|            0|            0|  0.00%|\n",
      "   134|         0|            0|            0|  0.00%|    def get_time_step(self):\n",
      "   135|         0|            0|            0|  0.00%|        '''Modify time step to add the fake actions'''\n",
      "   136|         0|            0|            0|  0.00%|        time_step = self._env.get_time_step()\n",
      "   137|         0|            0|            0|  0.00%|\n",
      "   138|         0|            0|            0|  0.00%|        for player in range(self.num_players):\n",
      "   139|         0|            0|            0|  0.00%|            time_step.observations['legal_actions'][player] = np.concatenate((time_step.observations['legal_actions'][player], np.array(time_step.observations['legal_actions'][player]) + self.num_actions))\n",
      "   140|         0|            0|            0|  0.00%|\n",
      "   141|         0|            0|            0|  0.00%|        # If state is terminal, punish current traps and then reset\n",
      "   142|         0|            0|            0|  0.00%|        penalties = np.zeros(self.num_players)\n",
      "   143|         0|            0|            0|  0.00%|        if time_step.last():\n",
      "   144|         0|            0|            0|  0.00%|            for player in range(self.num_players):\n",
      "   145|         0|            0|            0|  0.00%|                penalties[player] = sum(self.traps[player].values())\n",
      "   146|         0|            0|            0|  0.00%|            self.traps = defaultdict(lambda: defaultdict(float)) # Reset traps\n",
      "   147|         0|            0|            0|  0.00%|        else:\n",
      "   148|         0|            0|            0|  0.00%|            for player in range(self.num_players):\n",
      "   149|         0|            0|            0|  0.00%|                penalties[player] = self.traps[player][self._env._state.round]\n",
      "   150|         0|            0|            0|  0.00%|\n",
      "   151|         0|            0|            0|  0.00%|        time_step.rewards[:] = torch.tensor(time_step.rewards) - torch.tensor(penalties)\n",
      "   152|         0|            0|            0|  0.00%|\n",
      "   153|         0|            0|            0|  0.00%|        return time_step\n",
      "   154|         0|            0|            0|  0.00%|\n",
      "   155|         0|            0|            0|  0.00%|    def step(self, step_outputs):\n",
      "   156|         0|            0|            0|  0.00%|        player = self._env._state.current_player()\n",
      "   157|         0|            0|            0|  0.00%|        actions = []\n",
      "   158|         0|            0|            0|  0.00%|        for action in step_outputs:\n",
      "   159|         0|            0|            0|  0.00%|            if action >= self.num_actions:\n",
      "   160|         0|            0|            0|  0.00%|                self.traps[player][self._env._state.round + self.trap_delay] += self.trap_value\n",
      "   161|         0|            0|            0|  0.00%|                self.traps_triggered[player] += 1\n",
      "   162|         0|            0|            0|  0.00%|                actions.append(action - self.num_actions)\n",
      "   163|         0|            0|            0|  0.00%|            else:\n",
      "   164|         0|            0|            0|  0.00%|                actions.append(action)\n",
      "   165|         0|            0|            0|  0.00%|        _ = self._env.step(actions)\n",
      "   166|         0|            0|            0|  0.00%|        return self.get_time_step()\n",
      "   167|         0|            0|            0|  0.00%|\n",
      "   168|         0|            0|            0|  0.00%|    def reset(self):\n",
      "   169|         0|            0|            0|  0.00%|        _ = self._env.reset()\n",
      "   170|         0|            0|            0|  0.00%|        self.traps = defaultdict(lambda: defaultdict(float)) # Reset traps\n",
      "   171|         0|            0|            0|  0.00%|        for player in range(self.num_players):\n",
      "   172|         0|            0|            0|  0.00%|            self.traps_triggered_stats[player].append(self.traps_triggered[player])\n",
      "   173|         0|            0|            0|  0.00%|            self.traps_triggered[player] = 0\n",
      "   174|         0|            0|            0|  0.00%|        return self.get_time_step()\n",
      "   175|         0|            0|            0|  0.00%|\n",
      "   176|         0|            0|            0|  0.00%|    def traps_dict(self):\n",
      "   177|         0|            0|            0|  0.00%|        return {'traps': self.traps_triggered_stats}\n",
      "   178|         0|            0|            0|  0.00%|\n",
      "   179|         0|            0|            0|  0.00%|    def clear_traps(self):\n",
      "   180|         0|            0|            0|  0.00%|        self.traps_triggered_stats = defaultdict(list)\n",
      "   181|         0|            0|            0|  0.00%|        self.traps_triggered = defaultdict(int)\n",
      "   182|         0|            0|            0|  0.00%|\n",
      "   183|         0|            0|            0|  0.00%|class StateSavingEnvDecorator(EnvDecorator):\n",
      "   184|         0|            0|            0|  0.00%|\n",
      "   185|         0|            0|            0|  0.00%|    def __init__(self, env: Environment, num_states_to_save = 100) -> None:\n",
      "   186|         0|            0|            0|  0.00%|        super().__init__(env)\n",
      "   187|         0|            0|            0|  0.00%|        self.num_states_to_save = num_states_to_save\n",
      "   188|         0|            0|            0|  0.00%|        self.num_players = env.num_players\n",
      "   189|         0|            0|            0|  0.00%|        self.states = [collections.deque() for _ in range(self.num_players)]\n",
      "   190|         0|            0|            0|  0.00%|\n",
      "   191|         0|            0|            0|  0.00%|    def step(self, step_outputs):\n",
      "   192|         0|            0|            0|  0.00%|        time_step = self._env.get_time_step()\n",
      "   193|         0|            0|            0|  0.00%|        if not time_step.last():\n",
      "   194|         0|            0|            0|  0.00%|            current_player = time_step.current_player()\n",
      "   195|         0|            0|            0|  0.00%|            self.states[current_player].append(time_step.observations['info_state'][current_player])\n",
      "   196|         0|            0|            0|  0.00%|\n",
      "   197|         0|            0|            0|  0.00%|        _ = self._env.step(step_outputs)\n",
      "   198|         0|            0|            0|  0.00%|        return time_step\n",
      "   199|         0|            0|            0|  0.00%|\n",
      "   200|         0|            0|            0|  0.00%|    def get_states(self):\n",
      "   201|         0|            0|            0|  0.00%|        return [list(d) for d in self.states]\n",
      "   202|         0|            0|            0|  0.00%|\n",
      "   203|         0|            0|            0|  0.00%|    @staticmethod\n",
      "   204|         0|            0|            0|  0.00%|    def merge_states(sync_env):\n",
      "   205|         0|            0|            0|  0.00%|        d = []\n",
      "   206|         0|            0|            0|  0.00%|        for e in sync_env.envs:\n",
      "   207|         0|            0|            0|  0.00%|            d += e.get_states()\n",
      "   208|         0|            0|            0|  0.00%|        return d\n",
      "   209|         0|            0|            0|  0.00%|\n",
      "   210|         0|            0|            0|  0.00%|\n",
      "   211|         0|            0|            0|  0.00%|class AuctionStatTrackingDecorator(EnvDecorator):\n",
      "   212|         0|            0|            0|  0.00%|\n",
      "   213|         0|            0|            0|  0.00%|    def __init__(self, env: Environment, clear_on_report: bool = False) -> None:\n",
      "   214|         0|            0|            0|  0.00%|        super().__init__(env)\n",
      "   215|         0|            0|            0|  0.00%|        self.clear_on_report = clear_on_report\n",
      "   216|         0|            0|            0|  0.00%|        self.clear()\n",
      "   217|         0|            0|            0|  0.00%|\n",
      "   218|         0|            0|            0|  0.00%|    def clear(self):\n",
      "   219|         0|            0|            0|  0.00%|        self.rewards = defaultdict(list)\n",
      "   220|         0|            0|            0|  0.00%|        self.payments = defaultdict(list)\n",
      "   221|         0|            0|            0|  0.00%|        self.allocations = defaultdict(list)\n",
      "   222|         0|            0|            0|  0.00%|        self.auction_lengths = []\n",
      "   223|         0|            0|            0|  0.00%|        self.welfares = []\n",
      "   224|         0|            0|            0|  0.00%|        self.revenues = []\n",
      "   225|         0|            0|            0|  0.00%|\n",
      "   226|         0|            0|            0|  0.00%|    def step(self, step_outputs):\n",
      "   227|         0|            0|            0|  0.00%|        _ = self._env.step(step_outputs)\n",
      "   228|         0|            0|            0|  0.00%|        time_step = self._env.get_time_step()\n",
      "   229|         0|            0|            0|  0.00%|        state = self._env._state\n",
      "   230|         0|            0|            0|  0.00%|\n",
      "   231|         0|            0|            0|  0.00%|        if time_step.last():\n",
      "   232|         0|            0|            0|  0.00%|            for player_id, reward in enumerate(time_step.rewards):\n",
      "   233|         0|            0|            0|  0.00%|                self.rewards[player_id].append(reward)\n",
      "   234|         0|            0|            0|  0.00%|            for player_id, payment in enumerate(state.get_final_payments()):\n",
      "   235|         0|            0|            0|  0.00%|                self.payments[player_id].append(payment)\n",
      "   236|         0|            0|            0|  0.00%|            self.revenues.append(state.revenue)\n",
      "   237|         0|            0|            0|  0.00%|            for player_id, allocation in enumerate(state.get_allocation()):\n",
      "   238|         0|            0|            0|  0.00%|                self.allocations[player_id].append(allocation.tolist())\n",
      "   239|         0|            0|            0|  0.00%|            self.auction_lengths.append(state.round)\n",
      "   240|         0|            0|            0|  0.00%|            self.welfares.append(state.get_welfare())\n",
      "   241|         0|            0|            0|  0.00%|\n",
      "   242|         0|            0|            0|  0.00%|        return self.get_time_step()\n",
      "   243|         0|            0|            0|  0.00%|\n",
      "   244|         0|            0|            0|  0.00%|    def stats_dict(self):\n",
      "   245|         0|            0|            0|  0.00%|        return {\n",
      "   246|         0|            0|            0|  0.00%|            'raw_rewards': self.rewards,\n",
      "   247|         0|            0|            0|  0.00%|            'allocations': self.allocations,\n",
      "   248|         0|            0|            0|  0.00%|            'payments': self.payments,\n",
      "   249|         0|            0|            0|  0.00%|            'auction_lengths': self.auction_lengths,\n",
      "   250|         0|            0|            0|  0.00%|            'revenues': self.revenues,\n",
      "   251|         0|            0|            0|  0.00%|            'welfares': self.welfares,\n",
      "   252|         0|            0|            0|  0.00%|        }\n",
      "   253|         0|            0|            0|  0.00%|\n",
      "   254|         0|            0|            0|  0.00%|    @staticmethod\n",
      "   255|         0|            0|            0|  0.00%|    def merge_stats(sync_env):\n",
      "   256|         0|            0|            0|  0.00%|        d = []\n",
      "   257|         0|            0|            0|  0.00%|        for e in sync_env.envs:\n",
      "   258|         0|            0|            0|  0.00%|            stats = dict()\n",
      "   259|         0|            0|            0|  0.00%|            if hasattr(e, 'stats_dict'):\n",
      "   260|         0|            0|            0|  0.00%|                stats.update(e.stats_dict())\n",
      "   261|         0|            0|            0|  0.00%|                if e.clear_on_report:\n",
      "   262|         0|            0|            0|  0.00%|                    e.clear()\n",
      "   263|         0|            0|            0|  0.00%|\n",
      "   264|         0|            0|            0|  0.00%|            if hasattr(e, 'traps_dict'):\n",
      "   265|         0|            0|            0|  0.00%|                stats.update(e.traps_dict())\n",
      "   266|         0|            0|            0|  0.00%|                if e.clear_on_report:\n",
      "   267|         0|            0|            0|  0.00%|                    e.clear_traps()\n",
      "   268|         0|            0|            0|  0.00%|\n",
      "   269|         0|            0|            0|  0.00%|            d.append(stats)\n",
      "   270|         0|            0|            0|  0.00%|\n",
      "   271|         0|            0|            0|  0.00%|        stats_dict = d[0]\n",
      "   272|         0|            0|            0|  0.00%|        for other_dict in d[1:]:\n",
      "   273|         0|            0|            0|  0.00%|            for k, v in other_dict.items():\n",
      "   274|         0|            0|            0|  0.00%|                if isinstance(v, collections.defaultdict):\n",
      "   275|         0|            0|            0|  0.00%|                    for k2, v2 in v.items():\n",
      "   276|         0|            0|            0|  0.00%|                        stats_dict[k][k2] += v2\n",
      "   277|         0|            0|            0|  0.00%|                else:\n",
      "   278|         0|            0|            0|  0.00%|                    stats_dict[k] += v\n",
      "   279|         0|            0|            0|  0.00%|        return stats_dict\n",
      "   280|         0|            0|            0|  0.00%|\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py\n",
      "File duration: 15.2059s (2.45%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|\"\"\"\n",
      "     2|         0|            0|            0|  0.00%|Array methods which are called by both the C-code for the method\n",
      "     3|         0|            0|            0|  0.00%|and the Python code for the NumPy-namespace function\n",
      "     4|         0|            0|            0|  0.00%|\n",
      "     5|         0|            0|            0|  0.00%|\"\"\"\n",
      "     6|         0|            0|            0|  0.00%|import warnings\n",
      "     7|         0|            0|            0|  0.00%|from contextlib import nullcontext\n",
      "     8|         0|            0|            0|  0.00%|\n",
      "     9|         0|            0|            0|  0.00%|from numpy.core import multiarray as mu\n",
      "    10|         0|            0|            0|  0.00%|from numpy.core import umath as um\n",
      "    11|         0|            0|            0|  0.00%|from numpy.core.multiarray import asanyarray\n",
      "    12|         0|            0|            0|  0.00%|from numpy.core import numerictypes as nt\n",
      "    13|         0|            0|            0|  0.00%|from numpy.core import _exceptions\n",
      "    14|         0|            0|            0|  0.00%|from numpy._globals import _NoValue\n",
      "    15|         0|            0|            0|  0.00%|from numpy.compat import pickle, os_fspath\n",
      "    16|         0|            0|            0|  0.00%|\n",
      "    17|         0|            0|            0|  0.00%|# save those O(100) nanoseconds!\n",
      "    18|         0|            0|            0|  0.00%|umr_maximum = um.maximum.reduce\n",
      "    19|         0|            0|            0|  0.00%|umr_minimum = um.minimum.reduce\n",
      "    20|         0|            0|            0|  0.00%|umr_sum = um.add.reduce\n",
      "    21|         0|            0|            0|  0.00%|umr_prod = um.multiply.reduce\n",
      "    22|         0|            0|            0|  0.00%|umr_any = um.logical_or.reduce\n",
      "    23|         0|            0|            0|  0.00%|umr_all = um.logical_and.reduce\n",
      "    24|         0|            0|            0|  0.00%|\n",
      "    25|         0|            0|            0|  0.00%|# Complex types to -> (2,)float view for fast-path computation in _var()\n",
      "    26|         0|            0|            0|  0.00%|_complex_to_float = {\n",
      "    27|         0|            0|            0|  0.00%|    nt.dtype(nt.csingle) : nt.dtype(nt.single),\n",
      "    28|         0|            0|            0|  0.00%|    nt.dtype(nt.cdouble) : nt.dtype(nt.double),\n",
      "    29|         0|            0|            0|  0.00%|}\n",
      "    30|         0|            0|            0|  0.00%|# Special case for windows: ensure double takes precedence\n",
      "    31|         0|            0|            0|  0.00%|if nt.dtype(nt.longdouble) != nt.dtype(nt.double):\n",
      "    32|         0|            0|            0|  0.00%|    _complex_to_float.update({\n",
      "    33|         0|            0|            0|  0.00%|        nt.dtype(nt.clongdouble) : nt.dtype(nt.longdouble),\n",
      "    34|         0|            0|            0|  0.00%|    })\n",
      "    35|         0|            0|            0|  0.00%|\n",
      "    36|         0|            0|            0|  0.00%|# avoid keyword arguments to speed up parsing, saves about 15%-20% for very\n",
      "    37|         0|            0|            0|  0.00%|# small reductions\n",
      "    38|         0|            0|            0|  0.00%|def _amax(a, axis=None, out=None, keepdims=False,\n",
      "    39|         0|            0|            0|  0.00%|          initial=_NoValue, where=True):\n",
      "    40|         0|            0|            0|  0.00%|    return umr_maximum(a, axis, None, out, keepdims, initial, where)\n",
      "    41|         0|            0|            0|  0.00%|\n",
      "    42|         0|            0|            0|  0.00%|def _amin(a, axis=None, out=None, keepdims=False,\n",
      "    43|         0|            0|            0|  0.00%|          initial=_NoValue, where=True):\n",
      "    44|         0|            0|            0|  0.00%|    return umr_minimum(a, axis, None, out, keepdims, initial, where)\n",
      "    45|         0|            0|            0|  0.00%|\n",
      "    46|    424364|     0.695827|  1.63969e-06|  0.11%|def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "    47|         0|            0|            0|  0.00%|         initial=_NoValue, where=True):\n",
      "    48|    424364|      2.42175|  5.70679e-06|  0.39%|    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "    49|         0|            0|            0|  0.00%|\n",
      "    50|         0|            0|            0|  0.00%|def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "    51|         0|            0|            0|  0.00%|          initial=_NoValue, where=True):\n",
      "    52|         0|            0|            0|  0.00%|    return umr_prod(a, axis, dtype, out, keepdims, initial, where)\n",
      "    53|         0|            0|            0|  0.00%|\n",
      "    54|   1173142|      2.54714|  2.17121e-06|  0.41%|def _any(a, axis=None, dtype=None, out=None, keepdims=False, *, where=True):\n",
      "    55|         0|            0|            0|  0.00%|    # Parsing keyword arguments is currently fairly slow, so avoid it for now\n",
      "    56|   1173142|      2.16612|  1.84643e-06|  0.35%|    if where is True:\n",
      "    57|   1173142|      6.29734|  5.36792e-06|  1.01%|        return umr_any(a, axis, dtype, out, keepdims)\n",
      "    58|         0|            0|            0|  0.00%|    return umr_any(a, axis, dtype, out, keepdims, where=where)\n",
      "    59|         0|            0|            0|  0.00%|\n",
      "    60|     99816|     0.162547|  1.62847e-06|  0.03%|def _all(a, axis=None, dtype=None, out=None, keepdims=False, *, where=True):\n",
      "    61|         0|            0|            0|  0.00%|    # Parsing keyword arguments is currently fairly slow, so avoid it for now\n",
      "    62|     99816|     0.177467|  1.77794e-06|  0.03%|    if where is True:\n",
      "    63|     99816|     0.626265|  6.27419e-06|  0.10%|        return umr_all(a, axis, dtype, out, keepdims)\n",
      "    64|         0|            0|            0|  0.00%|    return umr_all(a, axis, dtype, out, keepdims, where=where)\n",
      "    65|         0|            0|            0|  0.00%|\n",
      "    66|       780|   0.00204492|  2.62169e-06|  0.00%|def _count_reduce_items(arr, axis, keepdims=False, where=True):\n",
      "    67|         0|            0|            0|  0.00%|    # fast-path for the default case\n",
      "    68|       780|   0.00185466|  2.37777e-06|  0.00%|    if where is True:\n",
      "    69|         0|            0|            0|  0.00%|        # no boolean mask given, calculate items according to axis\n",
      "    70|       780|   0.00167084|   2.1421e-06|  0.00%|        if axis is None:\n",
      "    71|       780|    0.0030756|  3.94308e-06|  0.00%|            axis = tuple(range(arr.ndim))\n",
      "    72|         0|            0|            0|  0.00%|        elif not isinstance(axis, tuple):\n",
      "    73|         0|            0|            0|  0.00%|            axis = (axis,)\n",
      "    74|       780|   0.00485253|   6.2212e-06|  0.00%|        items = nt.intp(1)\n",
      "    75|      1560|   0.00363946|  2.33299e-06|  0.00%|        for ax in axis:\n",
      "    76|       780|   0.00387096|  4.96277e-06|  0.00%|            items *= arr.shape[mu.normalize_axis_index(ax, arr.ndim)]\n",
      "    77|         0|            0|            0|  0.00%|    else:\n",
      "    78|         0|            0|            0|  0.00%|        # TODO: Optimize case when `where` is broadcast along a non-reduction\n",
      "    79|         0|            0|            0|  0.00%|        # axis and full sum is more excessive than needed.\n",
      "    80|         0|            0|            0|  0.00%|\n",
      "    81|         0|            0|            0|  0.00%|        # guarded to protect circular imports\n",
      "    82|         0|            0|            0|  0.00%|        from numpy.lib.stride_tricks import broadcast_to\n",
      "    83|         0|            0|            0|  0.00%|        # count True values in (potentially broadcasted) boolean mask\n",
      "    84|         0|            0|            0|  0.00%|        items = umr_sum(broadcast_to(where, arr.shape), axis, nt.intp, None,\n",
      "    85|         0|            0|            0|  0.00%|                        keepdims)\n",
      "    86|       780|   0.00139523|  1.78875e-06|  0.00%|    return items\n",
      "    87|         0|            0|            0|  0.00%|\n",
      "    88|         0|            0|            0|  0.00%|# Numpy 1.17.0, 2019-02-24\n",
      "    89|         0|            0|            0|  0.00%|# Various clip behavior deprecations, marked with _clip_dep as a prefix.\n",
      "    90|         0|            0|            0|  0.00%|\n",
      "    91|         0|            0|            0|  0.00%|def _clip_dep_is_scalar_nan(a):\n",
      "    92|         0|            0|            0|  0.00%|    # guarded to protect circular imports\n",
      "    93|         0|            0|            0|  0.00%|    from numpy.core.fromnumeric import ndim\n",
      "    94|         0|            0|            0|  0.00%|    if ndim(a) != 0:\n",
      "    95|         0|            0|            0|  0.00%|        return False\n",
      "    96|         0|            0|            0|  0.00%|    try:\n",
      "    97|         0|            0|            0|  0.00%|        return um.isnan(a)\n",
      "    98|         0|            0|            0|  0.00%|    except TypeError:\n",
      "    99|         0|            0|            0|  0.00%|        return False\n",
      "   100|         0|            0|            0|  0.00%|\n",
      "   101|         0|            0|            0|  0.00%|def _clip_dep_is_byte_swapped(a):\n",
      "   102|         0|            0|            0|  0.00%|    if isinstance(a, mu.ndarray):\n",
      "   103|         0|            0|            0|  0.00%|        return not a.dtype.isnative\n",
      "   104|         0|            0|            0|  0.00%|    return False\n",
      "   105|         0|            0|            0|  0.00%|\n",
      "   106|         0|            0|            0|  0.00%|def _clip_dep_invoke_with_casting(ufunc, *args, out=None, casting=None, **kwargs):\n",
      "   107|         0|            0|            0|  0.00%|    # normal path\n",
      "   108|         0|            0|            0|  0.00%|    if casting is not None:\n",
      "   109|         0|            0|            0|  0.00%|        return ufunc(*args, out=out, casting=casting, **kwargs)\n",
      "   110|         0|            0|            0|  0.00%|\n",
      "   111|         0|            0|            0|  0.00%|    # try to deal with broken casting rules\n",
      "   112|         0|            0|            0|  0.00%|    try:\n",
      "   113|         0|            0|            0|  0.00%|        return ufunc(*args, out=out, **kwargs)\n",
      "   114|         0|            0|            0|  0.00%|    except _exceptions._UFuncOutputCastingError as e:\n",
      "   115|         0|            0|            0|  0.00%|        # Numpy 1.17.0, 2019-02-24\n",
      "   116|         0|            0|            0|  0.00%|        warnings.warn(\n",
      "   117|         0|            0|            0|  0.00%|            \"Converting the output of clip from {!r} to {!r} is deprecated. \"\n",
      "   118|         0|            0|            0|  0.00%|            \"Pass `casting=\\\"unsafe\\\"` explicitly to silence this warning, or \"\n",
      "   119|         0|            0|            0|  0.00%|            \"correct the type of the variables.\".format(e.from_, e.to),\n",
      "   120|         0|            0|            0|  0.00%|            DeprecationWarning,\n",
      "   121|         0|            0|            0|  0.00%|            stacklevel=2\n",
      "   122|         0|            0|            0|  0.00%|        )\n",
      "   123|         0|            0|            0|  0.00%|        return ufunc(*args, out=out, casting=\"unsafe\", **kwargs)\n",
      "   124|         0|            0|            0|  0.00%|\n",
      "   125|         0|            0|            0|  0.00%|def _clip(a, min=None, max=None, out=None, *, casting=None, **kwargs):\n",
      "   126|         0|            0|            0|  0.00%|    if min is None and max is None:\n",
      "   127|         0|            0|            0|  0.00%|        raise ValueError(\"One of max or min must be given\")\n",
      "   128|         0|            0|            0|  0.00%|\n",
      "   129|         0|            0|            0|  0.00%|    # Numpy 1.17.0, 2019-02-24\n",
      "   130|         0|            0|            0|  0.00%|    # This deprecation probably incurs a substantial slowdown for small arrays,\n",
      "   131|         0|            0|            0|  0.00%|    # it will be good to get rid of it.\n",
      "   132|         0|            0|            0|  0.00%|    if not _clip_dep_is_byte_swapped(a) and not _clip_dep_is_byte_swapped(out):\n",
      "   133|         0|            0|            0|  0.00%|        using_deprecated_nan = False\n",
      "   134|         0|            0|            0|  0.00%|        if _clip_dep_is_scalar_nan(min):\n",
      "   135|         0|            0|            0|  0.00%|            min = -float('inf')\n",
      "   136|         0|            0|            0|  0.00%|            using_deprecated_nan = True\n",
      "   137|         0|            0|            0|  0.00%|        if _clip_dep_is_scalar_nan(max):\n",
      "   138|         0|            0|            0|  0.00%|            max = float('inf')\n",
      "   139|         0|            0|            0|  0.00%|            using_deprecated_nan = True\n",
      "   140|         0|            0|            0|  0.00%|        if using_deprecated_nan:\n",
      "   141|         0|            0|            0|  0.00%|            warnings.warn(\n",
      "   142|         0|            0|            0|  0.00%|                \"Passing `np.nan` to mean no clipping in np.clip has always \"\n",
      "   143|         0|            0|            0|  0.00%|                \"been unreliable, and is now deprecated. \"\n",
      "   144|         0|            0|            0|  0.00%|                \"In future, this will always return nan, like it already does \"\n",
      "   145|         0|            0|            0|  0.00%|                \"when min or max are arrays that contain nan. \"\n",
      "   146|         0|            0|            0|  0.00%|                \"To skip a bound, pass either None or an np.inf of an \"\n",
      "   147|         0|            0|            0|  0.00%|                \"appropriate sign.\",\n",
      "   148|         0|            0|            0|  0.00%|                DeprecationWarning,\n",
      "   149|         0|            0|            0|  0.00%|                stacklevel=2\n",
      "   150|         0|            0|            0|  0.00%|            )\n",
      "   151|         0|            0|            0|  0.00%|\n",
      "   152|         0|            0|            0|  0.00%|    if min is None:\n",
      "   153|         0|            0|            0|  0.00%|        return _clip_dep_invoke_with_casting(\n",
      "   154|         0|            0|            0|  0.00%|            um.minimum, a, max, out=out, casting=casting, **kwargs)\n",
      "   155|         0|            0|            0|  0.00%|    elif max is None:\n",
      "   156|         0|            0|            0|  0.00%|        return _clip_dep_invoke_with_casting(\n",
      "   157|         0|            0|            0|  0.00%|            um.maximum, a, min, out=out, casting=casting, **kwargs)\n",
      "   158|         0|            0|            0|  0.00%|    else:\n",
      "   159|         0|            0|            0|  0.00%|        return _clip_dep_invoke_with_casting(\n",
      "   160|         0|            0|            0|  0.00%|            um.clip, a, min, max, out=out, casting=casting, **kwargs)\n",
      "   161|         0|            0|            0|  0.00%|\n",
      "   162|         0|            0|            0|  0.00%|def _mean(a, axis=None, dtype=None, out=None, keepdims=False, *, where=True):\n",
      "   163|         0|            0|            0|  0.00%|    arr = asanyarray(a)\n",
      "   164|         0|            0|            0|  0.00%|\n",
      "   165|         0|            0|            0|  0.00%|    is_float16_result = False\n",
      "   166|         0|            0|            0|  0.00%|\n",
      "   167|         0|            0|            0|  0.00%|    rcount = _count_reduce_items(arr, axis, keepdims=keepdims, where=where)\n",
      "   168|         0|            0|            0|  0.00%|    if rcount == 0 if where is True else umr_any(rcount == 0, axis=None):\n",
      "   169|         0|            0|            0|  0.00%|        warnings.warn(\"Mean of empty slice.\", RuntimeWarning, stacklevel=2)\n",
      "   170|         0|            0|            0|  0.00%|\n",
      "   171|         0|            0|            0|  0.00%|    # Cast bool, unsigned int, and int to float64 by default\n",
      "   172|         0|            0|            0|  0.00%|    if dtype is None:\n",
      "   173|         0|            0|            0|  0.00%|        if issubclass(arr.dtype.type, (nt.integer, nt.bool_)):\n",
      "   174|         0|            0|            0|  0.00%|            dtype = mu.dtype('f8')\n",
      "   175|         0|            0|            0|  0.00%|        elif issubclass(arr.dtype.type, nt.float16):\n",
      "   176|         0|            0|            0|  0.00%|            dtype = mu.dtype('f4')\n",
      "   177|         0|            0|            0|  0.00%|            is_float16_result = True\n",
      "   178|         0|            0|            0|  0.00%|\n",
      "   179|         0|            0|            0|  0.00%|    ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "   180|         0|            0|            0|  0.00%|    if isinstance(ret, mu.ndarray):\n",
      "   181|         0|            0|            0|  0.00%|        ret = um.true_divide(\n",
      "   182|         0|            0|            0|  0.00%|                ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "   183|         0|            0|            0|  0.00%|        if is_float16_result and out is None:\n",
      "   184|         0|            0|            0|  0.00%|            ret = arr.dtype.type(ret)\n",
      "   185|         0|            0|            0|  0.00%|    elif hasattr(ret, 'dtype'):\n",
      "   186|         0|            0|            0|  0.00%|        if is_float16_result:\n",
      "   187|         0|            0|            0|  0.00%|            ret = arr.dtype.type(ret / rcount)\n",
      "   188|         0|            0|            0|  0.00%|        else:\n",
      "   189|         0|            0|            0|  0.00%|            ret = ret.dtype.type(ret / rcount)\n",
      "   190|         0|            0|            0|  0.00%|    else:\n",
      "   191|         0|            0|            0|  0.00%|        ret = ret / rcount\n",
      "   192|         0|            0|            0|  0.00%|\n",
      "   193|         0|            0|            0|  0.00%|    return ret\n",
      "   194|         0|            0|            0|  0.00%|\n",
      "   195|       780|   0.00298095|  3.82173e-06|  0.00%|def _var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False, *,\n",
      "   196|         0|            0|            0|  0.00%|         where=True):\n",
      "   197|       780|   0.00284505|   3.6475e-06|  0.00%|    arr = asanyarray(a)\n",
      "   198|         0|            0|            0|  0.00%|\n",
      "   199|       780|   0.00689983|  8.84594e-06|  0.00%|    rcount = _count_reduce_items(arr, axis, keepdims=keepdims, where=where)\n",
      "(call)|       780|    0.0224042|  2.87233e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py:66 _count_reduce_items\n",
      "   200|         0|            0|            0|  0.00%|    # Make this warning show up on top.\n",
      "   201|       780|   0.00308371|  3.95347e-06|  0.00%|    if ddof >= rcount if where is True else umr_any(ddof >= rcount, axis=None):\n",
      "   202|         0|            0|            0|  0.00%|        warnings.warn(\"Degrees of freedom <= 0 for slice\", RuntimeWarning,\n",
      "   203|         0|            0|            0|  0.00%|                      stacklevel=2)\n",
      "   204|         0|            0|            0|  0.00%|\n",
      "   205|         0|            0|            0|  0.00%|    # Cast bool, unsigned int, and int to float64 by default\n",
      "   206|       780|   0.00346041|  4.43642e-06|  0.00%|    if dtype is None and issubclass(arr.dtype.type, (nt.integer, nt.bool_)):\n",
      "   207|         0|            0|            0|  0.00%|        dtype = mu.dtype('f8')\n",
      "   208|         0|            0|            0|  0.00%|\n",
      "   209|         0|            0|            0|  0.00%|    # Compute the mean.\n",
      "   210|         0|            0|            0|  0.00%|    # Note that if dtype is not of inexact type then arraymean will\n",
      "   211|         0|            0|            0|  0.00%|    # not be either.\n",
      "   212|       780|   0.00960588|  1.23152e-05|  0.00%|    arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n",
      "   213|         0|            0|            0|  0.00%|    # The shape of rcount has to match arrmean to not change the shape of out\n",
      "   214|         0|            0|            0|  0.00%|    # in broadcasting. Otherwise, it cannot be stored back to arrmean.\n",
      "   215|       780|   0.00256753|   3.2917e-06|  0.00%|    if rcount.ndim == 0:\n",
      "   216|         0|            0|            0|  0.00%|        # fast-path for default case when where is True\n",
      "   217|       780|   0.00229001|  2.93591e-06|  0.00%|        div = rcount\n",
      "   218|         0|            0|            0|  0.00%|    else:\n",
      "   219|         0|            0|            0|  0.00%|        # matching rcount to arrmean when where is specified as array\n",
      "   220|         0|            0|            0|  0.00%|        div = rcount.reshape(arrmean.shape)\n",
      "   221|       780|   0.00235963|  3.02516e-06|  0.00%|    if isinstance(arrmean, mu.ndarray):\n",
      "   222|      1560|    0.0133266|  8.54272e-06|  0.00%|        arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "   223|       780|   0.00199914|    2.563e-06|  0.00%|                                 subok=False)\n",
      "   224|         0|            0|            0|  0.00%|    elif hasattr(arrmean, \"dtype\"):\n",
      "   225|         0|            0|            0|  0.00%|        arrmean = arrmean.dtype.type(arrmean / rcount)\n",
      "   226|         0|            0|            0|  0.00%|    else:\n",
      "   227|         0|            0|            0|  0.00%|        arrmean = arrmean / rcount\n",
      "   228|         0|            0|            0|  0.00%|\n",
      "   229|         0|            0|            0|  0.00%|    # Compute sum of squared deviations from mean\n",
      "   230|         0|            0|            0|  0.00%|    # Note that x may not be inexact and that we need it to be an array,\n",
      "   231|         0|            0|            0|  0.00%|    # not a scalar.\n",
      "   232|       780|   0.00499201|  6.40001e-06|  0.00%|    x = asanyarray(arr - arrmean)\n",
      "   233|         0|            0|            0|  0.00%|\n",
      "   234|       780|   0.00272679|  3.49589e-06|  0.00%|    if issubclass(arr.dtype.type, (nt.floating, nt.integer)):\n",
      "   235|       780|   0.00474477|  6.08304e-06|  0.00%|        x = um.multiply(x, x, out=x)\n",
      "   236|         0|            0|            0|  0.00%|    # Fast-paths for built-in complex types\n",
      "   237|         0|            0|            0|  0.00%|    elif x.dtype in _complex_to_float:\n",
      "   238|         0|            0|            0|  0.00%|        xv = x.view(dtype=(_complex_to_float[x.dtype], (2,)))\n",
      "   239|         0|            0|            0|  0.00%|        um.multiply(xv, xv, out=xv)\n",
      "   240|         0|            0|            0|  0.00%|        x = um.add(xv[..., 0], xv[..., 1], out=x.real).real\n",
      "   241|         0|            0|            0|  0.00%|    # Most general case; includes handling object arrays containing imaginary\n",
      "   242|         0|            0|            0|  0.00%|    # numbers and complex types with non-native byteorder\n",
      "   243|         0|            0|            0|  0.00%|    else:\n",
      "   244|         0|            0|            0|  0.00%|        x = um.multiply(x, um.conjugate(x), out=x).real\n",
      "   245|         0|            0|            0|  0.00%|\n",
      "   246|       780|   0.00462699|  5.93204e-06|  0.00%|    ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      "   247|         0|            0|            0|  0.00%|\n",
      "   248|         0|            0|            0|  0.00%|    # Compute degrees of freedom and make sure it is not negative.\n",
      "   249|       780|   0.00584245|  7.49032e-06|  0.00%|    rcount = um.maximum(rcount - ddof, 0)\n",
      "   250|         0|            0|            0|  0.00%|\n",
      "   251|         0|            0|            0|  0.00%|    # divide by degrees of freedom\n",
      "   252|       780|   0.00255585|  3.27673e-06|  0.00%|    if isinstance(ret, mu.ndarray):\n",
      "   253|         0|            0|            0|  0.00%|        ret = um.true_divide(\n",
      "   254|         0|            0|            0|  0.00%|                ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "   255|       780|   0.00238991|  3.06398e-06|  0.00%|    elif hasattr(ret, 'dtype'):\n",
      "   256|       780|   0.00753307|  9.65779e-06|  0.00%|        ret = ret.dtype.type(ret / rcount)\n",
      "   257|         0|            0|            0|  0.00%|    else:\n",
      "   258|         0|            0|            0|  0.00%|        ret = ret / rcount\n",
      "   259|         0|            0|            0|  0.00%|\n",
      "   260|       780|   0.00222611|  2.85399e-06|  0.00%|    return ret\n",
      "   261|         0|            0|            0|  0.00%|\n",
      "   262|         0|            0|            0|  0.00%|def _std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False, *,\n",
      "   263|         0|            0|            0|  0.00%|         where=True):\n",
      "   264|         0|            0|            0|  0.00%|    ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "   265|         0|            0|            0|  0.00%|               keepdims=keepdims, where=where)\n",
      "   266|         0|            0|            0|  0.00%|\n",
      "   267|         0|            0|            0|  0.00%|    if isinstance(ret, mu.ndarray):\n",
      "   268|         0|            0|            0|  0.00%|        ret = um.sqrt(ret, out=ret)\n",
      "   269|         0|            0|            0|  0.00%|    elif hasattr(ret, 'dtype'):\n",
      "   270|         0|            0|            0|  0.00%|        ret = ret.dtype.type(um.sqrt(ret))\n",
      "   271|         0|            0|            0|  0.00%|    else:\n",
      "   272|         0|            0|            0|  0.00%|        ret = um.sqrt(ret)\n",
      "   273|         0|            0|            0|  0.00%|\n",
      "   274|         0|            0|            0|  0.00%|    return ret\n",
      "   275|         0|            0|            0|  0.00%|\n",
      "   276|         0|            0|            0|  0.00%|def _ptp(a, axis=None, out=None, keepdims=False):\n",
      "   277|         0|            0|            0|  0.00%|    return um.subtract(\n",
      "   278|         0|            0|            0|  0.00%|        umr_maximum(a, axis, None, out, keepdims),\n",
      "   279|         0|            0|            0|  0.00%|        umr_minimum(a, axis, None, None, keepdims),\n",
      "   280|         0|            0|            0|  0.00%|        out\n",
      "   281|         0|            0|            0|  0.00%|    )\n",
      "   282|         0|            0|            0|  0.00%|\n",
      "   283|         0|            0|            0|  0.00%|def _dump(self, file, protocol=2):\n",
      "   284|         0|            0|            0|  0.00%|    if hasattr(file, 'write'):\n",
      "   285|         0|            0|            0|  0.00%|        ctx = nullcontext(file)\n",
      "   286|         0|            0|            0|  0.00%|    else:\n",
      "   287|         0|            0|            0|  0.00%|        ctx = open(os_fspath(file), \"wb\")\n",
      "   288|         0|            0|            0|  0.00%|    with ctx as f:\n",
      "   289|         0|            0|            0|  0.00%|        pickle.dump(self, f, protocol=protocol)\n",
      "   290|         0|            0|            0|  0.00%|\n",
      "   291|         0|            0|            0|  0.00%|def _dumps(self, protocol=2):\n",
      "   292|         0|            0|            0|  0.00%|    return pickle.dumps(self, protocol=protocol)\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/adam.py\n",
      "File duration: 13.3629s (2.15%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|import math\n",
      "     2|         0|            0|            0|  0.00%|import torch\n",
      "     3|         0|            0|            0|  0.00%|from torch import Tensor\n",
      "     4|         0|            0|            0|  0.00%|from .optimizer import Optimizer\n",
      "     5|         0|            0|            0|  0.00%|from typing import List, Optional\n",
      "     6|         0|            0|            0|  0.00%|\n",
      "     7|         0|            0|            0|  0.00%|\n",
      "     8|         0|            0|            0|  0.00%|class Adam(Optimizer):\n",
      "     9|         0|            0|            0|  0.00%|    r\"\"\"Implements Adam algorithm.\n",
      "    10|         0|            0|            0|  0.00%|\n",
      "    11|         0|            0|            0|  0.00%|    .. math::\n",
      "    12|         0|            0|            0|  0.00%|       \\begin{aligned}\n",
      "    13|         0|            0|            0|  0.00%|            &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      "    14|         0|            0|            0|  0.00%|            &\\textbf{input}      : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2\n",
      "    15|         0|            0|            0|  0.00%|                \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)}          \\\\\n",
      "    16|         0|            0|            0|  0.00%|            &\\hspace{13mm}      \\lambda \\text{ (weight decay)},  \\: \\textit{amsgrad},\n",
      "    17|         0|            0|            0|  0.00%|                \\:\\textit{maximize}                                                              \\\\\n",
      "    18|         0|            0|            0|  0.00%|            &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n",
      "    19|         0|            0|            0|  0.00%|                v_0\\leftarrow 0 \\text{ (second moment)},\\: \\widehat{v_0}^{max}\\leftarrow 0\\\\[-1.ex]\n",
      "    20|         0|            0|            0|  0.00%|            &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      "    21|         0|            0|            0|  0.00%|            &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
      "    22|         0|            0|            0|  0.00%|\n",
      "    23|         0|            0|            0|  0.00%|            &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n",
      "    24|         0|            0|            0|  0.00%|            &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n",
      "    25|         0|            0|            0|  0.00%|            &\\hspace{5mm}\\textbf{else}                                                           \\\\\n",
      "    26|         0|            0|            0|  0.00%|            &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n",
      "    27|         0|            0|            0|  0.00%|            &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n",
      "    28|         0|            0|            0|  0.00%|            &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n",
      "    29|         0|            0|            0|  0.00%|            &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n",
      "    30|         0|            0|            0|  0.00%|            &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n",
      "    31|         0|            0|            0|  0.00%|            &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n",
      "    32|         0|            0|            0|  0.00%|            &\\hspace{5mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                   \\\\\n",
      "    33|         0|            0|            0|  0.00%|            &\\hspace{5mm}\\textbf{if} \\: amsgrad                                                  \\\\\n",
      "    34|         0|            0|            0|  0.00%|            &\\hspace{10mm}\\widehat{v_t}^{max} \\leftarrow \\mathrm{max}(\\widehat{v_t}^{max},\n",
      "    35|         0|            0|            0|  0.00%|                \\widehat{v_t})                                                                   \\\\\n",
      "    36|         0|            0|            0|  0.00%|            &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\n",
      "    37|         0|            0|            0|  0.00%|                \\big(\\sqrt{\\widehat{v_t}^{max}} + \\epsilon \\big)                                 \\\\\n",
      "    38|         0|            0|            0|  0.00%|            &\\hspace{5mm}\\textbf{else}                                                           \\\\\n",
      "    39|         0|            0|            0|  0.00%|            &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\n",
      "    40|         0|            0|            0|  0.00%|                \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\n",
      "    41|         0|            0|            0|  0.00%|            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      "    42|         0|            0|            0|  0.00%|            &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
      "    43|         0|            0|            0|  0.00%|            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      "    44|         0|            0|            0|  0.00%|       \\end{aligned}\n",
      "    45|         0|            0|            0|  0.00%|\n",
      "    46|         0|            0|            0|  0.00%|    For further details regarding the algorithm we refer to `Adam: A Method for Stochastic Optimization`_.\n",
      "    47|         0|            0|            0|  0.00%|\n",
      "    48|         0|            0|            0|  0.00%|    Args:\n",
      "    49|         0|            0|            0|  0.00%|        params (iterable): iterable of parameters to optimize or dicts defining\n",
      "    50|         0|            0|            0|  0.00%|            parameter groups\n",
      "    51|         0|            0|            0|  0.00%|        lr (float, optional): learning rate (default: 1e-3)\n",
      "    52|         0|            0|            0|  0.00%|        betas (Tuple[float, float], optional): coefficients used for computing\n",
      "    53|         0|            0|            0|  0.00%|            running averages of gradient and its square (default: (0.9, 0.999))\n",
      "    54|         0|            0|            0|  0.00%|        eps (float, optional): term added to the denominator to improve\n",
      "    55|         0|            0|            0|  0.00%|            numerical stability (default: 1e-8)\n",
      "    56|         0|            0|            0|  0.00%|        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
      "    57|         0|            0|            0|  0.00%|        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
      "    58|         0|            0|            0|  0.00%|            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
      "    59|         0|            0|            0|  0.00%|            (default: False)\n",
      "    60|         0|            0|            0|  0.00%|        foreach (bool, optional): whether foreach implementation of optimizer\n",
      "    61|         0|            0|            0|  0.00%|            is used (default: None)\n",
      "    62|         0|            0|            0|  0.00%|        maximize (bool, optional): maximize the params based on the objective, instead of\n",
      "    63|         0|            0|            0|  0.00%|            minimizing (default: False)\n",
      "    64|         0|            0|            0|  0.00%|        capturable (bool, optional): whether this instance is safe to capture in a CUDA graph.\n",
      "    65|         0|            0|            0|  0.00%|            Passing True can impair ungraphed performance, so if you don't intend to\n",
      "    66|         0|            0|            0|  0.00%|            graph capture this instance, leave it False (default: False)\n",
      "    67|         0|            0|            0|  0.00%|\n",
      "    68|         0|            0|            0|  0.00%|    .. _Adam\\: A Method for Stochastic Optimization:\n",
      "    69|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1412.6980\n",
      "    70|         0|            0|            0|  0.00%|    .. _On the Convergence of Adam and Beyond:\n",
      "    71|         0|            0|            0|  0.00%|        https://openreview.net/forum?id=ryQu7f-RZ\n",
      "    72|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    73|         0|            0|            0|  0.00%|\n",
      "    74|         0|            0|            0|  0.00%|    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
      "    75|         0|            0|            0|  0.00%|                 weight_decay=0, amsgrad=False, *, foreach: Optional[bool] = None,\n",
      "    76|         0|            0|            0|  0.00%|                 maximize: bool = False, capturable: bool = False):\n",
      "    77|         0|            0|            0|  0.00%|        if not 0.0 <= lr:\n",
      "    78|         0|            0|            0|  0.00%|            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
      "    79|         0|            0|            0|  0.00%|        if not 0.0 <= eps:\n",
      "    80|         0|            0|            0|  0.00%|            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
      "    81|         0|            0|            0|  0.00%|        if not 0.0 <= betas[0] < 1.0:\n",
      "    82|         0|            0|            0|  0.00%|            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
      "    83|         0|            0|            0|  0.00%|        if not 0.0 <= betas[1] < 1.0:\n",
      "    84|         0|            0|            0|  0.00%|            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
      "    85|         0|            0|            0|  0.00%|        if not 0.0 <= weight_decay:\n",
      "    86|         0|            0|            0|  0.00%|            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
      "    87|         0|            0|            0|  0.00%|        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
      "    88|         0|            0|            0|  0.00%|                        weight_decay=weight_decay, amsgrad=amsgrad,\n",
      "    89|         0|            0|            0|  0.00%|                        maximize=maximize, foreach=foreach, capturable=capturable)\n",
      "    90|         0|            0|            0|  0.00%|        super(Adam, self).__init__(params, defaults)\n",
      "    91|         0|            0|            0|  0.00%|\n",
      "    92|         0|            0|            0|  0.00%|    def __setstate__(self, state):\n",
      "    93|         0|            0|            0|  0.00%|        super().__setstate__(state)\n",
      "    94|         0|            0|            0|  0.00%|        for group in self.param_groups:\n",
      "    95|         0|            0|            0|  0.00%|            group.setdefault('amsgrad', False)\n",
      "    96|         0|            0|            0|  0.00%|            group.setdefault('maximize', False)\n",
      "    97|         0|            0|            0|  0.00%|            group.setdefault('foreach', None)\n",
      "    98|         0|            0|            0|  0.00%|            group.setdefault('capturable', False)\n",
      "    99|         0|            0|            0|  0.00%|        state_values = list(self.state.values())\n",
      "   100|         0|            0|            0|  0.00%|        step_is_tensor = (len(state_values) != 0) and torch.is_tensor(state_values[0]['step'])\n",
      "   101|         0|            0|            0|  0.00%|        if not step_is_tensor:\n",
      "   102|         0|            0|            0|  0.00%|            for s in state_values:\n",
      "   103|         0|            0|            0|  0.00%|                s['step'] = torch.tensor(float(s['step']))\n",
      "   104|         0|            0|            0|  0.00%|\n",
      "   105|      6240|    0.0229981|  3.68559e-06|  0.00%|    @torch.no_grad()\n",
      "   106|         0|            0|            0|  0.00%|    def step(self, closure=None):\n",
      "   107|         0|            0|            0|  0.00%|        \"\"\"Performs a single optimization step.\n",
      "   108|         0|            0|            0|  0.00%|\n",
      "   109|         0|            0|            0|  0.00%|        Args:\n",
      "   110|         0|            0|            0|  0.00%|            closure (callable, optional): A closure that reevaluates the model\n",
      "   111|         0|            0|            0|  0.00%|                and returns the loss.\n",
      "   112|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   113|      6240|    0.0513487|  8.22896e-06|  0.01%|        self._cuda_graph_capture_health_check()\n",
      "(call)|      6240|    0.0923123|  1.47936e-05|  0.01%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:84 _cuda_graph_capture_health_check\n",
      "   114|         0|            0|            0|  0.00%|\n",
      "   115|      6240|    0.0210931|  3.38031e-06|  0.00%|        loss = None\n",
      "   116|      6240|    0.0210309|  3.37034e-06|  0.00%|        if closure is not None:\n",
      "   117|         0|            0|            0|  0.00%|            with torch.enable_grad():\n",
      "   118|         0|            0|            0|  0.00%|                loss = closure()\n",
      "   119|         0|            0|            0|  0.00%|\n",
      "   120|     12480|    0.0389371|  3.11996e-06|  0.01%|        for group in self.param_groups:\n",
      "   121|      6240|    0.0205383|   3.2914e-06|  0.00%|            params_with_grad = []\n",
      "   122|      6240|    0.0206664|  3.31192e-06|  0.00%|            grads = []\n",
      "   123|      6240|    0.0182297|  2.92143e-06|  0.00%|            exp_avgs = []\n",
      "   124|      6240|    0.0176616|  2.83038e-06|  0.00%|            exp_avg_sqs = []\n",
      "   125|      6240|     0.017359|  2.78189e-06|  0.00%|            max_exp_avg_sqs = []\n",
      "   126|      6240|    0.0165598|  2.65382e-06|  0.00%|            state_steps = []\n",
      "   127|      6240|    0.0192103|  3.07858e-06|  0.00%|            beta1, beta2 = group['betas']\n",
      "   128|         0|            0|            0|  0.00%|\n",
      "   129|     81120|     0.170443|  2.10113e-06|  0.03%|            for p in group['params']:\n",
      "   130|     74880|      0.43759|  5.84388e-06|  0.07%|                if p.grad is not None:\n",
      "(call)|     74880|     0.357773|  4.77795e-06|  0.06%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:1071 grad\n",
      "   131|     74880|     0.169042|   2.2575e-06|  0.03%|                    params_with_grad.append(p)\n",
      "   132|     74880|     0.428624|  5.72414e-06|  0.07%|                    if p.grad.is_sparse:\n",
      "(call)|     74880|      0.33458|  4.46822e-06|  0.05%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:1071 grad\n",
      "   133|         0|            0|            0|  0.00%|                        raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
      "   134|     74880|     0.428261|  5.71929e-06|  0.07%|                    grads.append(p.grad)\n",
      "(call)|     74880|     0.329651|  4.40239e-06|  0.05%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:1071 grad\n",
      "   135|         0|            0|            0|  0.00%|\n",
      "   136|     74880|     0.435969|  5.82224e-06|  0.07%|                    state = self.state[p]\n",
      "(call)|     74880|     0.336203|  4.48989e-06|  0.05%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:731 __hash__\n",
      "   137|         0|            0|            0|  0.00%|                    # Lazy state initialization\n",
      "   138|     74880|     0.168616|  2.25182e-06|  0.03%|                    if len(state) == 0:\n",
      "   139|         0|            0|            0|  0.00%|                        state['step'] = torch.zeros((1,), dtype=torch.float, device=p.device) \\\n",
      "   140|         0|            0|            0|  0.00%|                            if self.defaults['capturable'] else torch.tensor(0.)\n",
      "   141|         0|            0|            0|  0.00%|                        # Exponential moving average of gradient values\n",
      "   142|         0|            0|            0|  0.00%|                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
      "   143|         0|            0|            0|  0.00%|                        # Exponential moving average of squared gradient values\n",
      "   144|         0|            0|            0|  0.00%|                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
      "   145|         0|            0|            0|  0.00%|                        if group['amsgrad']:\n",
      "   146|         0|            0|            0|  0.00%|                            # Maintains max of all exp. moving avg. of sq. grad. values\n",
      "   147|         0|            0|            0|  0.00%|                            state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
      "   148|         0|            0|            0|  0.00%|\n",
      "   149|     74880|     0.167892|  2.24215e-06|  0.03%|                    exp_avgs.append(state['exp_avg'])\n",
      "   150|     74880|     0.159689|   2.1326e-06|  0.03%|                    exp_avg_sqs.append(state['exp_avg_sq'])\n",
      "   151|         0|            0|            0|  0.00%|\n",
      "   152|     74880|     0.155255|  2.07338e-06|  0.02%|                    if group['amsgrad']:\n",
      "   153|         0|            0|            0|  0.00%|                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
      "   154|         0|            0|            0|  0.00%|\n",
      "   155|     74880|     0.159538|  2.13058e-06|  0.03%|                    state_steps.append(state['step'])\n",
      "   156|         0|            0|            0|  0.00%|\n",
      "   157|     12480|    0.0773644|  6.19907e-06|  0.01%|            adam(params_with_grad,\n",
      "(call)|      6240|       9.9328|   0.00159179|  1.60%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/adam.py:176 adam\n",
      "   158|      6240|    0.0121679|  1.94999e-06|  0.00%|                 grads,\n",
      "   159|      6240|    0.0122528|  1.96359e-06|  0.00%|                 exp_avgs,\n",
      "   160|      6240|    0.0121477|  1.94674e-06|  0.00%|                 exp_avg_sqs,\n",
      "   161|      6240|    0.0122125|  1.95713e-06|  0.00%|                 max_exp_avg_sqs,\n",
      "   162|      6240|    0.0123506|  1.97926e-06|  0.00%|                 state_steps,\n",
      "   163|      6240|    0.0125325|  2.00841e-06|  0.00%|                 amsgrad=group['amsgrad'],\n",
      "   164|      6240|    0.0119481|  1.91476e-06|  0.00%|                 beta1=beta1,\n",
      "   165|      6240|    0.0121238|  1.94292e-06|  0.00%|                 beta2=beta2,\n",
      "   166|      6240|    0.0122108|  1.95687e-06|  0.00%|                 lr=group['lr'],\n",
      "   167|      6240|    0.0124035|  1.98774e-06|  0.00%|                 weight_decay=group['weight_decay'],\n",
      "   168|      6240|    0.0124671|  1.99794e-06|  0.00%|                 eps=group['eps'],\n",
      "   169|      6240|    0.0122466|   1.9626e-06|  0.00%|                 maximize=group['maximize'],\n",
      "   170|      6240|    0.0123572|  1.98033e-06|  0.00%|                 foreach=group['foreach'],\n",
      "   171|      6240|    0.0123231|  1.97486e-06|  0.00%|                 capturable=group['capturable'])\n",
      "   172|         0|            0|            0|  0.00%|\n",
      "   173|      6240|    0.0144439|  2.31472e-06|  0.00%|        return loss\n",
      "   174|         0|            0|            0|  0.00%|\n",
      "   175|         0|            0|            0|  0.00%|\n",
      "   176|      6240|    0.0168419|  2.69902e-06|  0.00%|def adam(params: List[Tensor],\n",
      "   177|         0|            0|            0|  0.00%|         grads: List[Tensor],\n",
      "   178|         0|            0|            0|  0.00%|         exp_avgs: List[Tensor],\n",
      "   179|         0|            0|            0|  0.00%|         exp_avg_sqs: List[Tensor],\n",
      "   180|         0|            0|            0|  0.00%|         max_exp_avg_sqs: List[Tensor],\n",
      "   181|         0|            0|            0|  0.00%|         state_steps: List[Tensor],\n",
      "   182|         0|            0|            0|  0.00%|         # kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\n",
      "   183|         0|            0|            0|  0.00%|         # setting this as kwarg for now as functional API is compiled by torch/distributed/optim\n",
      "   184|         0|            0|            0|  0.00%|         foreach: bool = None,\n",
      "   185|         0|            0|            0|  0.00%|         capturable: bool = False,\n",
      "   186|         0|            0|            0|  0.00%|         *,\n",
      "   187|         0|            0|            0|  0.00%|         amsgrad: bool,\n",
      "   188|         0|            0|            0|  0.00%|         beta1: float,\n",
      "   189|         0|            0|            0|  0.00%|         beta2: float,\n",
      "   190|         0|            0|            0|  0.00%|         lr: float,\n",
      "   191|         0|            0|            0|  0.00%|         weight_decay: float,\n",
      "   192|         0|            0|            0|  0.00%|         eps: float,\n",
      "   193|         0|            0|            0|  0.00%|         maximize: bool):\n",
      "   194|         0|            0|            0|  0.00%|    r\"\"\"Functional API that performs Adam algorithm computation.\n",
      "   195|         0|            0|            0|  0.00%|    See :class:`~torch.optim.Adam` for details.\n",
      "   196|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   197|         0|            0|            0|  0.00%|\n",
      "   198|     93600|     0.169234|  1.80806e-06|  0.03%|    if not all([isinstance(t, torch.Tensor) for t in state_steps]):\n",
      "(call)|      6240|     0.126747|   2.0312e-05|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/adam.py:198 <listcomp>\n",
      "   199|         0|            0|            0|  0.00%|        raise RuntimeError(\"API has changed, `state_steps` argument must contain a list of singleton tensors\")\n",
      "   200|         0|            0|            0|  0.00%|\n",
      "   201|      6240|    0.0140724|   2.2552e-06|  0.00%|    if foreach is None:\n",
      "   202|         0|            0|            0|  0.00%|        # Placeholder for more complex foreach logic to be added when value is not set\n",
      "   203|      6240|    0.0132313|   2.1204e-06|  0.00%|        foreach = False\n",
      "   204|         0|            0|            0|  0.00%|\n",
      "   205|      6240|    0.0130267|  2.08761e-06|  0.00%|    if foreach and torch.jit.is_scripting():\n",
      "   206|         0|            0|            0|  0.00%|        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n",
      "   207|         0|            0|            0|  0.00%|\n",
      "   208|      6240|    0.0124726|  1.99882e-06|  0.00%|    if foreach and not torch.jit.is_scripting():\n",
      "   209|         0|            0|            0|  0.00%|        func = _multi_tensor_adam\n",
      "   210|         0|            0|            0|  0.00%|    else:\n",
      "   211|      6240|    0.0130432|  2.09025e-06|  0.00%|        func = _single_tensor_adam\n",
      "   212|         0|            0|            0|  0.00%|\n",
      "   213|     12480|    0.0932784|  7.47423e-06|  0.02%|    func(params,\n",
      "(call)|      6240|      9.43287|   0.00151168|  1.52%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/adam.py:229 _single_tensor_adam\n",
      "   214|      6240|     0.012038|  1.92917e-06|  0.00%|         grads,\n",
      "   215|      6240|    0.0119538|  1.91568e-06|  0.00%|         exp_avgs,\n",
      "   216|      6240|    0.0120144|  1.92538e-06|  0.00%|         exp_avg_sqs,\n",
      "   217|      6240|    0.0117047|  1.87575e-06|  0.00%|         max_exp_avg_sqs,\n",
      "   218|      6240|    0.0120034|  1.92363e-06|  0.00%|         state_steps,\n",
      "   219|      6240|    0.0118821|  1.90418e-06|  0.00%|         amsgrad=amsgrad,\n",
      "   220|      6240|    0.0119133|  1.90918e-06|  0.00%|         beta1=beta1,\n",
      "   221|      6240|    0.0120091|  1.92454e-06|  0.00%|         beta2=beta2,\n",
      "   222|      6240|    0.0118697|  1.90219e-06|  0.00%|         lr=lr,\n",
      "   223|      6240|    0.0121062|  1.94009e-06|  0.00%|         weight_decay=weight_decay,\n",
      "   224|      6240|    0.0117934|  1.88996e-06|  0.00%|         eps=eps,\n",
      "   225|      6240|    0.0116608|  1.86872e-06|  0.00%|         maximize=maximize,\n",
      "   226|      6240|    0.0117767|  1.88729e-06|  0.00%|         capturable=capturable)\n",
      "   227|         0|            0|            0|  0.00%|\n",
      "   228|         0|            0|            0|  0.00%|\n",
      "   229|      6240|    0.0251417|  4.02912e-06|  0.00%|def _single_tensor_adam(params: List[Tensor],\n",
      "   230|         0|            0|            0|  0.00%|                        grads: List[Tensor],\n",
      "   231|         0|            0|            0|  0.00%|                        exp_avgs: List[Tensor],\n",
      "   232|         0|            0|            0|  0.00%|                        exp_avg_sqs: List[Tensor],\n",
      "   233|         0|            0|            0|  0.00%|                        max_exp_avg_sqs: List[Tensor],\n",
      "   234|         0|            0|            0|  0.00%|                        state_steps: List[Tensor],\n",
      "   235|         0|            0|            0|  0.00%|                        *,\n",
      "   236|         0|            0|            0|  0.00%|                        amsgrad: bool,\n",
      "   237|         0|            0|            0|  0.00%|                        beta1: float,\n",
      "   238|         0|            0|            0|  0.00%|                        beta2: float,\n",
      "   239|         0|            0|            0|  0.00%|                        lr: float,\n",
      "   240|         0|            0|            0|  0.00%|                        weight_decay: float,\n",
      "   241|         0|            0|            0|  0.00%|                        eps: float,\n",
      "   242|         0|            0|            0|  0.00%|                        maximize: bool,\n",
      "   243|         0|            0|            0|  0.00%|                        capturable: bool):\n",
      "   244|         0|            0|            0|  0.00%|\n",
      "   245|     81120|     0.246192|  3.03491e-06|  0.04%|    for i, param in enumerate(params):\n",
      "   246|         0|            0|            0|  0.00%|\n",
      "   247|     74880|     0.200861|  2.68243e-06|  0.03%|        grad = grads[i] if not maximize else -grads[i]\n",
      "   248|     74880|     0.194087|  2.59197e-06|  0.03%|        exp_avg = exp_avgs[i]\n",
      "   249|     74880|     0.191054|  2.55146e-06|  0.03%|        exp_avg_sq = exp_avg_sqs[i]\n",
      "   250|     74880|     0.187963|  2.51019e-06|  0.03%|        step_t = state_steps[i]\n",
      "   251|         0|            0|            0|  0.00%|\n",
      "   252|     74880|     0.185808|  2.48142e-06|  0.03%|        if capturable:\n",
      "   253|         0|            0|            0|  0.00%|            assert param.is_cuda and step_t.is_cuda, \"If capturable=True, params and state_steps must be CUDA tensors.\"\n",
      "   254|         0|            0|            0|  0.00%|\n",
      "   255|         0|            0|            0|  0.00%|        # update step\n",
      "   256|     74880|     0.673735|  8.99752e-06|  0.11%|        step_t += 1\n",
      "   257|         0|            0|            0|  0.00%|\n",
      "   258|     74880|     0.210582|  2.81227e-06|  0.03%|        if weight_decay != 0:\n",
      "   259|         0|            0|            0|  0.00%|            grad = grad.add(param, alpha=weight_decay)\n",
      "   260|         0|            0|            0|  0.00%|\n",
      "   261|         0|            0|            0|  0.00%|        # Decay the first and second moment running average coefficient\n",
      "   262|     74880|      1.22047|   1.6299e-05|  0.20%|        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
      "   263|     74880|      1.06793|  1.42619e-05|  0.17%|        exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)\n",
      "   264|         0|            0|            0|  0.00%|\n",
      "   265|     74880|     0.218818|  2.92225e-06|  0.04%|        if capturable:\n",
      "   266|         0|            0|            0|  0.00%|            step = step_t\n",
      "   267|         0|            0|            0|  0.00%|\n",
      "   268|         0|            0|            0|  0.00%|            # 1 - beta1 ** step can't be captured in a CUDA graph, even if step is a CUDA tensor\n",
      "   269|         0|            0|            0|  0.00%|            # (incurs \"RuntimeError: CUDA error: operation not permitted when stream is capturing\")\n",
      "   270|         0|            0|            0|  0.00%|            bias_correction1 = 1 - torch.pow(beta1, step)\n",
      "   271|         0|            0|            0|  0.00%|            bias_correction2 = 1 - torch.pow(beta2, step)\n",
      "   272|         0|            0|            0|  0.00%|\n",
      "   273|         0|            0|            0|  0.00%|            step_size = lr / bias_correction1\n",
      "   274|         0|            0|            0|  0.00%|            step_size_neg = step_size.neg()\n",
      "   275|         0|            0|            0|  0.00%|\n",
      "   276|         0|            0|            0|  0.00%|            bias_correction2_sqrt = bias_correction2.sqrt()\n",
      "   277|         0|            0|            0|  0.00%|\n",
      "   278|         0|            0|            0|  0.00%|            if amsgrad:\n",
      "   279|         0|            0|            0|  0.00%|                # Maintains the maximum of all 2nd moment running avg. till now\n",
      "   280|         0|            0|            0|  0.00%|                torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])\n",
      "   281|         0|            0|            0|  0.00%|                # Uses the max. for normalizing running avg. of gradient\n",
      "   282|         0|            0|            0|  0.00%|                # Folds in (admittedly ugly) 1-elem step_size math here to avoid extra param-set-sized read+write\n",
      "   283|         0|            0|            0|  0.00%|                # (can't fold it into addcdiv_ below because addcdiv_ requires value is a Number, not a Tensor)\n",
      "   284|         0|            0|            0|  0.00%|                denom = (max_exp_avg_sqs[i].sqrt() / (bias_correction2_sqrt * step_size_neg)).add_(eps / step_size_neg)\n",
      "   285|         0|            0|            0|  0.00%|            else:\n",
      "   286|         0|            0|            0|  0.00%|                denom = (exp_avg_sq.sqrt() / (bias_correction2_sqrt * step_size_neg)).add_(eps / step_size_neg)\n",
      "   287|         0|            0|            0|  0.00%|\n",
      "   288|         0|            0|            0|  0.00%|            param.addcdiv_(exp_avg, denom)\n",
      "   289|         0|            0|            0|  0.00%|        else:\n",
      "   290|     74880|     0.287337|  3.83729e-06|  0.05%|            step = step_t.item()\n",
      "   291|         0|            0|            0|  0.00%|\n",
      "   292|     74880|     0.264466|  3.53186e-06|  0.04%|            bias_correction1 = 1 - beta1 ** step\n",
      "   293|     74880|     0.209821|   2.8021e-06|  0.03%|            bias_correction2 = 1 - beta2 ** step\n",
      "   294|         0|            0|            0|  0.00%|\n",
      "   295|     74880|     0.200593|  2.67886e-06|  0.03%|            step_size = lr / bias_correction1\n",
      "   296|         0|            0|            0|  0.00%|\n",
      "   297|     74880|     0.219672|  2.93366e-06|  0.04%|            bias_correction2_sqrt = math.sqrt(bias_correction2)\n",
      "   298|         0|            0|            0|  0.00%|\n",
      "   299|     74880|     0.191556|  2.55817e-06|  0.03%|            if amsgrad:\n",
      "   300|         0|            0|            0|  0.00%|                # Maintains the maximum of all 2nd moment running avg. till now\n",
      "   301|         0|            0|            0|  0.00%|                torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])\n",
      "   302|         0|            0|            0|  0.00%|                # Use the max. for normalizing running avg. of gradient\n",
      "   303|         0|            0|            0|  0.00%|                denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n",
      "   304|         0|            0|            0|  0.00%|            else:\n",
      "   305|     74880|      2.53115|  3.38028e-05|  0.41%|                denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n",
      "   306|         0|            0|            0|  0.00%|\n",
      "   307|     74880|      0.90563|  1.20944e-05|  0.15%|            param.addcdiv_(exp_avg, denom, value=-step_size)\n",
      "   308|         0|            0|            0|  0.00%|\n",
      "   309|         0|            0|            0|  0.00%|\n",
      "   310|         0|            0|            0|  0.00%|def _multi_tensor_adam(params: List[Tensor],\n",
      "   311|         0|            0|            0|  0.00%|                       grads: List[Tensor],\n",
      "   312|         0|            0|            0|  0.00%|                       exp_avgs: List[Tensor],\n",
      "   313|         0|            0|            0|  0.00%|                       exp_avg_sqs: List[Tensor],\n",
      "   314|         0|            0|            0|  0.00%|                       max_exp_avg_sqs: List[Tensor],\n",
      "   315|         0|            0|            0|  0.00%|                       state_steps: List[Tensor],\n",
      "   316|         0|            0|            0|  0.00%|                       *,\n",
      "   317|         0|            0|            0|  0.00%|                       amsgrad: bool,\n",
      "   318|         0|            0|            0|  0.00%|                       beta1: float,\n",
      "   319|         0|            0|            0|  0.00%|                       beta2: float,\n",
      "   320|         0|            0|            0|  0.00%|                       lr: float,\n",
      "   321|         0|            0|            0|  0.00%|                       weight_decay: float,\n",
      "   322|         0|            0|            0|  0.00%|                       eps: float,\n",
      "   323|         0|            0|            0|  0.00%|                       maximize: bool,\n",
      "   324|         0|            0|            0|  0.00%|                       capturable: bool):\n",
      "   325|         0|            0|            0|  0.00%|    if len(params) == 0:\n",
      "   326|         0|            0|            0|  0.00%|        return\n",
      "   327|         0|            0|            0|  0.00%|\n",
      "   328|         0|            0|            0|  0.00%|    if capturable:\n",
      "   329|         0|            0|            0|  0.00%|        assert all(p.is_cuda and step.is_cuda for p, step in zip(params, state_steps)), \\\n",
      "   330|         0|            0|            0|  0.00%|            \"If capturable=True, params and state_steps must be CUDA tensors.\"\n",
      "   331|         0|            0|            0|  0.00%|\n",
      "   332|         0|            0|            0|  0.00%|    if maximize:\n",
      "   333|         0|            0|            0|  0.00%|        grads = torch._foreach_neg(tuple(grads))  # type: ignore[assignment]\n",
      "   334|         0|            0|            0|  0.00%|\n",
      "   335|         0|            0|            0|  0.00%|    # update steps\n",
      "   336|         0|            0|            0|  0.00%|    torch._foreach_add_(state_steps, 1)\n",
      "   337|         0|            0|            0|  0.00%|\n",
      "   338|         0|            0|            0|  0.00%|    if weight_decay != 0:\n",
      "   339|         0|            0|            0|  0.00%|        torch._foreach_add_(grads, params, alpha=weight_decay)\n",
      "   340|         0|            0|            0|  0.00%|\n",
      "   341|         0|            0|            0|  0.00%|    # Decay the first and second moment running average coefficient\n",
      "   342|         0|            0|            0|  0.00%|    torch._foreach_mul_(exp_avgs, beta1)\n",
      "   343|         0|            0|            0|  0.00%|    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)\n",
      "   344|         0|            0|            0|  0.00%|\n",
      "   345|         0|            0|            0|  0.00%|    torch._foreach_mul_(exp_avg_sqs, beta2)\n",
      "   346|         0|            0|            0|  0.00%|    torch._foreach_addcmul_(exp_avg_sqs, grads, grads, 1 - beta2)\n",
      "   347|         0|            0|            0|  0.00%|\n",
      "   348|         0|            0|            0|  0.00%|    if capturable:\n",
      "   349|         0|            0|            0|  0.00%|        # TODO: use foreach_pow if/when foreach_pow is added\n",
      "   350|         0|            0|            0|  0.00%|        bias_correction1 = [torch.pow(beta1, step) for step in state_steps]\n",
      "   351|         0|            0|            0|  0.00%|        bias_correction2 = [torch.pow(beta2, step) for step in state_steps]\n",
      "   352|         0|            0|            0|  0.00%|        # foreach_sub doesn't allow a scalar as the first arg\n",
      "   353|         0|            0|            0|  0.00%|        torch._foreach_sub_(bias_correction1, 1)\n",
      "   354|         0|            0|            0|  0.00%|        torch._foreach_sub_(bias_correction2, 1)\n",
      "   355|         0|            0|            0|  0.00%|        torch._foreach_neg_(bias_correction1)\n",
      "   356|         0|            0|            0|  0.00%|        torch._foreach_neg_(bias_correction2)\n",
      "   357|         0|            0|            0|  0.00%|\n",
      "   358|         0|            0|            0|  0.00%|        # foreach_div doesn't allow a scalar as the first arg\n",
      "   359|         0|            0|            0|  0.00%|        step_size = torch._foreach_div(bias_correction1, lr)\n",
      "   360|         0|            0|            0|  0.00%|        torch._foreach_reciprocal_(step_size)\n",
      "   361|         0|            0|            0|  0.00%|        torch._foreach_neg_(step_size)\n",
      "   362|         0|            0|            0|  0.00%|\n",
      "   363|         0|            0|            0|  0.00%|        bias_correction2_sqrt = torch._foreach_sqrt(bias_correction2)\n",
      "   364|         0|            0|            0|  0.00%|\n",
      "   365|         0|            0|            0|  0.00%|        if amsgrad:\n",
      "   366|         0|            0|            0|  0.00%|            # Maintains the maximum of all 2nd moment running avg. till now\n",
      "   367|         0|            0|            0|  0.00%|            max_exp_avg_sqs = torch._foreach_maximum(max_exp_avg_sqs, exp_avg_sqs)  # type: ignore[assignment]\n",
      "   368|         0|            0|            0|  0.00%|\n",
      "   369|         0|            0|            0|  0.00%|            # Use the max. for normalizing running avg. of gradient\n",
      "   370|         0|            0|            0|  0.00%|            max_exp_avg_sq_sqrt = torch._foreach_sqrt(max_exp_avg_sqs)\n",
      "   371|         0|            0|            0|  0.00%|            # Folds in (admittedly ugly) 1-elem step_size math here to avoid extra param-set-sized read+write\n",
      "   372|         0|            0|            0|  0.00%|            # (can't fold it into addcdiv_ below because addcdiv_ requires value is a Number, not a Tensor)\n",
      "   373|         0|            0|            0|  0.00%|            torch._foreach_div_(max_exp_avg_sq_sqrt, torch._foreach_mul(bias_correction2_sqrt, step_size))\n",
      "   374|         0|            0|            0|  0.00%|            eps_over_step_size = torch._foreach_div(step_size, eps)\n",
      "   375|         0|            0|            0|  0.00%|            torch._foreach_reciprocal_(eps_over_step_size)\n",
      "   376|         0|            0|            0|  0.00%|            denom = torch._foreach_add(max_exp_avg_sq_sqrt, eps_over_step_size)\n",
      "   377|         0|            0|            0|  0.00%|        else:\n",
      "   378|         0|            0|            0|  0.00%|            exp_avg_sq_sqrt = torch._foreach_sqrt(exp_avg_sqs)\n",
      "   379|         0|            0|            0|  0.00%|            torch._foreach_div_(exp_avg_sq_sqrt, torch._foreach_mul(bias_correction2_sqrt, step_size))\n",
      "   380|         0|            0|            0|  0.00%|            eps_over_step_size = torch._foreach_div(step_size, eps)\n",
      "   381|         0|            0|            0|  0.00%|            torch._foreach_reciprocal_(eps_over_step_size)\n",
      "   382|         0|            0|            0|  0.00%|            denom = torch._foreach_add(exp_avg_sq_sqrt, eps_over_step_size)\n",
      "   383|         0|            0|            0|  0.00%|\n",
      "   384|         0|            0|            0|  0.00%|        torch._foreach_addcdiv_(params, exp_avgs, denom)\n",
      "   385|         0|            0|            0|  0.00%|    else:\n",
      "   386|         0|            0|            0|  0.00%|        bias_correction1 = [1 - beta1 ** step.item() for step in state_steps]\n",
      "   387|         0|            0|            0|  0.00%|        bias_correction2 = [1 - beta2 ** step.item() for step in state_steps]\n",
      "   388|         0|            0|            0|  0.00%|\n",
      "   389|         0|            0|            0|  0.00%|        step_size = [(lr / bc) * -1 for bc in bias_correction1]\n",
      "   390|         0|            0|            0|  0.00%|\n",
      "   391|         0|            0|            0|  0.00%|        bias_correction2_sqrt = [math.sqrt(bc) for bc in bias_correction2]\n",
      "   392|         0|            0|            0|  0.00%|\n",
      "   393|         0|            0|            0|  0.00%|        if amsgrad:\n",
      "   394|         0|            0|            0|  0.00%|            # Maintains the maximum of all 2nd moment running avg. till now\n",
      "   395|         0|            0|            0|  0.00%|            max_exp_avg_sqs = torch._foreach_maximum(max_exp_avg_sqs, exp_avg_sqs)  # type: ignore[assignment]\n",
      "   396|         0|            0|            0|  0.00%|\n",
      "   397|         0|            0|            0|  0.00%|            # Use the max. for normalizing running avg. of gradient\n",
      "   398|         0|            0|            0|  0.00%|            max_exp_avg_sq_sqrt = torch._foreach_sqrt(max_exp_avg_sqs)\n",
      "   399|         0|            0|            0|  0.00%|            torch._foreach_div_(max_exp_avg_sq_sqrt, bias_correction2_sqrt)\n",
      "   400|         0|            0|            0|  0.00%|            denom = torch._foreach_add(max_exp_avg_sq_sqrt, eps)\n",
      "   401|         0|            0|            0|  0.00%|        else:\n",
      "   402|         0|            0|            0|  0.00%|            exp_avg_sq_sqrt = torch._foreach_sqrt(exp_avg_sqs)\n",
      "   403|         0|            0|            0|  0.00%|            torch._foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n",
      "   404|         0|            0|            0|  0.00%|            denom = torch._foreach_add(exp_avg_sq_sqrt, eps)\n",
      "   405|         0|            0|            0|  0.00%|\n",
      "   406|         0|            0|            0|  0.00%|        torch._foreach_addcdiv_(params, exp_avgs, denom, step_size)\n",
      "File: <__array_function__ internals>\n",
      "File duration: 12.228s (1.97%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|\n",
      "     2|         0|            0|            0|  0.00%|\n",
      "     3|         0|            0|            0|  0.00%|\n",
      "     4|         0|            0|            0|  0.00%|\n",
      "     5|         0|            0|            0|  0.00%|\n",
      "     6|         0|            0|            0|  0.00%|\n",
      "     7|         0|            0|            0|  0.00%|\n",
      "     8|         0|            0|            0|  0.00%|\n",
      "     9|         0|            0|            0|  0.00%|\n",
      "    10|         0|            0|            0|  0.00%|\n",
      "    11|         0|            0|            0|  0.00%|\n",
      "    12|         0|            0|            0|  0.00%|\n",
      "    13|         0|            0|            0|  0.00%|\n",
      "    14|         0|            0|            0|  0.00%|\n",
      "    15|         0|            0|            0|  0.00%|\n",
      "    16|         0|            0|            0|  0.00%|\n",
      "    17|         0|            0|            0|  0.00%|\n",
      "    18|         0|            0|            0|  0.00%|\n",
      "    19|         0|            0|            0|  0.00%|\n",
      "    20|         0|            0|            0|  0.00%|\n",
      "    21|         0|            0|            0|  0.00%|\n",
      "    22|         0|            0|            0|  0.00%|\n",
      "    23|         0|            0|            0|  0.00%|\n",
      "    24|         0|            0|            0|  0.00%|\n",
      "    25|         0|            0|            0|  0.00%|\n",
      "    26|         0|            0|            0|  0.00%|\n",
      "    27|         0|            0|            0|  0.00%|\n",
      "    28|         0|            0|            0|  0.00%|\n",
      "    29|         0|            0|            0|  0.00%|\n",
      "    30|         0|            0|            0|  0.00%|\n",
      "    31|         0|            0|            0|  0.00%|\n",
      "    32|         0|            0|            0|  0.00%|\n",
      "    33|         0|            0|            0|  0.00%|\n",
      "    34|         0|            0|            0|  0.00%|\n",
      "    35|         0|            0|            0|  0.00%|\n",
      "    36|         0|            0|            0|  0.00%|\n",
      "    37|         0|            0|            0|  0.00%|\n",
      "    38|         0|            0|            0|  0.00%|\n",
      "    39|         0|            0|            0|  0.00%|\n",
      "    40|         0|            0|            0|  0.00%|\n",
      "    41|         0|            0|            0|  0.00%|\n",
      "    42|         0|            0|            0|  0.00%|\n",
      "    43|         0|            0|            0|  0.00%|\n",
      "    44|         0|            0|            0|  0.00%|\n",
      "    45|         0|            0|            0|  0.00%|\n",
      "    46|         0|            0|            0|  0.00%|\n",
      "    47|         0|            0|            0|  0.00%|\n",
      "    48|         0|            0|            0|  0.00%|\n",
      "    49|         0|            0|            0|  0.00%|\n",
      "    50|         0|            0|            0|  0.00%|\n",
      "    51|         0|            0|            0|  0.00%|\n",
      "    52|         0|            0|            0|  0.00%|\n",
      "    53|         0|            0|            0|  0.00%|\n",
      "    54|         0|            0|            0|  0.00%|\n",
      "    55|         0|            0|            0|  0.00%|\n",
      "    56|         0|            0|            0|  0.00%|\n",
      "    57|         0|            0|            0|  0.00%|\n",
      "    58|         0|            0|            0|  0.00%|\n",
      "    59|         0|            0|            0|  0.00%|\n",
      "    60|         0|            0|            0|  0.00%|\n",
      "    61|         0|            0|            0|  0.00%|\n",
      "    62|         0|            0|            0|  0.00%|\n",
      "    63|         0|            0|            0|  0.00%|\n",
      "    64|         0|            0|            0|  0.00%|\n",
      "    65|         0|            0|            0|  0.00%|\n",
      "    66|         0|            0|            0|  0.00%|\n",
      "    67|         0|            0|            0|  0.00%|\n",
      "    68|         0|            0|            0|  0.00%|\n",
      "    69|         0|            0|            0|  0.00%|\n",
      "    70|         0|            0|            0|  0.00%|\n",
      "    71|         0|            0|            0|  0.00%|\n",
      "    72|         0|            0|            0|  0.00%|\n",
      "    73|         0|            0|            0|  0.00%|\n",
      "    74|         0|            0|            0|  0.00%|\n",
      "    75|         0|            0|            0|  0.00%|\n",
      "    76|         0|            0|            0|  0.00%|\n",
      "    77|         0|            0|            0|  0.00%|\n",
      "    78|         0|            0|            0|  0.00%|\n",
      "    79|         0|            0|            0|  0.00%|\n",
      "    80|         0|            0|            0|  0.00%|\n",
      "    81|         0|            0|            0|  0.00%|\n",
      "    82|         0|            0|            0|  0.00%|\n",
      "    83|         0|            0|            0|  0.00%|\n",
      "    84|         0|            0|            0|  0.00%|\n",
      "    85|         0|            0|            0|  0.00%|\n",
      "    86|         0|            0|            0|  0.00%|\n",
      "    87|         0|            0|            0|  0.00%|\n",
      "    88|         0|            0|            0|  0.00%|\n",
      "    89|         0|            0|            0|  0.00%|\n",
      "    90|         0|            0|            0|  0.00%|\n",
      "    91|         0|            0|            0|  0.00%|\n",
      "    92|         0|            0|            0|  0.00%|\n",
      "    93|         0|            0|            0|  0.00%|\n",
      "    94|         0|            0|            0|  0.00%|\n",
      "    95|         0|            0|            0|  0.00%|\n",
      "    96|         0|            0|            0|  0.00%|\n",
      "    97|         0|            0|            0|  0.00%|\n",
      "    98|         0|            0|            0|  0.00%|\n",
      "    99|         0|            0|            0|  0.00%|\n",
      "   100|         0|            0|            0|  0.00%|\n",
      "   101|         0|            0|            0|  0.00%|\n",
      "   102|         0|            0|            0|  0.00%|\n",
      "   103|         0|            0|            0|  0.00%|\n",
      "   104|         0|            0|            0|  0.00%|\n",
      "   105|         0|            0|            0|  0.00%|\n",
      "   106|         0|            0|            0|  0.00%|\n",
      "   107|         0|            0|            0|  0.00%|\n",
      "   108|         0|            0|            0|  0.00%|\n",
      "   109|         0|            0|            0|  0.00%|\n",
      "   110|         0|            0|            0|  0.00%|\n",
      "   111|         0|            0|            0|  0.00%|\n",
      "   112|         0|            0|            0|  0.00%|\n",
      "   113|         0|            0|            0|  0.00%|\n",
      "   114|         0|            0|            0|  0.00%|\n",
      "   115|         0|            0|            0|  0.00%|\n",
      "   116|         0|            0|            0|  0.00%|\n",
      "   117|         0|            0|            0|  0.00%|\n",
      "   118|         0|            0|            0|  0.00%|\n",
      "   119|         0|            0|            0|  0.00%|\n",
      "   120|         0|            0|            0|  0.00%|\n",
      "   121|         0|            0|            0|  0.00%|\n",
      "   122|         0|            0|            0|  0.00%|\n",
      "   123|         0|            0|            0|  0.00%|\n",
      "   124|         0|            0|            0|  0.00%|\n",
      "   125|         0|            0|            0|  0.00%|\n",
      "   126|         0|            0|            0|  0.00%|\n",
      "   127|         0|            0|            0|  0.00%|\n",
      "   128|         0|            0|            0|  0.00%|\n",
      "   129|         0|            0|            0|  0.00%|\n",
      "   130|         0|            0|            0|  0.00%|\n",
      "   131|         0|            0|            0|  0.00%|\n",
      "   132|         0|            0|            0|  0.00%|\n",
      "   133|         0|            0|            0|  0.00%|\n",
      "   134|         0|            0|            0|  0.00%|\n",
      "   135|         0|            0|            0|  0.00%|\n",
      "   136|         0|            0|            0|  0.00%|\n",
      "   137|         0|            0|            0|  0.00%|\n",
      "   138|         0|            0|            0|  0.00%|\n",
      "   139|         0|            0|            0|  0.00%|\n",
      "   140|         0|            0|            0|  0.00%|\n",
      "   141|         0|            0|            0|  0.00%|\n",
      "   142|         0|            0|            0|  0.00%|\n",
      "   143|         0|            0|            0|  0.00%|\n",
      "   144|         0|            0|            0|  0.00%|\n",
      "   145|         0|            0|            0|  0.00%|\n",
      "   146|         0|            0|            0|  0.00%|\n",
      "   147|         0|            0|            0|  0.00%|\n",
      "   148|         0|            0|            0|  0.00%|\n",
      "   149|         0|            0|            0|  0.00%|\n",
      "   150|         0|            0|            0|  0.00%|\n",
      "   151|         0|            0|            0|  0.00%|\n",
      "   152|         0|            0|            0|  0.00%|\n",
      "   153|         0|            0|            0|  0.00%|\n",
      "   154|         0|            0|            0|  0.00%|\n",
      "   155|         0|            0|            0|  0.00%|\n",
      "   156|         0|            0|            0|  0.00%|\n",
      "   157|         0|            0|            0|  0.00%|\n",
      "   158|         0|            0|            0|  0.00%|\n",
      "   159|         0|            0|            0|  0.00%|\n",
      "   160|         0|            0|            0|  0.00%|\n",
      "   161|         0|            0|            0|  0.00%|\n",
      "   162|         0|            0|            0|  0.00%|\n",
      "   163|         0|            0|            0|  0.00%|\n",
      "   164|         0|            0|            0|  0.00%|\n",
      "   165|         0|            0|            0|  0.00%|\n",
      "   166|         0|            0|            0|  0.00%|\n",
      "   167|         0|            0|            0|  0.00%|\n",
      "   168|         0|            0|            0|  0.00%|\n",
      "   169|         0|            0|            0|  0.00%|\n",
      "   170|         0|            0|            0|  0.00%|\n",
      "   171|         0|            0|            0|  0.00%|\n",
      "   172|         0|            0|            0|  0.00%|\n",
      "   173|         0|            0|            0|  0.00%|\n",
      "   174|         0|            0|            0|  0.00%|\n",
      "   175|         0|            0|            0|  0.00%|\n",
      "   176|         0|            0|            0|  0.00%|\n",
      "   177|    774510|      1.44263|  1.86263e-06|  0.23%|\n",
      "   178|         0|            0|            0|  0.00%|\n",
      "   179|    774510|      4.63329|  5.98222e-06|  0.75%|\n",
      "(call)|     99832|     0.335943|  3.36509e-06|  0.05%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/multiarray.py:1071 copyto\n",
      "(call)|     74880|     0.248099|  3.31329e-06|  0.04%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2491 _cumsum_dispatcher\n",
      "(call)|     74880|     0.243086|  3.24634e-06|  0.04%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:1315 _searchsorted_dispatcher\n",
      "(call)|    374390|      1.17765|  3.14551e-06|  0.19%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/multiarray.py:341 where\n",
      "(call)|     74874|     0.251587|  3.36014e-06|  0.04%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/numeric.py:72 _zeros_like_dispatcher\n",
      "(call)|     74874|      0.23245|  3.10455e-06|  0.04%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/multiarray.py:80 empty_like\n",
      "(call)|       780|    0.0030849|    3.955e-06|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3619 _var_dispatcher\n",
      "   180|   1549020|      4.89625|  3.16087e-06|  0.79%|\n",
      "(call)|     74880|      3.73868|  4.99289e-05|  0.60%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2495 cumsum\n",
      "(call)|     74880|      1.44825|   1.9341e-05|  0.23%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:1319 searchsorted\n",
      "(call)|     74874|      4.20472|  5.61572e-05|  0.68%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/numeric.py:76 zeros_like\n",
      "(call)|       780|     0.133711|  0.000171424|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3624 var\n",
      "   181|    774510|      1.25581|  1.62143e-06|  0.20%|\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/categorical.py\n",
      "File duration: 8.31147s (1.34%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|import torch\n",
      "     2|         0|            0|            0|  0.00%|from torch._six import nan\n",
      "     3|         0|            0|            0|  0.00%|from torch.distributions import constraints\n",
      "     4|         0|            0|            0|  0.00%|from torch.distributions.distribution import Distribution\n",
      "     5|         0|            0|            0|  0.00%|from torch.distributions.utils import probs_to_logits, logits_to_probs, lazy_property\n",
      "     6|         0|            0|            0|  0.00%|\n",
      "     7|         0|            0|            0|  0.00%|\n",
      "     8|         0|            0|            0|  0.00%|class Categorical(Distribution):\n",
      "     9|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "    10|         0|            0|            0|  0.00%|    Creates a categorical distribution parameterized by either :attr:`probs` or\n",
      "    11|         0|            0|            0|  0.00%|    :attr:`logits` (but not both).\n",
      "    12|         0|            0|            0|  0.00%|\n",
      "    13|         0|            0|            0|  0.00%|    .. note::\n",
      "    14|         0|            0|            0|  0.00%|        It is equivalent to the distribution that :func:`torch.multinomial`\n",
      "    15|         0|            0|            0|  0.00%|        samples from.\n",
      "    16|         0|            0|            0|  0.00%|\n",
      "    17|         0|            0|            0|  0.00%|    Samples are integers from :math:`\\{0, \\ldots, K-1\\}` where `K` is ``probs.size(-1)``.\n",
      "    18|         0|            0|            0|  0.00%|\n",
      "    19|         0|            0|            0|  0.00%|    If `probs` is 1-dimensional with length-`K`, each element is the relative probability\n",
      "    20|         0|            0|            0|  0.00%|    of sampling the class at that index.\n",
      "    21|         0|            0|            0|  0.00%|\n",
      "    22|         0|            0|            0|  0.00%|    If `probs` is N-dimensional, the first N-1 dimensions are treated as a batch of\n",
      "    23|         0|            0|            0|  0.00%|    relative probability vectors.\n",
      "    24|         0|            0|            0|  0.00%|\n",
      "    25|         0|            0|            0|  0.00%|    .. note:: The `probs` argument must be non-negative, finite and have a non-zero sum,\n",
      "    26|         0|            0|            0|  0.00%|              and it will be normalized to sum to 1 along the last dimension. :attr:`probs`\n",
      "    27|         0|            0|            0|  0.00%|              will return this normalized value.\n",
      "    28|         0|            0|            0|  0.00%|              The `logits` argument will be interpreted as unnormalized log probabilities\n",
      "    29|         0|            0|            0|  0.00%|              and can therefore be any real number. It will likewise be normalized so that\n",
      "    30|         0|            0|            0|  0.00%|              the resulting probabilities sum to 1 along the last dimension. :attr:`logits`\n",
      "    31|         0|            0|            0|  0.00%|              will return this normalized value.\n",
      "    32|         0|            0|            0|  0.00%|\n",
      "    33|         0|            0|            0|  0.00%|    See also: :func:`torch.multinomial`\n",
      "    34|         0|            0|            0|  0.00%|\n",
      "    35|         0|            0|            0|  0.00%|    Example::\n",
      "    36|         0|            0|            0|  0.00%|\n",
      "    37|         0|            0|            0|  0.00%|        >>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n",
      "    38|         0|            0|            0|  0.00%|        >>> m.sample()  # equal probability of 0, 1, 2, 3\n",
      "    39|         0|            0|            0|  0.00%|        tensor(3)\n",
      "    40|         0|            0|            0|  0.00%|\n",
      "    41|         0|            0|            0|  0.00%|    Args:\n",
      "    42|         0|            0|            0|  0.00%|        probs (Tensor): event probabilities\n",
      "    43|         0|            0|            0|  0.00%|        logits (Tensor): event log probabilities (unnormalized)\n",
      "    44|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    45|         0|            0|            0|  0.00%|    arg_constraints = {'probs': constraints.simplex,\n",
      "    46|         0|            0|            0|  0.00%|                       'logits': constraints.real_vector}\n",
      "    47|         0|            0|            0|  0.00%|    has_enumerate_support = True\n",
      "    48|         0|            0|            0|  0.00%|\n",
      "    49|     31590|    0.0947733|   3.0001e-06|  0.02%|    def __init__(self, probs=None, logits=None, validate_args=None):\n",
      "    50|     31590|    0.0928874|  2.94041e-06|  0.01%|        if (probs is None) == (logits is None):\n",
      "    51|         0|            0|            0|  0.00%|            raise ValueError(\"Either `probs` or `logits` must be specified, but not both.\")\n",
      "    52|     31590|     0.073601|  2.32988e-06|  0.01%|        if probs is not None:\n",
      "    53|         0|            0|            0|  0.00%|            if probs.dim() < 1:\n",
      "    54|         0|            0|            0|  0.00%|                raise ValueError(\"`probs` parameter must be at least one-dimensional.\")\n",
      "    55|         0|            0|            0|  0.00%|            self.probs = probs / probs.sum(-1, keepdim=True)\n",
      "    56|         0|            0|            0|  0.00%|        else:\n",
      "    57|     31590|    0.0888705|  2.81325e-06|  0.01%|            if logits.dim() < 1:\n",
      "    58|         0|            0|            0|  0.00%|                raise ValueError(\"`logits` parameter must be at least one-dimensional.\")\n",
      "    59|         0|            0|            0|  0.00%|            # Normalize\n",
      "    60|     31590|       2.1945|  6.94683e-05|  0.35%|            self.logits = logits - logits.logsumexp(dim=-1, keepdim=True)\n",
      "    61|     31590|      0.12092|  3.82781e-06|  0.02%|        self._param = self.probs if probs is not None else self.logits\n",
      "    62|     31590|     0.136669|  4.32633e-06|  0.02%|        self._num_events = self._param.size()[-1]\n",
      "    63|     31590|     0.157609|   4.9892e-06|  0.03%|        batch_shape = self._param.size()[:-1] if self._param.ndimension() > 1 else torch.Size()\n",
      "    64|     31590|      0.34199|  1.08259e-05|  0.06%|        super(Categorical, self).__init__(batch_shape, validate_args=validate_args)\n",
      "(call)|     31590|      4.97459|  0.000157473|  0.80%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/distribution.py:34 __init__\n",
      "    65|         0|            0|            0|  0.00%|\n",
      "    66|         0|            0|            0|  0.00%|    def expand(self, batch_shape, _instance=None):\n",
      "    67|         0|            0|            0|  0.00%|        new = self._get_checked_instance(Categorical, _instance)\n",
      "    68|         0|            0|            0|  0.00%|        batch_shape = torch.Size(batch_shape)\n",
      "    69|         0|            0|            0|  0.00%|        param_shape = batch_shape + torch.Size((self._num_events,))\n",
      "    70|         0|            0|            0|  0.00%|        if 'probs' in self.__dict__:\n",
      "    71|         0|            0|            0|  0.00%|            new.probs = self.probs.expand(param_shape)\n",
      "    72|         0|            0|            0|  0.00%|            new._param = new.probs\n",
      "    73|         0|            0|            0|  0.00%|        if 'logits' in self.__dict__:\n",
      "    74|         0|            0|            0|  0.00%|            new.logits = self.logits.expand(param_shape)\n",
      "    75|         0|            0|            0|  0.00%|            new._param = new.logits\n",
      "    76|         0|            0|            0|  0.00%|        new._num_events = self._num_events\n",
      "    77|         0|            0|            0|  0.00%|        super(Categorical, new).__init__(batch_shape, validate_args=False)\n",
      "    78|         0|            0|            0|  0.00%|        new._validate_args = self._validate_args\n",
      "    79|         0|            0|            0|  0.00%|        return new\n",
      "    80|         0|            0|            0|  0.00%|\n",
      "    81|         0|            0|            0|  0.00%|    def _new(self, *args, **kwargs):\n",
      "    82|         0|            0|            0|  0.00%|        return self._param.new(*args, **kwargs)\n",
      "    83|         0|            0|            0|  0.00%|\n",
      "    84|     31590|    0.0531228|  1.68163e-06|  0.01%|    @constraints.dependent_property(is_discrete=True, event_dim=0)\n",
      "    85|         0|            0|            0|  0.00%|    def support(self):\n",
      "    86|     31590|     0.222456|  7.04197e-06|  0.04%|        return constraints.integer_interval(0, self._num_events - 1)\n",
      "(call)|     31590|      0.27614|  8.74137e-06|  0.04%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/constraints.py:251 __init__\n",
      "    87|         0|            0|            0|  0.00%|\n",
      "    88|         0|            0|            0|  0.00%|    @lazy_property\n",
      "    89|         0|            0|            0|  0.00%|    def logits(self):\n",
      "    90|         0|            0|            0|  0.00%|        return probs_to_logits(self.probs)\n",
      "    91|         0|            0|            0|  0.00%|\n",
      "    92|     31590|    0.0449855|  1.42404e-06|  0.01%|    @lazy_property\n",
      "    93|         0|            0|            0|  0.00%|    def probs(self):\n",
      "    94|     31590|     0.195313|  6.18276e-06|  0.03%|        return logits_to_probs(self.logits)\n",
      "(call)|     31590|      1.11508|  3.52985e-05|  0.18%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/utils.py:65 logits_to_probs\n",
      "    95|         0|            0|            0|  0.00%|\n",
      "    96|         0|            0|            0|  0.00%|    @property\n",
      "    97|         0|            0|            0|  0.00%|    def param_shape(self):\n",
      "    98|         0|            0|            0|  0.00%|        return self._param.size()\n",
      "    99|         0|            0|            0|  0.00%|\n",
      "   100|         0|            0|            0|  0.00%|    @property\n",
      "   101|         0|            0|            0|  0.00%|    def mean(self):\n",
      "   102|         0|            0|            0|  0.00%|        return torch.full(self._extended_shape(), nan, dtype=self.probs.dtype, device=self.probs.device)\n",
      "   103|         0|            0|            0|  0.00%|\n",
      "   104|         0|            0|            0|  0.00%|    @property\n",
      "   105|         0|            0|            0|  0.00%|    def mode(self):\n",
      "   106|         0|            0|            0|  0.00%|        return self.probs.argmax(axis=-1)\n",
      "   107|         0|            0|            0|  0.00%|\n",
      "   108|         0|            0|            0|  0.00%|    @property\n",
      "   109|         0|            0|            0|  0.00%|    def variance(self):\n",
      "   110|         0|            0|            0|  0.00%|        return torch.full(self._extended_shape(), nan, dtype=self.probs.dtype, device=self.probs.device)\n",
      "   111|         0|            0|            0|  0.00%|\n",
      "   112|     24960|    0.0580456|  2.32555e-06|  0.01%|    def sample(self, sample_shape=torch.Size()):\n",
      "   113|     24960|    0.0667982|  2.67621e-06|  0.01%|        if not isinstance(sample_shape, torch.Size):\n",
      "   114|         0|            0|            0|  0.00%|            sample_shape = torch.Size(sample_shape)\n",
      "   115|     24960|     0.276103|  1.10618e-05|  0.04%|        probs_2d = self.probs.reshape(-1, self._num_events)\n",
      "(call)|     24960|       1.9088|  7.64744e-05|  0.31%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/utils.py:106 __get__\n",
      "   116|     24960|     0.370704|  1.48519e-05|  0.06%|        samples_2d = torch.multinomial(probs_2d, sample_shape.numel(), True).T\n",
      "   117|     24960|     0.262425|  1.05138e-05|  0.04%|        return samples_2d.reshape(self._extended_shape(sample_shape))\n",
      "(call)|     24960|     0.174944|  7.00899e-06|  0.03%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/distribution.py:241 _extended_shape\n",
      "   118|         0|            0|            0|  0.00%|\n",
      "   119|     31590|    0.0665579|  2.10693e-06|  0.01%|    def log_prob(self, value):\n",
      "   120|     31590|     0.072829|  2.30545e-06|  0.01%|        if self._validate_args:\n",
      "   121|     31590|     0.295252|  9.34637e-06|  0.05%|            self._validate_sample(value)\n",
      "(call)|     31590|      3.87566|  0.000122686|  0.62%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/distribution.py:255 _validate_sample\n",
      "   122|     31590|     0.253914|  8.03781e-06|  0.04%|        value = value.long().unsqueeze(-1)\n",
      "   123|     31590|     0.285117|  9.02556e-06|  0.05%|        value, log_pmf = torch.broadcast_tensors(value, self.logits)\n",
      "(call)|     31590|     0.910228|  2.88138e-05|  0.15%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/functional.py:44 broadcast_tensors\n",
      "   124|     31590|     0.281378|  8.90717e-06|  0.05%|        value = value[..., :1]\n",
      "   125|     31590|     0.592849|   1.8767e-05|  0.10%|        return log_pmf.gather(-1, value).squeeze(-1)\n",
      "   126|         0|            0|            0|  0.00%|\n",
      "   127|     31590|     0.074476|  2.35758e-06|  0.01%|    def entropy(self):\n",
      "   128|     31590|     0.132697|  4.20059e-06|  0.02%|        min_real = torch.finfo(self.logits.dtype).min\n",
      "   129|     31590|     0.392106|  1.24124e-05|  0.06%|        logits = torch.clamp(self.logits, min=min_real)\n",
      "   130|     31590|     0.357738|  1.13244e-05|  0.06%|        p_log_p = logits * self.probs\n",
      "(call)|      6630|      0.63276|  9.54389e-05|  0.10%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/utils.py:106 __get__\n",
      "   131|     31590|     0.654776|  2.07273e-05|  0.11%|        return -p_log_p.sum(-1)\n",
      "   132|         0|            0|            0|  0.00%|\n",
      "   133|         0|            0|            0|  0.00%|    def enumerate_support(self, expand=True):\n",
      "   134|         0|            0|            0|  0.00%|        num_events = self._num_events\n",
      "   135|         0|            0|            0|  0.00%|        values = torch.arange(num_events, dtype=torch.long, device=self._param.device)\n",
      "   136|         0|            0|            0|  0.00%|        values = values.view((-1,) + (1,) * len(self._batch_shape))\n",
      "   137|         0|            0|            0|  0.00%|        if expand:\n",
      "   138|         0|            0|            0|  0.00%|            values = values.expand((-1,) + self._batch_shape)\n",
      "   139|         0|            0|            0|  0.00%|        return values\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py\n",
      "File duration: 6.68738s (1.08%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|from collections import OrderedDict\n",
      "     2|         0|            0|            0|  0.00%|import enum\n",
      "     3|         0|            0|            0|  0.00%|import functools\n",
      "     4|         0|            0|            0|  0.00%|from numbers import Number\n",
      "     5|         0|            0|            0|  0.00%|from typing import Any, Dict, Optional, Tuple, Union\n",
      "     6|         0|            0|            0|  0.00%|import warnings\n",
      "     7|         0|            0|            0|  0.00%|import copyreg\n",
      "     8|         0|            0|            0|  0.00%|from copy import deepcopy\n",
      "     9|         0|            0|            0|  0.00%|\n",
      "    10|         0|            0|            0|  0.00%|import torch\n",
      "    11|         0|            0|            0|  0.00%|import torch._C as _C\n",
      "    12|         0|            0|            0|  0.00%|from torch._namedtensor_internals import (\n",
      "    13|         0|            0|            0|  0.00%|    update_names, check_serializing_named_tensor, resolve_ellipsis,\n",
      "    14|         0|            0|            0|  0.00%|    unzip_namedshape, single_ellipsis_index, is_ellipsis)\n",
      "    15|         0|            0|            0|  0.00%|from torch.overrides import (\n",
      "    16|         0|            0|            0|  0.00%|    has_torch_function, has_torch_function_unary, has_torch_function_variadic,\n",
      "    17|         0|            0|            0|  0.00%|    handle_torch_function, get_default_nowrap_functions)\n",
      "    18|         0|            0|            0|  0.00%|import torch.utils.hooks as hooks\n",
      "    19|         0|            0|            0|  0.00%|\n",
      "    20|         0|            0|            0|  0.00%|\n",
      "    21|         0|            0|            0|  0.00%|def _handle_torch_function_and_wrap_type_error_to_not_implemented(f):\n",
      "    22|         0|            0|            0|  0.00%|    # functools.wraps doesn't work well with methods in python 2\n",
      "    23|         0|            0|            0|  0.00%|    method_assignments = ('__name__', '__doc__')\n",
      "    24|         0|            0|            0|  0.00%|    assigned = functools.WRAPPER_ASSIGNMENTS\n",
      "    25|         0|            0|            0|  0.00%|\n",
      "    26|     43680|     0.120138|  2.75041e-06|  0.02%|    @functools.wraps(f, assigned=assigned)\n",
      "    27|         0|            0|            0|  0.00%|    def wrapped(*args, **kwargs):\n",
      "    28|     43680|    0.0813062|  1.86141e-06|  0.01%|        try:\n",
      "    29|         0|            0|            0|  0.00%|            # See https://github.com/pytorch/pytorch/issues/75462\n",
      "    30|     43680|    0.0773954|  1.77187e-06|  0.01%|            if has_torch_function(args):\n",
      "    31|         0|            0|            0|  0.00%|                return handle_torch_function(wrapped, args, *args, **kwargs)\n",
      "    32|     43680|     0.297176|  6.80347e-06|  0.05%|            return f(*args, **kwargs)\n",
      "(call)|     24960|     0.258296|  1.03484e-05|  0.04%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:637 __rsub__\n",
      "(call)|      6240|     0.102141|  1.63688e-05|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:641 __rdiv__\n",
      "    33|         0|            0|            0|  0.00%|        except TypeError:\n",
      "    34|         0|            0|            0|  0.00%|            return NotImplemented\n",
      "    35|         0|            0|            0|  0.00%|    return wrapped\n",
      "    36|         0|            0|            0|  0.00%|\n",
      "    37|         0|            0|            0|  0.00%|# Should not be used, this is kept only for BC of loading old serialized Tensor subclasses\n",
      "    38|         0|            0|            0|  0.00%|def _rebuild_from_type(func, type, args, dict):\n",
      "    39|         0|            0|            0|  0.00%|    if type is Tensor:\n",
      "    40|         0|            0|            0|  0.00%|        return func(*args)\n",
      "    41|         0|            0|            0|  0.00%|\n",
      "    42|         0|            0|            0|  0.00%|    ret = func(*args).as_subclass(type)\n",
      "    43|         0|            0|            0|  0.00%|    ret.__dict__ = dict\n",
      "    44|         0|            0|            0|  0.00%|    return ret\n",
      "    45|         0|            0|            0|  0.00%|\n",
      "    46|         0|            0|            0|  0.00%|def _rebuild_from_type_v2(func, new_type, args, state):\n",
      "    47|         0|            0|            0|  0.00%|    if new_type is Tensor:\n",
      "    48|         0|            0|            0|  0.00%|        return func(*args)\n",
      "    49|         0|            0|            0|  0.00%|\n",
      "    50|         0|            0|            0|  0.00%|    ret = func(*args)\n",
      "    51|         0|            0|            0|  0.00%|    if type(ret) is not new_type:\n",
      "    52|         0|            0|            0|  0.00%|        ret = ret.as_subclass(new_type)\n",
      "    53|         0|            0|            0|  0.00%|    # Tensor does define __setstate__ even though it doesn't define\n",
      "    54|         0|            0|            0|  0.00%|    # __getstate__. So only use __setstate__ if it is NOT the one defined\n",
      "    55|         0|            0|            0|  0.00%|    # on Tensor\n",
      "    56|         0|            0|            0|  0.00%|    if getattr(ret.__class__, \"__setstate__\", Tensor.__setstate__) is not Tensor.__setstate__:\n",
      "    57|         0|            0|            0|  0.00%|        ret.__setstate__(state)\n",
      "    58|         0|            0|            0|  0.00%|    else:\n",
      "    59|         0|            0|            0|  0.00%|        if isinstance(state, tuple):\n",
      "    60|         0|            0|            0|  0.00%|            if not len(state) == 2:\n",
      "    61|         0|            0|            0|  0.00%|                raise RuntimeError(f\"Invalid serialized state: {state}\")\n",
      "    62|         0|            0|            0|  0.00%|            dict_state = state[0]\n",
      "    63|         0|            0|            0|  0.00%|            slots_state = state[1]\n",
      "    64|         0|            0|            0|  0.00%|        else:\n",
      "    65|         0|            0|            0|  0.00%|            dict_state = state\n",
      "    66|         0|            0|            0|  0.00%|            slots_state = None\n",
      "    67|         0|            0|            0|  0.00%|\n",
      "    68|         0|            0|            0|  0.00%|        for k, v in dict_state.items():\n",
      "    69|         0|            0|            0|  0.00%|            setattr(ret, k, v)\n",
      "    70|         0|            0|            0|  0.00%|\n",
      "    71|         0|            0|            0|  0.00%|        if slots_state:\n",
      "    72|         0|            0|            0|  0.00%|            for k, v in slots_state.items():\n",
      "    73|         0|            0|            0|  0.00%|                setattr(ret, k, v)\n",
      "    74|         0|            0|            0|  0.00%|    return ret\n",
      "    75|         0|            0|            0|  0.00%|\n",
      "    76|         0|            0|            0|  0.00%|\n",
      "    77|         0|            0|            0|  0.00%|# NB: If you subclass Tensor, and want to share the subclassed class\n",
      "    78|         0|            0|            0|  0.00%|# across processes, you must also update torch/multiprocessing/reductions.py\n",
      "    79|         0|            0|            0|  0.00%|# to define a ForkingPickler serialization mode for the class.\n",
      "    80|         0|            0|            0|  0.00%|#\n",
      "    81|         0|            0|            0|  0.00%|# NB: If you add a new method to Tensor, you must update\n",
      "    82|         0|            0|            0|  0.00%|# torch/__init__.py.in to add a type annotation for your method;\n",
      "    83|         0|            0|            0|  0.00%|# otherwise, it will not show up in autocomplete.\n",
      "    84|         0|            0|            0|  0.00%|class Tensor(torch._C._TensorBase):\n",
      "    85|         0|            0|            0|  0.00%|    def __deepcopy__(self, memo):\n",
      "    86|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "    87|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__deepcopy__, (self,), self, memo)\n",
      "    88|         0|            0|            0|  0.00%|        if not self.is_leaf:\n",
      "    89|         0|            0|            0|  0.00%|            raise RuntimeError(\"Only Tensors created explicitly by the user \"\n",
      "    90|         0|            0|            0|  0.00%|                               \"(graph leaves) support the deepcopy protocol at the moment\")\n",
      "    91|         0|            0|            0|  0.00%|        if id(self) in memo:\n",
      "    92|         0|            0|            0|  0.00%|            return memo[id(self)]\n",
      "    93|         0|            0|            0|  0.00%|        with torch.no_grad():\n",
      "    94|         0|            0|            0|  0.00%|            # TODO: skipping storage copy is wrong for meta, as meta\n",
      "    95|         0|            0|            0|  0.00%|            # does accurate alias tracking; however, the code below\n",
      "    96|         0|            0|            0|  0.00%|            # doesn't work because of\n",
      "    97|         0|            0|            0|  0.00%|            # https://github.com/pytorch/pytorch/issues/47442\n",
      "    98|         0|            0|            0|  0.00%|            # Update the test in test_serialization if you remove 'meta' from here\n",
      "    99|         0|            0|            0|  0.00%|            if self.is_sparse or self.device.type in ['lazy', 'xla', 'mps', 'ort', 'meta', 'hpu'] or \\\n",
      "   100|         0|            0|            0|  0.00%|                    (type(self) is not Tensor and self.data_ptr() == 0):\n",
      "   101|         0|            0|            0|  0.00%|                new_tensor = self.clone()\n",
      "   102|         0|            0|            0|  0.00%|                if type(new_tensor) is not type(self):\n",
      "   103|         0|            0|            0|  0.00%|                    raise RuntimeError(\"The default implementation of __deepcopy__() for wrapper subclasses \"\n",
      "   104|         0|            0|            0|  0.00%|                                       \"only works for subclass types that implement clone() and for which \"\n",
      "   105|         0|            0|            0|  0.00%|                                       \"cloning returns another instance of the same subclass. You should either \"\n",
      "   106|         0|            0|            0|  0.00%|                                       \"properly implement clone() for your subclass or override __deepcopy__() \"\n",
      "   107|         0|            0|            0|  0.00%|                                       \"if it is intended behavior for clone() to return an instance of a \"\n",
      "   108|         0|            0|            0|  0.00%|                                       \"different type.\")\n",
      "   109|         0|            0|            0|  0.00%|            else:\n",
      "   110|         0|            0|            0|  0.00%|                new_storage = self.storage().__deepcopy__(memo)\n",
      "   111|         0|            0|            0|  0.00%|                if self.is_quantized:\n",
      "   112|         0|            0|            0|  0.00%|                    # quantizer_params can be different type based on torch attribute\n",
      "   113|         0|            0|            0|  0.00%|                    quantizer_params: Union[Tuple[torch.qscheme, float, int], Tuple[torch.qscheme, Tensor, Tensor, int]]\n",
      "   114|         0|            0|            0|  0.00%|                    if self.qscheme() == torch.per_tensor_affine:\n",
      "   115|         0|            0|            0|  0.00%|                        quantizer_params = self.qscheme(), self.q_scale(), self.q_zero_point()\n",
      "   116|         0|            0|            0|  0.00%|                    elif self.qscheme() in (torch.per_channel_affine, torch.per_channel_affine_float_qparams):\n",
      "   117|         0|            0|            0|  0.00%|                        quantizer_params = self.qscheme(), \\\n",
      "   118|         0|            0|            0|  0.00%|                            self.q_per_channel_scales(), \\\n",
      "   119|         0|            0|            0|  0.00%|                            self.q_per_channel_zero_points(), \\\n",
      "   120|         0|            0|            0|  0.00%|                            self.q_per_channel_axis()\n",
      "   121|         0|            0|            0|  0.00%|                    else:\n",
      "   122|         0|            0|            0|  0.00%|                        raise RuntimeError(f\"Unsupported qscheme {self.qscheme()} in deepcopy\")\n",
      "   123|         0|            0|            0|  0.00%|                    # TODO: Once we decide to break serialization FC, no longer\n",
      "   124|         0|            0|            0|  0.00%|                    # need to wrap with _TypedStorage\n",
      "   125|         0|            0|            0|  0.00%|                    new_tensor = torch._utils._rebuild_qtensor(\n",
      "   126|         0|            0|            0|  0.00%|                        torch.storage._TypedStorage(\n",
      "   127|         0|            0|            0|  0.00%|                            wrap_storage=new_storage._untyped(),\n",
      "   128|         0|            0|            0|  0.00%|                            dtype=self.dtype),\n",
      "   129|         0|            0|            0|  0.00%|                        self.storage_offset(),\n",
      "   130|         0|            0|            0|  0.00%|                        self.size(),\n",
      "   131|         0|            0|            0|  0.00%|                        self.stride(),\n",
      "   132|         0|            0|            0|  0.00%|                        quantizer_params,\n",
      "   133|         0|            0|            0|  0.00%|                        self.requires_grad,\n",
      "   134|         0|            0|            0|  0.00%|                        self._backward_hooks)\n",
      "   135|         0|            0|            0|  0.00%|                    if type(new_tensor) is not type(self):\n",
      "   136|         0|            0|            0|  0.00%|                        raise RuntimeError(\"The default implementation of __deepcopy__() for quantized tensors \"\n",
      "   137|         0|            0|            0|  0.00%|                                           \"expects the tensor returned by torch._utils._rebuild_qtensor() to \"\n",
      "   138|         0|            0|            0|  0.00%|                                           \"match the type of the instance being copied. If you encounter this, \"\n",
      "   139|         0|            0|            0|  0.00%|                                           \"please open an issue on PyTorch's GitHub.\")\n",
      "   140|         0|            0|            0|  0.00%|                else:\n",
      "   141|         0|            0|            0|  0.00%|                    new_tensor = self.new_empty([])\n",
      "   142|         0|            0|            0|  0.00%|                    if type(new_tensor) is not type(self):\n",
      "   143|         0|            0|            0|  0.00%|                        raise RuntimeError(\"The default implementation of __deepcopy__() for non-wrapper subclasses \"\n",
      "   144|         0|            0|            0|  0.00%|                                           \"only works for subclass types that implement new_empty() and for which \"\n",
      "   145|         0|            0|            0|  0.00%|                                           \"that function returns another instance of the same subclass. You should \"\n",
      "   146|         0|            0|            0|  0.00%|                                           \"either properly implement new_empty() for your subclass or override \"\n",
      "   147|         0|            0|            0|  0.00%|                                           \"__deepcopy__() if it is intended behavior for new_empty() to return \"\n",
      "   148|         0|            0|            0|  0.00%|                                           \"an instance of a different type.\")\n",
      "   149|         0|            0|            0|  0.00%|                    new_tensor.set_(new_storage, self.storage_offset(), self.size(), self.stride())\n",
      "   150|         0|            0|            0|  0.00%|                    if self.is_conj():\n",
      "   151|         0|            0|            0|  0.00%|                        new_tensor = new_tensor.conj_physical()\n",
      "   152|         0|            0|            0|  0.00%|                    if self.is_neg():\n",
      "   153|         0|            0|            0|  0.00%|                        new_tensor = new_tensor.neg()\n",
      "   154|         0|            0|            0|  0.00%|            if self.requires_grad:\n",
      "   155|         0|            0|            0|  0.00%|                new_tensor.requires_grad_()\n",
      "   156|         0|            0|            0|  0.00%|            if self.grad is not None:\n",
      "   157|         0|            0|            0|  0.00%|                new_tensor.grad = self.grad.__deepcopy__(memo)\n",
      "   158|         0|            0|            0|  0.00%|\n",
      "   159|         0|            0|            0|  0.00%|            if not type(self) is Tensor:\n",
      "   160|         0|            0|            0|  0.00%|                if type(new_tensor) is not type(self):\n",
      "   161|         0|            0|            0|  0.00%|                    raise RuntimeError(\"Type of deepcopy result does not match the type of the source tensor. \"\n",
      "   162|         0|            0|            0|  0.00%|                                       \"If you encounter this, please open an issue on PyTorch's GitHub.\")\n",
      "   163|         0|            0|            0|  0.00%|\n",
      "   164|         0|            0|            0|  0.00%|                # Plain Tensors don't have slots\n",
      "   165|         0|            0|            0|  0.00%|                slots_to_save = copyreg._slotnames(self.__class__)  # type: ignore[attr-defined]\n",
      "   166|         0|            0|            0|  0.00%|                for slot in slots_to_save:\n",
      "   167|         0|            0|            0|  0.00%|                    if hasattr(self, slot):\n",
      "   168|         0|            0|            0|  0.00%|                        setattr(new_tensor, slot, deepcopy(getattr(self, slot), memo))\n",
      "   169|         0|            0|            0|  0.00%|\n",
      "   170|         0|            0|            0|  0.00%|            new_tensor.__dict__ = deepcopy(self.__dict__, memo)\n",
      "   171|         0|            0|            0|  0.00%|\n",
      "   172|         0|            0|            0|  0.00%|            memo[id(self)] = new_tensor\n",
      "   173|         0|            0|            0|  0.00%|            return new_tensor\n",
      "   174|         0|            0|            0|  0.00%|\n",
      "   175|         0|            0|            0|  0.00%|    def __reduce_ex__(self, proto):\n",
      "   176|         0|            0|            0|  0.00%|        if type(self) is Tensor:\n",
      "   177|         0|            0|            0|  0.00%|            return self._reduce_ex_internal(proto)\n",
      "   178|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   179|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__reduce_ex__, (self,), self, proto)\n",
      "   180|         0|            0|            0|  0.00%|        func, args = self._reduce_ex_internal(proto)\n",
      "   181|         0|            0|            0|  0.00%|        # Get the state of the python subclass\n",
      "   182|         0|            0|            0|  0.00%|        # This loosely mimicks the function on the object class but since Tensor do not inherit\n",
      "   183|         0|            0|            0|  0.00%|        # from it, we cannot call that function directly\n",
      "   184|         0|            0|            0|  0.00%|        # https://github.com/python/cpython/blob/c83919bd635f4433f1c6ae8504996a9fe3c215e5/Objects/typeobject.c#L4891\n",
      "   185|         0|            0|            0|  0.00%|        getstate_fn = getattr(self, \"__getstate__\", None)\n",
      "   186|         0|            0|            0|  0.00%|        if getstate_fn:\n",
      "   187|         0|            0|            0|  0.00%|            state = getstate_fn()\n",
      "   188|         0|            0|            0|  0.00%|        else:\n",
      "   189|         0|            0|            0|  0.00%|            slots_to_save = copyreg._slotnames(self.__class__)  # type: ignore[attr-defined]\n",
      "   190|         0|            0|            0|  0.00%|            if slots_to_save:\n",
      "   191|         0|            0|            0|  0.00%|                state = (self.__dict__, {name: getattr(self, name) for name in slots_to_save if hasattr(self, name)})\n",
      "   192|         0|            0|            0|  0.00%|            else:\n",
      "   193|         0|            0|            0|  0.00%|                state = self.__dict__\n",
      "   194|         0|            0|            0|  0.00%|        return (_rebuild_from_type_v2, (func, type(self), args, state))\n",
      "   195|         0|            0|            0|  0.00%|\n",
      "   196|         0|            0|            0|  0.00%|    def storage(self):\n",
      "   197|         0|            0|            0|  0.00%|        r\"\"\"\n",
      "   198|         0|            0|            0|  0.00%|        storage() -> torch.Storage\n",
      "   199|         0|            0|            0|  0.00%|\n",
      "   200|         0|            0|            0|  0.00%|        Returns the underlying storage.\n",
      "   201|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   202|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   203|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.storage, (self,), self)\n",
      "   204|         0|            0|            0|  0.00%|\n",
      "   205|         0|            0|            0|  0.00%|        return torch._TypedStorage(wrap_storage=self._storage(), dtype=self.dtype)\n",
      "   206|         0|            0|            0|  0.00%|\n",
      "   207|         0|            0|            0|  0.00%|    def _reduce_ex_internal(self, proto):\n",
      "   208|         0|            0|            0|  0.00%|        check_serializing_named_tensor(self)\n",
      "   209|         0|            0|            0|  0.00%|        # See Note [Don't serialize hooks]\n",
      "   210|         0|            0|            0|  0.00%|        torch.utils.hooks.warn_if_has_hooks(self)\n",
      "   211|         0|            0|            0|  0.00%|        backward_hooks: Dict[Any, Any] = OrderedDict()\n",
      "   212|         0|            0|            0|  0.00%|        # Note: Numpy array is chosen to be the rebuild component for XLA, ORT Tensors.\n",
      "   213|         0|            0|            0|  0.00%|        # We considered a few options:\n",
      "   214|         0|            0|            0|  0.00%|        # 1. CPU tensor can't be used here.\n",
      "   215|         0|            0|            0|  0.00%|        #    Otherwise in torch.load CPU storage is reconstructed with randomly\n",
      "   216|         0|            0|            0|  0.00%|        #    initialized data, moved onto backend device, and then storage is updated\n",
      "   217|         0|            0|            0|  0.00%|        #    to the serialized content. This works perfectly for CPU/CUDA but not these backends;\n",
      "   218|         0|            0|            0|  0.00%|        #    their tensors are disconnected with storage so they don't get the update.\n",
      "   219|         0|            0|            0|  0.00%|        # 2. Python list is not a good fit due to performance reason.\n",
      "   220|         0|            0|            0|  0.00%|        #    `tolist()` converts every single element in the tensor into python objects\n",
      "   221|         0|            0|            0|  0.00%|        #    and serialize them one by one.\n",
      "   222|         0|            0|            0|  0.00%|        if self.device.type in ['xla', 'ort', 'mps', 'hpu']:\n",
      "   223|         0|            0|            0|  0.00%|            return (torch._utils._rebuild_device_tensor_from_numpy, (self.cpu().numpy(),\n",
      "   224|         0|            0|            0|  0.00%|                                                                     self.dtype,\n",
      "   225|         0|            0|            0|  0.00%|                                                                     str(self.device),\n",
      "   226|         0|            0|            0|  0.00%|                                                                     self.requires_grad))\n",
      "   227|         0|            0|            0|  0.00%|        if self.device.type == 'meta':\n",
      "   228|         0|            0|            0|  0.00%|            # NB: This implementation BREAKS storage sharing.  Current\n",
      "   229|         0|            0|            0|  0.00%|            # hypothesis is that no one cares for meta tensors.\n",
      "   230|         0|            0|            0|  0.00%|            arg_meta = (\n",
      "   231|         0|            0|            0|  0.00%|                self.dtype,\n",
      "   232|         0|            0|            0|  0.00%|                tuple(self.size()),\n",
      "   233|         0|            0|            0|  0.00%|                self.stride(),\n",
      "   234|         0|            0|            0|  0.00%|                self.requires_grad,\n",
      "   235|         0|            0|            0|  0.00%|            )\n",
      "   236|         0|            0|            0|  0.00%|            return (torch._utils._rebuild_meta_tensor_no_storage, arg_meta)\n",
      "   237|         0|            0|            0|  0.00%|        if self.is_quantized:\n",
      "   238|         0|            0|            0|  0.00%|            # quantizer_params can be different type based on torch attribute\n",
      "   239|         0|            0|            0|  0.00%|            quantizer_params: Union[Tuple[torch.qscheme, float, int], Tuple[Any, Tensor, Tensor, int]]\n",
      "   240|         0|            0|            0|  0.00%|            if self.qscheme() == torch.per_tensor_affine:\n",
      "   241|         0|            0|            0|  0.00%|                quantizer_params = (torch.per_tensor_affine,\n",
      "   242|         0|            0|            0|  0.00%|                                    self.q_scale(),\n",
      "   243|         0|            0|            0|  0.00%|                                    self.q_zero_point())\n",
      "   244|         0|            0|            0|  0.00%|            elif self.qscheme() in (torch.per_channel_affine, torch.per_channel_affine_float_qparams):\n",
      "   245|         0|            0|            0|  0.00%|                # convert scales and zero points to tuple to avoid recursive calls\n",
      "   246|         0|            0|            0|  0.00%|                # when/if we get multi-axis quantized tensors in the future, the shape\n",
      "   247|         0|            0|            0|  0.00%|                # is recoverable from the main tensor shape\n",
      "   248|         0|            0|            0|  0.00%|                quantizer_params = (torch.per_channel_affine,\n",
      "   249|         0|            0|            0|  0.00%|                                    self.q_per_channel_scales(),\n",
      "   250|         0|            0|            0|  0.00%|                                    self.q_per_channel_zero_points(),\n",
      "   251|         0|            0|            0|  0.00%|                                    self.q_per_channel_axis())\n",
      "   252|         0|            0|            0|  0.00%|            else:\n",
      "   253|         0|            0|            0|  0.00%|                raise RuntimeError(f\"Serialization is not supported for tensors of type {self.qscheme()}\")\n",
      "   254|         0|            0|            0|  0.00%|            # TODO: Once we decide to break serialization FC, no longer\n",
      "   255|         0|            0|            0|  0.00%|            # need to wrap with _TypedStorage\n",
      "   256|         0|            0|            0|  0.00%|            args_qtensor = (\n",
      "   257|         0|            0|            0|  0.00%|                torch.storage._TypedStorage(\n",
      "   258|         0|            0|            0|  0.00%|                    wrap_storage=self.storage()._untyped(),\n",
      "   259|         0|            0|            0|  0.00%|                    dtype=self.dtype),\n",
      "   260|         0|            0|            0|  0.00%|                self.storage_offset(),\n",
      "   261|         0|            0|            0|  0.00%|                tuple(self.size()),\n",
      "   262|         0|            0|            0|  0.00%|                self.stride(),\n",
      "   263|         0|            0|            0|  0.00%|                quantizer_params,\n",
      "   264|         0|            0|            0|  0.00%|                self.requires_grad,\n",
      "   265|         0|            0|            0|  0.00%|                backward_hooks)\n",
      "   266|         0|            0|            0|  0.00%|            return (torch._utils._rebuild_qtensor, args_qtensor)\n",
      "   267|         0|            0|            0|  0.00%|        elif self.is_sparse:\n",
      "   268|         0|            0|            0|  0.00%|            if self.layout == torch.sparse_coo:\n",
      "   269|         0|            0|            0|  0.00%|                args_sparse = (self.layout,\n",
      "   270|         0|            0|            0|  0.00%|                               (self._indices(),\n",
      "   271|         0|            0|            0|  0.00%|                                self._values(),\n",
      "   272|         0|            0|            0|  0.00%|                                self.size()))\n",
      "   273|         0|            0|            0|  0.00%|            else:\n",
      "   274|         0|            0|            0|  0.00%|                raise NotImplementedError(\n",
      "   275|         0|            0|            0|  0.00%|                    'sparse tensor __reduce_ex__ for layout `%s`' % (self.layout))\n",
      "   276|         0|            0|            0|  0.00%|            return (torch._utils._rebuild_sparse_tensor, args_sparse)\n",
      "   277|         0|            0|            0|  0.00%|        elif self.is_sparse_csr:\n",
      "   278|         0|            0|            0|  0.00%|            if self.layout == torch.sparse_csr:\n",
      "   279|         0|            0|            0|  0.00%|                args_sparse_csr = (self.layout,\n",
      "   280|         0|            0|            0|  0.00%|                                   (self.crow_indices(),\n",
      "   281|         0|            0|            0|  0.00%|                                    self.col_indices(),\n",
      "   282|         0|            0|            0|  0.00%|                                    self.values(),\n",
      "   283|         0|            0|            0|  0.00%|                                    self.size()))\n",
      "   284|         0|            0|            0|  0.00%|            else:\n",
      "   285|         0|            0|            0|  0.00%|                raise NotImplementedError(\n",
      "   286|         0|            0|            0|  0.00%|                    'sparse csr tensor __reduce_ex__ for layout `%s`' % (self.layout))\n",
      "   287|         0|            0|            0|  0.00%|            return (torch._utils._rebuild_sparse_csr_tensor, args_sparse_csr)\n",
      "   288|         0|            0|            0|  0.00%|        elif self.data_ptr() == 0 and type(self) is not torch.Tensor:\n",
      "   289|         0|            0|            0|  0.00%|            arg_wrapper_subclass = (\n",
      "   290|         0|            0|            0|  0.00%|                type(self),\n",
      "   291|         0|            0|            0|  0.00%|                self.dtype,\n",
      "   292|         0|            0|            0|  0.00%|                tuple(self.size()),\n",
      "   293|         0|            0|            0|  0.00%|                self.stride(),\n",
      "   294|         0|            0|            0|  0.00%|                self.storage_offset(),\n",
      "   295|         0|            0|            0|  0.00%|                self.layout,\n",
      "   296|         0|            0|            0|  0.00%|                self.device,\n",
      "   297|         0|            0|            0|  0.00%|                self.requires_grad\n",
      "   298|         0|            0|            0|  0.00%|            )\n",
      "   299|         0|            0|            0|  0.00%|            return (torch._utils._rebuild_wrapper_subclass, arg_wrapper_subclass)\n",
      "   300|         0|            0|            0|  0.00%|        else:\n",
      "   301|         0|            0|            0|  0.00%|            # TODO: Once we decide to break serialization FC, no longer\n",
      "   302|         0|            0|            0|  0.00%|            # need to wrap with _TypedStorage\n",
      "   303|         0|            0|            0|  0.00%|            args = (\n",
      "   304|         0|            0|            0|  0.00%|                torch.storage._TypedStorage(\n",
      "   305|         0|            0|            0|  0.00%|                    wrap_storage=self.storage()._untyped(),\n",
      "   306|         0|            0|            0|  0.00%|                    dtype=self.dtype),\n",
      "   307|         0|            0|            0|  0.00%|                self.storage_offset(),\n",
      "   308|         0|            0|            0|  0.00%|                tuple(self.size()),\n",
      "   309|         0|            0|            0|  0.00%|                self.stride(),\n",
      "   310|         0|            0|            0|  0.00%|                self.requires_grad,\n",
      "   311|         0|            0|            0|  0.00%|                backward_hooks)  # previously was self._backward_hooks\n",
      "   312|         0|            0|            0|  0.00%|            return (torch._utils._rebuild_tensor_v2, args)\n",
      "   313|         0|            0|            0|  0.00%|\n",
      "   314|         0|            0|            0|  0.00%|    def __setstate__(self, state):\n",
      "   315|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   316|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__setstate__, (self,), self, state)\n",
      "   317|         0|            0|            0|  0.00%|        # Warning: this method is NOT called when you torch.load() a tensor;\n",
      "   318|         0|            0|            0|  0.00%|        # that is managed by _rebuild_tensor_v2\n",
      "   319|         0|            0|            0|  0.00%|        if not self.is_leaf:\n",
      "   320|         0|            0|            0|  0.00%|            raise RuntimeError('__setstate__ can be only called on leaf Tensors')\n",
      "   321|         0|            0|            0|  0.00%|        if len(state) == 4:\n",
      "   322|         0|            0|            0|  0.00%|            # legacy serialization of Tensor\n",
      "   323|         0|            0|            0|  0.00%|            self.set_(*state)\n",
      "   324|         0|            0|            0|  0.00%|            return\n",
      "   325|         0|            0|            0|  0.00%|        elif len(state) == 5:\n",
      "   326|         0|            0|            0|  0.00%|            # legacy serialization of Variable\n",
      "   327|         0|            0|            0|  0.00%|            self.data = state[0]\n",
      "   328|         0|            0|            0|  0.00%|            state = (state[3], state[4], state[2])\n",
      "   329|         0|            0|            0|  0.00%|        # The setting of _backward_hooks is expected to be a no-op.\n",
      "   330|         0|            0|            0|  0.00%|        # See Note [Don't serialize hooks]\n",
      "   331|         0|            0|            0|  0.00%|        self.requires_grad, _, self._backward_hooks = state\n",
      "   332|         0|            0|            0|  0.00%|\n",
      "   333|         0|            0|            0|  0.00%|    def __repr__(self, *, tensor_contents=None):\n",
      "   334|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   335|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__repr__, (self,), self,\n",
      "   336|         0|            0|            0|  0.00%|                                         tensor_contents=tensor_contents)\n",
      "   337|         0|            0|            0|  0.00%|        # All strings are unicode in Python 3.\n",
      "   338|         0|            0|            0|  0.00%|        return torch._tensor_str._str(self, tensor_contents=tensor_contents)\n",
      "   339|         0|            0|            0|  0.00%|\n",
      "   340|      6240|    0.0154917|  2.48265e-06|  0.00%|    def backward(self, gradient=None, retain_graph=None, create_graph=False, inputs=None):\n",
      "   341|         0|            0|            0|  0.00%|        r\"\"\"Computes the gradient of current tensor w.r.t. graph leaves.\n",
      "   342|         0|            0|            0|  0.00%|\n",
      "   343|         0|            0|            0|  0.00%|        The graph is differentiated using the chain rule. If the tensor is\n",
      "   344|         0|            0|            0|  0.00%|        non-scalar (i.e. its data has more than one element) and requires\n",
      "   345|         0|            0|            0|  0.00%|        gradient, the function additionally requires specifying ``gradient``.\n",
      "   346|         0|            0|            0|  0.00%|        It should be a tensor of matching type and location, that contains\n",
      "   347|         0|            0|            0|  0.00%|        the gradient of the differentiated function w.r.t. ``self``.\n",
      "   348|         0|            0|            0|  0.00%|\n",
      "   349|         0|            0|            0|  0.00%|        This function accumulates gradients in the leaves - you might need to zero\n",
      "   350|         0|            0|            0|  0.00%|        ``.grad`` attributes or set them to ``None`` before calling it.\n",
      "   351|         0|            0|            0|  0.00%|        See :ref:`Default gradient layouts<default-grad-layouts>`\n",
      "   352|         0|            0|            0|  0.00%|        for details on the memory layout of accumulated gradients.\n",
      "   353|         0|            0|            0|  0.00%|\n",
      "   354|         0|            0|            0|  0.00%|        .. note::\n",
      "   355|         0|            0|            0|  0.00%|\n",
      "   356|         0|            0|            0|  0.00%|            If you run any forward ops, create ``gradient``, and/or call ``backward``\n",
      "   357|         0|            0|            0|  0.00%|            in a user-specified CUDA stream context, see\n",
      "   358|         0|            0|            0|  0.00%|            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n",
      "   359|         0|            0|            0|  0.00%|\n",
      "   360|         0|            0|            0|  0.00%|        .. note::\n",
      "   361|         0|            0|            0|  0.00%|\n",
      "   362|         0|            0|            0|  0.00%|            When ``inputs`` are provided and a given input is not a leaf,\n",
      "   363|         0|            0|            0|  0.00%|            the current implementation will call its grad_fn (though it is not strictly needed to get this gradients).\n",
      "   364|         0|            0|            0|  0.00%|            It is an implementation detail on which the user should not rely.\n",
      "   365|         0|            0|            0|  0.00%|            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.\n",
      "   366|         0|            0|            0|  0.00%|\n",
      "   367|         0|            0|            0|  0.00%|        Args:\n",
      "   368|         0|            0|            0|  0.00%|            gradient (Tensor or None): Gradient w.r.t. the\n",
      "   369|         0|            0|            0|  0.00%|                tensor. If it is a tensor, it will be automatically converted\n",
      "   370|         0|            0|            0|  0.00%|                to a Tensor that does not require grad unless ``create_graph`` is True.\n",
      "   371|         0|            0|            0|  0.00%|                None values can be specified for scalar Tensors or ones that\n",
      "   372|         0|            0|            0|  0.00%|                don't require grad. If a None value would be acceptable then\n",
      "   373|         0|            0|            0|  0.00%|                this argument is optional.\n",
      "   374|         0|            0|            0|  0.00%|            retain_graph (bool, optional): If ``False``, the graph used to compute\n",
      "   375|         0|            0|            0|  0.00%|                the grads will be freed. Note that in nearly all cases setting\n",
      "   376|         0|            0|            0|  0.00%|                this option to True is not needed and often can be worked around\n",
      "   377|         0|            0|            0|  0.00%|                in a much more efficient way. Defaults to the value of\n",
      "   378|         0|            0|            0|  0.00%|                ``create_graph``.\n",
      "   379|         0|            0|            0|  0.00%|            create_graph (bool, optional): If ``True``, graph of the derivative will\n",
      "   380|         0|            0|            0|  0.00%|                be constructed, allowing to compute higher order derivative\n",
      "   381|         0|            0|            0|  0.00%|                products. Defaults to ``False``.\n",
      "   382|         0|            0|            0|  0.00%|            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be\n",
      "   383|         0|            0|            0|  0.00%|                accumulated into ``.grad``. All other Tensors will be ignored. If not\n",
      "   384|         0|            0|            0|  0.00%|                provided, the gradient is accumulated into all the leaf Tensors that were\n",
      "   385|         0|            0|            0|  0.00%|                used to compute the attr::tensors.\n",
      "   386|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   387|      6240|    0.0138292|  2.21622e-06|  0.00%|        if has_torch_function_unary(self):\n",
      "   388|         0|            0|            0|  0.00%|            return handle_torch_function(\n",
      "   389|         0|            0|            0|  0.00%|                Tensor.backward,\n",
      "   390|         0|            0|            0|  0.00%|                (self,),\n",
      "   391|         0|            0|            0|  0.00%|                self,\n",
      "   392|         0|            0|            0|  0.00%|                gradient=gradient,\n",
      "   393|         0|            0|            0|  0.00%|                retain_graph=retain_graph,\n",
      "   394|         0|            0|            0|  0.00%|                create_graph=create_graph,\n",
      "   395|         0|            0|            0|  0.00%|                inputs=inputs)\n",
      "   396|      6240|      0.12937|  2.07323e-05|  0.02%|        torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "(call)|      6240|      5.61016|  0.000899065|  0.90%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:85 backward\n",
      "   397|         0|            0|            0|  0.00%|\n",
      "   398|         0|            0|            0|  0.00%|    def register_hook(self, hook):\n",
      "   399|         0|            0|            0|  0.00%|        r\"\"\"Registers a backward hook.\n",
      "   400|         0|            0|            0|  0.00%|\n",
      "   401|         0|            0|            0|  0.00%|        The hook will be called every time a gradient with respect to the\n",
      "   402|         0|            0|            0|  0.00%|        Tensor is computed. The hook should have the following signature::\n",
      "   403|         0|            0|            0|  0.00%|\n",
      "   404|         0|            0|            0|  0.00%|            hook(grad) -> Tensor or None\n",
      "   405|         0|            0|            0|  0.00%|\n",
      "   406|         0|            0|            0|  0.00%|\n",
      "   407|         0|            0|            0|  0.00%|        The hook should not modify its argument, but it can optionally return\n",
      "   408|         0|            0|            0|  0.00%|        a new gradient which will be used in place of :attr:`grad`.\n",
      "   409|         0|            0|            0|  0.00%|\n",
      "   410|         0|            0|            0|  0.00%|        This function returns a handle with a method ``handle.remove()``\n",
      "   411|         0|            0|            0|  0.00%|        that removes the hook from the module.\n",
      "   412|         0|            0|            0|  0.00%|\n",
      "   413|         0|            0|            0|  0.00%|        Example::\n",
      "   414|         0|            0|            0|  0.00%|\n",
      "   415|         0|            0|            0|  0.00%|            >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
      "   416|         0|            0|            0|  0.00%|            >>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n",
      "   417|         0|            0|            0|  0.00%|            >>> v.backward(torch.tensor([1., 2., 3.]))\n",
      "   418|         0|            0|            0|  0.00%|            >>> v.grad\n",
      "   419|         0|            0|            0|  0.00%|\n",
      "   420|         0|            0|            0|  0.00%|             2\n",
      "   421|         0|            0|            0|  0.00%|             4\n",
      "   422|         0|            0|            0|  0.00%|             6\n",
      "   423|         0|            0|            0|  0.00%|            [torch.FloatTensor of size (3,)]\n",
      "   424|         0|            0|            0|  0.00%|\n",
      "   425|         0|            0|            0|  0.00%|            >>> h.remove()  # removes the hook\n",
      "   426|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   427|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   428|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.register_hook, (self,), self, hook)\n",
      "   429|         0|            0|            0|  0.00%|        if not self.requires_grad:\n",
      "   430|         0|            0|            0|  0.00%|            raise RuntimeError(\"cannot register a hook on a tensor that \"\n",
      "   431|         0|            0|            0|  0.00%|                               \"doesn't require gradient\")\n",
      "   432|         0|            0|            0|  0.00%|        if self._backward_hooks is None:\n",
      "   433|         0|            0|            0|  0.00%|            self._backward_hooks = OrderedDict()\n",
      "   434|         0|            0|            0|  0.00%|            if self.grad_fn is not None:\n",
      "   435|         0|            0|            0|  0.00%|                self.grad_fn._register_hook_dict(self)\n",
      "   436|         0|            0|            0|  0.00%|        handle = hooks.RemovableHandle(self._backward_hooks)\n",
      "   437|         0|            0|            0|  0.00%|        self._backward_hooks[handle.id] = hook\n",
      "   438|         0|            0|            0|  0.00%|        return handle\n",
      "   439|         0|            0|            0|  0.00%|\n",
      "   440|         0|            0|            0|  0.00%|    def reinforce(self, reward):\n",
      "   441|         0|            0|            0|  0.00%|        def trim(str):\n",
      "   442|         0|            0|            0|  0.00%|            return '\\n'.join([line.strip() for line in str.split('\\n')])\n",
      "   443|         0|            0|            0|  0.00%|\n",
      "   444|         0|            0|            0|  0.00%|        raise RuntimeError(trim(r\"\"\"reinforce() was removed.\n",
      "   445|         0|            0|            0|  0.00%|            Use torch.distributions instead.\n",
      "   446|         0|            0|            0|  0.00%|            See https://pytorch.org/docs/master/distributions.html\n",
      "   447|         0|            0|            0|  0.00%|\n",
      "   448|         0|            0|            0|  0.00%|            Instead of:\n",
      "   449|         0|            0|            0|  0.00%|\n",
      "   450|         0|            0|            0|  0.00%|            probs = policy_network(state)\n",
      "   451|         0|            0|            0|  0.00%|            action = probs.multinomial()\n",
      "   452|         0|            0|            0|  0.00%|            next_state, reward = env.step(action)\n",
      "   453|         0|            0|            0|  0.00%|            action.reinforce(reward)\n",
      "   454|         0|            0|            0|  0.00%|            action.backward()\n",
      "   455|         0|            0|            0|  0.00%|\n",
      "   456|         0|            0|            0|  0.00%|            Use:\n",
      "   457|         0|            0|            0|  0.00%|\n",
      "   458|         0|            0|            0|  0.00%|            probs = policy_network(state)\n",
      "   459|         0|            0|            0|  0.00%|            # NOTE: categorical is equivalent to what used to be called multinomial\n",
      "   460|         0|            0|            0|  0.00%|            m = torch.distributions.Categorical(probs)\n",
      "   461|         0|            0|            0|  0.00%|            action = m.sample()\n",
      "   462|         0|            0|            0|  0.00%|            next_state, reward = env.step(action)\n",
      "   463|         0|            0|            0|  0.00%|            loss = -m.log_prob(action) * reward\n",
      "   464|         0|            0|            0|  0.00%|            loss.backward()\n",
      "   465|         0|            0|            0|  0.00%|        \"\"\"))\n",
      "   466|         0|            0|            0|  0.00%|\n",
      "   467|         0|            0|            0|  0.00%|    detach = _C._add_docstr(_C._TensorBase.detach, r\"\"\"\n",
      "   468|         0|            0|            0|  0.00%|    Returns a new Tensor, detached from the current graph.\n",
      "   469|         0|            0|            0|  0.00%|\n",
      "   470|         0|            0|            0|  0.00%|    The result will never require gradient.\n",
      "   471|         0|            0|            0|  0.00%|\n",
      "   472|         0|            0|            0|  0.00%|    This method also affects forward mode AD gradients and the result will never\n",
      "   473|         0|            0|            0|  0.00%|    have forward mode AD gradients.\n",
      "   474|         0|            0|            0|  0.00%|\n",
      "   475|         0|            0|            0|  0.00%|    .. note::\n",
      "   476|         0|            0|            0|  0.00%|\n",
      "   477|         0|            0|            0|  0.00%|      Returned Tensor shares the same storage with the original one.\n",
      "   478|         0|            0|            0|  0.00%|      In-place modifications on either of them will be seen, and may trigger\n",
      "   479|         0|            0|            0|  0.00%|      errors in correctness checks.\n",
      "   480|         0|            0|            0|  0.00%|      IMPORTANT NOTE: Previously, in-place size / stride / storage changes\n",
      "   481|         0|            0|            0|  0.00%|      (such as `resize_` / `resize_as_` / `set_` / `transpose_`) to the returned tensor\n",
      "   482|         0|            0|            0|  0.00%|      also update the original tensor. Now, these in-place changes will not update the\n",
      "   483|         0|            0|            0|  0.00%|      original tensor anymore, and will instead trigger an error.\n",
      "   484|         0|            0|            0|  0.00%|      For sparse tensors:\n",
      "   485|         0|            0|            0|  0.00%|      In-place indices / values changes (such as `zero_` / `copy_` / `add_`) to the\n",
      "   486|         0|            0|            0|  0.00%|      returned tensor will not update the original tensor anymore, and will instead\n",
      "   487|         0|            0|            0|  0.00%|      trigger an error.\n",
      "   488|         0|            0|            0|  0.00%|    \"\"\")\n",
      "   489|         0|            0|            0|  0.00%|\n",
      "   490|         0|            0|            0|  0.00%|    detach_ = _C._add_docstr(_C._TensorBase.detach_, r\"\"\"\n",
      "   491|         0|            0|            0|  0.00%|    Detaches the Tensor from the graph that created it, making it a leaf.\n",
      "   492|         0|            0|            0|  0.00%|    Views cannot be detached in-place.\n",
      "   493|         0|            0|            0|  0.00%|\n",
      "   494|         0|            0|            0|  0.00%|    This method also affects forward mode AD gradients and the result will never\n",
      "   495|         0|            0|            0|  0.00%|    have forward mode AD gradients.\n",
      "   496|         0|            0|            0|  0.00%|    \"\"\")\n",
      "   497|         0|            0|            0|  0.00%|\n",
      "   498|         0|            0|            0|  0.00%|    def is_shared(self):\n",
      "   499|         0|            0|            0|  0.00%|        r\"\"\"Checks if tensor is in shared memory.\n",
      "   500|         0|            0|            0|  0.00%|\n",
      "   501|         0|            0|            0|  0.00%|        This is always ``True`` for CUDA tensors.\n",
      "   502|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   503|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   504|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.is_shared, (self,), self)\n",
      "   505|         0|            0|            0|  0.00%|        return self.storage().is_shared()\n",
      "   506|         0|            0|            0|  0.00%|\n",
      "   507|         0|            0|            0|  0.00%|    def share_memory_(self):\n",
      "   508|         0|            0|            0|  0.00%|        r\"\"\"Moves the underlying storage to shared memory.\n",
      "   509|         0|            0|            0|  0.00%|\n",
      "   510|         0|            0|            0|  0.00%|        This is a no-op if the underlying storage is already in shared memory\n",
      "   511|         0|            0|            0|  0.00%|        and for CUDA tensors. Tensors in shared memory cannot be resized.\n",
      "   512|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   513|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   514|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.share_memory_, (self,), self)\n",
      "   515|         0|            0|            0|  0.00%|        self.storage().share_memory_()\n",
      "   516|         0|            0|            0|  0.00%|        return self\n",
      "   517|         0|            0|            0|  0.00%|\n",
      "   518|         0|            0|            0|  0.00%|    def __reversed__(self):\n",
      "   519|         0|            0|            0|  0.00%|        r\"\"\"Reverses the tensor along dimension 0.\"\"\"\n",
      "   520|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   521|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__reversed__, (self,), self)\n",
      "   522|         0|            0|            0|  0.00%|        if self.dim() == 0:\n",
      "   523|         0|            0|            0|  0.00%|            return self\n",
      "   524|         0|            0|            0|  0.00%|        else:\n",
      "   525|         0|            0|            0|  0.00%|            return self.flip(0)\n",
      "   526|         0|            0|            0|  0.00%|\n",
      "   527|         0|            0|            0|  0.00%|    def norm(self, p=\"fro\", dim=None, keepdim=False, dtype=None):\n",
      "   528|         0|            0|            0|  0.00%|        r\"\"\"See :func:`torch.norm`\"\"\"\n",
      "   529|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   530|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.norm, (self,), self, p=p, dim=dim, keepdim=keepdim, dtype=dtype)\n",
      "   531|         0|            0|            0|  0.00%|        return torch.norm(self, p, dim, keepdim, dtype=dtype)\n",
      "   532|         0|            0|            0|  0.00%|\n",
      "   533|         0|            0|            0|  0.00%|    def solve(self, other):\n",
      "   534|         0|            0|            0|  0.00%|        from ._linalg_utils import solve\n",
      "   535|         0|            0|            0|  0.00%|        return solve(self, other)\n",
      "   536|         0|            0|            0|  0.00%|\n",
      "   537|         0|            0|            0|  0.00%|    def lu(self, pivot=True, get_infos=False):\n",
      "   538|         0|            0|            0|  0.00%|        r\"\"\"See :func:`torch.lu`\"\"\"\n",
      "   539|         0|            0|            0|  0.00%|        # If get_infos is True, then we don't need to check for errors and vice versa\n",
      "   540|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   541|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.lu, (self,), self, pivot=pivot, get_infos=get_infos)\n",
      "   542|         0|            0|            0|  0.00%|\n",
      "   543|         0|            0|            0|  0.00%|        LU, pivots, infos = torch._lu_with_info(self, pivot=pivot, check_errors=(not get_infos))\n",
      "   544|         0|            0|            0|  0.00%|        if get_infos:\n",
      "   545|         0|            0|            0|  0.00%|            return LU, pivots, infos\n",
      "   546|         0|            0|            0|  0.00%|        else:\n",
      "   547|         0|            0|            0|  0.00%|            return LU, pivots\n",
      "   548|         0|            0|            0|  0.00%|\n",
      "   549|         0|            0|            0|  0.00%|    def stft(self, n_fft: int, hop_length: Optional[int] = None,\n",
      "   550|         0|            0|            0|  0.00%|             win_length: Optional[int] = None, window: 'Optional[Tensor]' = None,\n",
      "   551|         0|            0|            0|  0.00%|             center: bool = True, pad_mode: str = 'reflect', normalized: bool = False,\n",
      "   552|         0|            0|            0|  0.00%|             onesided: Optional[bool] = None, return_complex: Optional[bool] = None):\n",
      "   553|         0|            0|            0|  0.00%|        r\"\"\"See :func:`torch.stft`\n",
      "   554|         0|            0|            0|  0.00%|\n",
      "   555|         0|            0|            0|  0.00%|        .. warning::\n",
      "   556|         0|            0|            0|  0.00%|          This function changed signature at version 0.4.1. Calling with\n",
      "   557|         0|            0|            0|  0.00%|          the previous signature may cause error or return incorrect result.\n",
      "   558|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   559|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   560|         0|            0|            0|  0.00%|            return handle_torch_function(\n",
      "   561|         0|            0|            0|  0.00%|                Tensor.stft, (self,), self, n_fft, hop_length=hop_length,\n",
      "   562|         0|            0|            0|  0.00%|                win_length=win_length, window=window, center=center, pad_mode=pad_mode, normalized=normalized,\n",
      "   563|         0|            0|            0|  0.00%|                onesided=onesided, return_complex=return_complex\n",
      "   564|         0|            0|            0|  0.00%|            )\n",
      "   565|         0|            0|            0|  0.00%|        return torch.stft(self, n_fft, hop_length, win_length, window, center,\n",
      "   566|         0|            0|            0|  0.00%|                          pad_mode, normalized, onesided, return_complex=return_complex)\n",
      "   567|         0|            0|            0|  0.00%|\n",
      "   568|         0|            0|            0|  0.00%|    def istft(self, n_fft: int, hop_length: Optional[int] = None,\n",
      "   569|         0|            0|            0|  0.00%|              win_length: Optional[int] = None, window: 'Optional[Tensor]' = None,\n",
      "   570|         0|            0|            0|  0.00%|              center: bool = True, normalized: bool = False,\n",
      "   571|         0|            0|            0|  0.00%|              onesided: Optional[bool] = None, length: Optional[int] = None,\n",
      "   572|         0|            0|            0|  0.00%|              return_complex: bool = False):\n",
      "   573|         0|            0|            0|  0.00%|        r\"\"\"See :func:`torch.istft`\"\"\"\n",
      "   574|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   575|         0|            0|            0|  0.00%|            return handle_torch_function(\n",
      "   576|         0|            0|            0|  0.00%|                Tensor.istft, (self,), self, n_fft, hop_length=hop_length, win_length=win_length,\n",
      "   577|         0|            0|            0|  0.00%|                window=window, center=center, normalized=normalized, onesided=onesided, length=length,\n",
      "   578|         0|            0|            0|  0.00%|                return_complex=return_complex\n",
      "   579|         0|            0|            0|  0.00%|            )\n",
      "   580|         0|            0|            0|  0.00%|        return torch.istft(self, n_fft, hop_length, win_length, window, center,\n",
      "   581|         0|            0|            0|  0.00%|                           normalized, onesided, length, return_complex=return_complex)\n",
      "   582|         0|            0|            0|  0.00%|\n",
      "   583|         0|            0|            0|  0.00%|    def resize(self, *sizes):\n",
      "   584|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   585|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.resize, (self,), self, *sizes)\n",
      "   586|         0|            0|            0|  0.00%|        warnings.warn(\"non-inplace resize is deprecated\")\n",
      "   587|         0|            0|            0|  0.00%|        from torch.autograd._functions import Resize\n",
      "   588|         0|            0|            0|  0.00%|        return Resize.apply(self, sizes)\n",
      "   589|         0|            0|            0|  0.00%|\n",
      "   590|         0|            0|            0|  0.00%|    def resize_as(self, tensor):\n",
      "   591|         0|            0|            0|  0.00%|        if has_torch_function_variadic(self, tensor):\n",
      "   592|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.resize_as, (self, tensor), self, tensor)\n",
      "   593|         0|            0|            0|  0.00%|        warnings.warn(\"non-inplace resize_as is deprecated\")\n",
      "   594|         0|            0|            0|  0.00%|        from torch.autograd._functions import Resize\n",
      "   595|         0|            0|            0|  0.00%|        return Resize.apply(self, tensor.size())\n",
      "   596|         0|            0|            0|  0.00%|\n",
      "   597|         0|            0|            0|  0.00%|    def split(self, split_size, dim=0):\n",
      "   598|         0|            0|            0|  0.00%|        r\"\"\"See :func:`torch.split`\n",
      "   599|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   600|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   601|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.split, (self,), self, split_size, dim=dim)\n",
      "   602|         0|            0|            0|  0.00%|        if isinstance(split_size, int):\n",
      "   603|         0|            0|            0|  0.00%|            return super(Tensor, self).split(split_size, dim)\n",
      "   604|         0|            0|            0|  0.00%|        elif isinstance(split_size, Tensor):\n",
      "   605|         0|            0|            0|  0.00%|            try:\n",
      "   606|         0|            0|            0|  0.00%|                split_size = int(split_size)\n",
      "   607|         0|            0|            0|  0.00%|                return super(Tensor, self).split(split_size, dim)\n",
      "   608|         0|            0|            0|  0.00%|            except ValueError:\n",
      "   609|         0|            0|            0|  0.00%|                return super(Tensor, self).split_with_sizes(split_size, dim)\n",
      "   610|         0|            0|            0|  0.00%|        else:\n",
      "   611|         0|            0|            0|  0.00%|            return super(Tensor, self).split_with_sizes(split_size, dim)\n",
      "   612|         0|            0|            0|  0.00%|\n",
      "   613|         0|            0|            0|  0.00%|    def unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None):\n",
      "   614|         0|            0|            0|  0.00%|        r\"\"\"Returns the unique elements of the input tensor.\n",
      "   615|         0|            0|            0|  0.00%|\n",
      "   616|         0|            0|            0|  0.00%|        See :func:`torch.unique`\n",
      "   617|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   618|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   619|         0|            0|            0|  0.00%|            return handle_torch_function(\n",
      "   620|         0|            0|            0|  0.00%|                Tensor.unique, (self,), self, sorted=sorted, return_inverse=return_inverse,\n",
      "   621|         0|            0|            0|  0.00%|                return_counts=return_counts, dim=dim\n",
      "   622|         0|            0|            0|  0.00%|            )\n",
      "   623|         0|            0|            0|  0.00%|        return torch.unique(self, sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim)\n",
      "   624|         0|            0|            0|  0.00%|\n",
      "   625|         0|            0|            0|  0.00%|    def unique_consecutive(self, return_inverse=False, return_counts=False, dim=None):\n",
      "   626|         0|            0|            0|  0.00%|        r\"\"\"Eliminates all but the first element from every consecutive group of equivalent elements.\n",
      "   627|         0|            0|            0|  0.00%|\n",
      "   628|         0|            0|            0|  0.00%|        See :func:`torch.unique_consecutive`\n",
      "   629|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   630|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   631|         0|            0|            0|  0.00%|            return handle_torch_function(\n",
      "   632|         0|            0|            0|  0.00%|                Tensor.unique_consecutive, (self,), self, return_inverse=return_inverse,\n",
      "   633|         0|            0|            0|  0.00%|                return_counts=return_counts, dim=dim\n",
      "   634|         0|            0|            0|  0.00%|            )\n",
      "   635|         0|            0|            0|  0.00%|        return torch.unique_consecutive(self, return_inverse=return_inverse, return_counts=return_counts, dim=dim)\n",
      "   636|         0|            0|            0|  0.00%|\n",
      "   637|     24960|    0.0307291|  1.23113e-06|  0.00%|    @_handle_torch_function_and_wrap_type_error_to_not_implemented\n",
      "   638|         0|            0|            0|  0.00%|    def __rsub__(self, other):\n",
      "   639|     24960|     0.227567|  9.11728e-06|  0.04%|        return _C._VariableFunctions.rsub(self, other)\n",
      "   640|         0|            0|            0|  0.00%|\n",
      "   641|      6240|   0.00877094|   1.4056e-06|  0.00%|    @_handle_torch_function_and_wrap_type_error_to_not_implemented\n",
      "   642|         0|            0|            0|  0.00%|    def __rdiv__(self, other):\n",
      "   643|      6240|    0.0933702|  1.49632e-05|  0.02%|        return self.reciprocal() * other\n",
      "   644|         0|            0|            0|  0.00%|\n",
      "   645|         0|            0|            0|  0.00%|    __rtruediv__ = __rdiv__\n",
      "   646|         0|            0|            0|  0.00%|    __itruediv__ = _C._TensorBase.__idiv__\n",
      "   647|         0|            0|            0|  0.00%|\n",
      "   648|         0|            0|            0|  0.00%|    __pow__ = _handle_torch_function_and_wrap_type_error_to_not_implemented(_C._TensorBase.pow)\n",
      "   649|         0|            0|            0|  0.00%|    __ipow__ = _handle_torch_function_and_wrap_type_error_to_not_implemented(_C._TensorBase.pow_)\n",
      "   650|         0|            0|            0|  0.00%|\n",
      "   651|         0|            0|            0|  0.00%|    @_handle_torch_function_and_wrap_type_error_to_not_implemented\n",
      "   652|         0|            0|            0|  0.00%|    def __rmod__(self, other):\n",
      "   653|         0|            0|            0|  0.00%|        return torch.remainder(other, self)\n",
      "   654|         0|            0|            0|  0.00%|\n",
      "   655|         0|            0|            0|  0.00%|    def __format__(self, format_spec):\n",
      "   656|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   657|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__format__, (self,), self, format_spec)\n",
      "   658|         0|            0|            0|  0.00%|        if self.dim() == 0 and not self.is_meta:\n",
      "   659|         0|            0|            0|  0.00%|            return self.item().__format__(format_spec)\n",
      "   660|         0|            0|            0|  0.00%|        return object.__format__(self, format_spec)\n",
      "   661|         0|            0|            0|  0.00%|\n",
      "   662|         0|            0|            0|  0.00%|    @_handle_torch_function_and_wrap_type_error_to_not_implemented\n",
      "   663|         0|            0|            0|  0.00%|    def __rpow__(self, other):\n",
      "   664|         0|            0|            0|  0.00%|        dtype = torch.result_type(other, self)\n",
      "   665|         0|            0|            0|  0.00%|        return torch.tensor(other, dtype=dtype, device=self.device) ** self\n",
      "   666|         0|            0|            0|  0.00%|\n",
      "   667|         0|            0|            0|  0.00%|    @_handle_torch_function_and_wrap_type_error_to_not_implemented\n",
      "   668|         0|            0|            0|  0.00%|    def __floordiv__(self, other):\n",
      "   669|         0|            0|            0|  0.00%|        warnings.warn(\"__floordiv__ is deprecated, and its behavior will change in a future version of pytorch. \"\n",
      "   670|         0|            0|            0|  0.00%|                      \"It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). \"\n",
      "   671|         0|            0|            0|  0.00%|                      \"This results in incorrect rounding for negative values. \"\n",
      "   672|         0|            0|            0|  0.00%|                      \"To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), \"\n",
      "   673|         0|            0|            0|  0.00%|                      \"or for actual floor division, use torch.div(a, b, rounding_mode='floor').\", stacklevel=3)\n",
      "   674|         0|            0|            0|  0.00%|        return torch.div(self, other, rounding_mode='trunc')\n",
      "   675|         0|            0|            0|  0.00%|\n",
      "   676|         0|            0|            0|  0.00%|    @_handle_torch_function_and_wrap_type_error_to_not_implemented\n",
      "   677|         0|            0|            0|  0.00%|    def __rfloordiv__(self, other):\n",
      "   678|         0|            0|            0|  0.00%|        warnings.warn(\"__rfloordiv__ is deprecated, and its behavior will change in a future version of pytorch. \"\n",
      "   679|         0|            0|            0|  0.00%|                      \"It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). \"\n",
      "   680|         0|            0|            0|  0.00%|                      \"This results in incorrect rounding for negative values. \"\n",
      "   681|         0|            0|            0|  0.00%|                      \"To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), \"\n",
      "   682|         0|            0|            0|  0.00%|                      \"or for actual floor division, use torch.div(a, b, rounding_mode='floor').\", stacklevel=3)\n",
      "   683|         0|            0|            0|  0.00%|        return torch.div(other, self, rounding_mode='trunc')\n",
      "   684|         0|            0|            0|  0.00%|\n",
      "   685|         0|            0|            0|  0.00%|    @_handle_torch_function_and_wrap_type_error_to_not_implemented\n",
      "   686|         0|            0|            0|  0.00%|    def __rlshift__(self, other):\n",
      "   687|         0|            0|            0|  0.00%|        return torch.bitwise_left_shift(other, self)\n",
      "   688|         0|            0|            0|  0.00%|\n",
      "   689|         0|            0|            0|  0.00%|    @_handle_torch_function_and_wrap_type_error_to_not_implemented\n",
      "   690|         0|            0|            0|  0.00%|    def __rrshift__(self, other):\n",
      "   691|         0|            0|            0|  0.00%|        return torch.bitwise_right_shift(other, self)\n",
      "   692|         0|            0|            0|  0.00%|\n",
      "   693|         0|            0|            0|  0.00%|    @_handle_torch_function_and_wrap_type_error_to_not_implemented\n",
      "   694|         0|            0|            0|  0.00%|    def __rmatmul__(self, other):\n",
      "   695|         0|            0|            0|  0.00%|        return torch.matmul(other, self)\n",
      "   696|         0|            0|            0|  0.00%|\n",
      "   697|         0|            0|            0|  0.00%|    __pos__ = _C._TensorBase.positive\n",
      "   698|         0|            0|            0|  0.00%|    __neg__ = _C._TensorBase.neg\n",
      "   699|         0|            0|            0|  0.00%|    __abs__ = _C._TensorBase.abs\n",
      "   700|         0|            0|            0|  0.00%|\n",
      "   701|         0|            0|            0|  0.00%|    def __len__(self):\n",
      "   702|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   703|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__len__, (self,), self)\n",
      "   704|         0|            0|            0|  0.00%|        if self.dim() == 0:\n",
      "   705|         0|            0|            0|  0.00%|            raise TypeError(\"len() of a 0-d tensor\")\n",
      "   706|         0|            0|            0|  0.00%|        if torch._C._get_tracing_state():\n",
      "   707|         0|            0|            0|  0.00%|            warnings.warn('Using len to get tensor shape might cause the trace to be incorrect. '\n",
      "   708|         0|            0|            0|  0.00%|                          'Recommended usage would be tensor.shape[0]. '\n",
      "   709|         0|            0|            0|  0.00%|                          'Passing a tensor of different shape might lead to errors or silently give '\n",
      "   710|         0|            0|            0|  0.00%|                          'incorrect results.', category=torch.jit.TracerWarning, stacklevel=2)\n",
      "   711|         0|            0|            0|  0.00%|        return self.shape[0]\n",
      "   712|         0|            0|            0|  0.00%|\n",
      "   713|     49920|     0.133488|  2.67405e-06|  0.02%|    def __iter__(self):\n",
      "   714|         0|            0|            0|  0.00%|        # NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\n",
      "   715|         0|            0|            0|  0.00%|        # generator and don't eagerly perform all the indexes.  This could\n",
      "   716|         0|            0|            0|  0.00%|        # save us work, and also helps keep trace ordering deterministic\n",
      "   717|         0|            0|            0|  0.00%|        # (e.g., if you zip(*hiddens), the eager map will force all the\n",
      "   718|         0|            0|            0|  0.00%|        # indexes of hiddens[0] before hiddens[1], while the generator\n",
      "   719|         0|            0|            0|  0.00%|        # map will interleave them.)\n",
      "   720|         0|            0|            0|  0.00%|        # NB: We have intentionally skipped __torch_function__ dispatch here.\n",
      "   721|         0|            0|            0|  0.00%|        # See gh-54457\n",
      "   722|     49920|     0.121713|  2.43815e-06|  0.02%|        if self.dim() == 0:\n",
      "   723|         0|            0|            0|  0.00%|            raise TypeError('iteration over a 0-d tensor')\n",
      "   724|     49920|     0.127294|  2.54996e-06|  0.02%|        if torch._C._get_tracing_state():\n",
      "   725|         0|            0|            0|  0.00%|            warnings.warn('Iterating over a tensor might cause the trace to be incorrect. '\n",
      "   726|         0|            0|            0|  0.00%|                          'Passing a tensor of different shape won\\'t change the number of '\n",
      "   727|         0|            0|            0|  0.00%|                          'iterations executed (and might lead to errors or silently give '\n",
      "   728|         0|            0|            0|  0.00%|                          'incorrect results).', category=torch.jit.TracerWarning, stacklevel=2)\n",
      "   729|     49920|      0.41697|  8.35277e-06|  0.07%|        return iter(self.unbind(0))\n",
      "   730|         0|            0|            0|  0.00%|\n",
      "   731|    224640|     0.285309|  1.27007e-06|  0.05%|    def __hash__(self):\n",
      "   732|    224640|     0.363783|   1.6194e-06|  0.06%|        if has_torch_function_unary(self):\n",
      "   733|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__hash__, (self,), self)\n",
      "   734|    224640|     0.342812|  1.52605e-06|  0.06%|        return id(self)\n",
      "   735|         0|            0|            0|  0.00%|\n",
      "   736|         0|            0|            0|  0.00%|    def __dir__(self):\n",
      "   737|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   738|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__dir__, (self,), self)\n",
      "   739|         0|            0|            0|  0.00%|        tensor_methods = dir(self.__class__)\n",
      "   740|         0|            0|            0|  0.00%|        tensor_methods.remove('volatile')  # deprecated\n",
      "   741|         0|            0|            0|  0.00%|        attrs = list(self.__dict__.keys())\n",
      "   742|         0|            0|            0|  0.00%|        keys = tensor_methods + attrs\n",
      "   743|         0|            0|            0|  0.00%|\n",
      "   744|         0|            0|            0|  0.00%|        # property only available dense, cuda tensors\n",
      "   745|         0|            0|            0|  0.00%|        if (not self.is_cuda) or self.is_sparse:\n",
      "   746|         0|            0|            0|  0.00%|            keys.remove(\"__cuda_array_interface__\")\n",
      "   747|         0|            0|            0|  0.00%|\n",
      "   748|         0|            0|            0|  0.00%|        return sorted(keys)\n",
      "   749|         0|            0|            0|  0.00%|\n",
      "   750|         0|            0|            0|  0.00%|    # Numpy array interface, to support `numpy.asarray(tensor) -> ndarray`\n",
      "   751|         0|            0|            0|  0.00%|    __array_priority__ = 1000    # prefer Tensor ops over numpy ones\n",
      "   752|         0|            0|            0|  0.00%|\n",
      "   753|         0|            0|            0|  0.00%|    def __array__(self, dtype=None):\n",
      "   754|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   755|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__array__, (self,), self, dtype=dtype)\n",
      "   756|         0|            0|            0|  0.00%|        if dtype is None:\n",
      "   757|         0|            0|            0|  0.00%|            return self.numpy()\n",
      "   758|         0|            0|            0|  0.00%|        else:\n",
      "   759|         0|            0|            0|  0.00%|            return self.numpy().astype(dtype, copy=False)\n",
      "   760|         0|            0|            0|  0.00%|\n",
      "   761|         0|            0|            0|  0.00%|    # Wrap Numpy array again in a suitable tensor when done, to support e.g.\n",
      "   762|         0|            0|            0|  0.00%|    # `numpy.sin(tensor) -> tensor` or `numpy.greater(tensor, 0) -> ByteTensor`\n",
      "   763|         0|            0|            0|  0.00%|    def __array_wrap__(self, array):\n",
      "   764|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   765|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__array_wrap__, (self,), self, array=array)\n",
      "   766|         0|            0|            0|  0.00%|        if array.dtype == bool:\n",
      "   767|         0|            0|            0|  0.00%|            # Workaround, torch has no built-in bool tensor\n",
      "   768|         0|            0|            0|  0.00%|            array = array.astype('uint8')\n",
      "   769|         0|            0|            0|  0.00%|        return torch.from_numpy(array)\n",
      "   770|         0|            0|            0|  0.00%|\n",
      "   771|         0|            0|            0|  0.00%|    def __contains__(self, element):\n",
      "   772|         0|            0|            0|  0.00%|        r\"\"\"Check if `element` is present in tensor\n",
      "   773|         0|            0|            0|  0.00%|\n",
      "   774|         0|            0|            0|  0.00%|        Args:\n",
      "   775|         0|            0|            0|  0.00%|            element (Tensor or scalar): element to be checked\n",
      "   776|         0|            0|            0|  0.00%|                for presence in current tensor\"\n",
      "   777|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   778|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   779|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__contains__, (self,), self, element)\n",
      "   780|         0|            0|            0|  0.00%|        if isinstance(element, (torch.Tensor, Number)):\n",
      "   781|         0|            0|            0|  0.00%|            # type hint doesn't understand the __contains__ result array\n",
      "   782|         0|            0|            0|  0.00%|            return (element == self).any().item()  # type: ignore[union-attr]\n",
      "   783|         0|            0|            0|  0.00%|\n",
      "   784|         0|            0|            0|  0.00%|        raise RuntimeError(\n",
      "   785|         0|            0|            0|  0.00%|            \"Tensor.__contains__ only supports Tensor or scalar, but you passed in a %s.\" %\n",
      "   786|         0|            0|            0|  0.00%|            type(element)\n",
      "   787|         0|            0|            0|  0.00%|        )\n",
      "   788|         0|            0|            0|  0.00%|\n",
      "   789|         0|            0|            0|  0.00%|    @property\n",
      "   790|         0|            0|            0|  0.00%|    def __cuda_array_interface__(self):\n",
      "   791|         0|            0|            0|  0.00%|        \"\"\"Array view description for cuda tensors.\n",
      "   792|         0|            0|            0|  0.00%|\n",
      "   793|         0|            0|            0|  0.00%|        See:\n",
      "   794|         0|            0|            0|  0.00%|        https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html\n",
      "   795|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   796|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   797|         0|            0|            0|  0.00%|            # TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185\n",
      "   798|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__cuda_array_interface__.__get__, (self,), self)  # type: ignore[attr-defined]\n",
      "   799|         0|            0|            0|  0.00%|\n",
      "   800|         0|            0|            0|  0.00%|        # raise AttributeError for unsupported tensors, so that\n",
      "   801|         0|            0|            0|  0.00%|        # hasattr(cpu_tensor, \"__cuda_array_interface__\") is False.\n",
      "   802|         0|            0|            0|  0.00%|        if not self.is_cuda:\n",
      "   803|         0|            0|            0|  0.00%|            raise AttributeError(\n",
      "   804|         0|            0|            0|  0.00%|                \"Can't get __cuda_array_interface__ on non-CUDA tensor type: %s \"\n",
      "   805|         0|            0|            0|  0.00%|                \"If CUDA data is required use tensor.cuda() to copy tensor to device memory.\" %\n",
      "   806|         0|            0|            0|  0.00%|                self.type()\n",
      "   807|         0|            0|            0|  0.00%|            )\n",
      "   808|         0|            0|            0|  0.00%|\n",
      "   809|         0|            0|            0|  0.00%|        if self.is_sparse:\n",
      "   810|         0|            0|            0|  0.00%|            raise AttributeError(\n",
      "   811|         0|            0|            0|  0.00%|                \"Can't get __cuda_array_interface__ on sparse type: %s \"\n",
      "   812|         0|            0|            0|  0.00%|                \"Use Tensor.to_dense() to convert to a dense tensor first.\" %\n",
      "   813|         0|            0|            0|  0.00%|                self.type()\n",
      "   814|         0|            0|            0|  0.00%|            )\n",
      "   815|         0|            0|            0|  0.00%|\n",
      "   816|         0|            0|            0|  0.00%|        # RuntimeError, matching tensor.__array__() behavior.\n",
      "   817|         0|            0|            0|  0.00%|        if self.requires_grad:\n",
      "   818|         0|            0|            0|  0.00%|            raise RuntimeError(\n",
      "   819|         0|            0|            0|  0.00%|                \"Can't get __cuda_array_interface__ on Variable that requires grad. \"\n",
      "   820|         0|            0|            0|  0.00%|                \"If gradients aren't required, use var.detach() to get Variable that doesn't require grad.\"\n",
      "   821|         0|            0|            0|  0.00%|            )\n",
      "   822|         0|            0|            0|  0.00%|\n",
      "   823|         0|            0|            0|  0.00%|        # CUDA devices are little-endian and tensors are stored in native byte\n",
      "   824|         0|            0|            0|  0.00%|        # order. 1-byte entries are endian-agnostic.\n",
      "   825|         0|            0|            0|  0.00%|        typestr = {\n",
      "   826|         0|            0|            0|  0.00%|            torch.complex64: \"<c8\",\n",
      "   827|         0|            0|            0|  0.00%|            torch.complex128: \"<c16\",\n",
      "   828|         0|            0|            0|  0.00%|            torch.float16: \"<f2\",\n",
      "   829|         0|            0|            0|  0.00%|            torch.float32: \"<f4\",\n",
      "   830|         0|            0|            0|  0.00%|            torch.float64: \"<f8\",\n",
      "   831|         0|            0|            0|  0.00%|            torch.uint8: \"|u1\",\n",
      "   832|         0|            0|            0|  0.00%|            torch.int8: \"|i1\",\n",
      "   833|         0|            0|            0|  0.00%|            torch.int16: \"<i2\",\n",
      "   834|         0|            0|            0|  0.00%|            torch.int32: \"<i4\",\n",
      "   835|         0|            0|            0|  0.00%|            torch.int64: \"<i8\",\n",
      "   836|         0|            0|            0|  0.00%|        }[self.dtype]\n",
      "   837|         0|            0|            0|  0.00%|\n",
      "   838|         0|            0|            0|  0.00%|        itemsize = self.storage().element_size()\n",
      "   839|         0|            0|            0|  0.00%|\n",
      "   840|         0|            0|            0|  0.00%|        shape = tuple(self.shape)\n",
      "   841|         0|            0|            0|  0.00%|        if self.is_contiguous():\n",
      "   842|         0|            0|            0|  0.00%|            # __cuda_array_interface__ v2 requires the strides to be omitted\n",
      "   843|         0|            0|            0|  0.00%|            # (either not set or set to None) for C-contiguous arrays.\n",
      "   844|         0|            0|            0|  0.00%|            strides = None\n",
      "   845|         0|            0|            0|  0.00%|        else:\n",
      "   846|         0|            0|            0|  0.00%|            strides = tuple(s * itemsize for s in self.stride())\n",
      "   847|         0|            0|            0|  0.00%|        data_ptr = self.data_ptr() if self.numel() > 0 else 0\n",
      "   848|         0|            0|            0|  0.00%|        data = (data_ptr, False)  # read-only is false\n",
      "   849|         0|            0|            0|  0.00%|\n",
      "   850|         0|            0|            0|  0.00%|        return dict(typestr=typestr, shape=shape, strides=strides, data=data, version=2)\n",
      "   851|         0|            0|            0|  0.00%|\n",
      "   852|         0|            0|            0|  0.00%|    def storage_type(self):\n",
      "   853|         0|            0|            0|  0.00%|        r\"\"\"storage_type() -> type\n",
      "   854|         0|            0|            0|  0.00%|\n",
      "   855|         0|            0|            0|  0.00%|        Returns the type of the underlying storage.\n",
      "   856|         0|            0|            0|  0.00%|\n",
      "   857|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   858|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   859|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.storage_type, (self,), self)\n",
      "   860|         0|            0|            0|  0.00%|\n",
      "   861|         0|            0|            0|  0.00%|        return self.storage()._get_legacy_storage_class()\n",
      "   862|         0|            0|            0|  0.00%|\n",
      "   863|         0|            0|            0|  0.00%|    def refine_names(self, *names):\n",
      "   864|         0|            0|            0|  0.00%|        r\"\"\"Refines the dimension names of :attr:`self` according to :attr:`names`.\n",
      "   865|         0|            0|            0|  0.00%|\n",
      "   866|         0|            0|            0|  0.00%|        Refining is a special case of renaming that \"lifts\" unnamed dimensions.\n",
      "   867|         0|            0|            0|  0.00%|        A ``None`` dim can be refined to have any name; a named dim can only be\n",
      "   868|         0|            0|            0|  0.00%|        refined to have the same name.\n",
      "   869|         0|            0|            0|  0.00%|\n",
      "   870|         0|            0|            0|  0.00%|        Because named tensors can coexist with unnamed tensors, refining names\n",
      "   871|         0|            0|            0|  0.00%|        gives a nice way to write named-tensor-aware code that works with both\n",
      "   872|         0|            0|            0|  0.00%|        named and unnamed tensors.\n",
      "   873|         0|            0|            0|  0.00%|\n",
      "   874|         0|            0|            0|  0.00%|        :attr:`names` may contain up to one Ellipsis (``...``).\n",
      "   875|         0|            0|            0|  0.00%|        The Ellipsis is expanded greedily; it is expanded in-place to fill\n",
      "   876|         0|            0|            0|  0.00%|        :attr:`names` to the same length as ``self.dim()`` using names from the\n",
      "   877|         0|            0|            0|  0.00%|        corresponding indices of ``self.names``.\n",
      "   878|         0|            0|            0|  0.00%|\n",
      "   879|         0|            0|            0|  0.00%|        Python 2 does not support Ellipsis but one may use a string literal\n",
      "   880|         0|            0|            0|  0.00%|        instead (``'...'``).\n",
      "   881|         0|            0|            0|  0.00%|\n",
      "   882|         0|            0|            0|  0.00%|        Args:\n",
      "   883|         0|            0|            0|  0.00%|            names (iterable of str): The desired names of the output tensor. May\n",
      "   884|         0|            0|            0|  0.00%|                contain up to one Ellipsis.\n",
      "   885|         0|            0|            0|  0.00%|\n",
      "   886|         0|            0|            0|  0.00%|        Examples::\n",
      "   887|         0|            0|            0|  0.00%|\n",
      "   888|         0|            0|            0|  0.00%|            >>> imgs = torch.randn(32, 3, 128, 128)\n",
      "   889|         0|            0|            0|  0.00%|            >>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n",
      "   890|         0|            0|            0|  0.00%|            >>> named_imgs.names\n",
      "   891|         0|            0|            0|  0.00%|            ('N', 'C', 'H', 'W')\n",
      "   892|         0|            0|            0|  0.00%|\n",
      "   893|         0|            0|            0|  0.00%|            >>> tensor = torch.randn(2, 3, 5, 7, 11)\n",
      "   894|         0|            0|            0|  0.00%|            >>> tensor = tensor.refine_names('A', ..., 'B', 'C')\n",
      "   895|         0|            0|            0|  0.00%|            >>> tensor.names\n",
      "   896|         0|            0|            0|  0.00%|            ('A', None, None, 'B', 'C')\n",
      "   897|         0|            0|            0|  0.00%|\n",
      "   898|         0|            0|            0|  0.00%|        .. warning::\n",
      "   899|         0|            0|            0|  0.00%|            The named tensor API is experimental and subject to change.\n",
      "   900|         0|            0|            0|  0.00%|\n",
      "   901|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   902|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   903|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.refine_names, (self,), self, *names)\n",
      "   904|         0|            0|            0|  0.00%|        names = resolve_ellipsis(names, self.names, 'refine_names')\n",
      "   905|         0|            0|            0|  0.00%|        return super(Tensor, self).refine_names(names)\n",
      "   906|         0|            0|            0|  0.00%|\n",
      "   907|         0|            0|            0|  0.00%|    def align_to(self, *names):\n",
      "   908|         0|            0|            0|  0.00%|        r\"\"\"Permutes the dimensions of the :attr:`self` tensor to match the order\n",
      "   909|         0|            0|            0|  0.00%|        specified in :attr:`names`, adding size-one dims for any new names.\n",
      "   910|         0|            0|            0|  0.00%|\n",
      "   911|         0|            0|            0|  0.00%|        All of the dims of :attr:`self` must be named in order to use this method.\n",
      "   912|         0|            0|            0|  0.00%|        The resulting tensor is a view on the original tensor.\n",
      "   913|         0|            0|            0|  0.00%|\n",
      "   914|         0|            0|            0|  0.00%|        All dimension names of :attr:`self` must be present in :attr:`names`.\n",
      "   915|         0|            0|            0|  0.00%|        :attr:`names` may contain additional names that are not in ``self.names``;\n",
      "   916|         0|            0|            0|  0.00%|        the output tensor has a size-one dimension for each of those new names.\n",
      "   917|         0|            0|            0|  0.00%|\n",
      "   918|         0|            0|            0|  0.00%|        :attr:`names` may contain up to one Ellipsis (``...``).\n",
      "   919|         0|            0|            0|  0.00%|        The Ellipsis is expanded to be equal to all dimension names of :attr:`self`\n",
      "   920|         0|            0|            0|  0.00%|        that are not mentioned in :attr:`names`, in the order that they appear\n",
      "   921|         0|            0|            0|  0.00%|        in :attr:`self`.\n",
      "   922|         0|            0|            0|  0.00%|\n",
      "   923|         0|            0|            0|  0.00%|        Python 2 does not support Ellipsis but one may use a string literal\n",
      "   924|         0|            0|            0|  0.00%|        instead (``'...'``).\n",
      "   925|         0|            0|            0|  0.00%|\n",
      "   926|         0|            0|            0|  0.00%|        Args:\n",
      "   927|         0|            0|            0|  0.00%|            names (iterable of str): The desired dimension ordering of the\n",
      "   928|         0|            0|            0|  0.00%|                output tensor. May contain up to one Ellipsis that is expanded\n",
      "   929|         0|            0|            0|  0.00%|                to all unmentioned dim names of :attr:`self`.\n",
      "   930|         0|            0|            0|  0.00%|\n",
      "   931|         0|            0|            0|  0.00%|        Examples::\n",
      "   932|         0|            0|            0|  0.00%|\n",
      "   933|         0|            0|            0|  0.00%|            >>> tensor = torch.randn(2, 2, 2, 2, 2, 2)\n",
      "   934|         0|            0|            0|  0.00%|            >>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n",
      "   935|         0|            0|            0|  0.00%|\n",
      "   936|         0|            0|            0|  0.00%|            # Move the F and E dims to the front while keeping the rest in order\n",
      "   937|         0|            0|            0|  0.00%|            >>> named_tensor.align_to('F', 'E', ...)\n",
      "   938|         0|            0|            0|  0.00%|\n",
      "   939|         0|            0|            0|  0.00%|        .. warning::\n",
      "   940|         0|            0|            0|  0.00%|            The named tensor API is experimental and subject to change.\n",
      "   941|         0|            0|            0|  0.00%|\n",
      "   942|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   943|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   944|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.align_to, (self,), self, *names)\n",
      "   945|         0|            0|            0|  0.00%|        ellipsis_idx = single_ellipsis_index(names, 'align_to')\n",
      "   946|         0|            0|            0|  0.00%|        if ellipsis_idx is None:\n",
      "   947|         0|            0|            0|  0.00%|            return super(Tensor, self).align_to(names)\n",
      "   948|         0|            0|            0|  0.00%|        return super(Tensor, self).align_to(\n",
      "   949|         0|            0|            0|  0.00%|            [name for name in names if not is_ellipsis(name)],\n",
      "   950|         0|            0|            0|  0.00%|            ellipsis_idx)\n",
      "   951|         0|            0|            0|  0.00%|\n",
      "   952|         0|            0|            0|  0.00%|    def unflatten(self, dim, sizes):\n",
      "   953|         0|            0|            0|  0.00%|        r\"\"\"Expands the dimension :attr:`dim` of the :attr:`self` tensor over multiple dimensions\n",
      "   954|         0|            0|            0|  0.00%|        of sizes given by :attr:`sizes`.\n",
      "   955|         0|            0|            0|  0.00%|\n",
      "   956|         0|            0|            0|  0.00%|        * :attr:`sizes` is the new shape of the unflattened dimension and it can be a `Tuple[int]` as well\n",
      "   957|         0|            0|            0|  0.00%|          as `torch.Size` if :attr:`self` is a `Tensor`, or `namedshape` (Tuple[(name: str, size: int)])\n",
      "   958|         0|            0|            0|  0.00%|          if :attr:`self` is a `NamedTensor`. The total number of elements in sizes must match the number\n",
      "   959|         0|            0|            0|  0.00%|          of elements in the original dim being unflattened.\n",
      "   960|         0|            0|            0|  0.00%|\n",
      "   961|         0|            0|            0|  0.00%|        Args:\n",
      "   962|         0|            0|            0|  0.00%|            dim (Union[int, str]): Dimension to unflatten\n",
      "   963|         0|            0|            0|  0.00%|            sizes (Union[Tuple[int] or torch.Size, Tuple[Tuple[str, int]]]): New shape of the unflattened dimension\n",
      "   964|         0|            0|            0|  0.00%|\n",
      "   965|         0|            0|            0|  0.00%|        Examples:\n",
      "   966|         0|            0|            0|  0.00%|            >>> torch.randn(3, 4, 1).unflatten(1, (2, 2)).shape\n",
      "   967|         0|            0|            0|  0.00%|            torch.Size([3, 2, 2, 1])\n",
      "   968|         0|            0|            0|  0.00%|            >>> torch.randn(3, 4, 1).unflatten(1, (-1, 2)).shape # the size -1 is inferred from the size of dimension 1\n",
      "   969|         0|            0|            0|  0.00%|            torch.Size([3, 2, 2, 1])\n",
      "   970|         0|            0|            0|  0.00%|            >>> torch.randn(2, 4, names=('A', 'B')).unflatten('B', (('B1', 2), ('B2', 2)))\n",
      "   971|         0|            0|            0|  0.00%|            tensor([[[-1.1772,  0.0180],\n",
      "   972|         0|            0|            0|  0.00%|                    [ 0.2412,  0.1431]],\n",
      "   973|         0|            0|            0|  0.00%|                    [[-1.1819, -0.8899],\n",
      "   974|         0|            0|            0|  0.00%|                    [ 1.5813,  0.2274]]], names=('A', 'B1', 'B2'))\n",
      "   975|         0|            0|            0|  0.00%|            >>> torch.randn(2, names=('A',)).unflatten('A', (('B1', -1), ('B2', 1)))\n",
      "   976|         0|            0|            0|  0.00%|            tensor([[-0.8591],\n",
      "   977|         0|            0|            0|  0.00%|                    [ 0.3100]], names=('B1', 'B2'))\n",
      "   978|         0|            0|            0|  0.00%|\n",
      "   979|         0|            0|            0|  0.00%|        .. warning::\n",
      "   980|         0|            0|            0|  0.00%|            The named tensor API is experimental and subject to change.\n",
      "   981|         0|            0|            0|  0.00%|\n",
      "   982|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   983|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   984|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.unflatten, (self,), self, dim, sizes)\n",
      "   985|         0|            0|            0|  0.00%|\n",
      "   986|         0|            0|            0|  0.00%|        if not sizes:\n",
      "   987|         0|            0|            0|  0.00%|            raise RuntimeError(\"unflatten: sizes must be non-empty\")\n",
      "   988|         0|            0|            0|  0.00%|\n",
      "   989|         0|            0|            0|  0.00%|        names = None\n",
      "   990|         0|            0|            0|  0.00%|        if isinstance(sizes, OrderedDict) or (isinstance(sizes, (tuple, list)) and isinstance(sizes[0], (tuple, list))):\n",
      "   991|         0|            0|            0|  0.00%|            names, sizes = unzip_namedshape(sizes)\n",
      "   992|         0|            0|            0|  0.00%|        return super(Tensor, self).unflatten(dim, sizes, names)\n",
      "   993|         0|            0|            0|  0.00%|\n",
      "   994|         0|            0|            0|  0.00%|\n",
      "   995|         0|            0|            0|  0.00%|    def rename_(self, *names, **rename_map):\n",
      "   996|         0|            0|            0|  0.00%|        \"\"\"In-place version of :meth:`~Tensor.rename`.\"\"\"\n",
      "   997|         0|            0|            0|  0.00%|\n",
      "   998|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "   999|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.rename_, (self,), self, *names, **rename_map)\n",
      "  1000|         0|            0|            0|  0.00%|\n",
      "  1001|         0|            0|            0|  0.00%|        # Note [rename_ / rename API]\n",
      "  1002|         0|            0|            0|  0.00%|        # The Python API for these is different from the C++ API. In Python:\n",
      "  1003|         0|            0|            0|  0.00%|        # 1) tensor.rename(*names) takes a vararglist of names\n",
      "  1004|         0|            0|            0|  0.00%|        # 2) tensor.rename(**rename_map) takes a map of names to rename.\n",
      "  1005|         0|            0|            0|  0.00%|        # C++ is static, making it difficult to implement similar behavior.\n",
      "  1006|         0|            0|            0|  0.00%|        return update_names(self, names, rename_map, inplace=True)\n",
      "  1007|         0|            0|            0|  0.00%|\n",
      "  1008|         0|            0|            0|  0.00%|    def rename(self, *names, **rename_map):\n",
      "  1009|         0|            0|            0|  0.00%|        \"\"\"Renames dimension names of :attr:`self`.\n",
      "  1010|         0|            0|            0|  0.00%|\n",
      "  1011|         0|            0|            0|  0.00%|        There are two main usages:\n",
      "  1012|         0|            0|            0|  0.00%|\n",
      "  1013|         0|            0|            0|  0.00%|        ``self.rename(**rename_map)`` returns a view on tensor that has dims\n",
      "  1014|         0|            0|            0|  0.00%|        renamed as specified in the mapping :attr:`rename_map`.\n",
      "  1015|         0|            0|            0|  0.00%|\n",
      "  1016|         0|            0|            0|  0.00%|        ``self.rename(*names)`` returns a view on tensor, renaming all\n",
      "  1017|         0|            0|            0|  0.00%|        dimensions positionally using :attr:`names`.\n",
      "  1018|         0|            0|            0|  0.00%|        Use ``self.rename(None)`` to drop names on a tensor.\n",
      "  1019|         0|            0|            0|  0.00%|\n",
      "  1020|         0|            0|            0|  0.00%|        One cannot specify both positional args :attr:`names` and keyword args\n",
      "  1021|         0|            0|            0|  0.00%|        :attr:`rename_map`.\n",
      "  1022|         0|            0|            0|  0.00%|\n",
      "  1023|         0|            0|            0|  0.00%|        Examples::\n",
      "  1024|         0|            0|            0|  0.00%|\n",
      "  1025|         0|            0|            0|  0.00%|            >>> imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))\n",
      "  1026|         0|            0|            0|  0.00%|            >>> renamed_imgs = imgs.rename(N='batch', C='channels')\n",
      "  1027|         0|            0|            0|  0.00%|            >>> renamed_imgs.names\n",
      "  1028|         0|            0|            0|  0.00%|            ('batch', 'channels', 'H', 'W')\n",
      "  1029|         0|            0|            0|  0.00%|\n",
      "  1030|         0|            0|            0|  0.00%|            >>> renamed_imgs = imgs.rename(None)\n",
      "  1031|         0|            0|            0|  0.00%|            >>> renamed_imgs.names\n",
      "  1032|         0|            0|            0|  0.00%|            (None,)\n",
      "  1033|         0|            0|            0|  0.00%|\n",
      "  1034|         0|            0|            0|  0.00%|            >>> renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width')\n",
      "  1035|         0|            0|            0|  0.00%|            >>> renamed_imgs.names\n",
      "  1036|         0|            0|            0|  0.00%|            ('batch', 'channel', 'height', 'width')\n",
      "  1037|         0|            0|            0|  0.00%|\n",
      "  1038|         0|            0|            0|  0.00%|        .. warning::\n",
      "  1039|         0|            0|            0|  0.00%|            The named tensor API is experimental and subject to change.\n",
      "  1040|         0|            0|            0|  0.00%|\n",
      "  1041|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1042|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "  1043|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.rename, (self,), self, *names, **rename_map)\n",
      "  1044|         0|            0|            0|  0.00%|\n",
      "  1045|         0|            0|            0|  0.00%|        # See Note [rename_ / rename API]\n",
      "  1046|         0|            0|            0|  0.00%|        return update_names(self, names, rename_map, inplace=False)\n",
      "  1047|         0|            0|            0|  0.00%|\n",
      "  1048|         0|            0|            0|  0.00%|    def to_sparse_coo(self):\n",
      "  1049|         0|            0|            0|  0.00%|        \"\"\" Convert a tensor to :ref:`coordinate format <sparse-coo-docs>`.\n",
      "  1050|         0|            0|            0|  0.00%|\n",
      "  1051|         0|            0|            0|  0.00%|       Examples::\n",
      "  1052|         0|            0|            0|  0.00%|\n",
      "  1053|         0|            0|            0|  0.00%|            >>> dense = torch.randn(5, 5)\n",
      "  1054|         0|            0|            0|  0.00%|            >>> sparse = dense.to_sparse_coo()\n",
      "  1055|         0|            0|            0|  0.00%|            >>> sparse._nnz()\n",
      "  1056|         0|            0|            0|  0.00%|            25\n",
      "  1057|         0|            0|            0|  0.00%|\n",
      "  1058|         0|            0|            0|  0.00%|       \"\"\"\n",
      "  1059|         0|            0|            0|  0.00%|        return self.to_sparse()\n",
      "  1060|         0|            0|            0|  0.00%|\n",
      "  1061|         0|            0|            0|  0.00%|    def _update_names(self, names, inplace):\n",
      "  1062|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "  1063|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor._update_names, (self,), self, names, inplace)\n",
      "  1064|         0|            0|            0|  0.00%|\n",
      "  1065|         0|            0|            0|  0.00%|        # See Note [rename_ / rename API]\n",
      "  1066|         0|            0|            0|  0.00%|        if inplace:\n",
      "  1067|         0|            0|            0|  0.00%|            return super(Tensor, self).rename_(names)\n",
      "  1068|         0|            0|            0|  0.00%|        else:\n",
      "  1069|         0|            0|            0|  0.00%|            return super(Tensor, self).rename(names)\n",
      "  1070|         0|            0|            0|  0.00%|\n",
      "  1071|    829920|      1.16378|  1.40228e-06|  0.19%|    @property\n",
      "  1072|         0|            0|            0|  0.00%|    def grad(self):\n",
      "  1073|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1074|         0|            0|            0|  0.00%|        This attribute is ``None`` by default and becomes a Tensor the first time a call to\n",
      "  1075|         0|            0|            0|  0.00%|        :func:`backward` computes gradients for ``self``.\n",
      "  1076|         0|            0|            0|  0.00%|        The attribute will then contain the gradients computed and future calls to\n",
      "  1077|         0|            0|            0|  0.00%|        :func:`backward` will accumulate (add) gradients into it.\n",
      "  1078|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1079|    829920|      1.28473|  1.54802e-06|  0.21%|        if has_torch_function_unary(self):\n",
      "  1080|         0|            0|            0|  0.00%|            # TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185\n",
      "  1081|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.grad.__get__, (self,), self)  # type: ignore[attr-defined]\n",
      "  1082|         0|            0|            0|  0.00%|\n",
      "  1083|    829920|      1.35235|   1.6295e-06|  0.22%|        return self._grad\n",
      "  1084|         0|            0|            0|  0.00%|\n",
      "  1085|         0|            0|            0|  0.00%|    @grad.setter\n",
      "  1086|         0|            0|            0|  0.00%|    def grad(self, new_grad):\n",
      "  1087|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "  1088|         0|            0|            0|  0.00%|            # TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185\n",
      "  1089|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.grad.__set__, (self,), self, new_grad)  # type: ignore[attr-defined]\n",
      "  1090|         0|            0|            0|  0.00%|        self._grad = new_grad\n",
      "  1091|         0|            0|            0|  0.00%|\n",
      "  1092|         0|            0|            0|  0.00%|    @grad.deleter\n",
      "  1093|         0|            0|            0|  0.00%|    def grad(self):\n",
      "  1094|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "  1095|         0|            0|            0|  0.00%|            # TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185\n",
      "  1096|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.grad.__delete__, (self,), self)  # type: ignore[attr-defined]\n",
      "  1097|         0|            0|            0|  0.00%|        del self._grad\n",
      "  1098|         0|            0|            0|  0.00%|\n",
      "  1099|         0|            0|            0|  0.00%|    @classmethod\n",
      "  1100|         0|            0|            0|  0.00%|    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
      "  1101|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1102|         0|            0|            0|  0.00%|        This __torch_function__ implementation wraps subclasses such that\n",
      "  1103|         0|            0|            0|  0.00%|        methods called on subclasses return a subclass instance instead of\n",
      "  1104|         0|            0|            0|  0.00%|        a ``torch.Tensor`` instance.\n",
      "  1105|         0|            0|            0|  0.00%|\n",
      "  1106|         0|            0|            0|  0.00%|        One corollary to this is that you need coverage for torch.Tensor\n",
      "  1107|         0|            0|            0|  0.00%|        methods if implementing __torch_function__ for subclasses.\n",
      "  1108|         0|            0|            0|  0.00%|\n",
      "  1109|         0|            0|            0|  0.00%|        We recommend always calling ``super().__torch_function__`` as the base\n",
      "  1110|         0|            0|            0|  0.00%|        case when doing the above.\n",
      "  1111|         0|            0|            0|  0.00%|\n",
      "  1112|         0|            0|            0|  0.00%|        While not mandatory, we recommend making `__torch_function__` a classmethod.\n",
      "  1113|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1114|         0|            0|            0|  0.00%|        if kwargs is None:\n",
      "  1115|         0|            0|            0|  0.00%|            kwargs = {}\n",
      "  1116|         0|            0|            0|  0.00%|\n",
      "  1117|         0|            0|            0|  0.00%|        if not all(issubclass(cls, t) for t in types):\n",
      "  1118|         0|            0|            0|  0.00%|            return NotImplemented\n",
      "  1119|         0|            0|            0|  0.00%|\n",
      "  1120|         0|            0|            0|  0.00%|        with _C.DisableTorchFunction():\n",
      "  1121|         0|            0|            0|  0.00%|            ret = func(*args, **kwargs)\n",
      "  1122|         0|            0|            0|  0.00%|            if func in get_default_nowrap_functions():\n",
      "  1123|         0|            0|            0|  0.00%|                return ret\n",
      "  1124|         0|            0|            0|  0.00%|            else:\n",
      "  1125|         0|            0|            0|  0.00%|                return _convert(ret, cls)\n",
      "  1126|         0|            0|            0|  0.00%|\n",
      "  1127|         0|            0|            0|  0.00%|    __torch_dispatch__ = _C._disabled_torch_dispatch_impl\n",
      "  1128|         0|            0|            0|  0.00%|\n",
      "  1129|         0|            0|            0|  0.00%|    def __dlpack__(self, stream=None):\n",
      "  1130|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1131|         0|            0|            0|  0.00%|        Creates a DLpack `capsule https://data-apis.org/array-api/latest/design_topics/data_interchange.html#data-interchange`_\n",
      "  1132|         0|            0|            0|  0.00%|        of the current tensor to be exported to other libraries.\n",
      "  1133|         0|            0|            0|  0.00%|\n",
      "  1134|         0|            0|            0|  0.00%|        This function will be called from the `from_dlpack` method\n",
      "  1135|         0|            0|            0|  0.00%|        of the library that will consume the capsule. `from_dlpack` passes the current\n",
      "  1136|         0|            0|            0|  0.00%|        stream to this method as part of the specification.\n",
      "  1137|         0|            0|            0|  0.00%|\n",
      "  1138|         0|            0|            0|  0.00%|        Args:\n",
      "  1139|         0|            0|            0|  0.00%|            stream (integer or None): An optional Python integer representing a\n",
      "  1140|         0|            0|            0|  0.00%|            pointer to a CUDA stream. The current stream is synchronized with\n",
      "  1141|         0|            0|            0|  0.00%|            this stream before the capsule is created, and since the capsule\n",
      "  1142|         0|            0|            0|  0.00%|            shares its storage with the tensor this make it safe to access from\n",
      "  1143|         0|            0|            0|  0.00%|            both streams.  If None or -1 is passed then no synchronization is performed.\n",
      "  1144|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1145|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "  1146|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__dlpack__, (self,), self, stream)\n",
      "  1147|         0|            0|            0|  0.00%|\n",
      "  1148|         0|            0|            0|  0.00%|        # DLPack capsules can't capture all of PyTorch's semantics,\n",
      "  1149|         0|            0|            0|  0.00%|        # so we prohibit exporting tensors that would lose their properties like\n",
      "  1150|         0|            0|            0|  0.00%|        # requires_grad and having the conjugate bit set.\n",
      "  1151|         0|            0|            0|  0.00%|        if self.requires_grad:\n",
      "  1152|         0|            0|            0|  0.00%|            raise RuntimeError('Can\\'t export tensors that require gradient, use tensor.detach()')\n",
      "  1153|         0|            0|            0|  0.00%|        if self.is_conj():\n",
      "  1154|         0|            0|            0|  0.00%|            raise RuntimeError('Can\\'t export tensors with the conjugate bit set')\n",
      "  1155|         0|            0|            0|  0.00%|        if self.layout != torch.strided:\n",
      "  1156|         0|            0|            0|  0.00%|            raise RuntimeError('Can\\'t export tensors with layout other than torch.strided')\n",
      "  1157|         0|            0|            0|  0.00%|\n",
      "  1158|         0|            0|            0|  0.00%|        if stream is not None and type(stream) is not int:\n",
      "  1159|         0|            0|            0|  0.00%|            # Stream pointers in CUDA/ROCm are uniquely numbered and can\n",
      "  1160|         0|            0|            0|  0.00%|            # be retrieved from their integer value.\n",
      "  1161|         0|            0|            0|  0.00%|            raise TypeError('stream must be ``int`` or ``none``')\n",
      "  1162|         0|            0|            0|  0.00%|        elif stream is not None and stream != -1:\n",
      "  1163|         0|            0|            0|  0.00%|            if self.device.type == 'cuda':\n",
      "  1164|         0|            0|            0|  0.00%|                stream = torch.cuda.ExternalStream(stream)\n",
      "  1165|         0|            0|            0|  0.00%|                # Only synchronize on different streams\n",
      "  1166|         0|            0|            0|  0.00%|                if stream != torch.cuda.current_stream:\n",
      "  1167|         0|            0|            0|  0.00%|                    event = torch.cuda.Event()\n",
      "  1168|         0|            0|            0|  0.00%|                    event.record(torch.cuda.current_stream())\n",
      "  1169|         0|            0|            0|  0.00%|                    stream.wait_event(event)\n",
      "  1170|         0|            0|            0|  0.00%|        return torch.to_dlpack(self)\n",
      "  1171|         0|            0|            0|  0.00%|\n",
      "  1172|         0|            0|            0|  0.00%|    def __dlpack_device__(self) -> Tuple[enum.IntEnum, int]:\n",
      "  1173|         0|            0|            0|  0.00%|        # Avoid circular import\n",
      "  1174|         0|            0|            0|  0.00%|        from torch.utils.dlpack import DLDeviceType\n",
      "  1175|         0|            0|            0|  0.00%|        if has_torch_function_unary(self):\n",
      "  1176|         0|            0|            0|  0.00%|            return handle_torch_function(Tensor.__dlpack_device__, (self,), self)\n",
      "  1177|         0|            0|            0|  0.00%|        idx = self.device.index if self.device.index is not None else 0\n",
      "  1178|         0|            0|            0|  0.00%|        if self.device.type == 'cuda' and torch.version.hip is not None:\n",
      "  1179|         0|            0|            0|  0.00%|            device_type = DLDeviceType.kDLROCM\n",
      "  1180|         0|            0|            0|  0.00%|        elif self.device.type == 'cpu' and self.is_pinned():\n",
      "  1181|         0|            0|            0|  0.00%|            device_type = DLDeviceType.kDLCPUPinned\n",
      "  1182|         0|            0|            0|  0.00%|        elif self.device.type == 'cuda':\n",
      "  1183|         0|            0|            0|  0.00%|            device_type = DLDeviceType.kDLGPU\n",
      "  1184|         0|            0|            0|  0.00%|        elif self.device.type == 'cpu':\n",
      "  1185|         0|            0|            0|  0.00%|            device_type = DLDeviceType.kDLCPU\n",
      "  1186|         0|            0|            0|  0.00%|        else:\n",
      "  1187|         0|            0|            0|  0.00%|            raise ValueError('Unknown device type {} for Dlpack'.format(self.device.type))\n",
      "  1188|         0|            0|            0|  0.00%|        return (device_type, idx)\n",
      "  1189|         0|            0|            0|  0.00%|\n",
      "  1190|         0|            0|            0|  0.00%|    __module__ = 'torch'\n",
      "  1191|         0|            0|            0|  0.00%|\n",
      "  1192|         0|            0|            0|  0.00%|def _convert(ret, cls):\n",
      "  1193|         0|            0|            0|  0.00%|    if cls is Tensor:\n",
      "  1194|         0|            0|            0|  0.00%|        return ret\n",
      "  1195|         0|            0|            0|  0.00%|\n",
      "  1196|         0|            0|            0|  0.00%|    if isinstance(ret, Tensor) and not isinstance(ret, cls):\n",
      "  1197|         0|            0|            0|  0.00%|        ret = ret.as_subclass(cls)\n",
      "  1198|         0|            0|            0|  0.00%|\n",
      "  1199|         0|            0|            0|  0.00%|    if isinstance(ret, (tuple, list)):\n",
      "  1200|         0|            0|            0|  0.00%|        # Also handles things like namedtuples\n",
      "  1201|         0|            0|            0|  0.00%|        ret = type(ret)(_convert(r, cls) for r in ret)\n",
      "  1202|         0|            0|            0|  0.00%|\n",
      "  1203|         0|            0|            0|  0.00%|    return ret\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/linear.py\n",
      "File duration: 6.30551s (1.02%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|import math\n",
      "     2|         0|            0|            0|  0.00%|from typing import Any\n",
      "     3|         0|            0|            0|  0.00%|\n",
      "     4|         0|            0|            0|  0.00%|import torch\n",
      "     5|         0|            0|            0|  0.00%|from torch import Tensor\n",
      "     6|         0|            0|            0|  0.00%|from torch.nn.parameter import Parameter, UninitializedParameter\n",
      "     7|         0|            0|            0|  0.00%|from .. import functional as F\n",
      "     8|         0|            0|            0|  0.00%|from .. import init\n",
      "     9|         0|            0|            0|  0.00%|from .module import Module\n",
      "    10|         0|            0|            0|  0.00%|from .lazy import LazyModuleMixin\n",
      "    11|         0|            0|            0|  0.00%|\n",
      "    12|         0|            0|            0|  0.00%|\n",
      "    13|         0|            0|            0|  0.00%|__all__ = [\n",
      "    14|         0|            0|            0|  0.00%|    'Bilinear',\n",
      "    15|         0|            0|            0|  0.00%|    'Identity',\n",
      "    16|         0|            0|            0|  0.00%|    'LazyLinear',\n",
      "    17|         0|            0|            0|  0.00%|    'Linear',\n",
      "    18|         0|            0|            0|  0.00%|]\n",
      "    19|         0|            0|            0|  0.00%|\n",
      "    20|         0|            0|            0|  0.00%|\n",
      "    21|         0|            0|            0|  0.00%|class Identity(Module):\n",
      "    22|         0|            0|            0|  0.00%|    r\"\"\"A placeholder identity operator that is argument-insensitive.\n",
      "    23|         0|            0|            0|  0.00%|\n",
      "    24|         0|            0|            0|  0.00%|    Args:\n",
      "    25|         0|            0|            0|  0.00%|        args: any argument (unused)\n",
      "    26|         0|            0|            0|  0.00%|        kwargs: any keyword argument (unused)\n",
      "    27|         0|            0|            0|  0.00%|\n",
      "    28|         0|            0|            0|  0.00%|    Shape:\n",
      "    29|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "    30|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "    31|         0|            0|            0|  0.00%|\n",
      "    32|         0|            0|            0|  0.00%|    Examples::\n",
      "    33|         0|            0|            0|  0.00%|\n",
      "    34|         0|            0|            0|  0.00%|        >>> m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)\n",
      "    35|         0|            0|            0|  0.00%|        >>> input = torch.randn(128, 20)\n",
      "    36|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "    37|         0|            0|            0|  0.00%|        >>> print(output.size())\n",
      "    38|         0|            0|            0|  0.00%|        torch.Size([128, 20])\n",
      "    39|         0|            0|            0|  0.00%|\n",
      "    40|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    41|         0|            0|            0|  0.00%|    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
      "    42|         0|            0|            0|  0.00%|        super(Identity, self).__init__()\n",
      "    43|         0|            0|            0|  0.00%|\n",
      "    44|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "    45|         0|            0|            0|  0.00%|        return input\n",
      "    46|         0|            0|            0|  0.00%|\n",
      "    47|         0|            0|            0|  0.00%|\n",
      "    48|         0|            0|            0|  0.00%|class Linear(Module):\n",
      "    49|         0|            0|            0|  0.00%|    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
      "    50|         0|            0|            0|  0.00%|\n",
      "    51|         0|            0|            0|  0.00%|    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "    52|         0|            0|            0|  0.00%|\n",
      "    53|         0|            0|            0|  0.00%|    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "    54|         0|            0|            0|  0.00%|\n",
      "    55|         0|            0|            0|  0.00%|    Args:\n",
      "    56|         0|            0|            0|  0.00%|        in_features: size of each input sample\n",
      "    57|         0|            0|            0|  0.00%|        out_features: size of each output sample\n",
      "    58|         0|            0|            0|  0.00%|        bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "    59|         0|            0|            0|  0.00%|            Default: ``True``\n",
      "    60|         0|            0|            0|  0.00%|\n",
      "    61|         0|            0|            0|  0.00%|    Shape:\n",
      "    62|         0|            0|            0|  0.00%|        - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      "    63|         0|            0|            0|  0.00%|          dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      "    64|         0|            0|            0|  0.00%|        - Output: :math:`(*, H_{out})` where all but the last dimension\n",
      "    65|         0|            0|            0|  0.00%|          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      "    66|         0|            0|            0|  0.00%|\n",
      "    67|         0|            0|            0|  0.00%|    Attributes:\n",
      "    68|         0|            0|            0|  0.00%|        weight: the learnable weights of the module of shape\n",
      "    69|         0|            0|            0|  0.00%|            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "    70|         0|            0|            0|  0.00%|            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "    71|         0|            0|            0|  0.00%|            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "    72|         0|            0|            0|  0.00%|        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "    73|         0|            0|            0|  0.00%|                If :attr:`bias` is ``True``, the values are initialized from\n",
      "    74|         0|            0|            0|  0.00%|                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "    75|         0|            0|            0|  0.00%|                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "    76|         0|            0|            0|  0.00%|\n",
      "    77|         0|            0|            0|  0.00%|    Examples::\n",
      "    78|         0|            0|            0|  0.00%|\n",
      "    79|         0|            0|            0|  0.00%|        >>> m = nn.Linear(20, 30)\n",
      "    80|         0|            0|            0|  0.00%|        >>> input = torch.randn(128, 20)\n",
      "    81|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "    82|         0|            0|            0|  0.00%|        >>> print(output.size())\n",
      "    83|         0|            0|            0|  0.00%|        torch.Size([128, 30])\n",
      "    84|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    85|         0|            0|            0|  0.00%|    __constants__ = ['in_features', 'out_features']\n",
      "    86|         0|            0|            0|  0.00%|    in_features: int\n",
      "    87|         0|            0|            0|  0.00%|    out_features: int\n",
      "    88|         0|            0|            0|  0.00%|    weight: Tensor\n",
      "    89|         0|            0|            0|  0.00%|\n",
      "    90|         0|            0|            0|  0.00%|    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
      "    91|         0|            0|            0|  0.00%|                 device=None, dtype=None) -> None:\n",
      "    92|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "    93|         0|            0|            0|  0.00%|        super(Linear, self).__init__()\n",
      "    94|         0|            0|            0|  0.00%|        self.in_features = in_features\n",
      "    95|         0|            0|            0|  0.00%|        self.out_features = out_features\n",
      "    96|         0|            0|            0|  0.00%|        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
      "    97|         0|            0|            0|  0.00%|        if bias:\n",
      "    98|         0|            0|            0|  0.00%|            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
      "    99|         0|            0|            0|  0.00%|        else:\n",
      "   100|         0|            0|            0|  0.00%|            self.register_parameter('bias', None)\n",
      "   101|         0|            0|            0|  0.00%|        self.reset_parameters()\n",
      "   102|         0|            0|            0|  0.00%|\n",
      "   103|         0|            0|            0|  0.00%|    def reset_parameters(self) -> None:\n",
      "   104|         0|            0|            0|  0.00%|        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
      "   105|         0|            0|            0|  0.00%|        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n",
      "   106|         0|            0|            0|  0.00%|        # https://github.com/pytorch/pytorch/issues/57109\n",
      "   107|         0|            0|            0|  0.00%|        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
      "   108|         0|            0|            0|  0.00%|        if self.bias is not None:\n",
      "   109|         0|            0|            0|  0.00%|            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
      "   110|         0|            0|            0|  0.00%|            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
      "   111|         0|            0|            0|  0.00%|            init.uniform_(self.bias, -bound, bound)\n",
      "   112|         0|            0|            0|  0.00%|\n",
      "   113|    190710|     0.300633|  1.57639e-06|  0.05%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   114|    190710|      6.00487|  3.14869e-05|  0.97%|        return F.linear(input, self.weight, self.bias)\n",
      "(call)|    381420|      3.81974|  1.00145e-05|  0.62%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194 __getattr__\n",
      "   115|         0|            0|            0|  0.00%|\n",
      "   116|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   117|         0|            0|            0|  0.00%|        return 'in_features={}, out_features={}, bias={}'.format(\n",
      "   118|         0|            0|            0|  0.00%|            self.in_features, self.out_features, self.bias is not None\n",
      "   119|         0|            0|            0|  0.00%|        )\n",
      "   120|         0|            0|            0|  0.00%|\n",
      "   121|         0|            0|            0|  0.00%|\n",
      "   122|         0|            0|            0|  0.00%|# This class exists solely to avoid triggering an obscure error when scripting\n",
      "   123|         0|            0|            0|  0.00%|# an improperly quantized attention layer. See this issue for details:\n",
      "   124|         0|            0|            0|  0.00%|# https://github.com/pytorch/pytorch/issues/58969\n",
      "   125|         0|            0|            0|  0.00%|# TODO: fail fast on quantization API usage error, then remove this class\n",
      "   126|         0|            0|            0|  0.00%|# and replace uses of it with plain Linear\n",
      "   127|         0|            0|            0|  0.00%|class NonDynamicallyQuantizableLinear(Linear):\n",
      "   128|         0|            0|            0|  0.00%|    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
      "   129|         0|            0|            0|  0.00%|                 device=None, dtype=None) -> None:\n",
      "   130|         0|            0|            0|  0.00%|        super().__init__(in_features, out_features, bias=bias,\n",
      "   131|         0|            0|            0|  0.00%|                         device=device, dtype=dtype)\n",
      "   132|         0|            0|            0|  0.00%|\n",
      "   133|         0|            0|            0|  0.00%|\n",
      "   134|         0|            0|            0|  0.00%|class Bilinear(Module):\n",
      "   135|         0|            0|            0|  0.00%|    r\"\"\"Applies a bilinear transformation to the incoming data:\n",
      "   136|         0|            0|            0|  0.00%|    :math:`y = x_1^T A x_2 + b`\n",
      "   137|         0|            0|            0|  0.00%|\n",
      "   138|         0|            0|            0|  0.00%|    Args:\n",
      "   139|         0|            0|            0|  0.00%|        in1_features: size of each first input sample\n",
      "   140|         0|            0|            0|  0.00%|        in2_features: size of each second input sample\n",
      "   141|         0|            0|            0|  0.00%|        out_features: size of each output sample\n",
      "   142|         0|            0|            0|  0.00%|        bias: If set to False, the layer will not learn an additive bias.\n",
      "   143|         0|            0|            0|  0.00%|            Default: ``True``\n",
      "   144|         0|            0|            0|  0.00%|\n",
      "   145|         0|            0|            0|  0.00%|    Shape:\n",
      "   146|         0|            0|            0|  0.00%|        - Input1: :math:`(*, H_{in1})` where :math:`H_{in1}=\\text{in1\\_features}` and\n",
      "   147|         0|            0|            0|  0.00%|          :math:`*` means any number of additional dimensions including none. All but the last dimension\n",
      "   148|         0|            0|            0|  0.00%|          of the inputs should be the same.\n",
      "   149|         0|            0|            0|  0.00%|        - Input2: :math:`(*, H_{in2})` where :math:`H_{in2}=\\text{in2\\_features}`.\n",
      "   150|         0|            0|            0|  0.00%|        - Output: :math:`(*, H_{out})` where :math:`H_{out}=\\text{out\\_features}`\n",
      "   151|         0|            0|            0|  0.00%|          and all but the last dimension are the same shape as the input.\n",
      "   152|         0|            0|            0|  0.00%|\n",
      "   153|         0|            0|            0|  0.00%|    Attributes:\n",
      "   154|         0|            0|            0|  0.00%|        weight: the learnable weights of the module of shape\n",
      "   155|         0|            0|            0|  0.00%|            :math:`(\\text{out\\_features}, \\text{in1\\_features}, \\text{in2\\_features})`.\n",
      "   156|         0|            0|            0|  0.00%|            The values are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "   157|         0|            0|            0|  0.00%|            :math:`k = \\frac{1}{\\text{in1\\_features}}`\n",
      "   158|         0|            0|            0|  0.00%|        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "   159|         0|            0|            0|  0.00%|                If :attr:`bias` is ``True``, the values are initialized from\n",
      "   160|         0|            0|            0|  0.00%|                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "   161|         0|            0|            0|  0.00%|                :math:`k = \\frac{1}{\\text{in1\\_features}}`\n",
      "   162|         0|            0|            0|  0.00%|\n",
      "   163|         0|            0|            0|  0.00%|    Examples::\n",
      "   164|         0|            0|            0|  0.00%|\n",
      "   165|         0|            0|            0|  0.00%|        >>> m = nn.Bilinear(20, 30, 40)\n",
      "   166|         0|            0|            0|  0.00%|        >>> input1 = torch.randn(128, 20)\n",
      "   167|         0|            0|            0|  0.00%|        >>> input2 = torch.randn(128, 30)\n",
      "   168|         0|            0|            0|  0.00%|        >>> output = m(input1, input2)\n",
      "   169|         0|            0|            0|  0.00%|        >>> print(output.size())\n",
      "   170|         0|            0|            0|  0.00%|        torch.Size([128, 40])\n",
      "   171|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   172|         0|            0|            0|  0.00%|    __constants__ = ['in1_features', 'in2_features', 'out_features']\n",
      "   173|         0|            0|            0|  0.00%|    in1_features: int\n",
      "   174|         0|            0|            0|  0.00%|    in2_features: int\n",
      "   175|         0|            0|            0|  0.00%|    out_features: int\n",
      "   176|         0|            0|            0|  0.00%|    weight: Tensor\n",
      "   177|         0|            0|            0|  0.00%|\n",
      "   178|         0|            0|            0|  0.00%|    def __init__(self, in1_features: int, in2_features: int, out_features: int, bias: bool = True,\n",
      "   179|         0|            0|            0|  0.00%|                 device=None, dtype=None) -> None:\n",
      "   180|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "   181|         0|            0|            0|  0.00%|        super(Bilinear, self).__init__()\n",
      "   182|         0|            0|            0|  0.00%|        self.in1_features = in1_features\n",
      "   183|         0|            0|            0|  0.00%|        self.in2_features = in2_features\n",
      "   184|         0|            0|            0|  0.00%|        self.out_features = out_features\n",
      "   185|         0|            0|            0|  0.00%|        self.weight = Parameter(torch.empty((out_features, in1_features, in2_features), **factory_kwargs))\n",
      "   186|         0|            0|            0|  0.00%|\n",
      "   187|         0|            0|            0|  0.00%|        if bias:\n",
      "   188|         0|            0|            0|  0.00%|            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
      "   189|         0|            0|            0|  0.00%|        else:\n",
      "   190|         0|            0|            0|  0.00%|            self.register_parameter('bias', None)\n",
      "   191|         0|            0|            0|  0.00%|        self.reset_parameters()\n",
      "   192|         0|            0|            0|  0.00%|\n",
      "   193|         0|            0|            0|  0.00%|    def reset_parameters(self) -> None:\n",
      "   194|         0|            0|            0|  0.00%|        bound = 1 / math.sqrt(self.weight.size(1))\n",
      "   195|         0|            0|            0|  0.00%|        init.uniform_(self.weight, -bound, bound)\n",
      "   196|         0|            0|            0|  0.00%|        if self.bias is not None:\n",
      "   197|         0|            0|            0|  0.00%|            init.uniform_(self.bias, -bound, bound)\n",
      "   198|         0|            0|            0|  0.00%|\n",
      "   199|         0|            0|            0|  0.00%|    def forward(self, input1: Tensor, input2: Tensor) -> Tensor:\n",
      "   200|         0|            0|            0|  0.00%|        return F.bilinear(input1, input2, self.weight, self.bias)\n",
      "   201|         0|            0|            0|  0.00%|\n",
      "   202|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   203|         0|            0|            0|  0.00%|        return 'in1_features={}, in2_features={}, out_features={}, bias={}'.format(\n",
      "   204|         0|            0|            0|  0.00%|            self.in1_features, self.in2_features, self.out_features, self.bias is not None\n",
      "   205|         0|            0|            0|  0.00%|        )\n",
      "   206|         0|            0|            0|  0.00%|\n",
      "   207|         0|            0|            0|  0.00%|\n",
      "   208|         0|            0|            0|  0.00%|class LazyLinear(LazyModuleMixin, Linear):\n",
      "   209|         0|            0|            0|  0.00%|    r\"\"\"A :class:`torch.nn.Linear` module where `in_features` is inferred.\n",
      "   210|         0|            0|            0|  0.00%|\n",
      "   211|         0|            0|            0|  0.00%|    In this module, the `weight` and `bias` are of :class:`torch.nn.UninitializedParameter`\n",
      "   212|         0|            0|            0|  0.00%|    class. They will be initialized after the first call to ``forward`` is done and the\n",
      "   213|         0|            0|            0|  0.00%|    module will become a regular :class:`torch.nn.Linear` module. The ``in_features`` argument\n",
      "   214|         0|            0|            0|  0.00%|    of the :class:`Linear` is inferred from the ``input.shape[-1]``.\n",
      "   215|         0|            0|            0|  0.00%|\n",
      "   216|         0|            0|            0|  0.00%|    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation\n",
      "   217|         0|            0|            0|  0.00%|    on lazy modules and their limitations.\n",
      "   218|         0|            0|            0|  0.00%|\n",
      "   219|         0|            0|            0|  0.00%|    Args:\n",
      "   220|         0|            0|            0|  0.00%|        out_features: size of each output sample\n",
      "   221|         0|            0|            0|  0.00%|        bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "   222|         0|            0|            0|  0.00%|            Default: ``True``\n",
      "   223|         0|            0|            0|  0.00%|\n",
      "   224|         0|            0|            0|  0.00%|    Attributes:\n",
      "   225|         0|            0|            0|  0.00%|        weight: the learnable weights of the module of shape\n",
      "   226|         0|            0|            0|  0.00%|            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "   227|         0|            0|            0|  0.00%|            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "   228|         0|            0|            0|  0.00%|            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "   229|         0|            0|            0|  0.00%|        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "   230|         0|            0|            0|  0.00%|                If :attr:`bias` is ``True``, the values are initialized from\n",
      "   231|         0|            0|            0|  0.00%|                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "   232|         0|            0|            0|  0.00%|                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "   233|         0|            0|            0|  0.00%|\n",
      "   234|         0|            0|            0|  0.00%|\n",
      "   235|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   236|         0|            0|            0|  0.00%|\n",
      "   237|         0|            0|            0|  0.00%|    cls_to_become = Linear  # type: ignore[assignment]\n",
      "   238|         0|            0|            0|  0.00%|    weight: UninitializedParameter\n",
      "   239|         0|            0|            0|  0.00%|    bias: UninitializedParameter  # type: ignore[assignment]\n",
      "   240|         0|            0|            0|  0.00%|\n",
      "   241|         0|            0|            0|  0.00%|    def __init__(self, out_features: int, bias: bool = True,\n",
      "   242|         0|            0|            0|  0.00%|                 device=None, dtype=None) -> None:\n",
      "   243|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "   244|         0|            0|            0|  0.00%|        # bias is hardcoded to False to avoid creating tensor\n",
      "   245|         0|            0|            0|  0.00%|        # that will soon be overwritten.\n",
      "   246|         0|            0|            0|  0.00%|        super().__init__(0, 0, False)\n",
      "   247|         0|            0|            0|  0.00%|        self.weight = UninitializedParameter(**factory_kwargs)\n",
      "   248|         0|            0|            0|  0.00%|        self.out_features = out_features\n",
      "   249|         0|            0|            0|  0.00%|        if bias:\n",
      "   250|         0|            0|            0|  0.00%|            self.bias = UninitializedParameter(**factory_kwargs)\n",
      "   251|         0|            0|            0|  0.00%|\n",
      "   252|         0|            0|            0|  0.00%|    def reset_parameters(self) -> None:\n",
      "   253|         0|            0|            0|  0.00%|        if not self.has_uninitialized_params() and self.in_features != 0:\n",
      "   254|         0|            0|            0|  0.00%|            super().reset_parameters()\n",
      "   255|         0|            0|            0|  0.00%|\n",
      "   256|         0|            0|            0|  0.00%|    def initialize_parameters(self, input) -> None:  # type: ignore[override]\n",
      "   257|         0|            0|            0|  0.00%|        if self.has_uninitialized_params():\n",
      "   258|         0|            0|            0|  0.00%|            with torch.no_grad():\n",
      "   259|         0|            0|            0|  0.00%|                self.in_features = input.shape[-1]\n",
      "   260|         0|            0|            0|  0.00%|                self.weight.materialize((self.out_features, self.in_features))\n",
      "   261|         0|            0|            0|  0.00%|                if self.bias is not None:\n",
      "   262|         0|            0|            0|  0.00%|                    self.bias.materialize((self.out_features,))\n",
      "   263|         0|            0|            0|  0.00%|                self.reset_parameters()\n",
      "   264|         0|            0|            0|  0.00%|# TODO: PartialLinear - maybe in sparse?\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py\n",
      "File duration: 5.70345s (0.92%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|\"\"\"Module containing non-deprecated functions borrowed from Numeric.\n",
      "     2|         0|            0|            0|  0.00%|\n",
      "     3|         0|            0|            0|  0.00%|\"\"\"\n",
      "     4|         0|            0|            0|  0.00%|import functools\n",
      "     5|         0|            0|            0|  0.00%|import types\n",
      "     6|         0|            0|            0|  0.00%|import warnings\n",
      "     7|         0|            0|            0|  0.00%|\n",
      "     8|         0|            0|            0|  0.00%|import numpy as np\n",
      "     9|         0|            0|            0|  0.00%|from . import multiarray as mu\n",
      "    10|         0|            0|            0|  0.00%|from . import overrides\n",
      "    11|         0|            0|            0|  0.00%|from . import umath as um\n",
      "    12|         0|            0|            0|  0.00%|from . import numerictypes as nt\n",
      "    13|         0|            0|            0|  0.00%|from .multiarray import asarray, array, asanyarray, concatenate\n",
      "    14|         0|            0|            0|  0.00%|from . import _methods\n",
      "    15|         0|            0|            0|  0.00%|\n",
      "    16|         0|            0|            0|  0.00%|_dt_ = nt.sctype2char\n",
      "    17|         0|            0|            0|  0.00%|\n",
      "    18|         0|            0|            0|  0.00%|# functions that are methods\n",
      "    19|         0|            0|            0|  0.00%|__all__ = [\n",
      "    20|         0|            0|            0|  0.00%|    'alen', 'all', 'alltrue', 'amax', 'amin', 'any', 'argmax',\n",
      "    21|         0|            0|            0|  0.00%|    'argmin', 'argpartition', 'argsort', 'around', 'choose', 'clip',\n",
      "    22|         0|            0|            0|  0.00%|    'compress', 'cumprod', 'cumproduct', 'cumsum', 'diagonal', 'mean',\n",
      "    23|         0|            0|            0|  0.00%|    'ndim', 'nonzero', 'partition', 'prod', 'product', 'ptp', 'put',\n",
      "    24|         0|            0|            0|  0.00%|    'ravel', 'repeat', 'reshape', 'resize', 'round_',\n",
      "    25|         0|            0|            0|  0.00%|    'searchsorted', 'shape', 'size', 'sometrue', 'sort', 'squeeze',\n",
      "    26|         0|            0|            0|  0.00%|    'std', 'sum', 'swapaxes', 'take', 'trace', 'transpose', 'var',\n",
      "    27|         0|            0|            0|  0.00%|]\n",
      "    28|         0|            0|            0|  0.00%|\n",
      "    29|         0|            0|            0|  0.00%|_gentype = types.GeneratorType\n",
      "    30|         0|            0|            0|  0.00%|# save away Python sum\n",
      "    31|         0|            0|            0|  0.00%|_sum_ = sum\n",
      "    32|         0|            0|            0|  0.00%|\n",
      "    33|         0|            0|            0|  0.00%|array_function_dispatch = functools.partial(\n",
      "    34|         0|            0|            0|  0.00%|    overrides.array_function_dispatch, module='numpy')\n",
      "    35|         0|            0|            0|  0.00%|\n",
      "    36|         0|            0|            0|  0.00%|\n",
      "    37|         0|            0|            0|  0.00%|# functions that are now methods\n",
      "    38|     74880|      0.12308|  1.64369e-06|  0.02%|def _wrapit(obj, method, *args, **kwds):\n",
      "    39|     74880|     0.140971|  1.88262e-06|  0.02%|    try:\n",
      "    40|     74880|     0.333759|  4.45725e-06|  0.05%|        wrap = obj.__array_wrap__\n",
      "    41|     74880|     0.173955|  2.32311e-06|  0.03%|    except AttributeError:\n",
      "    42|     74880|     0.163429|  2.18254e-06|  0.03%|        wrap = None\n",
      "    43|     74880|      1.05537|  1.40941e-05|  0.17%|    result = getattr(asarray(obj), method)(*args, **kwds)\n",
      "    44|     74880|     0.147825|  1.97416e-06|  0.02%|    if wrap:\n",
      "    45|         0|            0|            0|  0.00%|        if not isinstance(result, mu.ndarray):\n",
      "    46|         0|            0|            0|  0.00%|            result = asarray(result)\n",
      "    47|         0|            0|            0|  0.00%|        result = wrap(result)\n",
      "    48|     74880|     0.123697|  1.65193e-06|  0.02%|    return result\n",
      "    49|         0|            0|            0|  0.00%|\n",
      "    50|         0|            0|            0|  0.00%|\n",
      "    51|    149760|     0.234149|   1.5635e-06|  0.04%|def _wrapfunc(obj, method, *args, **kwds):\n",
      "    52|    149760|     0.298306|   1.9919e-06|  0.05%|    bound = getattr(obj, method, None)\n",
      "    53|    149760|     0.245899|  1.64196e-06|  0.04%|    if bound is None:\n",
      "    54|     74880|      0.49974|  6.67388e-06|  0.08%|        return _wrapit(obj, method, *args, **kwds)\n",
      "(call)|     74880|      2.26208|  3.02094e-05|  0.36%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:38 _wrapit\n",
      "    55|         0|            0|            0|  0.00%|\n",
      "    56|     74880|     0.110254|   1.4724e-06|  0.02%|    try:\n",
      "    57|     74880|     0.337742|  4.51044e-06|  0.05%|        return bound(*args, **kwds)\n",
      "    58|         0|            0|            0|  0.00%|    except TypeError:\n",
      "    59|         0|            0|            0|  0.00%|        # A TypeError occurs if the object does have such a method in its\n",
      "    60|         0|            0|            0|  0.00%|        # class, but its signature is not identical to that of NumPy's. This\n",
      "    61|         0|            0|            0|  0.00%|        # situation has occurred in the case of a downstream library like\n",
      "    62|         0|            0|            0|  0.00%|        # 'pandas'.\n",
      "    63|         0|            0|            0|  0.00%|        #\n",
      "    64|         0|            0|            0|  0.00%|        # Call _wrapit from within the except clause to ensure a potential\n",
      "    65|         0|            0|            0|  0.00%|        # exception has a traceback chain.\n",
      "    66|         0|            0|            0|  0.00%|        return _wrapit(obj, method, *args, **kwds)\n",
      "    67|         0|            0|            0|  0.00%|\n",
      "    68|         0|            0|            0|  0.00%|\n",
      "    69|         0|            0|            0|  0.00%|def _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs):\n",
      "    70|         0|            0|            0|  0.00%|    passkwargs = {k: v for k, v in kwargs.items()\n",
      "    71|         0|            0|            0|  0.00%|                  if v is not np._NoValue}\n",
      "    72|         0|            0|            0|  0.00%|\n",
      "    73|         0|            0|            0|  0.00%|    if type(obj) is not mu.ndarray:\n",
      "    74|         0|            0|            0|  0.00%|        try:\n",
      "    75|         0|            0|            0|  0.00%|            reduction = getattr(obj, method)\n",
      "    76|         0|            0|            0|  0.00%|        except AttributeError:\n",
      "    77|         0|            0|            0|  0.00%|            pass\n",
      "    78|         0|            0|            0|  0.00%|        else:\n",
      "    79|         0|            0|            0|  0.00%|            # This branch is needed for reductions like any which don't\n",
      "    80|         0|            0|            0|  0.00%|            # support a dtype.\n",
      "    81|         0|            0|            0|  0.00%|            if dtype is not None:\n",
      "    82|         0|            0|            0|  0.00%|                return reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\n",
      "    83|         0|            0|            0|  0.00%|            else:\n",
      "    84|         0|            0|            0|  0.00%|                return reduction(axis=axis, out=out, **passkwargs)\n",
      "    85|         0|            0|            0|  0.00%|\n",
      "    86|         0|            0|            0|  0.00%|    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "    87|         0|            0|            0|  0.00%|\n",
      "    88|         0|            0|            0|  0.00%|\n",
      "    89|         0|            0|            0|  0.00%|def _take_dispatcher(a, indices, axis=None, out=None, mode=None):\n",
      "    90|         0|            0|            0|  0.00%|    return (a, out)\n",
      "    91|         0|            0|            0|  0.00%|\n",
      "    92|         0|            0|            0|  0.00%|\n",
      "    93|         0|            0|            0|  0.00%|@array_function_dispatch(_take_dispatcher)\n",
      "    94|         0|            0|            0|  0.00%|def take(a, indices, axis=None, out=None, mode='raise'):\n",
      "    95|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    96|         0|            0|            0|  0.00%|    Take elements from an array along an axis.\n",
      "    97|         0|            0|            0|  0.00%|\n",
      "    98|         0|            0|            0|  0.00%|    When axis is not None, this function does the same thing as \"fancy\"\n",
      "    99|         0|            0|            0|  0.00%|    indexing (indexing arrays using arrays); however, it can be easier to use\n",
      "   100|         0|            0|            0|  0.00%|    if you need elements along a given axis. A call such as\n",
      "   101|         0|            0|            0|  0.00%|    ``np.take(arr, indices, axis=3)`` is equivalent to\n",
      "   102|         0|            0|            0|  0.00%|    ``arr[:,:,:,indices,...]``.\n",
      "   103|         0|            0|            0|  0.00%|\n",
      "   104|         0|            0|            0|  0.00%|    Explained without fancy indexing, this is equivalent to the following use\n",
      "   105|         0|            0|            0|  0.00%|    of `ndindex`, which sets each of ``ii``, ``jj``, and ``kk`` to a tuple of\n",
      "   106|         0|            0|            0|  0.00%|    indices::\n",
      "   107|         0|            0|            0|  0.00%|\n",
      "   108|         0|            0|            0|  0.00%|        Ni, Nk = a.shape[:axis], a.shape[axis+1:]\n",
      "   109|         0|            0|            0|  0.00%|        Nj = indices.shape\n",
      "   110|         0|            0|            0|  0.00%|        for ii in ndindex(Ni):\n",
      "   111|         0|            0|            0|  0.00%|            for jj in ndindex(Nj):\n",
      "   112|         0|            0|            0|  0.00%|                for kk in ndindex(Nk):\n",
      "   113|         0|            0|            0|  0.00%|                    out[ii + jj + kk] = a[ii + (indices[jj],) + kk]\n",
      "   114|         0|            0|            0|  0.00%|\n",
      "   115|         0|            0|            0|  0.00%|    Parameters\n",
      "   116|         0|            0|            0|  0.00%|    ----------\n",
      "   117|         0|            0|            0|  0.00%|    a : array_like (Ni..., M, Nk...)\n",
      "   118|         0|            0|            0|  0.00%|        The source array.\n",
      "   119|         0|            0|            0|  0.00%|    indices : array_like (Nj...)\n",
      "   120|         0|            0|            0|  0.00%|        The indices of the values to extract.\n",
      "   121|         0|            0|            0|  0.00%|\n",
      "   122|         0|            0|            0|  0.00%|        .. versionadded:: 1.8.0\n",
      "   123|         0|            0|            0|  0.00%|\n",
      "   124|         0|            0|            0|  0.00%|        Also allow scalars for indices.\n",
      "   125|         0|            0|            0|  0.00%|    axis : int, optional\n",
      "   126|         0|            0|            0|  0.00%|        The axis over which to select values. By default, the flattened\n",
      "   127|         0|            0|            0|  0.00%|        input array is used.\n",
      "   128|         0|            0|            0|  0.00%|    out : ndarray, optional (Ni..., Nj..., Nk...)\n",
      "   129|         0|            0|            0|  0.00%|        If provided, the result will be placed in this array. It should\n",
      "   130|         0|            0|            0|  0.00%|        be of the appropriate shape and dtype. Note that `out` is always\n",
      "   131|         0|            0|            0|  0.00%|        buffered if `mode='raise'`; use other modes for better performance.\n",
      "   132|         0|            0|            0|  0.00%|    mode : {'raise', 'wrap', 'clip'}, optional\n",
      "   133|         0|            0|            0|  0.00%|        Specifies how out-of-bounds indices will behave.\n",
      "   134|         0|            0|            0|  0.00%|\n",
      "   135|         0|            0|            0|  0.00%|        * 'raise' -- raise an error (default)\n",
      "   136|         0|            0|            0|  0.00%|        * 'wrap' -- wrap around\n",
      "   137|         0|            0|            0|  0.00%|        * 'clip' -- clip to the range\n",
      "   138|         0|            0|            0|  0.00%|\n",
      "   139|         0|            0|            0|  0.00%|        'clip' mode means that all indices that are too large are replaced\n",
      "   140|         0|            0|            0|  0.00%|        by the index that addresses the last element along that axis. Note\n",
      "   141|         0|            0|            0|  0.00%|        that this disables indexing with negative numbers.\n",
      "   142|         0|            0|            0|  0.00%|\n",
      "   143|         0|            0|            0|  0.00%|    Returns\n",
      "   144|         0|            0|            0|  0.00%|    -------\n",
      "   145|         0|            0|            0|  0.00%|    out : ndarray (Ni..., Nj..., Nk...)\n",
      "   146|         0|            0|            0|  0.00%|        The returned array has the same type as `a`.\n",
      "   147|         0|            0|            0|  0.00%|\n",
      "   148|         0|            0|            0|  0.00%|    See Also\n",
      "   149|         0|            0|            0|  0.00%|    --------\n",
      "   150|         0|            0|            0|  0.00%|    compress : Take elements using a boolean mask\n",
      "   151|         0|            0|            0|  0.00%|    ndarray.take : equivalent method\n",
      "   152|         0|            0|            0|  0.00%|    take_along_axis : Take elements by matching the array and the index arrays\n",
      "   153|         0|            0|            0|  0.00%|\n",
      "   154|         0|            0|            0|  0.00%|    Notes\n",
      "   155|         0|            0|            0|  0.00%|    -----\n",
      "   156|         0|            0|            0|  0.00%|\n",
      "   157|         0|            0|            0|  0.00%|    By eliminating the inner loop in the description above, and using `s_` to\n",
      "   158|         0|            0|            0|  0.00%|    build simple slice objects, `take` can be expressed  in terms of applying\n",
      "   159|         0|            0|            0|  0.00%|    fancy indexing to each 1-d slice::\n",
      "   160|         0|            0|            0|  0.00%|\n",
      "   161|         0|            0|            0|  0.00%|        Ni, Nk = a.shape[:axis], a.shape[axis+1:]\n",
      "   162|         0|            0|            0|  0.00%|        for ii in ndindex(Ni):\n",
      "   163|         0|            0|            0|  0.00%|            for kk in ndindex(Nj):\n",
      "   164|         0|            0|            0|  0.00%|                out[ii + s_[...,] + kk] = a[ii + s_[:,] + kk][indices]\n",
      "   165|         0|            0|            0|  0.00%|\n",
      "   166|         0|            0|            0|  0.00%|    For this reason, it is equivalent to (but faster than) the following use\n",
      "   167|         0|            0|            0|  0.00%|    of `apply_along_axis`::\n",
      "   168|         0|            0|            0|  0.00%|\n",
      "   169|         0|            0|            0|  0.00%|        out = np.apply_along_axis(lambda a_1d: a_1d[indices], axis, a)\n",
      "   170|         0|            0|            0|  0.00%|\n",
      "   171|         0|            0|            0|  0.00%|    Examples\n",
      "   172|         0|            0|            0|  0.00%|    --------\n",
      "   173|         0|            0|            0|  0.00%|    >>> a = [4, 3, 5, 7, 6, 8]\n",
      "   174|         0|            0|            0|  0.00%|    >>> indices = [0, 1, 4]\n",
      "   175|         0|            0|            0|  0.00%|    >>> np.take(a, indices)\n",
      "   176|         0|            0|            0|  0.00%|    array([4, 3, 6])\n",
      "   177|         0|            0|            0|  0.00%|\n",
      "   178|         0|            0|            0|  0.00%|    In this example if `a` is an ndarray, \"fancy\" indexing can be used.\n",
      "   179|         0|            0|            0|  0.00%|\n",
      "   180|         0|            0|            0|  0.00%|    >>> a = np.array(a)\n",
      "   181|         0|            0|            0|  0.00%|    >>> a[indices]\n",
      "   182|         0|            0|            0|  0.00%|    array([4, 3, 6])\n",
      "   183|         0|            0|            0|  0.00%|\n",
      "   184|         0|            0|            0|  0.00%|    If `indices` is not one dimensional, the output also has these dimensions.\n",
      "   185|         0|            0|            0|  0.00%|\n",
      "   186|         0|            0|            0|  0.00%|    >>> np.take(a, [[0, 1], [2, 3]])\n",
      "   187|         0|            0|            0|  0.00%|    array([[4, 3],\n",
      "   188|         0|            0|            0|  0.00%|           [5, 7]])\n",
      "   189|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   190|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)\n",
      "   191|         0|            0|            0|  0.00%|\n",
      "   192|         0|            0|            0|  0.00%|\n",
      "   193|         0|            0|            0|  0.00%|def _reshape_dispatcher(a, newshape, order=None):\n",
      "   194|         0|            0|            0|  0.00%|    return (a,)\n",
      "   195|         0|            0|            0|  0.00%|\n",
      "   196|         0|            0|            0|  0.00%|\n",
      "   197|         0|            0|            0|  0.00%|# not deprecated --- copy if necessary, view otherwise\n",
      "   198|         0|            0|            0|  0.00%|@array_function_dispatch(_reshape_dispatcher)\n",
      "   199|         0|            0|            0|  0.00%|def reshape(a, newshape, order='C'):\n",
      "   200|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   201|         0|            0|            0|  0.00%|    Gives a new shape to an array without changing its data.\n",
      "   202|         0|            0|            0|  0.00%|\n",
      "   203|         0|            0|            0|  0.00%|    Parameters\n",
      "   204|         0|            0|            0|  0.00%|    ----------\n",
      "   205|         0|            0|            0|  0.00%|    a : array_like\n",
      "   206|         0|            0|            0|  0.00%|        Array to be reshaped.\n",
      "   207|         0|            0|            0|  0.00%|    newshape : int or tuple of ints\n",
      "   208|         0|            0|            0|  0.00%|        The new shape should be compatible with the original shape. If\n",
      "   209|         0|            0|            0|  0.00%|        an integer, then the result will be a 1-D array of that length.\n",
      "   210|         0|            0|            0|  0.00%|        One shape dimension can be -1. In this case, the value is\n",
      "   211|         0|            0|            0|  0.00%|        inferred from the length of the array and remaining dimensions.\n",
      "   212|         0|            0|            0|  0.00%|    order : {'C', 'F', 'A'}, optional\n",
      "   213|         0|            0|            0|  0.00%|        Read the elements of `a` using this index order, and place the\n",
      "   214|         0|            0|            0|  0.00%|        elements into the reshaped array using this index order.  'C'\n",
      "   215|         0|            0|            0|  0.00%|        means to read / write the elements using C-like index order,\n",
      "   216|         0|            0|            0|  0.00%|        with the last axis index changing fastest, back to the first\n",
      "   217|         0|            0|            0|  0.00%|        axis index changing slowest. 'F' means to read / write the\n",
      "   218|         0|            0|            0|  0.00%|        elements using Fortran-like index order, with the first index\n",
      "   219|         0|            0|            0|  0.00%|        changing fastest, and the last index changing slowest. Note that\n",
      "   220|         0|            0|            0|  0.00%|        the 'C' and 'F' options take no account of the memory layout of\n",
      "   221|         0|            0|            0|  0.00%|        the underlying array, and only refer to the order of indexing.\n",
      "   222|         0|            0|            0|  0.00%|        'A' means to read / write the elements in Fortran-like index\n",
      "   223|         0|            0|            0|  0.00%|        order if `a` is Fortran *contiguous* in memory, C-like order\n",
      "   224|         0|            0|            0|  0.00%|        otherwise.\n",
      "   225|         0|            0|            0|  0.00%|\n",
      "   226|         0|            0|            0|  0.00%|    Returns\n",
      "   227|         0|            0|            0|  0.00%|    -------\n",
      "   228|         0|            0|            0|  0.00%|    reshaped_array : ndarray\n",
      "   229|         0|            0|            0|  0.00%|        This will be a new view object if possible; otherwise, it will\n",
      "   230|         0|            0|            0|  0.00%|        be a copy.  Note there is no guarantee of the *memory layout* (C- or\n",
      "   231|         0|            0|            0|  0.00%|        Fortran- contiguous) of the returned array.\n",
      "   232|         0|            0|            0|  0.00%|\n",
      "   233|         0|            0|            0|  0.00%|    See Also\n",
      "   234|         0|            0|            0|  0.00%|    --------\n",
      "   235|         0|            0|            0|  0.00%|    ndarray.reshape : Equivalent method.\n",
      "   236|         0|            0|            0|  0.00%|\n",
      "   237|         0|            0|            0|  0.00%|    Notes\n",
      "   238|         0|            0|            0|  0.00%|    -----\n",
      "   239|         0|            0|            0|  0.00%|    It is not always possible to change the shape of an array without\n",
      "   240|         0|            0|            0|  0.00%|    copying the data. If you want an error to be raised when the data is copied,\n",
      "   241|         0|            0|            0|  0.00%|    you should assign the new shape to the shape attribute of the array::\n",
      "   242|         0|            0|            0|  0.00%|\n",
      "   243|         0|            0|            0|  0.00%|     >>> a = np.zeros((10, 2))\n",
      "   244|         0|            0|            0|  0.00%|\n",
      "   245|         0|            0|            0|  0.00%|     # A transpose makes the array non-contiguous\n",
      "   246|         0|            0|            0|  0.00%|     >>> b = a.T\n",
      "   247|         0|            0|            0|  0.00%|\n",
      "   248|         0|            0|            0|  0.00%|     # Taking a view makes it possible to modify the shape without modifying\n",
      "   249|         0|            0|            0|  0.00%|     # the initial object.\n",
      "   250|         0|            0|            0|  0.00%|     >>> c = b.view()\n",
      "   251|         0|            0|            0|  0.00%|     >>> c.shape = (20)\n",
      "   252|         0|            0|            0|  0.00%|     Traceback (most recent call last):\n",
      "   253|         0|            0|            0|  0.00%|        ...\n",
      "   254|         0|            0|            0|  0.00%|     AttributeError: Incompatible shape for in-place modification. Use\n",
      "   255|         0|            0|            0|  0.00%|     `.reshape()` to make a copy with the desired shape.\n",
      "   256|         0|            0|            0|  0.00%|\n",
      "   257|         0|            0|            0|  0.00%|    The `order` keyword gives the index ordering both for *fetching* the values\n",
      "   258|         0|            0|            0|  0.00%|    from `a`, and then *placing* the values into the output array.\n",
      "   259|         0|            0|            0|  0.00%|    For example, let's say you have an array:\n",
      "   260|         0|            0|            0|  0.00%|\n",
      "   261|         0|            0|            0|  0.00%|    >>> a = np.arange(6).reshape((3, 2))\n",
      "   262|         0|            0|            0|  0.00%|    >>> a\n",
      "   263|         0|            0|            0|  0.00%|    array([[0, 1],\n",
      "   264|         0|            0|            0|  0.00%|           [2, 3],\n",
      "   265|         0|            0|            0|  0.00%|           [4, 5]])\n",
      "   266|         0|            0|            0|  0.00%|\n",
      "   267|         0|            0|            0|  0.00%|    You can think of reshaping as first raveling the array (using the given\n",
      "   268|         0|            0|            0|  0.00%|    index order), then inserting the elements from the raveled array into the\n",
      "   269|         0|            0|            0|  0.00%|    new array using the same kind of index ordering as was used for the\n",
      "   270|         0|            0|            0|  0.00%|    raveling.\n",
      "   271|         0|            0|            0|  0.00%|\n",
      "   272|         0|            0|            0|  0.00%|    >>> np.reshape(a, (2, 3)) # C-like index ordering\n",
      "   273|         0|            0|            0|  0.00%|    array([[0, 1, 2],\n",
      "   274|         0|            0|            0|  0.00%|           [3, 4, 5]])\n",
      "   275|         0|            0|            0|  0.00%|    >>> np.reshape(np.ravel(a), (2, 3)) # equivalent to C ravel then C reshape\n",
      "   276|         0|            0|            0|  0.00%|    array([[0, 1, 2],\n",
      "   277|         0|            0|            0|  0.00%|           [3, 4, 5]])\n",
      "   278|         0|            0|            0|  0.00%|    >>> np.reshape(a, (2, 3), order='F') # Fortran-like index ordering\n",
      "   279|         0|            0|            0|  0.00%|    array([[0, 4, 3],\n",
      "   280|         0|            0|            0|  0.00%|           [2, 1, 5]])\n",
      "   281|         0|            0|            0|  0.00%|    >>> np.reshape(np.ravel(a, order='F'), (2, 3), order='F')\n",
      "   282|         0|            0|            0|  0.00%|    array([[0, 4, 3],\n",
      "   283|         0|            0|            0|  0.00%|           [2, 1, 5]])\n",
      "   284|         0|            0|            0|  0.00%|\n",
      "   285|         0|            0|            0|  0.00%|    Examples\n",
      "   286|         0|            0|            0|  0.00%|    --------\n",
      "   287|         0|            0|            0|  0.00%|    >>> a = np.array([[1,2,3], [4,5,6]])\n",
      "   288|         0|            0|            0|  0.00%|    >>> np.reshape(a, 6)\n",
      "   289|         0|            0|            0|  0.00%|    array([1, 2, 3, 4, 5, 6])\n",
      "   290|         0|            0|            0|  0.00%|    >>> np.reshape(a, 6, order='F')\n",
      "   291|         0|            0|            0|  0.00%|    array([1, 4, 2, 5, 3, 6])\n",
      "   292|         0|            0|            0|  0.00%|\n",
      "   293|         0|            0|            0|  0.00%|    >>> np.reshape(a, (3,-1))       # the unspecified value is inferred to be 2\n",
      "   294|         0|            0|            0|  0.00%|    array([[1, 2],\n",
      "   295|         0|            0|            0|  0.00%|           [3, 4],\n",
      "   296|         0|            0|            0|  0.00%|           [5, 6]])\n",
      "   297|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   298|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'reshape', newshape, order=order)\n",
      "   299|         0|            0|            0|  0.00%|\n",
      "   300|         0|            0|            0|  0.00%|\n",
      "   301|         0|            0|            0|  0.00%|def _choose_dispatcher(a, choices, out=None, mode=None):\n",
      "   302|         0|            0|            0|  0.00%|    yield a\n",
      "   303|         0|            0|            0|  0.00%|    yield from choices\n",
      "   304|         0|            0|            0|  0.00%|    yield out\n",
      "   305|         0|            0|            0|  0.00%|\n",
      "   306|         0|            0|            0|  0.00%|\n",
      "   307|         0|            0|            0|  0.00%|@array_function_dispatch(_choose_dispatcher)\n",
      "   308|         0|            0|            0|  0.00%|def choose(a, choices, out=None, mode='raise'):\n",
      "   309|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   310|         0|            0|            0|  0.00%|    Construct an array from an index array and a list of arrays to choose from.\n",
      "   311|         0|            0|            0|  0.00%|\n",
      "   312|         0|            0|            0|  0.00%|    First of all, if confused or uncertain, definitely look at the Examples -\n",
      "   313|         0|            0|            0|  0.00%|    in its full generality, this function is less simple than it might\n",
      "   314|         0|            0|            0|  0.00%|    seem from the following code description (below ndi =\n",
      "   315|         0|            0|            0|  0.00%|    `numpy.lib.index_tricks`):\n",
      "   316|         0|            0|            0|  0.00%|\n",
      "   317|         0|            0|            0|  0.00%|    ``np.choose(a,c) == np.array([c[a[I]][I] for I in ndi.ndindex(a.shape)])``.\n",
      "   318|         0|            0|            0|  0.00%|\n",
      "   319|         0|            0|            0|  0.00%|    But this omits some subtleties.  Here is a fully general summary:\n",
      "   320|         0|            0|            0|  0.00%|\n",
      "   321|         0|            0|            0|  0.00%|    Given an \"index\" array (`a`) of integers and a sequence of ``n`` arrays\n",
      "   322|         0|            0|            0|  0.00%|    (`choices`), `a` and each choice array are first broadcast, as necessary,\n",
      "   323|         0|            0|            0|  0.00%|    to arrays of a common shape; calling these *Ba* and *Bchoices[i], i =\n",
      "   324|         0|            0|            0|  0.00%|    0,...,n-1* we have that, necessarily, ``Ba.shape == Bchoices[i].shape``\n",
      "   325|         0|            0|            0|  0.00%|    for each ``i``.  Then, a new array with shape ``Ba.shape`` is created as\n",
      "   326|         0|            0|            0|  0.00%|    follows:\n",
      "   327|         0|            0|            0|  0.00%|\n",
      "   328|         0|            0|            0|  0.00%|    * if ``mode='raise'`` (the default), then, first of all, each element of\n",
      "   329|         0|            0|            0|  0.00%|      ``a`` (and thus ``Ba``) must be in the range ``[0, n-1]``; now, suppose\n",
      "   330|         0|            0|            0|  0.00%|      that ``i`` (in that range) is the value at the ``(j0, j1, ..., jm)``\n",
      "   331|         0|            0|            0|  0.00%|      position in ``Ba`` - then the value at the same position in the new array\n",
      "   332|         0|            0|            0|  0.00%|      is the value in ``Bchoices[i]`` at that same position;\n",
      "   333|         0|            0|            0|  0.00%|\n",
      "   334|         0|            0|            0|  0.00%|    * if ``mode='wrap'``, values in `a` (and thus `Ba`) may be any (signed)\n",
      "   335|         0|            0|            0|  0.00%|      integer; modular arithmetic is used to map integers outside the range\n",
      "   336|         0|            0|            0|  0.00%|      `[0, n-1]` back into that range; and then the new array is constructed\n",
      "   337|         0|            0|            0|  0.00%|      as above;\n",
      "   338|         0|            0|            0|  0.00%|\n",
      "   339|         0|            0|            0|  0.00%|    * if ``mode='clip'``, values in `a` (and thus ``Ba``) may be any (signed)\n",
      "   340|         0|            0|            0|  0.00%|      integer; negative integers are mapped to 0; values greater than ``n-1``\n",
      "   341|         0|            0|            0|  0.00%|      are mapped to ``n-1``; and then the new array is constructed as above.\n",
      "   342|         0|            0|            0|  0.00%|\n",
      "   343|         0|            0|            0|  0.00%|    Parameters\n",
      "   344|         0|            0|            0|  0.00%|    ----------\n",
      "   345|         0|            0|            0|  0.00%|    a : int array\n",
      "   346|         0|            0|            0|  0.00%|        This array must contain integers in ``[0, n-1]``, where ``n`` is the\n",
      "   347|         0|            0|            0|  0.00%|        number of choices, unless ``mode=wrap`` or ``mode=clip``, in which\n",
      "   348|         0|            0|            0|  0.00%|        cases any integers are permissible.\n",
      "   349|         0|            0|            0|  0.00%|    choices : sequence of arrays\n",
      "   350|         0|            0|            0|  0.00%|        Choice arrays. `a` and all of the choices must be broadcastable to the\n",
      "   351|         0|            0|            0|  0.00%|        same shape.  If `choices` is itself an array (not recommended), then\n",
      "   352|         0|            0|            0|  0.00%|        its outermost dimension (i.e., the one corresponding to\n",
      "   353|         0|            0|            0|  0.00%|        ``choices.shape[0]``) is taken as defining the \"sequence\".\n",
      "   354|         0|            0|            0|  0.00%|    out : array, optional\n",
      "   355|         0|            0|            0|  0.00%|        If provided, the result will be inserted into this array. It should\n",
      "   356|         0|            0|            0|  0.00%|        be of the appropriate shape and dtype. Note that `out` is always\n",
      "   357|         0|            0|            0|  0.00%|        buffered if ``mode='raise'``; use other modes for better performance.\n",
      "   358|         0|            0|            0|  0.00%|    mode : {'raise' (default), 'wrap', 'clip'}, optional\n",
      "   359|         0|            0|            0|  0.00%|        Specifies how indices outside ``[0, n-1]`` will be treated:\n",
      "   360|         0|            0|            0|  0.00%|\n",
      "   361|         0|            0|            0|  0.00%|          * 'raise' : an exception is raised\n",
      "   362|         0|            0|            0|  0.00%|          * 'wrap' : value becomes value mod ``n``\n",
      "   363|         0|            0|            0|  0.00%|          * 'clip' : values < 0 are mapped to 0, values > n-1 are mapped to n-1\n",
      "   364|         0|            0|            0|  0.00%|\n",
      "   365|         0|            0|            0|  0.00%|    Returns\n",
      "   366|         0|            0|            0|  0.00%|    -------\n",
      "   367|         0|            0|            0|  0.00%|    merged_array : array\n",
      "   368|         0|            0|            0|  0.00%|        The merged result.\n",
      "   369|         0|            0|            0|  0.00%|\n",
      "   370|         0|            0|            0|  0.00%|    Raises\n",
      "   371|         0|            0|            0|  0.00%|    ------\n",
      "   372|         0|            0|            0|  0.00%|    ValueError: shape mismatch\n",
      "   373|         0|            0|            0|  0.00%|        If `a` and each choice array are not all broadcastable to the same\n",
      "   374|         0|            0|            0|  0.00%|        shape.\n",
      "   375|         0|            0|            0|  0.00%|\n",
      "   376|         0|            0|            0|  0.00%|    See Also\n",
      "   377|         0|            0|            0|  0.00%|    --------\n",
      "   378|         0|            0|            0|  0.00%|    ndarray.choose : equivalent method\n",
      "   379|         0|            0|            0|  0.00%|    numpy.take_along_axis : Preferable if `choices` is an array\n",
      "   380|         0|            0|            0|  0.00%|\n",
      "   381|         0|            0|            0|  0.00%|    Notes\n",
      "   382|         0|            0|            0|  0.00%|    -----\n",
      "   383|         0|            0|            0|  0.00%|    To reduce the chance of misinterpretation, even though the following\n",
      "   384|         0|            0|            0|  0.00%|    \"abuse\" is nominally supported, `choices` should neither be, nor be\n",
      "   385|         0|            0|            0|  0.00%|    thought of as, a single array, i.e., the outermost sequence-like container\n",
      "   386|         0|            0|            0|  0.00%|    should be either a list or a tuple.\n",
      "   387|         0|            0|            0|  0.00%|\n",
      "   388|         0|            0|            0|  0.00%|    Examples\n",
      "   389|         0|            0|            0|  0.00%|    --------\n",
      "   390|         0|            0|            0|  0.00%|\n",
      "   391|         0|            0|            0|  0.00%|    >>> choices = [[0, 1, 2, 3], [10, 11, 12, 13],\n",
      "   392|         0|            0|            0|  0.00%|    ...   [20, 21, 22, 23], [30, 31, 32, 33]]\n",
      "   393|         0|            0|            0|  0.00%|    >>> np.choose([2, 3, 1, 0], choices\n",
      "   394|         0|            0|            0|  0.00%|    ... # the first element of the result will be the first element of the\n",
      "   395|         0|            0|            0|  0.00%|    ... # third (2+1) \"array\" in choices, namely, 20; the second element\n",
      "   396|         0|            0|            0|  0.00%|    ... # will be the second element of the fourth (3+1) choice array, i.e.,\n",
      "   397|         0|            0|            0|  0.00%|    ... # 31, etc.\n",
      "   398|         0|            0|            0|  0.00%|    ... )\n",
      "   399|         0|            0|            0|  0.00%|    array([20, 31, 12,  3])\n",
      "   400|         0|            0|            0|  0.00%|    >>> np.choose([2, 4, 1, 0], choices, mode='clip') # 4 goes to 3 (4-1)\n",
      "   401|         0|            0|            0|  0.00%|    array([20, 31, 12,  3])\n",
      "   402|         0|            0|            0|  0.00%|    >>> # because there are 4 choice arrays\n",
      "   403|         0|            0|            0|  0.00%|    >>> np.choose([2, 4, 1, 0], choices, mode='wrap') # 4 goes to (4 mod 4)\n",
      "   404|         0|            0|            0|  0.00%|    array([20,  1, 12,  3])\n",
      "   405|         0|            0|            0|  0.00%|    >>> # i.e., 0\n",
      "   406|         0|            0|            0|  0.00%|\n",
      "   407|         0|            0|            0|  0.00%|    A couple examples illustrating how choose broadcasts:\n",
      "   408|         0|            0|            0|  0.00%|\n",
      "   409|         0|            0|            0|  0.00%|    >>> a = [[1, 0, 1], [0, 1, 0], [1, 0, 1]]\n",
      "   410|         0|            0|            0|  0.00%|    >>> choices = [-10, 10]\n",
      "   411|         0|            0|            0|  0.00%|    >>> np.choose(a, choices)\n",
      "   412|         0|            0|            0|  0.00%|    array([[ 10, -10,  10],\n",
      "   413|         0|            0|            0|  0.00%|           [-10,  10, -10],\n",
      "   414|         0|            0|            0|  0.00%|           [ 10, -10,  10]])\n",
      "   415|         0|            0|            0|  0.00%|\n",
      "   416|         0|            0|            0|  0.00%|    >>> # With thanks to Anne Archibald\n",
      "   417|         0|            0|            0|  0.00%|    >>> a = np.array([0, 1]).reshape((2,1,1))\n",
      "   418|         0|            0|            0|  0.00%|    >>> c1 = np.array([1, 2, 3]).reshape((1,3,1))\n",
      "   419|         0|            0|            0|  0.00%|    >>> c2 = np.array([-1, -2, -3, -4, -5]).reshape((1,1,5))\n",
      "   420|         0|            0|            0|  0.00%|    >>> np.choose(a, (c1, c2)) # result is 2x3x5, res[0,:,:]=c1, res[1,:,:]=c2\n",
      "   421|         0|            0|            0|  0.00%|    array([[[ 1,  1,  1,  1,  1],\n",
      "   422|         0|            0|            0|  0.00%|            [ 2,  2,  2,  2,  2],\n",
      "   423|         0|            0|            0|  0.00%|            [ 3,  3,  3,  3,  3]],\n",
      "   424|         0|            0|            0|  0.00%|           [[-1, -2, -3, -4, -5],\n",
      "   425|         0|            0|            0|  0.00%|            [-1, -2, -3, -4, -5],\n",
      "   426|         0|            0|            0|  0.00%|            [-1, -2, -3, -4, -5]]])\n",
      "   427|         0|            0|            0|  0.00%|\n",
      "   428|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   429|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'choose', choices, out=out, mode=mode)\n",
      "   430|         0|            0|            0|  0.00%|\n",
      "   431|         0|            0|            0|  0.00%|\n",
      "   432|         0|            0|            0|  0.00%|def _repeat_dispatcher(a, repeats, axis=None):\n",
      "   433|         0|            0|            0|  0.00%|    return (a,)\n",
      "   434|         0|            0|            0|  0.00%|\n",
      "   435|         0|            0|            0|  0.00%|\n",
      "   436|         0|            0|            0|  0.00%|@array_function_dispatch(_repeat_dispatcher)\n",
      "   437|         0|            0|            0|  0.00%|def repeat(a, repeats, axis=None):\n",
      "   438|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   439|         0|            0|            0|  0.00%|    Repeat elements of an array.\n",
      "   440|         0|            0|            0|  0.00%|\n",
      "   441|         0|            0|            0|  0.00%|    Parameters\n",
      "   442|         0|            0|            0|  0.00%|    ----------\n",
      "   443|         0|            0|            0|  0.00%|    a : array_like\n",
      "   444|         0|            0|            0|  0.00%|        Input array.\n",
      "   445|         0|            0|            0|  0.00%|    repeats : int or array of ints\n",
      "   446|         0|            0|            0|  0.00%|        The number of repetitions for each element.  `repeats` is broadcasted\n",
      "   447|         0|            0|            0|  0.00%|        to fit the shape of the given axis.\n",
      "   448|         0|            0|            0|  0.00%|    axis : int, optional\n",
      "   449|         0|            0|            0|  0.00%|        The axis along which to repeat values.  By default, use the\n",
      "   450|         0|            0|            0|  0.00%|        flattened input array, and return a flat output array.\n",
      "   451|         0|            0|            0|  0.00%|\n",
      "   452|         0|            0|            0|  0.00%|    Returns\n",
      "   453|         0|            0|            0|  0.00%|    -------\n",
      "   454|         0|            0|            0|  0.00%|    repeated_array : ndarray\n",
      "   455|         0|            0|            0|  0.00%|        Output array which has the same shape as `a`, except along\n",
      "   456|         0|            0|            0|  0.00%|        the given axis.\n",
      "   457|         0|            0|            0|  0.00%|\n",
      "   458|         0|            0|            0|  0.00%|    See Also\n",
      "   459|         0|            0|            0|  0.00%|    --------\n",
      "   460|         0|            0|            0|  0.00%|    tile : Tile an array.\n",
      "   461|         0|            0|            0|  0.00%|    unique : Find the unique elements of an array.\n",
      "   462|         0|            0|            0|  0.00%|\n",
      "   463|         0|            0|            0|  0.00%|    Examples\n",
      "   464|         0|            0|            0|  0.00%|    --------\n",
      "   465|         0|            0|            0|  0.00%|    >>> np.repeat(3, 4)\n",
      "   466|         0|            0|            0|  0.00%|    array([3, 3, 3, 3])\n",
      "   467|         0|            0|            0|  0.00%|    >>> x = np.array([[1,2],[3,4]])\n",
      "   468|         0|            0|            0|  0.00%|    >>> np.repeat(x, 2)\n",
      "   469|         0|            0|            0|  0.00%|    array([1, 1, 2, 2, 3, 3, 4, 4])\n",
      "   470|         0|            0|            0|  0.00%|    >>> np.repeat(x, 3, axis=1)\n",
      "   471|         0|            0|            0|  0.00%|    array([[1, 1, 1, 2, 2, 2],\n",
      "   472|         0|            0|            0|  0.00%|           [3, 3, 3, 4, 4, 4]])\n",
      "   473|         0|            0|            0|  0.00%|    >>> np.repeat(x, [1, 2], axis=0)\n",
      "   474|         0|            0|            0|  0.00%|    array([[1, 2],\n",
      "   475|         0|            0|            0|  0.00%|           [3, 4],\n",
      "   476|         0|            0|            0|  0.00%|           [3, 4]])\n",
      "   477|         0|            0|            0|  0.00%|\n",
      "   478|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   479|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'repeat', repeats, axis=axis)\n",
      "   480|         0|            0|            0|  0.00%|\n",
      "   481|         0|            0|            0|  0.00%|\n",
      "   482|         0|            0|            0|  0.00%|def _put_dispatcher(a, ind, v, mode=None):\n",
      "   483|         0|            0|            0|  0.00%|    return (a, ind, v)\n",
      "   484|         0|            0|            0|  0.00%|\n",
      "   485|         0|            0|            0|  0.00%|\n",
      "   486|         0|            0|            0|  0.00%|@array_function_dispatch(_put_dispatcher)\n",
      "   487|         0|            0|            0|  0.00%|def put(a, ind, v, mode='raise'):\n",
      "   488|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   489|         0|            0|            0|  0.00%|    Replaces specified elements of an array with given values.\n",
      "   490|         0|            0|            0|  0.00%|\n",
      "   491|         0|            0|            0|  0.00%|    The indexing works on the flattened target array. `put` is roughly\n",
      "   492|         0|            0|            0|  0.00%|    equivalent to:\n",
      "   493|         0|            0|            0|  0.00%|\n",
      "   494|         0|            0|            0|  0.00%|    ::\n",
      "   495|         0|            0|            0|  0.00%|\n",
      "   496|         0|            0|            0|  0.00%|        a.flat[ind] = v\n",
      "   497|         0|            0|            0|  0.00%|\n",
      "   498|         0|            0|            0|  0.00%|    Parameters\n",
      "   499|         0|            0|            0|  0.00%|    ----------\n",
      "   500|         0|            0|            0|  0.00%|    a : ndarray\n",
      "   501|         0|            0|            0|  0.00%|        Target array.\n",
      "   502|         0|            0|            0|  0.00%|    ind : array_like\n",
      "   503|         0|            0|            0|  0.00%|        Target indices, interpreted as integers.\n",
      "   504|         0|            0|            0|  0.00%|    v : array_like\n",
      "   505|         0|            0|            0|  0.00%|        Values to place in `a` at target indices. If `v` is shorter than\n",
      "   506|         0|            0|            0|  0.00%|        `ind` it will be repeated as necessary.\n",
      "   507|         0|            0|            0|  0.00%|    mode : {'raise', 'wrap', 'clip'}, optional\n",
      "   508|         0|            0|            0|  0.00%|        Specifies how out-of-bounds indices will behave.\n",
      "   509|         0|            0|            0|  0.00%|\n",
      "   510|         0|            0|            0|  0.00%|        * 'raise' -- raise an error (default)\n",
      "   511|         0|            0|            0|  0.00%|        * 'wrap' -- wrap around\n",
      "   512|         0|            0|            0|  0.00%|        * 'clip' -- clip to the range\n",
      "   513|         0|            0|            0|  0.00%|\n",
      "   514|         0|            0|            0|  0.00%|        'clip' mode means that all indices that are too large are replaced\n",
      "   515|         0|            0|            0|  0.00%|        by the index that addresses the last element along that axis. Note\n",
      "   516|         0|            0|            0|  0.00%|        that this disables indexing with negative numbers. In 'raise' mode,\n",
      "   517|         0|            0|            0|  0.00%|        if an exception occurs the target array may still be modified.\n",
      "   518|         0|            0|            0|  0.00%|\n",
      "   519|         0|            0|            0|  0.00%|    See Also\n",
      "   520|         0|            0|            0|  0.00%|    --------\n",
      "   521|         0|            0|            0|  0.00%|    putmask, place\n",
      "   522|         0|            0|            0|  0.00%|    put_along_axis : Put elements by matching the array and the index arrays\n",
      "   523|         0|            0|            0|  0.00%|\n",
      "   524|         0|            0|            0|  0.00%|    Examples\n",
      "   525|         0|            0|            0|  0.00%|    --------\n",
      "   526|         0|            0|            0|  0.00%|    >>> a = np.arange(5)\n",
      "   527|         0|            0|            0|  0.00%|    >>> np.put(a, [0, 2], [-44, -55])\n",
      "   528|         0|            0|            0|  0.00%|    >>> a\n",
      "   529|         0|            0|            0|  0.00%|    array([-44,   1, -55,   3,   4])\n",
      "   530|         0|            0|            0|  0.00%|\n",
      "   531|         0|            0|            0|  0.00%|    >>> a = np.arange(5)\n",
      "   532|         0|            0|            0|  0.00%|    >>> np.put(a, 22, -5, mode='clip')\n",
      "   533|         0|            0|            0|  0.00%|    >>> a\n",
      "   534|         0|            0|            0|  0.00%|    array([ 0,  1,  2,  3, -5])\n",
      "   535|         0|            0|            0|  0.00%|\n",
      "   536|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   537|         0|            0|            0|  0.00%|    try:\n",
      "   538|         0|            0|            0|  0.00%|        put = a.put\n",
      "   539|         0|            0|            0|  0.00%|    except AttributeError as e:\n",
      "   540|         0|            0|            0|  0.00%|        raise TypeError(\"argument 1 must be numpy.ndarray, \"\n",
      "   541|         0|            0|            0|  0.00%|                        \"not {name}\".format(name=type(a).__name__)) from e\n",
      "   542|         0|            0|            0|  0.00%|\n",
      "   543|         0|            0|            0|  0.00%|    return put(ind, v, mode=mode)\n",
      "   544|         0|            0|            0|  0.00%|\n",
      "   545|         0|            0|            0|  0.00%|\n",
      "   546|         0|            0|            0|  0.00%|def _swapaxes_dispatcher(a, axis1, axis2):\n",
      "   547|         0|            0|            0|  0.00%|    return (a,)\n",
      "   548|         0|            0|            0|  0.00%|\n",
      "   549|         0|            0|            0|  0.00%|\n",
      "   550|         0|            0|            0|  0.00%|@array_function_dispatch(_swapaxes_dispatcher)\n",
      "   551|         0|            0|            0|  0.00%|def swapaxes(a, axis1, axis2):\n",
      "   552|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   553|         0|            0|            0|  0.00%|    Interchange two axes of an array.\n",
      "   554|         0|            0|            0|  0.00%|\n",
      "   555|         0|            0|            0|  0.00%|    Parameters\n",
      "   556|         0|            0|            0|  0.00%|    ----------\n",
      "   557|         0|            0|            0|  0.00%|    a : array_like\n",
      "   558|         0|            0|            0|  0.00%|        Input array.\n",
      "   559|         0|            0|            0|  0.00%|    axis1 : int\n",
      "   560|         0|            0|            0|  0.00%|        First axis.\n",
      "   561|         0|            0|            0|  0.00%|    axis2 : int\n",
      "   562|         0|            0|            0|  0.00%|        Second axis.\n",
      "   563|         0|            0|            0|  0.00%|\n",
      "   564|         0|            0|            0|  0.00%|    Returns\n",
      "   565|         0|            0|            0|  0.00%|    -------\n",
      "   566|         0|            0|            0|  0.00%|    a_swapped : ndarray\n",
      "   567|         0|            0|            0|  0.00%|        For NumPy >= 1.10.0, if `a` is an ndarray, then a view of `a` is\n",
      "   568|         0|            0|            0|  0.00%|        returned; otherwise a new array is created. For earlier NumPy\n",
      "   569|         0|            0|            0|  0.00%|        versions a view of `a` is returned only if the order of the\n",
      "   570|         0|            0|            0|  0.00%|        axes is changed, otherwise the input array is returned.\n",
      "   571|         0|            0|            0|  0.00%|\n",
      "   572|         0|            0|            0|  0.00%|    Examples\n",
      "   573|         0|            0|            0|  0.00%|    --------\n",
      "   574|         0|            0|            0|  0.00%|    >>> x = np.array([[1,2,3]])\n",
      "   575|         0|            0|            0|  0.00%|    >>> np.swapaxes(x,0,1)\n",
      "   576|         0|            0|            0|  0.00%|    array([[1],\n",
      "   577|         0|            0|            0|  0.00%|           [2],\n",
      "   578|         0|            0|            0|  0.00%|           [3]])\n",
      "   579|         0|            0|            0|  0.00%|\n",
      "   580|         0|            0|            0|  0.00%|    >>> x = np.array([[[0,1],[2,3]],[[4,5],[6,7]]])\n",
      "   581|         0|            0|            0|  0.00%|    >>> x\n",
      "   582|         0|            0|            0|  0.00%|    array([[[0, 1],\n",
      "   583|         0|            0|            0|  0.00%|            [2, 3]],\n",
      "   584|         0|            0|            0|  0.00%|           [[4, 5],\n",
      "   585|         0|            0|            0|  0.00%|            [6, 7]]])\n",
      "   586|         0|            0|            0|  0.00%|\n",
      "   587|         0|            0|            0|  0.00%|    >>> np.swapaxes(x,0,2)\n",
      "   588|         0|            0|            0|  0.00%|    array([[[0, 4],\n",
      "   589|         0|            0|            0|  0.00%|            [2, 6]],\n",
      "   590|         0|            0|            0|  0.00%|           [[1, 5],\n",
      "   591|         0|            0|            0|  0.00%|            [3, 7]]])\n",
      "   592|         0|            0|            0|  0.00%|\n",
      "   593|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   594|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'swapaxes', axis1, axis2)\n",
      "   595|         0|            0|            0|  0.00%|\n",
      "   596|         0|            0|            0|  0.00%|\n",
      "   597|         0|            0|            0|  0.00%|def _transpose_dispatcher(a, axes=None):\n",
      "   598|         0|            0|            0|  0.00%|    return (a,)\n",
      "   599|         0|            0|            0|  0.00%|\n",
      "   600|         0|            0|            0|  0.00%|\n",
      "   601|         0|            0|            0|  0.00%|@array_function_dispatch(_transpose_dispatcher)\n",
      "   602|         0|            0|            0|  0.00%|def transpose(a, axes=None):\n",
      "   603|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   604|         0|            0|            0|  0.00%|    Reverse or permute the axes of an array; returns the modified array.\n",
      "   605|         0|            0|            0|  0.00%|\n",
      "   606|         0|            0|            0|  0.00%|    For an array a with two axes, transpose(a) gives the matrix transpose.\n",
      "   607|         0|            0|            0|  0.00%|\n",
      "   608|         0|            0|            0|  0.00%|    Refer to `numpy.ndarray.transpose` for full documentation.\n",
      "   609|         0|            0|            0|  0.00%|\n",
      "   610|         0|            0|            0|  0.00%|    Parameters\n",
      "   611|         0|            0|            0|  0.00%|    ----------\n",
      "   612|         0|            0|            0|  0.00%|    a : array_like\n",
      "   613|         0|            0|            0|  0.00%|        Input array.\n",
      "   614|         0|            0|            0|  0.00%|    axes : tuple or list of ints, optional\n",
      "   615|         0|            0|            0|  0.00%|        If specified, it must be a tuple or list which contains a permutation of\n",
      "   616|         0|            0|            0|  0.00%|        [0,1,..,N-1] where N is the number of axes of a.  The i'th axis of the\n",
      "   617|         0|            0|            0|  0.00%|        returned array will correspond to the axis numbered ``axes[i]`` of the\n",
      "   618|         0|            0|            0|  0.00%|        input.  If not specified, defaults to ``range(a.ndim)[::-1]``, which\n",
      "   619|         0|            0|            0|  0.00%|        reverses the order of the axes.\n",
      "   620|         0|            0|            0|  0.00%|\n",
      "   621|         0|            0|            0|  0.00%|    Returns\n",
      "   622|         0|            0|            0|  0.00%|    -------\n",
      "   623|         0|            0|            0|  0.00%|    p : ndarray\n",
      "   624|         0|            0|            0|  0.00%|        `a` with its axes permuted.  A view is returned whenever\n",
      "   625|         0|            0|            0|  0.00%|        possible.\n",
      "   626|         0|            0|            0|  0.00%|\n",
      "   627|         0|            0|            0|  0.00%|    See Also\n",
      "   628|         0|            0|            0|  0.00%|    --------\n",
      "   629|         0|            0|            0|  0.00%|    ndarray.transpose : Equivalent method\n",
      "   630|         0|            0|            0|  0.00%|    moveaxis\n",
      "   631|         0|            0|            0|  0.00%|    argsort\n",
      "   632|         0|            0|            0|  0.00%|\n",
      "   633|         0|            0|            0|  0.00%|    Notes\n",
      "   634|         0|            0|            0|  0.00%|    -----\n",
      "   635|         0|            0|            0|  0.00%|    Use `transpose(a, argsort(axes))` to invert the transposition of tensors\n",
      "   636|         0|            0|            0|  0.00%|    when using the `axes` keyword argument.\n",
      "   637|         0|            0|            0|  0.00%|\n",
      "   638|         0|            0|            0|  0.00%|    Transposing a 1-D array returns an unchanged view of the original array.\n",
      "   639|         0|            0|            0|  0.00%|\n",
      "   640|         0|            0|            0|  0.00%|    Examples\n",
      "   641|         0|            0|            0|  0.00%|    --------\n",
      "   642|         0|            0|            0|  0.00%|    >>> x = np.arange(4).reshape((2,2))\n",
      "   643|         0|            0|            0|  0.00%|    >>> x\n",
      "   644|         0|            0|            0|  0.00%|    array([[0, 1],\n",
      "   645|         0|            0|            0|  0.00%|           [2, 3]])\n",
      "   646|         0|            0|            0|  0.00%|\n",
      "   647|         0|            0|            0|  0.00%|    >>> np.transpose(x)\n",
      "   648|         0|            0|            0|  0.00%|    array([[0, 2],\n",
      "   649|         0|            0|            0|  0.00%|           [1, 3]])\n",
      "   650|         0|            0|            0|  0.00%|\n",
      "   651|         0|            0|            0|  0.00%|    >>> x = np.ones((1, 2, 3))\n",
      "   652|         0|            0|            0|  0.00%|    >>> np.transpose(x, (1, 0, 2)).shape\n",
      "   653|         0|            0|            0|  0.00%|    (2, 1, 3)\n",
      "   654|         0|            0|            0|  0.00%|\n",
      "   655|         0|            0|            0|  0.00%|    >>> x = np.ones((2, 3, 4, 5))\n",
      "   656|         0|            0|            0|  0.00%|    >>> np.transpose(x).shape\n",
      "   657|         0|            0|            0|  0.00%|    (5, 4, 3, 2)\n",
      "   658|         0|            0|            0|  0.00%|\n",
      "   659|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   660|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'transpose', axes)\n",
      "   661|         0|            0|            0|  0.00%|\n",
      "   662|         0|            0|            0|  0.00%|\n",
      "   663|         0|            0|            0|  0.00%|def _partition_dispatcher(a, kth, axis=None, kind=None, order=None):\n",
      "   664|         0|            0|            0|  0.00%|    return (a,)\n",
      "   665|         0|            0|            0|  0.00%|\n",
      "   666|         0|            0|            0|  0.00%|\n",
      "   667|         0|            0|            0|  0.00%|@array_function_dispatch(_partition_dispatcher)\n",
      "   668|         0|            0|            0|  0.00%|def partition(a, kth, axis=-1, kind='introselect', order=None):\n",
      "   669|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   670|         0|            0|            0|  0.00%|    Return a partitioned copy of an array.\n",
      "   671|         0|            0|            0|  0.00%|\n",
      "   672|         0|            0|            0|  0.00%|    Creates a copy of the array with its elements rearranged in such a\n",
      "   673|         0|            0|            0|  0.00%|    way that the value of the element in k-th position is in the\n",
      "   674|         0|            0|            0|  0.00%|    position it would be in a sorted array. All elements smaller than\n",
      "   675|         0|            0|            0|  0.00%|    the k-th element are moved before this element and all equal or\n",
      "   676|         0|            0|            0|  0.00%|    greater are moved behind it. The ordering of the elements in the two\n",
      "   677|         0|            0|            0|  0.00%|    partitions is undefined.\n",
      "   678|         0|            0|            0|  0.00%|\n",
      "   679|         0|            0|            0|  0.00%|    .. versionadded:: 1.8.0\n",
      "   680|         0|            0|            0|  0.00%|\n",
      "   681|         0|            0|            0|  0.00%|    Parameters\n",
      "   682|         0|            0|            0|  0.00%|    ----------\n",
      "   683|         0|            0|            0|  0.00%|    a : array_like\n",
      "   684|         0|            0|            0|  0.00%|        Array to be sorted.\n",
      "   685|         0|            0|            0|  0.00%|    kth : int or sequence of ints\n",
      "   686|         0|            0|            0|  0.00%|        Element index to partition by. The k-th value of the element\n",
      "   687|         0|            0|            0|  0.00%|        will be in its final sorted position and all smaller elements\n",
      "   688|         0|            0|            0|  0.00%|        will be moved before it and all equal or greater elements behind\n",
      "   689|         0|            0|            0|  0.00%|        it. The order of all elements in the partitions is undefined. If\n",
      "   690|         0|            0|            0|  0.00%|        provided with a sequence of k-th it will partition all elements\n",
      "   691|         0|            0|            0|  0.00%|        indexed by k-th  of them into their sorted position at once.\n",
      "   692|         0|            0|            0|  0.00%|\n",
      "   693|         0|            0|            0|  0.00%|        .. deprecated:: 1.22.0\n",
      "   694|         0|            0|            0|  0.00%|            Passing booleans as index is deprecated.\n",
      "   695|         0|            0|            0|  0.00%|    axis : int or None, optional\n",
      "   696|         0|            0|            0|  0.00%|        Axis along which to sort. If None, the array is flattened before\n",
      "   697|         0|            0|            0|  0.00%|        sorting. The default is -1, which sorts along the last axis.\n",
      "   698|         0|            0|            0|  0.00%|    kind : {'introselect'}, optional\n",
      "   699|         0|            0|            0|  0.00%|        Selection algorithm. Default is 'introselect'.\n",
      "   700|         0|            0|            0|  0.00%|    order : str or list of str, optional\n",
      "   701|         0|            0|            0|  0.00%|        When `a` is an array with fields defined, this argument\n",
      "   702|         0|            0|            0|  0.00%|        specifies which fields to compare first, second, etc.  A single\n",
      "   703|         0|            0|            0|  0.00%|        field can be specified as a string.  Not all fields need be\n",
      "   704|         0|            0|            0|  0.00%|        specified, but unspecified fields will still be used, in the\n",
      "   705|         0|            0|            0|  0.00%|        order in which they come up in the dtype, to break ties.\n",
      "   706|         0|            0|            0|  0.00%|\n",
      "   707|         0|            0|            0|  0.00%|    Returns\n",
      "   708|         0|            0|            0|  0.00%|    -------\n",
      "   709|         0|            0|            0|  0.00%|    partitioned_array : ndarray\n",
      "   710|         0|            0|            0|  0.00%|        Array of the same type and shape as `a`.\n",
      "   711|         0|            0|            0|  0.00%|\n",
      "   712|         0|            0|            0|  0.00%|    See Also\n",
      "   713|         0|            0|            0|  0.00%|    --------\n",
      "   714|         0|            0|            0|  0.00%|    ndarray.partition : Method to sort an array in-place.\n",
      "   715|         0|            0|            0|  0.00%|    argpartition : Indirect partition.\n",
      "   716|         0|            0|            0|  0.00%|    sort : Full sorting\n",
      "   717|         0|            0|            0|  0.00%|\n",
      "   718|         0|            0|            0|  0.00%|    Notes\n",
      "   719|         0|            0|            0|  0.00%|    -----\n",
      "   720|         0|            0|            0|  0.00%|    The various selection algorithms are characterized by their average\n",
      "   721|         0|            0|            0|  0.00%|    speed, worst case performance, work space size, and whether they are\n",
      "   722|         0|            0|            0|  0.00%|    stable. A stable sort keeps items with the same key in the same\n",
      "   723|         0|            0|            0|  0.00%|    relative order. The available algorithms have the following\n",
      "   724|         0|            0|            0|  0.00%|    properties:\n",
      "   725|         0|            0|            0|  0.00%|\n",
      "   726|         0|            0|            0|  0.00%|    ================= ======= ============= ============ =======\n",
      "   727|         0|            0|            0|  0.00%|       kind            speed   worst case    work space  stable\n",
      "   728|         0|            0|            0|  0.00%|    ================= ======= ============= ============ =======\n",
      "   729|         0|            0|            0|  0.00%|    'introselect'        1        O(n)           0         no\n",
      "   730|         0|            0|            0|  0.00%|    ================= ======= ============= ============ =======\n",
      "   731|         0|            0|            0|  0.00%|\n",
      "   732|         0|            0|            0|  0.00%|    All the partition algorithms make temporary copies of the data when\n",
      "   733|         0|            0|            0|  0.00%|    partitioning along any but the last axis.  Consequently,\n",
      "   734|         0|            0|            0|  0.00%|    partitioning along the last axis is faster and uses less space than\n",
      "   735|         0|            0|            0|  0.00%|    partitioning along any other axis.\n",
      "   736|         0|            0|            0|  0.00%|\n",
      "   737|         0|            0|            0|  0.00%|    The sort order for complex numbers is lexicographic. If both the\n",
      "   738|         0|            0|            0|  0.00%|    real and imaginary parts are non-nan then the order is determined by\n",
      "   739|         0|            0|            0|  0.00%|    the real parts except when they are equal, in which case the order\n",
      "   740|         0|            0|            0|  0.00%|    is determined by the imaginary parts.\n",
      "   741|         0|            0|            0|  0.00%|\n",
      "   742|         0|            0|            0|  0.00%|    Examples\n",
      "   743|         0|            0|            0|  0.00%|    --------\n",
      "   744|         0|            0|            0|  0.00%|    >>> a = np.array([3, 4, 2, 1])\n",
      "   745|         0|            0|            0|  0.00%|    >>> np.partition(a, 3)\n",
      "   746|         0|            0|            0|  0.00%|    array([2, 1, 3, 4])\n",
      "   747|         0|            0|            0|  0.00%|\n",
      "   748|         0|            0|            0|  0.00%|    >>> np.partition(a, (1, 3))\n",
      "   749|         0|            0|            0|  0.00%|    array([1, 2, 3, 4])\n",
      "   750|         0|            0|            0|  0.00%|\n",
      "   751|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   752|         0|            0|            0|  0.00%|    if axis is None:\n",
      "   753|         0|            0|            0|  0.00%|        # flatten returns (1, N) for np.matrix, so always use the last axis\n",
      "   754|         0|            0|            0|  0.00%|        a = asanyarray(a).flatten()\n",
      "   755|         0|            0|            0|  0.00%|        axis = -1\n",
      "   756|         0|            0|            0|  0.00%|    else:\n",
      "   757|         0|            0|            0|  0.00%|        a = asanyarray(a).copy(order=\"K\")\n",
      "   758|         0|            0|            0|  0.00%|    a.partition(kth, axis=axis, kind=kind, order=order)\n",
      "   759|         0|            0|            0|  0.00%|    return a\n",
      "   760|         0|            0|            0|  0.00%|\n",
      "   761|         0|            0|            0|  0.00%|\n",
      "   762|         0|            0|            0|  0.00%|def _argpartition_dispatcher(a, kth, axis=None, kind=None, order=None):\n",
      "   763|         0|            0|            0|  0.00%|    return (a,)\n",
      "   764|         0|            0|            0|  0.00%|\n",
      "   765|         0|            0|            0|  0.00%|\n",
      "   766|         0|            0|            0|  0.00%|@array_function_dispatch(_argpartition_dispatcher)\n",
      "   767|         0|            0|            0|  0.00%|def argpartition(a, kth, axis=-1, kind='introselect', order=None):\n",
      "   768|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   769|         0|            0|            0|  0.00%|    Perform an indirect partition along the given axis using the\n",
      "   770|         0|            0|            0|  0.00%|    algorithm specified by the `kind` keyword. It returns an array of\n",
      "   771|         0|            0|            0|  0.00%|    indices of the same shape as `a` that index data along the given\n",
      "   772|         0|            0|            0|  0.00%|    axis in partitioned order.\n",
      "   773|         0|            0|            0|  0.00%|\n",
      "   774|         0|            0|            0|  0.00%|    .. versionadded:: 1.8.0\n",
      "   775|         0|            0|            0|  0.00%|\n",
      "   776|         0|            0|            0|  0.00%|    Parameters\n",
      "   777|         0|            0|            0|  0.00%|    ----------\n",
      "   778|         0|            0|            0|  0.00%|    a : array_like\n",
      "   779|         0|            0|            0|  0.00%|        Array to sort.\n",
      "   780|         0|            0|            0|  0.00%|    kth : int or sequence of ints\n",
      "   781|         0|            0|            0|  0.00%|        Element index to partition by. The k-th element will be in its\n",
      "   782|         0|            0|            0|  0.00%|        final sorted position and all smaller elements will be moved\n",
      "   783|         0|            0|            0|  0.00%|        before it and all larger elements behind it. The order all\n",
      "   784|         0|            0|            0|  0.00%|        elements in the partitions is undefined. If provided with a\n",
      "   785|         0|            0|            0|  0.00%|        sequence of k-th it will partition all of them into their sorted\n",
      "   786|         0|            0|            0|  0.00%|        position at once.\n",
      "   787|         0|            0|            0|  0.00%|\n",
      "   788|         0|            0|            0|  0.00%|        .. deprecated:: 1.22.0\n",
      "   789|         0|            0|            0|  0.00%|            Passing booleans as index is deprecated.\n",
      "   790|         0|            0|            0|  0.00%|    axis : int or None, optional\n",
      "   791|         0|            0|            0|  0.00%|        Axis along which to sort. The default is -1 (the last axis). If\n",
      "   792|         0|            0|            0|  0.00%|        None, the flattened array is used.\n",
      "   793|         0|            0|            0|  0.00%|    kind : {'introselect'}, optional\n",
      "   794|         0|            0|            0|  0.00%|        Selection algorithm. Default is 'introselect'\n",
      "   795|         0|            0|            0|  0.00%|    order : str or list of str, optional\n",
      "   796|         0|            0|            0|  0.00%|        When `a` is an array with fields defined, this argument\n",
      "   797|         0|            0|            0|  0.00%|        specifies which fields to compare first, second, etc. A single\n",
      "   798|         0|            0|            0|  0.00%|        field can be specified as a string, and not all fields need be\n",
      "   799|         0|            0|            0|  0.00%|        specified, but unspecified fields will still be used, in the\n",
      "   800|         0|            0|            0|  0.00%|        order in which they come up in the dtype, to break ties.\n",
      "   801|         0|            0|            0|  0.00%|\n",
      "   802|         0|            0|            0|  0.00%|    Returns\n",
      "   803|         0|            0|            0|  0.00%|    -------\n",
      "   804|         0|            0|            0|  0.00%|    index_array : ndarray, int\n",
      "   805|         0|            0|            0|  0.00%|        Array of indices that partition `a` along the specified axis.\n",
      "   806|         0|            0|            0|  0.00%|        If `a` is one-dimensional, ``a[index_array]`` yields a partitioned `a`.\n",
      "   807|         0|            0|            0|  0.00%|        More generally, ``np.take_along_axis(a, index_array, axis=a)`` always\n",
      "   808|         0|            0|            0|  0.00%|        yields the partitioned `a`, irrespective of dimensionality.\n",
      "   809|         0|            0|            0|  0.00%|\n",
      "   810|         0|            0|            0|  0.00%|    See Also\n",
      "   811|         0|            0|            0|  0.00%|    --------\n",
      "   812|         0|            0|            0|  0.00%|    partition : Describes partition algorithms used.\n",
      "   813|         0|            0|            0|  0.00%|    ndarray.partition : Inplace partition.\n",
      "   814|         0|            0|            0|  0.00%|    argsort : Full indirect sort.\n",
      "   815|         0|            0|            0|  0.00%|    take_along_axis : Apply ``index_array`` from argpartition\n",
      "   816|         0|            0|            0|  0.00%|                      to an array as if by calling partition.\n",
      "   817|         0|            0|            0|  0.00%|\n",
      "   818|         0|            0|            0|  0.00%|    Notes\n",
      "   819|         0|            0|            0|  0.00%|    -----\n",
      "   820|         0|            0|            0|  0.00%|    See `partition` for notes on the different selection algorithms.\n",
      "   821|         0|            0|            0|  0.00%|\n",
      "   822|         0|            0|            0|  0.00%|    Examples\n",
      "   823|         0|            0|            0|  0.00%|    --------\n",
      "   824|         0|            0|            0|  0.00%|    One dimensional array:\n",
      "   825|         0|            0|            0|  0.00%|\n",
      "   826|         0|            0|            0|  0.00%|    >>> x = np.array([3, 4, 2, 1])\n",
      "   827|         0|            0|            0|  0.00%|    >>> x[np.argpartition(x, 3)]\n",
      "   828|         0|            0|            0|  0.00%|    array([2, 1, 3, 4])\n",
      "   829|         0|            0|            0|  0.00%|    >>> x[np.argpartition(x, (1, 3))]\n",
      "   830|         0|            0|            0|  0.00%|    array([1, 2, 3, 4])\n",
      "   831|         0|            0|            0|  0.00%|\n",
      "   832|         0|            0|            0|  0.00%|    >>> x = [3, 4, 2, 1]\n",
      "   833|         0|            0|            0|  0.00%|    >>> np.array(x)[np.argpartition(x, 3)]\n",
      "   834|         0|            0|            0|  0.00%|    array([2, 1, 3, 4])\n",
      "   835|         0|            0|            0|  0.00%|\n",
      "   836|         0|            0|            0|  0.00%|    Multi-dimensional array:\n",
      "   837|         0|            0|            0|  0.00%|\n",
      "   838|         0|            0|            0|  0.00%|    >>> x = np.array([[3, 4, 2], [1, 3, 1]])\n",
      "   839|         0|            0|            0|  0.00%|    >>> index_array = np.argpartition(x, kth=1, axis=-1)\n",
      "   840|         0|            0|            0|  0.00%|    >>> np.take_along_axis(x, index_array, axis=-1)  # same as np.partition(x, kth=1)\n",
      "   841|         0|            0|            0|  0.00%|    array([[2, 3, 4],\n",
      "   842|         0|            0|            0|  0.00%|           [1, 1, 3]])\n",
      "   843|         0|            0|            0|  0.00%|\n",
      "   844|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   845|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'argpartition', kth, axis=axis, kind=kind, order=order)\n",
      "   846|         0|            0|            0|  0.00%|\n",
      "   847|         0|            0|            0|  0.00%|\n",
      "   848|         0|            0|            0|  0.00%|def _sort_dispatcher(a, axis=None, kind=None, order=None):\n",
      "   849|         0|            0|            0|  0.00%|    return (a,)\n",
      "   850|         0|            0|            0|  0.00%|\n",
      "   851|         0|            0|            0|  0.00%|\n",
      "   852|         0|            0|            0|  0.00%|@array_function_dispatch(_sort_dispatcher)\n",
      "   853|         0|            0|            0|  0.00%|def sort(a, axis=-1, kind=None, order=None):\n",
      "   854|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   855|         0|            0|            0|  0.00%|    Return a sorted copy of an array.\n",
      "   856|         0|            0|            0|  0.00%|\n",
      "   857|         0|            0|            0|  0.00%|    Parameters\n",
      "   858|         0|            0|            0|  0.00%|    ----------\n",
      "   859|         0|            0|            0|  0.00%|    a : array_like\n",
      "   860|         0|            0|            0|  0.00%|        Array to be sorted.\n",
      "   861|         0|            0|            0|  0.00%|    axis : int or None, optional\n",
      "   862|         0|            0|            0|  0.00%|        Axis along which to sort. If None, the array is flattened before\n",
      "   863|         0|            0|            0|  0.00%|        sorting. The default is -1, which sorts along the last axis.\n",
      "   864|         0|            0|            0|  0.00%|    kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n",
      "   865|         0|            0|            0|  0.00%|        Sorting algorithm. The default is 'quicksort'. Note that both 'stable'\n",
      "   866|         0|            0|            0|  0.00%|        and 'mergesort' use timsort or radix sort under the covers and, in general,\n",
      "   867|         0|            0|            0|  0.00%|        the actual implementation will vary with data type. The 'mergesort' option\n",
      "   868|         0|            0|            0|  0.00%|        is retained for backwards compatibility.\n",
      "   869|         0|            0|            0|  0.00%|\n",
      "   870|         0|            0|            0|  0.00%|        .. versionchanged:: 1.15.0.\n",
      "   871|         0|            0|            0|  0.00%|           The 'stable' option was added.\n",
      "   872|         0|            0|            0|  0.00%|\n",
      "   873|         0|            0|            0|  0.00%|    order : str or list of str, optional\n",
      "   874|         0|            0|            0|  0.00%|        When `a` is an array with fields defined, this argument specifies\n",
      "   875|         0|            0|            0|  0.00%|        which fields to compare first, second, etc.  A single field can\n",
      "   876|         0|            0|            0|  0.00%|        be specified as a string, and not all fields need be specified,\n",
      "   877|         0|            0|            0|  0.00%|        but unspecified fields will still be used, in the order in which\n",
      "   878|         0|            0|            0|  0.00%|        they come up in the dtype, to break ties.\n",
      "   879|         0|            0|            0|  0.00%|\n",
      "   880|         0|            0|            0|  0.00%|    Returns\n",
      "   881|         0|            0|            0|  0.00%|    -------\n",
      "   882|         0|            0|            0|  0.00%|    sorted_array : ndarray\n",
      "   883|         0|            0|            0|  0.00%|        Array of the same type and shape as `a`.\n",
      "   884|         0|            0|            0|  0.00%|\n",
      "   885|         0|            0|            0|  0.00%|    See Also\n",
      "   886|         0|            0|            0|  0.00%|    --------\n",
      "   887|         0|            0|            0|  0.00%|    ndarray.sort : Method to sort an array in-place.\n",
      "   888|         0|            0|            0|  0.00%|    argsort : Indirect sort.\n",
      "   889|         0|            0|            0|  0.00%|    lexsort : Indirect stable sort on multiple keys.\n",
      "   890|         0|            0|            0|  0.00%|    searchsorted : Find elements in a sorted array.\n",
      "   891|         0|            0|            0|  0.00%|    partition : Partial sort.\n",
      "   892|         0|            0|            0|  0.00%|\n",
      "   893|         0|            0|            0|  0.00%|    Notes\n",
      "   894|         0|            0|            0|  0.00%|    -----\n",
      "   895|         0|            0|            0|  0.00%|    The various sorting algorithms are characterized by their average speed,\n",
      "   896|         0|            0|            0|  0.00%|    worst case performance, work space size, and whether they are stable. A\n",
      "   897|         0|            0|            0|  0.00%|    stable sort keeps items with the same key in the same relative\n",
      "   898|         0|            0|            0|  0.00%|    order. The four algorithms implemented in NumPy have the following\n",
      "   899|         0|            0|            0|  0.00%|    properties:\n",
      "   900|         0|            0|            0|  0.00%|\n",
      "   901|         0|            0|            0|  0.00%|    =========== ======= ============= ============ ========\n",
      "   902|         0|            0|            0|  0.00%|       kind      speed   worst case    work space   stable\n",
      "   903|         0|            0|            0|  0.00%|    =========== ======= ============= ============ ========\n",
      "   904|         0|            0|            0|  0.00%|    'quicksort'    1     O(n^2)            0          no\n",
      "   905|         0|            0|            0|  0.00%|    'heapsort'     3     O(n*log(n))       0          no\n",
      "   906|         0|            0|            0|  0.00%|    'mergesort'    2     O(n*log(n))      ~n/2        yes\n",
      "   907|         0|            0|            0|  0.00%|    'timsort'      2     O(n*log(n))      ~n/2        yes\n",
      "   908|         0|            0|            0|  0.00%|    =========== ======= ============= ============ ========\n",
      "   909|         0|            0|            0|  0.00%|\n",
      "   910|         0|            0|            0|  0.00%|    .. note:: The datatype determines which of 'mergesort' or 'timsort'\n",
      "   911|         0|            0|            0|  0.00%|       is actually used, even if 'mergesort' is specified. User selection\n",
      "   912|         0|            0|            0|  0.00%|       at a finer scale is not currently available.\n",
      "   913|         0|            0|            0|  0.00%|\n",
      "   914|         0|            0|            0|  0.00%|    All the sort algorithms make temporary copies of the data when\n",
      "   915|         0|            0|            0|  0.00%|    sorting along any but the last axis.  Consequently, sorting along\n",
      "   916|         0|            0|            0|  0.00%|    the last axis is faster and uses less space than sorting along\n",
      "   917|         0|            0|            0|  0.00%|    any other axis.\n",
      "   918|         0|            0|            0|  0.00%|\n",
      "   919|         0|            0|            0|  0.00%|    The sort order for complex numbers is lexicographic. If both the real\n",
      "   920|         0|            0|            0|  0.00%|    and imaginary parts are non-nan then the order is determined by the\n",
      "   921|         0|            0|            0|  0.00%|    real parts except when they are equal, in which case the order is\n",
      "   922|         0|            0|            0|  0.00%|    determined by the imaginary parts.\n",
      "   923|         0|            0|            0|  0.00%|\n",
      "   924|         0|            0|            0|  0.00%|    Previous to numpy 1.4.0 sorting real and complex arrays containing nan\n",
      "   925|         0|            0|            0|  0.00%|    values led to undefined behaviour. In numpy versions >= 1.4.0 nan\n",
      "   926|         0|            0|            0|  0.00%|    values are sorted to the end. The extended sort order is:\n",
      "   927|         0|            0|            0|  0.00%|\n",
      "   928|         0|            0|            0|  0.00%|      * Real: [R, nan]\n",
      "   929|         0|            0|            0|  0.00%|      * Complex: [R + Rj, R + nanj, nan + Rj, nan + nanj]\n",
      "   930|         0|            0|            0|  0.00%|\n",
      "   931|         0|            0|            0|  0.00%|    where R is a non-nan real value. Complex values with the same nan\n",
      "   932|         0|            0|            0|  0.00%|    placements are sorted according to the non-nan part if it exists.\n",
      "   933|         0|            0|            0|  0.00%|    Non-nan values are sorted as before.\n",
      "   934|         0|            0|            0|  0.00%|\n",
      "   935|         0|            0|            0|  0.00%|    .. versionadded:: 1.12.0\n",
      "   936|         0|            0|            0|  0.00%|\n",
      "   937|         0|            0|            0|  0.00%|    quicksort has been changed to `introsort <https://en.wikipedia.org/wiki/Introsort>`_.\n",
      "   938|         0|            0|            0|  0.00%|    When sorting does not make enough progress it switches to\n",
      "   939|         0|            0|            0|  0.00%|    `heapsort <https://en.wikipedia.org/wiki/Heapsort>`_.\n",
      "   940|         0|            0|            0|  0.00%|    This implementation makes quicksort O(n*log(n)) in the worst case.\n",
      "   941|         0|            0|            0|  0.00%|\n",
      "   942|         0|            0|            0|  0.00%|    'stable' automatically chooses the best stable sorting algorithm\n",
      "   943|         0|            0|            0|  0.00%|    for the data type being sorted.\n",
      "   944|         0|            0|            0|  0.00%|    It, along with 'mergesort' is currently mapped to\n",
      "   945|         0|            0|            0|  0.00%|    `timsort <https://en.wikipedia.org/wiki/Timsort>`_\n",
      "   946|         0|            0|            0|  0.00%|    or `radix sort <https://en.wikipedia.org/wiki/Radix_sort>`_\n",
      "   947|         0|            0|            0|  0.00%|    depending on the data type.\n",
      "   948|         0|            0|            0|  0.00%|    API forward compatibility currently limits the\n",
      "   949|         0|            0|            0|  0.00%|    ability to select the implementation and it is hardwired for the different\n",
      "   950|         0|            0|            0|  0.00%|    data types.\n",
      "   951|         0|            0|            0|  0.00%|\n",
      "   952|         0|            0|            0|  0.00%|    .. versionadded:: 1.17.0\n",
      "   953|         0|            0|            0|  0.00%|\n",
      "   954|         0|            0|            0|  0.00%|    Timsort is added for better performance on already or nearly\n",
      "   955|         0|            0|            0|  0.00%|    sorted data. On random data timsort is almost identical to\n",
      "   956|         0|            0|            0|  0.00%|    mergesort. It is now used for stable sort while quicksort is still the\n",
      "   957|         0|            0|            0|  0.00%|    default sort if none is chosen. For timsort details, refer to\n",
      "   958|         0|            0|            0|  0.00%|    `CPython listsort.txt <https://github.com/python/cpython/blob/3.7/Objects/listsort.txt>`_.\n",
      "   959|         0|            0|            0|  0.00%|    'mergesort' and 'stable' are mapped to radix sort for integer data types. Radix sort is an\n",
      "   960|         0|            0|            0|  0.00%|    O(n) sort instead of O(n log n).\n",
      "   961|         0|            0|            0|  0.00%|\n",
      "   962|         0|            0|            0|  0.00%|    .. versionchanged:: 1.18.0\n",
      "   963|         0|            0|            0|  0.00%|\n",
      "   964|         0|            0|            0|  0.00%|    NaT now sorts to the end of arrays for consistency with NaN.\n",
      "   965|         0|            0|            0|  0.00%|\n",
      "   966|         0|            0|            0|  0.00%|    Examples\n",
      "   967|         0|            0|            0|  0.00%|    --------\n",
      "   968|         0|            0|            0|  0.00%|    >>> a = np.array([[1,4],[3,1]])\n",
      "   969|         0|            0|            0|  0.00%|    >>> np.sort(a)                # sort along the last axis\n",
      "   970|         0|            0|            0|  0.00%|    array([[1, 4],\n",
      "   971|         0|            0|            0|  0.00%|           [1, 3]])\n",
      "   972|         0|            0|            0|  0.00%|    >>> np.sort(a, axis=None)     # sort the flattened array\n",
      "   973|         0|            0|            0|  0.00%|    array([1, 1, 3, 4])\n",
      "   974|         0|            0|            0|  0.00%|    >>> np.sort(a, axis=0)        # sort along the first axis\n",
      "   975|         0|            0|            0|  0.00%|    array([[1, 1],\n",
      "   976|         0|            0|            0|  0.00%|           [3, 4]])\n",
      "   977|         0|            0|            0|  0.00%|\n",
      "   978|         0|            0|            0|  0.00%|    Use the `order` keyword to specify a field to use when sorting a\n",
      "   979|         0|            0|            0|  0.00%|    structured array:\n",
      "   980|         0|            0|            0|  0.00%|\n",
      "   981|         0|            0|            0|  0.00%|    >>> dtype = [('name', 'S10'), ('height', float), ('age', int)]\n",
      "   982|         0|            0|            0|  0.00%|    >>> values = [('Arthur', 1.8, 41), ('Lancelot', 1.9, 38),\n",
      "   983|         0|            0|            0|  0.00%|    ...           ('Galahad', 1.7, 38)]\n",
      "   984|         0|            0|            0|  0.00%|    >>> a = np.array(values, dtype=dtype)       # create a structured array\n",
      "   985|         0|            0|            0|  0.00%|    >>> np.sort(a, order='height')                        # doctest: +SKIP\n",
      "   986|         0|            0|            0|  0.00%|    array([('Galahad', 1.7, 38), ('Arthur', 1.8, 41),\n",
      "   987|         0|            0|            0|  0.00%|           ('Lancelot', 1.8999999999999999, 38)],\n",
      "   988|         0|            0|            0|  0.00%|          dtype=[('name', '|S10'), ('height', '<f8'), ('age', '<i4')])\n",
      "   989|         0|            0|            0|  0.00%|\n",
      "   990|         0|            0|            0|  0.00%|    Sort by age, then height if ages are equal:\n",
      "   991|         0|            0|            0|  0.00%|\n",
      "   992|         0|            0|            0|  0.00%|    >>> np.sort(a, order=['age', 'height'])               # doctest: +SKIP\n",
      "   993|         0|            0|            0|  0.00%|    array([('Galahad', 1.7, 38), ('Lancelot', 1.8999999999999999, 38),\n",
      "   994|         0|            0|            0|  0.00%|           ('Arthur', 1.8, 41)],\n",
      "   995|         0|            0|            0|  0.00%|          dtype=[('name', '|S10'), ('height', '<f8'), ('age', '<i4')])\n",
      "   996|         0|            0|            0|  0.00%|\n",
      "   997|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   998|         0|            0|            0|  0.00%|    if axis is None:\n",
      "   999|         0|            0|            0|  0.00%|        # flatten returns (1, N) for np.matrix, so always use the last axis\n",
      "  1000|         0|            0|            0|  0.00%|        a = asanyarray(a).flatten()\n",
      "  1001|         0|            0|            0|  0.00%|        axis = -1\n",
      "  1002|         0|            0|            0|  0.00%|    else:\n",
      "  1003|         0|            0|            0|  0.00%|        a = asanyarray(a).copy(order=\"K\")\n",
      "  1004|         0|            0|            0|  0.00%|    a.sort(axis=axis, kind=kind, order=order)\n",
      "  1005|         0|            0|            0|  0.00%|    return a\n",
      "  1006|         0|            0|            0|  0.00%|\n",
      "  1007|         0|            0|            0|  0.00%|\n",
      "  1008|         0|            0|            0|  0.00%|def _argsort_dispatcher(a, axis=None, kind=None, order=None):\n",
      "  1009|         0|            0|            0|  0.00%|    return (a,)\n",
      "  1010|         0|            0|            0|  0.00%|\n",
      "  1011|         0|            0|            0|  0.00%|\n",
      "  1012|         0|            0|            0|  0.00%|@array_function_dispatch(_argsort_dispatcher)\n",
      "  1013|         0|            0|            0|  0.00%|def argsort(a, axis=-1, kind=None, order=None):\n",
      "  1014|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1015|         0|            0|            0|  0.00%|    Returns the indices that would sort an array.\n",
      "  1016|         0|            0|            0|  0.00%|\n",
      "  1017|         0|            0|            0|  0.00%|    Perform an indirect sort along the given axis using the algorithm specified\n",
      "  1018|         0|            0|            0|  0.00%|    by the `kind` keyword. It returns an array of indices of the same shape as\n",
      "  1019|         0|            0|            0|  0.00%|    `a` that index data along the given axis in sorted order.\n",
      "  1020|         0|            0|            0|  0.00%|\n",
      "  1021|         0|            0|            0|  0.00%|    Parameters\n",
      "  1022|         0|            0|            0|  0.00%|    ----------\n",
      "  1023|         0|            0|            0|  0.00%|    a : array_like\n",
      "  1024|         0|            0|            0|  0.00%|        Array to sort.\n",
      "  1025|         0|            0|            0|  0.00%|    axis : int or None, optional\n",
      "  1026|         0|            0|            0|  0.00%|        Axis along which to sort.  The default is -1 (the last axis). If None,\n",
      "  1027|         0|            0|            0|  0.00%|        the flattened array is used.\n",
      "  1028|         0|            0|            0|  0.00%|    kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n",
      "  1029|         0|            0|            0|  0.00%|        Sorting algorithm. The default is 'quicksort'. Note that both 'stable'\n",
      "  1030|         0|            0|            0|  0.00%|        and 'mergesort' use timsort under the covers and, in general, the\n",
      "  1031|         0|            0|            0|  0.00%|        actual implementation will vary with data type. The 'mergesort' option\n",
      "  1032|         0|            0|            0|  0.00%|        is retained for backwards compatibility.\n",
      "  1033|         0|            0|            0|  0.00%|\n",
      "  1034|         0|            0|            0|  0.00%|        .. versionchanged:: 1.15.0.\n",
      "  1035|         0|            0|            0|  0.00%|           The 'stable' option was added.\n",
      "  1036|         0|            0|            0|  0.00%|    order : str or list of str, optional\n",
      "  1037|         0|            0|            0|  0.00%|        When `a` is an array with fields defined, this argument specifies\n",
      "  1038|         0|            0|            0|  0.00%|        which fields to compare first, second, etc.  A single field can\n",
      "  1039|         0|            0|            0|  0.00%|        be specified as a string, and not all fields need be specified,\n",
      "  1040|         0|            0|            0|  0.00%|        but unspecified fields will still be used, in the order in which\n",
      "  1041|         0|            0|            0|  0.00%|        they come up in the dtype, to break ties.\n",
      "  1042|         0|            0|            0|  0.00%|\n",
      "  1043|         0|            0|            0|  0.00%|    Returns\n",
      "  1044|         0|            0|            0|  0.00%|    -------\n",
      "  1045|         0|            0|            0|  0.00%|    index_array : ndarray, int\n",
      "  1046|         0|            0|            0|  0.00%|        Array of indices that sort `a` along the specified `axis`.\n",
      "  1047|         0|            0|            0|  0.00%|        If `a` is one-dimensional, ``a[index_array]`` yields a sorted `a`.\n",
      "  1048|         0|            0|            0|  0.00%|        More generally, ``np.take_along_axis(a, index_array, axis=axis)``\n",
      "  1049|         0|            0|            0|  0.00%|        always yields the sorted `a`, irrespective of dimensionality.\n",
      "  1050|         0|            0|            0|  0.00%|\n",
      "  1051|         0|            0|            0|  0.00%|    See Also\n",
      "  1052|         0|            0|            0|  0.00%|    --------\n",
      "  1053|         0|            0|            0|  0.00%|    sort : Describes sorting algorithms used.\n",
      "  1054|         0|            0|            0|  0.00%|    lexsort : Indirect stable sort with multiple keys.\n",
      "  1055|         0|            0|            0|  0.00%|    ndarray.sort : Inplace sort.\n",
      "  1056|         0|            0|            0|  0.00%|    argpartition : Indirect partial sort.\n",
      "  1057|         0|            0|            0|  0.00%|    take_along_axis : Apply ``index_array`` from argsort\n",
      "  1058|         0|            0|            0|  0.00%|                      to an array as if by calling sort.\n",
      "  1059|         0|            0|            0|  0.00%|\n",
      "  1060|         0|            0|            0|  0.00%|    Notes\n",
      "  1061|         0|            0|            0|  0.00%|    -----\n",
      "  1062|         0|            0|            0|  0.00%|    See `sort` for notes on the different sorting algorithms.\n",
      "  1063|         0|            0|            0|  0.00%|\n",
      "  1064|         0|            0|            0|  0.00%|    As of NumPy 1.4.0 `argsort` works with real/complex arrays containing\n",
      "  1065|         0|            0|            0|  0.00%|    nan values. The enhanced sort order is documented in `sort`.\n",
      "  1066|         0|            0|            0|  0.00%|\n",
      "  1067|         0|            0|            0|  0.00%|    Examples\n",
      "  1068|         0|            0|            0|  0.00%|    --------\n",
      "  1069|         0|            0|            0|  0.00%|    One dimensional array:\n",
      "  1070|         0|            0|            0|  0.00%|\n",
      "  1071|         0|            0|            0|  0.00%|    >>> x = np.array([3, 1, 2])\n",
      "  1072|         0|            0|            0|  0.00%|    >>> np.argsort(x)\n",
      "  1073|         0|            0|            0|  0.00%|    array([1, 2, 0])\n",
      "  1074|         0|            0|            0|  0.00%|\n",
      "  1075|         0|            0|            0|  0.00%|    Two-dimensional array:\n",
      "  1076|         0|            0|            0|  0.00%|\n",
      "  1077|         0|            0|            0|  0.00%|    >>> x = np.array([[0, 3], [2, 2]])\n",
      "  1078|         0|            0|            0|  0.00%|    >>> x\n",
      "  1079|         0|            0|            0|  0.00%|    array([[0, 3],\n",
      "  1080|         0|            0|            0|  0.00%|           [2, 2]])\n",
      "  1081|         0|            0|            0|  0.00%|\n",
      "  1082|         0|            0|            0|  0.00%|    >>> ind = np.argsort(x, axis=0)  # sorts along first axis (down)\n",
      "  1083|         0|            0|            0|  0.00%|    >>> ind\n",
      "  1084|         0|            0|            0|  0.00%|    array([[0, 1],\n",
      "  1085|         0|            0|            0|  0.00%|           [1, 0]])\n",
      "  1086|         0|            0|            0|  0.00%|    >>> np.take_along_axis(x, ind, axis=0)  # same as np.sort(x, axis=0)\n",
      "  1087|         0|            0|            0|  0.00%|    array([[0, 2],\n",
      "  1088|         0|            0|            0|  0.00%|           [2, 3]])\n",
      "  1089|         0|            0|            0|  0.00%|\n",
      "  1090|         0|            0|            0|  0.00%|    >>> ind = np.argsort(x, axis=1)  # sorts along last axis (across)\n",
      "  1091|         0|            0|            0|  0.00%|    >>> ind\n",
      "  1092|         0|            0|            0|  0.00%|    array([[0, 1],\n",
      "  1093|         0|            0|            0|  0.00%|           [0, 1]])\n",
      "  1094|         0|            0|            0|  0.00%|    >>> np.take_along_axis(x, ind, axis=1)  # same as np.sort(x, axis=1)\n",
      "  1095|         0|            0|            0|  0.00%|    array([[0, 3],\n",
      "  1096|         0|            0|            0|  0.00%|           [2, 2]])\n",
      "  1097|         0|            0|            0|  0.00%|\n",
      "  1098|         0|            0|            0|  0.00%|    Indices of the sorted elements of a N-dimensional array:\n",
      "  1099|         0|            0|            0|  0.00%|\n",
      "  1100|         0|            0|            0|  0.00%|    >>> ind = np.unravel_index(np.argsort(x, axis=None), x.shape)\n",
      "  1101|         0|            0|            0|  0.00%|    >>> ind\n",
      "  1102|         0|            0|            0|  0.00%|    (array([0, 1, 1, 0]), array([0, 0, 1, 1]))\n",
      "  1103|         0|            0|            0|  0.00%|    >>> x[ind]  # same as np.sort(x, axis=None)\n",
      "  1104|         0|            0|            0|  0.00%|    array([0, 2, 2, 3])\n",
      "  1105|         0|            0|            0|  0.00%|\n",
      "  1106|         0|            0|            0|  0.00%|    Sorting with keys:\n",
      "  1107|         0|            0|            0|  0.00%|\n",
      "  1108|         0|            0|            0|  0.00%|    >>> x = np.array([(1, 0), (0, 1)], dtype=[('x', '<i4'), ('y', '<i4')])\n",
      "  1109|         0|            0|            0|  0.00%|    >>> x\n",
      "  1110|         0|            0|            0|  0.00%|    array([(1, 0), (0, 1)],\n",
      "  1111|         0|            0|            0|  0.00%|          dtype=[('x', '<i4'), ('y', '<i4')])\n",
      "  1112|         0|            0|            0|  0.00%|\n",
      "  1113|         0|            0|            0|  0.00%|    >>> np.argsort(x, order=('x','y'))\n",
      "  1114|         0|            0|            0|  0.00%|    array([1, 0])\n",
      "  1115|         0|            0|            0|  0.00%|\n",
      "  1116|         0|            0|            0|  0.00%|    >>> np.argsort(x, order=('y','x'))\n",
      "  1117|         0|            0|            0|  0.00%|    array([0, 1])\n",
      "  1118|         0|            0|            0|  0.00%|\n",
      "  1119|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1120|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'argsort', axis=axis, kind=kind, order=order)\n",
      "  1121|         0|            0|            0|  0.00%|\n",
      "  1122|         0|            0|            0|  0.00%|\n",
      "  1123|         0|            0|            0|  0.00%|def _argmax_dispatcher(a, axis=None, out=None, *, keepdims=np._NoValue):\n",
      "  1124|         0|            0|            0|  0.00%|    return (a, out)\n",
      "  1125|         0|            0|            0|  0.00%|\n",
      "  1126|         0|            0|            0|  0.00%|\n",
      "  1127|         0|            0|            0|  0.00%|@array_function_dispatch(_argmax_dispatcher)\n",
      "  1128|         0|            0|            0|  0.00%|def argmax(a, axis=None, out=None, *, keepdims=np._NoValue):\n",
      "  1129|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1130|         0|            0|            0|  0.00%|    Returns the indices of the maximum values along an axis.\n",
      "  1131|         0|            0|            0|  0.00%|\n",
      "  1132|         0|            0|            0|  0.00%|    Parameters\n",
      "  1133|         0|            0|            0|  0.00%|    ----------\n",
      "  1134|         0|            0|            0|  0.00%|    a : array_like\n",
      "  1135|         0|            0|            0|  0.00%|        Input array.\n",
      "  1136|         0|            0|            0|  0.00%|    axis : int, optional\n",
      "  1137|         0|            0|            0|  0.00%|        By default, the index is into the flattened array, otherwise\n",
      "  1138|         0|            0|            0|  0.00%|        along the specified axis.\n",
      "  1139|         0|            0|            0|  0.00%|    out : array, optional\n",
      "  1140|         0|            0|            0|  0.00%|        If provided, the result will be inserted into this array. It should\n",
      "  1141|         0|            0|            0|  0.00%|        be of the appropriate shape and dtype.\n",
      "  1142|         0|            0|            0|  0.00%|    keepdims : bool, optional\n",
      "  1143|         0|            0|            0|  0.00%|        If this is set to True, the axes which are reduced are left\n",
      "  1144|         0|            0|            0|  0.00%|        in the result as dimensions with size one. With this option,\n",
      "  1145|         0|            0|            0|  0.00%|        the result will broadcast correctly against the array.\n",
      "  1146|         0|            0|            0|  0.00%|\n",
      "  1147|         0|            0|            0|  0.00%|        .. versionadded:: 1.22.0\n",
      "  1148|         0|            0|            0|  0.00%|\n",
      "  1149|         0|            0|            0|  0.00%|    Returns\n",
      "  1150|         0|            0|            0|  0.00%|    -------\n",
      "  1151|         0|            0|            0|  0.00%|    index_array : ndarray of ints\n",
      "  1152|         0|            0|            0|  0.00%|        Array of indices into the array. It has the same shape as `a.shape`\n",
      "  1153|         0|            0|            0|  0.00%|        with the dimension along `axis` removed. If `keepdims` is set to True,\n",
      "  1154|         0|            0|            0|  0.00%|        then the size of `axis` will be 1 with the resulting array having same\n",
      "  1155|         0|            0|            0|  0.00%|        shape as `a.shape`.\n",
      "  1156|         0|            0|            0|  0.00%|\n",
      "  1157|         0|            0|            0|  0.00%|    See Also\n",
      "  1158|         0|            0|            0|  0.00%|    --------\n",
      "  1159|         0|            0|            0|  0.00%|    ndarray.argmax, argmin\n",
      "  1160|         0|            0|            0|  0.00%|    amax : The maximum value along a given axis.\n",
      "  1161|         0|            0|            0|  0.00%|    unravel_index : Convert a flat index into an index tuple.\n",
      "  1162|         0|            0|            0|  0.00%|    take_along_axis : Apply ``np.expand_dims(index_array, axis)``\n",
      "  1163|         0|            0|            0|  0.00%|                      from argmax to an array as if by calling max.\n",
      "  1164|         0|            0|            0|  0.00%|\n",
      "  1165|         0|            0|            0|  0.00%|    Notes\n",
      "  1166|         0|            0|            0|  0.00%|    -----\n",
      "  1167|         0|            0|            0|  0.00%|    In case of multiple occurrences of the maximum values, the indices\n",
      "  1168|         0|            0|            0|  0.00%|    corresponding to the first occurrence are returned.\n",
      "  1169|         0|            0|            0|  0.00%|\n",
      "  1170|         0|            0|            0|  0.00%|    Examples\n",
      "  1171|         0|            0|            0|  0.00%|    --------\n",
      "  1172|         0|            0|            0|  0.00%|    >>> a = np.arange(6).reshape(2,3) + 10\n",
      "  1173|         0|            0|            0|  0.00%|    >>> a\n",
      "  1174|         0|            0|            0|  0.00%|    array([[10, 11, 12],\n",
      "  1175|         0|            0|            0|  0.00%|           [13, 14, 15]])\n",
      "  1176|         0|            0|            0|  0.00%|    >>> np.argmax(a)\n",
      "  1177|         0|            0|            0|  0.00%|    5\n",
      "  1178|         0|            0|            0|  0.00%|    >>> np.argmax(a, axis=0)\n",
      "  1179|         0|            0|            0|  0.00%|    array([1, 1, 1])\n",
      "  1180|         0|            0|            0|  0.00%|    >>> np.argmax(a, axis=1)\n",
      "  1181|         0|            0|            0|  0.00%|    array([2, 2])\n",
      "  1182|         0|            0|            0|  0.00%|\n",
      "  1183|         0|            0|            0|  0.00%|    Indexes of the maximal elements of a N-dimensional array:\n",
      "  1184|         0|            0|            0|  0.00%|\n",
      "  1185|         0|            0|            0|  0.00%|    >>> ind = np.unravel_index(np.argmax(a, axis=None), a.shape)\n",
      "  1186|         0|            0|            0|  0.00%|    >>> ind\n",
      "  1187|         0|            0|            0|  0.00%|    (1, 2)\n",
      "  1188|         0|            0|            0|  0.00%|    >>> a[ind]\n",
      "  1189|         0|            0|            0|  0.00%|    15\n",
      "  1190|         0|            0|            0|  0.00%|\n",
      "  1191|         0|            0|            0|  0.00%|    >>> b = np.arange(6)\n",
      "  1192|         0|            0|            0|  0.00%|    >>> b[1] = 5\n",
      "  1193|         0|            0|            0|  0.00%|    >>> b\n",
      "  1194|         0|            0|            0|  0.00%|    array([0, 5, 2, 3, 4, 5])\n",
      "  1195|         0|            0|            0|  0.00%|    >>> np.argmax(b)  # Only the first occurrence is returned.\n",
      "  1196|         0|            0|            0|  0.00%|    1\n",
      "  1197|         0|            0|            0|  0.00%|\n",
      "  1198|         0|            0|            0|  0.00%|    >>> x = np.array([[4,2,3], [1,0,3]])\n",
      "  1199|         0|            0|            0|  0.00%|    >>> index_array = np.argmax(x, axis=-1)\n",
      "  1200|         0|            0|            0|  0.00%|    >>> # Same as np.amax(x, axis=-1, keepdims=True)\n",
      "  1201|         0|            0|            0|  0.00%|    >>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1)\n",
      "  1202|         0|            0|            0|  0.00%|    array([[4],\n",
      "  1203|         0|            0|            0|  0.00%|           [3]])\n",
      "  1204|         0|            0|            0|  0.00%|    >>> # Same as np.amax(x, axis=-1)\n",
      "  1205|         0|            0|            0|  0.00%|    >>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1).squeeze(axis=-1)\n",
      "  1206|         0|            0|            0|  0.00%|    array([4, 3])\n",
      "  1207|         0|            0|            0|  0.00%|\n",
      "  1208|         0|            0|            0|  0.00%|    Setting `keepdims` to `True`,\n",
      "  1209|         0|            0|            0|  0.00%|\n",
      "  1210|         0|            0|            0|  0.00%|    >>> x = np.arange(24).reshape((2, 3, 4))\n",
      "  1211|         0|            0|            0|  0.00%|    >>> res = np.argmax(x, axis=1, keepdims=True)\n",
      "  1212|         0|            0|            0|  0.00%|    >>> res.shape\n",
      "  1213|         0|            0|            0|  0.00%|    (2, 1, 4)\n",
      "  1214|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1215|         0|            0|            0|  0.00%|    kwds = {'keepdims': keepdims} if keepdims is not np._NoValue else {}\n",
      "  1216|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'argmax', axis=axis, out=out, **kwds)\n",
      "  1217|         0|            0|            0|  0.00%|\n",
      "  1218|         0|            0|            0|  0.00%|\n",
      "  1219|         0|            0|            0|  0.00%|def _argmin_dispatcher(a, axis=None, out=None, *, keepdims=np._NoValue):\n",
      "  1220|         0|            0|            0|  0.00%|    return (a, out)\n",
      "  1221|         0|            0|            0|  0.00%|\n",
      "  1222|         0|            0|            0|  0.00%|\n",
      "  1223|         0|            0|            0|  0.00%|@array_function_dispatch(_argmin_dispatcher)\n",
      "  1224|         0|            0|            0|  0.00%|def argmin(a, axis=None, out=None, *, keepdims=np._NoValue):\n",
      "  1225|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1226|         0|            0|            0|  0.00%|    Returns the indices of the minimum values along an axis.\n",
      "  1227|         0|            0|            0|  0.00%|\n",
      "  1228|         0|            0|            0|  0.00%|    Parameters\n",
      "  1229|         0|            0|            0|  0.00%|    ----------\n",
      "  1230|         0|            0|            0|  0.00%|    a : array_like\n",
      "  1231|         0|            0|            0|  0.00%|        Input array.\n",
      "  1232|         0|            0|            0|  0.00%|    axis : int, optional\n",
      "  1233|         0|            0|            0|  0.00%|        By default, the index is into the flattened array, otherwise\n",
      "  1234|         0|            0|            0|  0.00%|        along the specified axis.\n",
      "  1235|         0|            0|            0|  0.00%|    out : array, optional\n",
      "  1236|         0|            0|            0|  0.00%|        If provided, the result will be inserted into this array. It should\n",
      "  1237|         0|            0|            0|  0.00%|        be of the appropriate shape and dtype.\n",
      "  1238|         0|            0|            0|  0.00%|    keepdims : bool, optional\n",
      "  1239|         0|            0|            0|  0.00%|        If this is set to True, the axes which are reduced are left\n",
      "  1240|         0|            0|            0|  0.00%|        in the result as dimensions with size one. With this option,\n",
      "  1241|         0|            0|            0|  0.00%|        the result will broadcast correctly against the array.\n",
      "  1242|         0|            0|            0|  0.00%|\n",
      "  1243|         0|            0|            0|  0.00%|        .. versionadded:: 1.22.0\n",
      "  1244|         0|            0|            0|  0.00%|\n",
      "  1245|         0|            0|            0|  0.00%|    Returns\n",
      "  1246|         0|            0|            0|  0.00%|    -------\n",
      "  1247|         0|            0|            0|  0.00%|    index_array : ndarray of ints\n",
      "  1248|         0|            0|            0|  0.00%|        Array of indices into the array. It has the same shape as `a.shape`\n",
      "  1249|         0|            0|            0|  0.00%|        with the dimension along `axis` removed. If `keepdims` is set to True,\n",
      "  1250|         0|            0|            0|  0.00%|        then the size of `axis` will be 1 with the resulting array having same\n",
      "  1251|         0|            0|            0|  0.00%|        shape as `a.shape`.\n",
      "  1252|         0|            0|            0|  0.00%|\n",
      "  1253|         0|            0|            0|  0.00%|    See Also\n",
      "  1254|         0|            0|            0|  0.00%|    --------\n",
      "  1255|         0|            0|            0|  0.00%|    ndarray.argmin, argmax\n",
      "  1256|         0|            0|            0|  0.00%|    amin : The minimum value along a given axis.\n",
      "  1257|         0|            0|            0|  0.00%|    unravel_index : Convert a flat index into an index tuple.\n",
      "  1258|         0|            0|            0|  0.00%|    take_along_axis : Apply ``np.expand_dims(index_array, axis)``\n",
      "  1259|         0|            0|            0|  0.00%|                      from argmin to an array as if by calling min.\n",
      "  1260|         0|            0|            0|  0.00%|\n",
      "  1261|         0|            0|            0|  0.00%|    Notes\n",
      "  1262|         0|            0|            0|  0.00%|    -----\n",
      "  1263|         0|            0|            0|  0.00%|    In case of multiple occurrences of the minimum values, the indices\n",
      "  1264|         0|            0|            0|  0.00%|    corresponding to the first occurrence are returned.\n",
      "  1265|         0|            0|            0|  0.00%|\n",
      "  1266|         0|            0|            0|  0.00%|    Examples\n",
      "  1267|         0|            0|            0|  0.00%|    --------\n",
      "  1268|         0|            0|            0|  0.00%|    >>> a = np.arange(6).reshape(2,3) + 10\n",
      "  1269|         0|            0|            0|  0.00%|    >>> a\n",
      "  1270|         0|            0|            0|  0.00%|    array([[10, 11, 12],\n",
      "  1271|         0|            0|            0|  0.00%|           [13, 14, 15]])\n",
      "  1272|         0|            0|            0|  0.00%|    >>> np.argmin(a)\n",
      "  1273|         0|            0|            0|  0.00%|    0\n",
      "  1274|         0|            0|            0|  0.00%|    >>> np.argmin(a, axis=0)\n",
      "  1275|         0|            0|            0|  0.00%|    array([0, 0, 0])\n",
      "  1276|         0|            0|            0|  0.00%|    >>> np.argmin(a, axis=1)\n",
      "  1277|         0|            0|            0|  0.00%|    array([0, 0])\n",
      "  1278|         0|            0|            0|  0.00%|\n",
      "  1279|         0|            0|            0|  0.00%|    Indices of the minimum elements of a N-dimensional array:\n",
      "  1280|         0|            0|            0|  0.00%|\n",
      "  1281|         0|            0|            0|  0.00%|    >>> ind = np.unravel_index(np.argmin(a, axis=None), a.shape)\n",
      "  1282|         0|            0|            0|  0.00%|    >>> ind\n",
      "  1283|         0|            0|            0|  0.00%|    (0, 0)\n",
      "  1284|         0|            0|            0|  0.00%|    >>> a[ind]\n",
      "  1285|         0|            0|            0|  0.00%|    10\n",
      "  1286|         0|            0|            0|  0.00%|\n",
      "  1287|         0|            0|            0|  0.00%|    >>> b = np.arange(6) + 10\n",
      "  1288|         0|            0|            0|  0.00%|    >>> b[4] = 10\n",
      "  1289|         0|            0|            0|  0.00%|    >>> b\n",
      "  1290|         0|            0|            0|  0.00%|    array([10, 11, 12, 13, 10, 15])\n",
      "  1291|         0|            0|            0|  0.00%|    >>> np.argmin(b)  # Only the first occurrence is returned.\n",
      "  1292|         0|            0|            0|  0.00%|    0\n",
      "  1293|         0|            0|            0|  0.00%|\n",
      "  1294|         0|            0|            0|  0.00%|    >>> x = np.array([[4,2,3], [1,0,3]])\n",
      "  1295|         0|            0|            0|  0.00%|    >>> index_array = np.argmin(x, axis=-1)\n",
      "  1296|         0|            0|            0|  0.00%|    >>> # Same as np.amin(x, axis=-1, keepdims=True)\n",
      "  1297|         0|            0|            0|  0.00%|    >>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1)\n",
      "  1298|         0|            0|            0|  0.00%|    array([[2],\n",
      "  1299|         0|            0|            0|  0.00%|           [0]])\n",
      "  1300|         0|            0|            0|  0.00%|    >>> # Same as np.amax(x, axis=-1)\n",
      "  1301|         0|            0|            0|  0.00%|    >>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1).squeeze(axis=-1)\n",
      "  1302|         0|            0|            0|  0.00%|    array([2, 0])\n",
      "  1303|         0|            0|            0|  0.00%|\n",
      "  1304|         0|            0|            0|  0.00%|    Setting `keepdims` to `True`,\n",
      "  1305|         0|            0|            0|  0.00%|\n",
      "  1306|         0|            0|            0|  0.00%|    >>> x = np.arange(24).reshape((2, 3, 4))\n",
      "  1307|         0|            0|            0|  0.00%|    >>> res = np.argmin(x, axis=1, keepdims=True)\n",
      "  1308|         0|            0|            0|  0.00%|    >>> res.shape\n",
      "  1309|         0|            0|            0|  0.00%|    (2, 1, 4)\n",
      "  1310|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1311|         0|            0|            0|  0.00%|    kwds = {'keepdims': keepdims} if keepdims is not np._NoValue else {}\n",
      "  1312|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'argmin', axis=axis, out=out, **kwds)\n",
      "  1313|         0|            0|            0|  0.00%|\n",
      "  1314|         0|            0|            0|  0.00%|\n",
      "  1315|     74880|     0.106677|  1.42464e-06|  0.02%|def _searchsorted_dispatcher(a, v, side=None, sorter=None):\n",
      "  1316|     74880|     0.136409|   1.8217e-06|  0.02%|    return (a, v, sorter)\n",
      "  1317|         0|            0|            0|  0.00%|\n",
      "  1318|         0|            0|            0|  0.00%|\n",
      "  1319|     74880|     0.119801|   1.5999e-06|  0.02%|@array_function_dispatch(_searchsorted_dispatcher)\n",
      "  1320|         0|            0|            0|  0.00%|def searchsorted(a, v, side='left', sorter=None):\n",
      "  1321|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1322|         0|            0|            0|  0.00%|    Find indices where elements should be inserted to maintain order.\n",
      "  1323|         0|            0|            0|  0.00%|\n",
      "  1324|         0|            0|            0|  0.00%|    Find the indices into a sorted array `a` such that, if the\n",
      "  1325|         0|            0|            0|  0.00%|    corresponding elements in `v` were inserted before the indices, the\n",
      "  1326|         0|            0|            0|  0.00%|    order of `a` would be preserved.\n",
      "  1327|         0|            0|            0|  0.00%|\n",
      "  1328|         0|            0|            0|  0.00%|    Assuming that `a` is sorted:\n",
      "  1329|         0|            0|            0|  0.00%|\n",
      "  1330|         0|            0|            0|  0.00%|    ======  ============================\n",
      "  1331|         0|            0|            0|  0.00%|    `side`  returned index `i` satisfies\n",
      "  1332|         0|            0|            0|  0.00%|    ======  ============================\n",
      "  1333|         0|            0|            0|  0.00%|    left    ``a[i-1] < v <= a[i]``\n",
      "  1334|         0|            0|            0|  0.00%|    right   ``a[i-1] <= v < a[i]``\n",
      "  1335|         0|            0|            0|  0.00%|    ======  ============================\n",
      "  1336|         0|            0|            0|  0.00%|\n",
      "  1337|         0|            0|            0|  0.00%|    Parameters\n",
      "  1338|         0|            0|            0|  0.00%|    ----------\n",
      "  1339|         0|            0|            0|  0.00%|    a : 1-D array_like\n",
      "  1340|         0|            0|            0|  0.00%|        Input array. If `sorter` is None, then it must be sorted in\n",
      "  1341|         0|            0|            0|  0.00%|        ascending order, otherwise `sorter` must be an array of indices\n",
      "  1342|         0|            0|            0|  0.00%|        that sort it.\n",
      "  1343|         0|            0|            0|  0.00%|    v : array_like\n",
      "  1344|         0|            0|            0|  0.00%|        Values to insert into `a`.\n",
      "  1345|         0|            0|            0|  0.00%|    side : {'left', 'right'}, optional\n",
      "  1346|         0|            0|            0|  0.00%|        If 'left', the index of the first suitable location found is given.\n",
      "  1347|         0|            0|            0|  0.00%|        If 'right', return the last such index.  If there is no suitable\n",
      "  1348|         0|            0|            0|  0.00%|        index, return either 0 or N (where N is the length of `a`).\n",
      "  1349|         0|            0|            0|  0.00%|    sorter : 1-D array_like, optional\n",
      "  1350|         0|            0|            0|  0.00%|        Optional array of integer indices that sort array a into ascending\n",
      "  1351|         0|            0|            0|  0.00%|        order. They are typically the result of argsort.\n",
      "  1352|         0|            0|            0|  0.00%|\n",
      "  1353|         0|            0|            0|  0.00%|        .. versionadded:: 1.7.0\n",
      "  1354|         0|            0|            0|  0.00%|\n",
      "  1355|         0|            0|            0|  0.00%|    Returns\n",
      "  1356|         0|            0|            0|  0.00%|    -------\n",
      "  1357|         0|            0|            0|  0.00%|    indices : int or array of ints\n",
      "  1358|         0|            0|            0|  0.00%|        Array of insertion points with the same shape as `v`,\n",
      "  1359|         0|            0|            0|  0.00%|        or an integer if `v` is a scalar.\n",
      "  1360|         0|            0|            0|  0.00%|\n",
      "  1361|         0|            0|            0|  0.00%|    See Also\n",
      "  1362|         0|            0|            0|  0.00%|    --------\n",
      "  1363|         0|            0|            0|  0.00%|    sort : Return a sorted copy of an array.\n",
      "  1364|         0|            0|            0|  0.00%|    histogram : Produce histogram from 1-D data.\n",
      "  1365|         0|            0|            0|  0.00%|\n",
      "  1366|         0|            0|            0|  0.00%|    Notes\n",
      "  1367|         0|            0|            0|  0.00%|    -----\n",
      "  1368|         0|            0|            0|  0.00%|    Binary search is used to find the required insertion points.\n",
      "  1369|         0|            0|            0|  0.00%|\n",
      "  1370|         0|            0|            0|  0.00%|    As of NumPy 1.4.0 `searchsorted` works with real/complex arrays containing\n",
      "  1371|         0|            0|            0|  0.00%|    `nan` values. The enhanced sort order is documented in `sort`.\n",
      "  1372|         0|            0|            0|  0.00%|\n",
      "  1373|         0|            0|            0|  0.00%|    This function uses the same algorithm as the builtin python `bisect.bisect_left`\n",
      "  1374|         0|            0|            0|  0.00%|    (``side='left'``) and `bisect.bisect_right` (``side='right'``) functions,\n",
      "  1375|         0|            0|            0|  0.00%|    which is also vectorized in the `v` argument.\n",
      "  1376|         0|            0|            0|  0.00%|\n",
      "  1377|         0|            0|            0|  0.00%|    Examples\n",
      "  1378|         0|            0|            0|  0.00%|    --------\n",
      "  1379|         0|            0|            0|  0.00%|    >>> np.searchsorted([1,2,3,4,5], 3)\n",
      "  1380|         0|            0|            0|  0.00%|    2\n",
      "  1381|         0|            0|            0|  0.00%|    >>> np.searchsorted([1,2,3,4,5], 3, side='right')\n",
      "  1382|         0|            0|            0|  0.00%|    3\n",
      "  1383|         0|            0|            0|  0.00%|    >>> np.searchsorted([1,2,3,4,5], [-10, 10, 2, 3])\n",
      "  1384|         0|            0|            0|  0.00%|    array([0, 5, 1, 2])\n",
      "  1385|         0|            0|            0|  0.00%|\n",
      "  1386|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1387|     74880|     0.499744|  6.67393e-06|  0.08%|    return _wrapfunc(a, 'searchsorted', v, side=side, sorter=sorter)\n",
      "(call)|     74880|     0.828706|  1.10671e-05|  0.13%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:51 _wrapfunc\n",
      "  1388|         0|            0|            0|  0.00%|\n",
      "  1389|         0|            0|            0|  0.00%|\n",
      "  1390|         0|            0|            0|  0.00%|def _resize_dispatcher(a, new_shape):\n",
      "  1391|         0|            0|            0|  0.00%|    return (a,)\n",
      "  1392|         0|            0|            0|  0.00%|\n",
      "  1393|         0|            0|            0|  0.00%|\n",
      "  1394|         0|            0|            0|  0.00%|@array_function_dispatch(_resize_dispatcher)\n",
      "  1395|         0|            0|            0|  0.00%|def resize(a, new_shape):\n",
      "  1396|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1397|         0|            0|            0|  0.00%|    Return a new array with the specified shape.\n",
      "  1398|         0|            0|            0|  0.00%|\n",
      "  1399|         0|            0|            0|  0.00%|    If the new array is larger than the original array, then the new\n",
      "  1400|         0|            0|            0|  0.00%|    array is filled with repeated copies of `a`.  Note that this behavior\n",
      "  1401|         0|            0|            0|  0.00%|    is different from a.resize(new_shape) which fills with zeros instead\n",
      "  1402|         0|            0|            0|  0.00%|    of repeated copies of `a`.\n",
      "  1403|         0|            0|            0|  0.00%|\n",
      "  1404|         0|            0|            0|  0.00%|    Parameters\n",
      "  1405|         0|            0|            0|  0.00%|    ----------\n",
      "  1406|         0|            0|            0|  0.00%|    a : array_like\n",
      "  1407|         0|            0|            0|  0.00%|        Array to be resized.\n",
      "  1408|         0|            0|            0|  0.00%|\n",
      "  1409|         0|            0|            0|  0.00%|    new_shape : int or tuple of int\n",
      "  1410|         0|            0|            0|  0.00%|        Shape of resized array.\n",
      "  1411|         0|            0|            0|  0.00%|\n",
      "  1412|         0|            0|            0|  0.00%|    Returns\n",
      "  1413|         0|            0|            0|  0.00%|    -------\n",
      "  1414|         0|            0|            0|  0.00%|    reshaped_array : ndarray\n",
      "  1415|         0|            0|            0|  0.00%|        The new array is formed from the data in the old array, repeated\n",
      "  1416|         0|            0|            0|  0.00%|        if necessary to fill out the required number of elements.  The\n",
      "  1417|         0|            0|            0|  0.00%|        data are repeated iterating over the array in C-order.\n",
      "  1418|         0|            0|            0|  0.00%|\n",
      "  1419|         0|            0|            0|  0.00%|    See Also\n",
      "  1420|         0|            0|            0|  0.00%|    --------\n",
      "  1421|         0|            0|            0|  0.00%|    numpy.reshape : Reshape an array without changing the total size.\n",
      "  1422|         0|            0|            0|  0.00%|    numpy.pad : Enlarge and pad an array.\n",
      "  1423|         0|            0|            0|  0.00%|    numpy.repeat : Repeat elements of an array.\n",
      "  1424|         0|            0|            0|  0.00%|    ndarray.resize : resize an array in-place.\n",
      "  1425|         0|            0|            0|  0.00%|\n",
      "  1426|         0|            0|            0|  0.00%|    Notes\n",
      "  1427|         0|            0|            0|  0.00%|    -----\n",
      "  1428|         0|            0|            0|  0.00%|    When the total size of the array does not change `~numpy.reshape` should\n",
      "  1429|         0|            0|            0|  0.00%|    be used.  In most other cases either indexing (to reduce the size)\n",
      "  1430|         0|            0|            0|  0.00%|    or padding (to increase the size) may be a more appropriate solution.\n",
      "  1431|         0|            0|            0|  0.00%|\n",
      "  1432|         0|            0|            0|  0.00%|    Warning: This functionality does **not** consider axes separately,\n",
      "  1433|         0|            0|            0|  0.00%|    i.e. it does not apply interpolation/extrapolation.\n",
      "  1434|         0|            0|            0|  0.00%|    It fills the return array with the required number of elements, iterating\n",
      "  1435|         0|            0|            0|  0.00%|    over `a` in C-order, disregarding axes (and cycling back from the start if\n",
      "  1436|         0|            0|            0|  0.00%|    the new shape is larger).  This functionality is therefore not suitable to\n",
      "  1437|         0|            0|            0|  0.00%|    resize images, or data where each axis represents a separate and distinct\n",
      "  1438|         0|            0|            0|  0.00%|    entity.\n",
      "  1439|         0|            0|            0|  0.00%|\n",
      "  1440|         0|            0|            0|  0.00%|    Examples\n",
      "  1441|         0|            0|            0|  0.00%|    --------\n",
      "  1442|         0|            0|            0|  0.00%|    >>> a=np.array([[0,1],[2,3]])\n",
      "  1443|         0|            0|            0|  0.00%|    >>> np.resize(a,(2,3))\n",
      "  1444|         0|            0|            0|  0.00%|    array([[0, 1, 2],\n",
      "  1445|         0|            0|            0|  0.00%|           [3, 0, 1]])\n",
      "  1446|         0|            0|            0|  0.00%|    >>> np.resize(a,(1,4))\n",
      "  1447|         0|            0|            0|  0.00%|    array([[0, 1, 2, 3]])\n",
      "  1448|         0|            0|            0|  0.00%|    >>> np.resize(a,(2,4))\n",
      "  1449|         0|            0|            0|  0.00%|    array([[0, 1, 2, 3],\n",
      "  1450|         0|            0|            0|  0.00%|           [0, 1, 2, 3]])\n",
      "  1451|         0|            0|            0|  0.00%|\n",
      "  1452|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1453|         0|            0|            0|  0.00%|    if isinstance(new_shape, (int, nt.integer)):\n",
      "  1454|         0|            0|            0|  0.00%|        new_shape = (new_shape,)\n",
      "  1455|         0|            0|            0|  0.00%|\n",
      "  1456|         0|            0|            0|  0.00%|    a = ravel(a)\n",
      "  1457|         0|            0|            0|  0.00%|\n",
      "  1458|         0|            0|            0|  0.00%|    new_size = 1\n",
      "  1459|         0|            0|            0|  0.00%|    for dim_length in new_shape:\n",
      "  1460|         0|            0|            0|  0.00%|        new_size *= dim_length\n",
      "  1461|         0|            0|            0|  0.00%|        if dim_length < 0:\n",
      "  1462|         0|            0|            0|  0.00%|            raise ValueError('all elements of `new_shape` must be non-negative')\n",
      "  1463|         0|            0|            0|  0.00%|\n",
      "  1464|         0|            0|            0|  0.00%|    if a.size == 0 or new_size == 0:\n",
      "  1465|         0|            0|            0|  0.00%|        # First case must zero fill. The second would have repeats == 0.\n",
      "  1466|         0|            0|            0|  0.00%|        return np.zeros_like(a, shape=new_shape)\n",
      "  1467|         0|            0|            0|  0.00%|\n",
      "  1468|         0|            0|            0|  0.00%|    repeats = -(-new_size // a.size)  # ceil division\n",
      "  1469|         0|            0|            0|  0.00%|    a = concatenate((a,) * repeats)[:new_size]\n",
      "  1470|         0|            0|            0|  0.00%|\n",
      "  1471|         0|            0|            0|  0.00%|    return reshape(a, new_shape)\n",
      "  1472|         0|            0|            0|  0.00%|\n",
      "  1473|         0|            0|            0|  0.00%|\n",
      "  1474|         0|            0|            0|  0.00%|def _squeeze_dispatcher(a, axis=None):\n",
      "  1475|         0|            0|            0|  0.00%|    return (a,)\n",
      "  1476|         0|            0|            0|  0.00%|\n",
      "  1477|         0|            0|            0|  0.00%|\n",
      "  1478|         0|            0|            0|  0.00%|@array_function_dispatch(_squeeze_dispatcher)\n",
      "  1479|         0|            0|            0|  0.00%|def squeeze(a, axis=None):\n",
      "  1480|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1481|         0|            0|            0|  0.00%|    Remove axes of length one from `a`.\n",
      "  1482|         0|            0|            0|  0.00%|\n",
      "  1483|         0|            0|            0|  0.00%|    Parameters\n",
      "  1484|         0|            0|            0|  0.00%|    ----------\n",
      "  1485|         0|            0|            0|  0.00%|    a : array_like\n",
      "  1486|         0|            0|            0|  0.00%|        Input data.\n",
      "  1487|         0|            0|            0|  0.00%|    axis : None or int or tuple of ints, optional\n",
      "  1488|         0|            0|            0|  0.00%|        .. versionadded:: 1.7.0\n",
      "  1489|         0|            0|            0|  0.00%|\n",
      "  1490|         0|            0|            0|  0.00%|        Selects a subset of the entries of length one in the\n",
      "  1491|         0|            0|            0|  0.00%|        shape. If an axis is selected with shape entry greater than\n",
      "  1492|         0|            0|            0|  0.00%|        one, an error is raised.\n",
      "  1493|         0|            0|            0|  0.00%|\n",
      "  1494|         0|            0|            0|  0.00%|    Returns\n",
      "  1495|         0|            0|            0|  0.00%|    -------\n",
      "  1496|         0|            0|            0|  0.00%|    squeezed : ndarray\n",
      "  1497|         0|            0|            0|  0.00%|        The input array, but with all or a subset of the\n",
      "  1498|         0|            0|            0|  0.00%|        dimensions of length 1 removed. This is always `a` itself\n",
      "  1499|         0|            0|            0|  0.00%|        or a view into `a`. Note that if all axes are squeezed,\n",
      "  1500|         0|            0|            0|  0.00%|        the result is a 0d array and not a scalar.\n",
      "  1501|         0|            0|            0|  0.00%|\n",
      "  1502|         0|            0|            0|  0.00%|    Raises\n",
      "  1503|         0|            0|            0|  0.00%|    ------\n",
      "  1504|         0|            0|            0|  0.00%|    ValueError\n",
      "  1505|         0|            0|            0|  0.00%|        If `axis` is not None, and an axis being squeezed is not of length 1\n",
      "  1506|         0|            0|            0|  0.00%|\n",
      "  1507|         0|            0|            0|  0.00%|    See Also\n",
      "  1508|         0|            0|            0|  0.00%|    --------\n",
      "  1509|         0|            0|            0|  0.00%|    expand_dims : The inverse operation, adding entries of length one\n",
      "  1510|         0|            0|            0|  0.00%|    reshape : Insert, remove, and combine dimensions, and resize existing ones\n",
      "  1511|         0|            0|            0|  0.00%|\n",
      "  1512|         0|            0|            0|  0.00%|    Examples\n",
      "  1513|         0|            0|            0|  0.00%|    --------\n",
      "  1514|         0|            0|            0|  0.00%|    >>> x = np.array([[[0], [1], [2]]])\n",
      "  1515|         0|            0|            0|  0.00%|    >>> x.shape\n",
      "  1516|         0|            0|            0|  0.00%|    (1, 3, 1)\n",
      "  1517|         0|            0|            0|  0.00%|    >>> np.squeeze(x).shape\n",
      "  1518|         0|            0|            0|  0.00%|    (3,)\n",
      "  1519|         0|            0|            0|  0.00%|    >>> np.squeeze(x, axis=0).shape\n",
      "  1520|         0|            0|            0|  0.00%|    (3, 1)\n",
      "  1521|         0|            0|            0|  0.00%|    >>> np.squeeze(x, axis=1).shape\n",
      "  1522|         0|            0|            0|  0.00%|    Traceback (most recent call last):\n",
      "  1523|         0|            0|            0|  0.00%|    ...\n",
      "  1524|         0|            0|            0|  0.00%|    ValueError: cannot select an axis to squeeze out which has size not equal to one\n",
      "  1525|         0|            0|            0|  0.00%|    >>> np.squeeze(x, axis=2).shape\n",
      "  1526|         0|            0|            0|  0.00%|    (1, 3)\n",
      "  1527|         0|            0|            0|  0.00%|    >>> x = np.array([[1234]])\n",
      "  1528|         0|            0|            0|  0.00%|    >>> x.shape\n",
      "  1529|         0|            0|            0|  0.00%|    (1, 1)\n",
      "  1530|         0|            0|            0|  0.00%|    >>> np.squeeze(x)\n",
      "  1531|         0|            0|            0|  0.00%|    array(1234)  # 0d array\n",
      "  1532|         0|            0|            0|  0.00%|    >>> np.squeeze(x).shape\n",
      "  1533|         0|            0|            0|  0.00%|    ()\n",
      "  1534|         0|            0|            0|  0.00%|    >>> np.squeeze(x)[()]\n",
      "  1535|         0|            0|            0|  0.00%|    1234\n",
      "  1536|         0|            0|            0|  0.00%|\n",
      "  1537|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1538|         0|            0|            0|  0.00%|    try:\n",
      "  1539|         0|            0|            0|  0.00%|        squeeze = a.squeeze\n",
      "  1540|         0|            0|            0|  0.00%|    except AttributeError:\n",
      "  1541|         0|            0|            0|  0.00%|        return _wrapit(a, 'squeeze', axis=axis)\n",
      "  1542|         0|            0|            0|  0.00%|    if axis is None:\n",
      "  1543|         0|            0|            0|  0.00%|        return squeeze()\n",
      "  1544|         0|            0|            0|  0.00%|    else:\n",
      "  1545|         0|            0|            0|  0.00%|        return squeeze(axis=axis)\n",
      "  1546|         0|            0|            0|  0.00%|\n",
      "  1547|         0|            0|            0|  0.00%|\n",
      "  1548|         0|            0|            0|  0.00%|def _diagonal_dispatcher(a, offset=None, axis1=None, axis2=None):\n",
      "  1549|         0|            0|            0|  0.00%|    return (a,)\n",
      "  1550|         0|            0|            0|  0.00%|\n",
      "  1551|         0|            0|            0|  0.00%|\n",
      "  1552|         0|            0|            0|  0.00%|@array_function_dispatch(_diagonal_dispatcher)\n",
      "  1553|         0|            0|            0|  0.00%|def diagonal(a, offset=0, axis1=0, axis2=1):\n",
      "  1554|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1555|         0|            0|            0|  0.00%|    Return specified diagonals.\n",
      "  1556|         0|            0|            0|  0.00%|\n",
      "  1557|         0|            0|            0|  0.00%|    If `a` is 2-D, returns the diagonal of `a` with the given offset,\n",
      "  1558|         0|            0|            0|  0.00%|    i.e., the collection of elements of the form ``a[i, i+offset]``.  If\n",
      "  1559|         0|            0|            0|  0.00%|    `a` has more than two dimensions, then the axes specified by `axis1`\n",
      "  1560|         0|            0|            0|  0.00%|    and `axis2` are used to determine the 2-D sub-array whose diagonal is\n",
      "  1561|         0|            0|            0|  0.00%|    returned.  The shape of the resulting array can be determined by\n",
      "  1562|         0|            0|            0|  0.00%|    removing `axis1` and `axis2` and appending an index to the right equal\n",
      "  1563|         0|            0|            0|  0.00%|    to the size of the resulting diagonals.\n",
      "  1564|         0|            0|            0|  0.00%|\n",
      "  1565|         0|            0|            0|  0.00%|    In versions of NumPy prior to 1.7, this function always returned a new,\n",
      "  1566|         0|            0|            0|  0.00%|    independent array containing a copy of the values in the diagonal.\n",
      "  1567|         0|            0|            0|  0.00%|\n",
      "  1568|         0|            0|            0|  0.00%|    In NumPy 1.7 and 1.8, it continues to return a copy of the diagonal,\n",
      "  1569|         0|            0|            0|  0.00%|    but depending on this fact is deprecated. Writing to the resulting\n",
      "  1570|         0|            0|            0|  0.00%|    array continues to work as it used to, but a FutureWarning is issued.\n",
      "  1571|         0|            0|            0|  0.00%|\n",
      "  1572|         0|            0|            0|  0.00%|    Starting in NumPy 1.9 it returns a read-only view on the original array.\n",
      "  1573|         0|            0|            0|  0.00%|    Attempting to write to the resulting array will produce an error.\n",
      "  1574|         0|            0|            0|  0.00%|\n",
      "  1575|         0|            0|            0|  0.00%|    In some future release, it will return a read/write view and writing to\n",
      "  1576|         0|            0|            0|  0.00%|    the returned array will alter your original array.  The returned array\n",
      "  1577|         0|            0|            0|  0.00%|    will have the same type as the input array.\n",
      "  1578|         0|            0|            0|  0.00%|\n",
      "  1579|         0|            0|            0|  0.00%|    If you don't write to the array returned by this function, then you can\n",
      "  1580|         0|            0|            0|  0.00%|    just ignore all of the above.\n",
      "  1581|         0|            0|            0|  0.00%|\n",
      "  1582|         0|            0|            0|  0.00%|    If you depend on the current behavior, then we suggest copying the\n",
      "  1583|         0|            0|            0|  0.00%|    returned array explicitly, i.e., use ``np.diagonal(a).copy()`` instead\n",
      "  1584|         0|            0|            0|  0.00%|    of just ``np.diagonal(a)``. This will work with both past and future\n",
      "  1585|         0|            0|            0|  0.00%|    versions of NumPy.\n",
      "  1586|         0|            0|            0|  0.00%|\n",
      "  1587|         0|            0|            0|  0.00%|    Parameters\n",
      "  1588|         0|            0|            0|  0.00%|    ----------\n",
      "  1589|         0|            0|            0|  0.00%|    a : array_like\n",
      "  1590|         0|            0|            0|  0.00%|        Array from which the diagonals are taken.\n",
      "  1591|         0|            0|            0|  0.00%|    offset : int, optional\n",
      "  1592|         0|            0|            0|  0.00%|        Offset of the diagonal from the main diagonal.  Can be positive or\n",
      "  1593|         0|            0|            0|  0.00%|        negative.  Defaults to main diagonal (0).\n",
      "  1594|         0|            0|            0|  0.00%|    axis1 : int, optional\n",
      "  1595|         0|            0|            0|  0.00%|        Axis to be used as the first axis of the 2-D sub-arrays from which\n",
      "  1596|         0|            0|            0|  0.00%|        the diagonals should be taken.  Defaults to first axis (0).\n",
      "  1597|         0|            0|            0|  0.00%|    axis2 : int, optional\n",
      "  1598|         0|            0|            0|  0.00%|        Axis to be used as the second axis of the 2-D sub-arrays from\n",
      "  1599|         0|            0|            0|  0.00%|        which the diagonals should be taken. Defaults to second axis (1).\n",
      "  1600|         0|            0|            0|  0.00%|\n",
      "  1601|         0|            0|            0|  0.00%|    Returns\n",
      "  1602|         0|            0|            0|  0.00%|    -------\n",
      "  1603|         0|            0|            0|  0.00%|    array_of_diagonals : ndarray\n",
      "  1604|         0|            0|            0|  0.00%|        If `a` is 2-D, then a 1-D array containing the diagonal and of the\n",
      "  1605|         0|            0|            0|  0.00%|        same type as `a` is returned unless `a` is a `matrix`, in which case\n",
      "  1606|         0|            0|            0|  0.00%|        a 1-D array rather than a (2-D) `matrix` is returned in order to\n",
      "  1607|         0|            0|            0|  0.00%|        maintain backward compatibility.\n",
      "  1608|         0|            0|            0|  0.00%|\n",
      "  1609|         0|            0|            0|  0.00%|        If ``a.ndim > 2``, then the dimensions specified by `axis1` and `axis2`\n",
      "  1610|         0|            0|            0|  0.00%|        are removed, and a new axis inserted at the end corresponding to the\n",
      "  1611|         0|            0|            0|  0.00%|        diagonal.\n",
      "  1612|         0|            0|            0|  0.00%|\n",
      "  1613|         0|            0|            0|  0.00%|    Raises\n",
      "  1614|         0|            0|            0|  0.00%|    ------\n",
      "  1615|         0|            0|            0|  0.00%|    ValueError\n",
      "  1616|         0|            0|            0|  0.00%|        If the dimension of `a` is less than 2.\n",
      "  1617|         0|            0|            0|  0.00%|\n",
      "  1618|         0|            0|            0|  0.00%|    See Also\n",
      "  1619|         0|            0|            0|  0.00%|    --------\n",
      "  1620|         0|            0|            0|  0.00%|    diag : MATLAB work-a-like for 1-D and 2-D arrays.\n",
      "  1621|         0|            0|            0|  0.00%|    diagflat : Create diagonal arrays.\n",
      "  1622|         0|            0|            0|  0.00%|    trace : Sum along diagonals.\n",
      "  1623|         0|            0|            0|  0.00%|\n",
      "  1624|         0|            0|            0|  0.00%|    Examples\n",
      "  1625|         0|            0|            0|  0.00%|    --------\n",
      "  1626|         0|            0|            0|  0.00%|    >>> a = np.arange(4).reshape(2,2)\n",
      "  1627|         0|            0|            0|  0.00%|    >>> a\n",
      "  1628|         0|            0|            0|  0.00%|    array([[0, 1],\n",
      "  1629|         0|            0|            0|  0.00%|           [2, 3]])\n",
      "  1630|         0|            0|            0|  0.00%|    >>> a.diagonal()\n",
      "  1631|         0|            0|            0|  0.00%|    array([0, 3])\n",
      "  1632|         0|            0|            0|  0.00%|    >>> a.diagonal(1)\n",
      "  1633|         0|            0|            0|  0.00%|    array([1])\n",
      "  1634|         0|            0|            0|  0.00%|\n",
      "  1635|         0|            0|            0|  0.00%|    A 3-D example:\n",
      "  1636|         0|            0|            0|  0.00%|\n",
      "  1637|         0|            0|            0|  0.00%|    >>> a = np.arange(8).reshape(2,2,2); a\n",
      "  1638|         0|            0|            0|  0.00%|    array([[[0, 1],\n",
      "  1639|         0|            0|            0|  0.00%|            [2, 3]],\n",
      "  1640|         0|            0|            0|  0.00%|           [[4, 5],\n",
      "  1641|         0|            0|            0|  0.00%|            [6, 7]]])\n",
      "  1642|         0|            0|            0|  0.00%|    >>> a.diagonal(0,  # Main diagonals of two arrays created by skipping\n",
      "  1643|         0|            0|            0|  0.00%|    ...            0,  # across the outer(left)-most axis last and\n",
      "  1644|         0|            0|            0|  0.00%|    ...            1)  # the \"middle\" (row) axis first.\n",
      "  1645|         0|            0|            0|  0.00%|    array([[0, 6],\n",
      "  1646|         0|            0|            0|  0.00%|           [1, 7]])\n",
      "  1647|         0|            0|            0|  0.00%|\n",
      "  1648|         0|            0|            0|  0.00%|    The sub-arrays whose main diagonals we just obtained; note that each\n",
      "  1649|         0|            0|            0|  0.00%|    corresponds to fixing the right-most (column) axis, and that the\n",
      "  1650|         0|            0|            0|  0.00%|    diagonals are \"packed\" in rows.\n",
      "  1651|         0|            0|            0|  0.00%|\n",
      "  1652|         0|            0|            0|  0.00%|    >>> a[:,:,0]  # main diagonal is [0 6]\n",
      "  1653|         0|            0|            0|  0.00%|    array([[0, 2],\n",
      "  1654|         0|            0|            0|  0.00%|           [4, 6]])\n",
      "  1655|         0|            0|            0|  0.00%|    >>> a[:,:,1]  # main diagonal is [1 7]\n",
      "  1656|         0|            0|            0|  0.00%|    array([[1, 3],\n",
      "  1657|         0|            0|            0|  0.00%|           [5, 7]])\n",
      "  1658|         0|            0|            0|  0.00%|\n",
      "  1659|         0|            0|            0|  0.00%|    The anti-diagonal can be obtained by reversing the order of elements\n",
      "  1660|         0|            0|            0|  0.00%|    using either `numpy.flipud` or `numpy.fliplr`.\n",
      "  1661|         0|            0|            0|  0.00%|\n",
      "  1662|         0|            0|            0|  0.00%|    >>> a = np.arange(9).reshape(3, 3)\n",
      "  1663|         0|            0|            0|  0.00%|    >>> a\n",
      "  1664|         0|            0|            0|  0.00%|    array([[0, 1, 2],\n",
      "  1665|         0|            0|            0|  0.00%|           [3, 4, 5],\n",
      "  1666|         0|            0|            0|  0.00%|           [6, 7, 8]])\n",
      "  1667|         0|            0|            0|  0.00%|    >>> np.fliplr(a).diagonal()  # Horizontal flip\n",
      "  1668|         0|            0|            0|  0.00%|    array([2, 4, 6])\n",
      "  1669|         0|            0|            0|  0.00%|    >>> np.flipud(a).diagonal()  # Vertical flip\n",
      "  1670|         0|            0|            0|  0.00%|    array([6, 4, 2])\n",
      "  1671|         0|            0|            0|  0.00%|\n",
      "  1672|         0|            0|            0|  0.00%|    Note that the order in which the diagonal is retrieved varies depending\n",
      "  1673|         0|            0|            0|  0.00%|    on the flip function.\n",
      "  1674|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1675|         0|            0|            0|  0.00%|    if isinstance(a, np.matrix):\n",
      "  1676|         0|            0|            0|  0.00%|        # Make diagonal of matrix 1-D to preserve backward compatibility.\n",
      "  1677|         0|            0|            0|  0.00%|        return asarray(a).diagonal(offset=offset, axis1=axis1, axis2=axis2)\n",
      "  1678|         0|            0|            0|  0.00%|    else:\n",
      "  1679|         0|            0|            0|  0.00%|        return asanyarray(a).diagonal(offset=offset, axis1=axis1, axis2=axis2)\n",
      "  1680|         0|            0|            0|  0.00%|\n",
      "  1681|         0|            0|            0|  0.00%|\n",
      "  1682|         0|            0|            0|  0.00%|def _trace_dispatcher(\n",
      "  1683|         0|            0|            0|  0.00%|        a, offset=None, axis1=None, axis2=None, dtype=None, out=None):\n",
      "  1684|         0|            0|            0|  0.00%|    return (a, out)\n",
      "  1685|         0|            0|            0|  0.00%|\n",
      "  1686|         0|            0|            0|  0.00%|\n",
      "  1687|         0|            0|            0|  0.00%|@array_function_dispatch(_trace_dispatcher)\n",
      "  1688|         0|            0|            0|  0.00%|def trace(a, offset=0, axis1=0, axis2=1, dtype=None, out=None):\n",
      "  1689|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1690|         0|            0|            0|  0.00%|    Return the sum along diagonals of the array.\n",
      "  1691|         0|            0|            0|  0.00%|\n",
      "  1692|         0|            0|            0|  0.00%|    If `a` is 2-D, the sum along its diagonal with the given offset\n",
      "  1693|         0|            0|            0|  0.00%|    is returned, i.e., the sum of elements ``a[i,i+offset]`` for all i.\n",
      "  1694|         0|            0|            0|  0.00%|\n",
      "  1695|         0|            0|            0|  0.00%|    If `a` has more than two dimensions, then the axes specified by axis1 and\n",
      "  1696|         0|            0|            0|  0.00%|    axis2 are used to determine the 2-D sub-arrays whose traces are returned.\n",
      "  1697|         0|            0|            0|  0.00%|    The shape of the resulting array is the same as that of `a` with `axis1`\n",
      "  1698|         0|            0|            0|  0.00%|    and `axis2` removed.\n",
      "  1699|         0|            0|            0|  0.00%|\n",
      "  1700|         0|            0|            0|  0.00%|    Parameters\n",
      "  1701|         0|            0|            0|  0.00%|    ----------\n",
      "  1702|         0|            0|            0|  0.00%|    a : array_like\n",
      "  1703|         0|            0|            0|  0.00%|        Input array, from which the diagonals are taken.\n",
      "  1704|         0|            0|            0|  0.00%|    offset : int, optional\n",
      "  1705|         0|            0|            0|  0.00%|        Offset of the diagonal from the main diagonal. Can be both positive\n",
      "  1706|         0|            0|            0|  0.00%|        and negative. Defaults to 0.\n",
      "  1707|         0|            0|            0|  0.00%|    axis1, axis2 : int, optional\n",
      "  1708|         0|            0|            0|  0.00%|        Axes to be used as the first and second axis of the 2-D sub-arrays\n",
      "  1709|         0|            0|            0|  0.00%|        from which the diagonals should be taken. Defaults are the first two\n",
      "  1710|         0|            0|            0|  0.00%|        axes of `a`.\n",
      "  1711|         0|            0|            0|  0.00%|    dtype : dtype, optional\n",
      "  1712|         0|            0|            0|  0.00%|        Determines the data-type of the returned array and of the accumulator\n",
      "  1713|         0|            0|            0|  0.00%|        where the elements are summed. If dtype has the value None and `a` is\n",
      "  1714|         0|            0|            0|  0.00%|        of integer type of precision less than the default integer\n",
      "  1715|         0|            0|            0|  0.00%|        precision, then the default integer precision is used. Otherwise,\n",
      "  1716|         0|            0|            0|  0.00%|        the precision is the same as that of `a`.\n",
      "  1717|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  1718|         0|            0|            0|  0.00%|        Array into which the output is placed. Its type is preserved and\n",
      "  1719|         0|            0|            0|  0.00%|        it must be of the right shape to hold the output.\n",
      "  1720|         0|            0|            0|  0.00%|\n",
      "  1721|         0|            0|            0|  0.00%|    Returns\n",
      "  1722|         0|            0|            0|  0.00%|    -------\n",
      "  1723|         0|            0|            0|  0.00%|    sum_along_diagonals : ndarray\n",
      "  1724|         0|            0|            0|  0.00%|        If `a` is 2-D, the sum along the diagonal is returned.  If `a` has\n",
      "  1725|         0|            0|            0|  0.00%|        larger dimensions, then an array of sums along diagonals is returned.\n",
      "  1726|         0|            0|            0|  0.00%|\n",
      "  1727|         0|            0|            0|  0.00%|    See Also\n",
      "  1728|         0|            0|            0|  0.00%|    --------\n",
      "  1729|         0|            0|            0|  0.00%|    diag, diagonal, diagflat\n",
      "  1730|         0|            0|            0|  0.00%|\n",
      "  1731|         0|            0|            0|  0.00%|    Examples\n",
      "  1732|         0|            0|            0|  0.00%|    --------\n",
      "  1733|         0|            0|            0|  0.00%|    >>> np.trace(np.eye(3))\n",
      "  1734|         0|            0|            0|  0.00%|    3.0\n",
      "  1735|         0|            0|            0|  0.00%|    >>> a = np.arange(8).reshape((2,2,2))\n",
      "  1736|         0|            0|            0|  0.00%|    >>> np.trace(a)\n",
      "  1737|         0|            0|            0|  0.00%|    array([6, 8])\n",
      "  1738|         0|            0|            0|  0.00%|\n",
      "  1739|         0|            0|            0|  0.00%|    >>> a = np.arange(24).reshape((2,2,2,3))\n",
      "  1740|         0|            0|            0|  0.00%|    >>> np.trace(a).shape\n",
      "  1741|         0|            0|            0|  0.00%|    (2, 3)\n",
      "  1742|         0|            0|            0|  0.00%|\n",
      "  1743|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1744|         0|            0|            0|  0.00%|    if isinstance(a, np.matrix):\n",
      "  1745|         0|            0|            0|  0.00%|        # Get trace of matrix via an array to preserve backward compatibility.\n",
      "  1746|         0|            0|            0|  0.00%|        return asarray(a).trace(offset=offset, axis1=axis1, axis2=axis2, dtype=dtype, out=out)\n",
      "  1747|         0|            0|            0|  0.00%|    else:\n",
      "  1748|         0|            0|            0|  0.00%|        return asanyarray(a).trace(offset=offset, axis1=axis1, axis2=axis2, dtype=dtype, out=out)\n",
      "  1749|         0|            0|            0|  0.00%|\n",
      "  1750|         0|            0|            0|  0.00%|\n",
      "  1751|         0|            0|            0|  0.00%|def _ravel_dispatcher(a, order=None):\n",
      "  1752|         0|            0|            0|  0.00%|    return (a,)\n",
      "  1753|         0|            0|            0|  0.00%|\n",
      "  1754|         0|            0|            0|  0.00%|\n",
      "  1755|         0|            0|            0|  0.00%|@array_function_dispatch(_ravel_dispatcher)\n",
      "  1756|         0|            0|            0|  0.00%|def ravel(a, order='C'):\n",
      "  1757|         0|            0|            0|  0.00%|    \"\"\"Return a contiguous flattened array.\n",
      "  1758|         0|            0|            0|  0.00%|\n",
      "  1759|         0|            0|            0|  0.00%|    A 1-D array, containing the elements of the input, is returned.  A copy is\n",
      "  1760|         0|            0|            0|  0.00%|    made only if needed.\n",
      "  1761|         0|            0|            0|  0.00%|\n",
      "  1762|         0|            0|            0|  0.00%|    As of NumPy 1.10, the returned array will have the same type as the input\n",
      "  1763|         0|            0|            0|  0.00%|    array. (for example, a masked array will be returned for a masked array\n",
      "  1764|         0|            0|            0|  0.00%|    input)\n",
      "  1765|         0|            0|            0|  0.00%|\n",
      "  1766|         0|            0|            0|  0.00%|    Parameters\n",
      "  1767|         0|            0|            0|  0.00%|    ----------\n",
      "  1768|         0|            0|            0|  0.00%|    a : array_like\n",
      "  1769|         0|            0|            0|  0.00%|        Input array.  The elements in `a` are read in the order specified by\n",
      "  1770|         0|            0|            0|  0.00%|        `order`, and packed as a 1-D array.\n",
      "  1771|         0|            0|            0|  0.00%|    order : {'C','F', 'A', 'K'}, optional\n",
      "  1772|         0|            0|            0|  0.00%|\n",
      "  1773|         0|            0|            0|  0.00%|        The elements of `a` are read using this index order. 'C' means\n",
      "  1774|         0|            0|            0|  0.00%|        to index the elements in row-major, C-style order,\n",
      "  1775|         0|            0|            0|  0.00%|        with the last axis index changing fastest, back to the first\n",
      "  1776|         0|            0|            0|  0.00%|        axis index changing slowest.  'F' means to index the elements\n",
      "  1777|         0|            0|            0|  0.00%|        in column-major, Fortran-style order, with the\n",
      "  1778|         0|            0|            0|  0.00%|        first index changing fastest, and the last index changing\n",
      "  1779|         0|            0|            0|  0.00%|        slowest. Note that the 'C' and 'F' options take no account of\n",
      "  1780|         0|            0|            0|  0.00%|        the memory layout of the underlying array, and only refer to\n",
      "  1781|         0|            0|            0|  0.00%|        the order of axis indexing.  'A' means to read the elements in\n",
      "  1782|         0|            0|            0|  0.00%|        Fortran-like index order if `a` is Fortran *contiguous* in\n",
      "  1783|         0|            0|            0|  0.00%|        memory, C-like order otherwise.  'K' means to read the\n",
      "  1784|         0|            0|            0|  0.00%|        elements in the order they occur in memory, except for\n",
      "  1785|         0|            0|            0|  0.00%|        reversing the data when strides are negative.  By default, 'C'\n",
      "  1786|         0|            0|            0|  0.00%|        index order is used.\n",
      "  1787|         0|            0|            0|  0.00%|\n",
      "  1788|         0|            0|            0|  0.00%|    Returns\n",
      "  1789|         0|            0|            0|  0.00%|    -------\n",
      "  1790|         0|            0|            0|  0.00%|    y : array_like\n",
      "  1791|         0|            0|            0|  0.00%|        y is an array of the same subtype as `a`, with shape ``(a.size,)``.\n",
      "  1792|         0|            0|            0|  0.00%|        Note that matrices are special cased for backward compatibility, if `a`\n",
      "  1793|         0|            0|            0|  0.00%|        is a matrix, then y is a 1-D ndarray.\n",
      "  1794|         0|            0|            0|  0.00%|\n",
      "  1795|         0|            0|            0|  0.00%|    See Also\n",
      "  1796|         0|            0|            0|  0.00%|    --------\n",
      "  1797|         0|            0|            0|  0.00%|    ndarray.flat : 1-D iterator over an array.\n",
      "  1798|         0|            0|            0|  0.00%|    ndarray.flatten : 1-D array copy of the elements of an array\n",
      "  1799|         0|            0|            0|  0.00%|                      in row-major order.\n",
      "  1800|         0|            0|            0|  0.00%|    ndarray.reshape : Change the shape of an array without changing its data.\n",
      "  1801|         0|            0|            0|  0.00%|\n",
      "  1802|         0|            0|            0|  0.00%|    Notes\n",
      "  1803|         0|            0|            0|  0.00%|    -----\n",
      "  1804|         0|            0|            0|  0.00%|    In row-major, C-style order, in two dimensions, the row index\n",
      "  1805|         0|            0|            0|  0.00%|    varies the slowest, and the column index the quickest.  This can\n",
      "  1806|         0|            0|            0|  0.00%|    be generalized to multiple dimensions, where row-major order\n",
      "  1807|         0|            0|            0|  0.00%|    implies that the index along the first axis varies slowest, and\n",
      "  1808|         0|            0|            0|  0.00%|    the index along the last quickest.  The opposite holds for\n",
      "  1809|         0|            0|            0|  0.00%|    column-major, Fortran-style index ordering.\n",
      "  1810|         0|            0|            0|  0.00%|\n",
      "  1811|         0|            0|            0|  0.00%|    When a view is desired in as many cases as possible, ``arr.reshape(-1)``\n",
      "  1812|         0|            0|            0|  0.00%|    may be preferable.\n",
      "  1813|         0|            0|            0|  0.00%|\n",
      "  1814|         0|            0|            0|  0.00%|    Examples\n",
      "  1815|         0|            0|            0|  0.00%|    --------\n",
      "  1816|         0|            0|            0|  0.00%|    It is equivalent to ``reshape(-1, order=order)``.\n",
      "  1817|         0|            0|            0|  0.00%|\n",
      "  1818|         0|            0|            0|  0.00%|    >>> x = np.array([[1, 2, 3], [4, 5, 6]])\n",
      "  1819|         0|            0|            0|  0.00%|    >>> np.ravel(x)\n",
      "  1820|         0|            0|            0|  0.00%|    array([1, 2, 3, 4, 5, 6])\n",
      "  1821|         0|            0|            0|  0.00%|\n",
      "  1822|         0|            0|            0|  0.00%|    >>> x.reshape(-1)\n",
      "  1823|         0|            0|            0|  0.00%|    array([1, 2, 3, 4, 5, 6])\n",
      "  1824|         0|            0|            0|  0.00%|\n",
      "  1825|         0|            0|            0|  0.00%|    >>> np.ravel(x, order='F')\n",
      "  1826|         0|            0|            0|  0.00%|    array([1, 4, 2, 5, 3, 6])\n",
      "  1827|         0|            0|            0|  0.00%|\n",
      "  1828|         0|            0|            0|  0.00%|    When ``order`` is 'A', it will preserve the array's 'C' or 'F' ordering:\n",
      "  1829|         0|            0|            0|  0.00%|\n",
      "  1830|         0|            0|            0|  0.00%|    >>> np.ravel(x.T)\n",
      "  1831|         0|            0|            0|  0.00%|    array([1, 4, 2, 5, 3, 6])\n",
      "  1832|         0|            0|            0|  0.00%|    >>> np.ravel(x.T, order='A')\n",
      "  1833|         0|            0|            0|  0.00%|    array([1, 2, 3, 4, 5, 6])\n",
      "  1834|         0|            0|            0|  0.00%|\n",
      "  1835|         0|            0|            0|  0.00%|    When ``order`` is 'K', it will preserve orderings that are neither 'C'\n",
      "  1836|         0|            0|            0|  0.00%|    nor 'F', but won't reverse axes:\n",
      "  1837|         0|            0|            0|  0.00%|\n",
      "  1838|         0|            0|            0|  0.00%|    >>> a = np.arange(3)[::-1]; a\n",
      "  1839|         0|            0|            0|  0.00%|    array([2, 1, 0])\n",
      "  1840|         0|            0|            0|  0.00%|    >>> a.ravel(order='C')\n",
      "  1841|         0|            0|            0|  0.00%|    array([2, 1, 0])\n",
      "  1842|         0|            0|            0|  0.00%|    >>> a.ravel(order='K')\n",
      "  1843|         0|            0|            0|  0.00%|    array([2, 1, 0])\n",
      "  1844|         0|            0|            0|  0.00%|\n",
      "  1845|         0|            0|            0|  0.00%|    >>> a = np.arange(12).reshape(2,3,2).swapaxes(1,2); a\n",
      "  1846|         0|            0|            0|  0.00%|    array([[[ 0,  2,  4],\n",
      "  1847|         0|            0|            0|  0.00%|            [ 1,  3,  5]],\n",
      "  1848|         0|            0|            0|  0.00%|           [[ 6,  8, 10],\n",
      "  1849|         0|            0|            0|  0.00%|            [ 7,  9, 11]]])\n",
      "  1850|         0|            0|            0|  0.00%|    >>> a.ravel(order='C')\n",
      "  1851|         0|            0|            0|  0.00%|    array([ 0,  2,  4,  1,  3,  5,  6,  8, 10,  7,  9, 11])\n",
      "  1852|         0|            0|            0|  0.00%|    >>> a.ravel(order='K')\n",
      "  1853|         0|            0|            0|  0.00%|    array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "  1854|         0|            0|            0|  0.00%|\n",
      "  1855|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1856|         0|            0|            0|  0.00%|    if isinstance(a, np.matrix):\n",
      "  1857|         0|            0|            0|  0.00%|        return asarray(a).ravel(order=order)\n",
      "  1858|         0|            0|            0|  0.00%|    else:\n",
      "  1859|         0|            0|            0|  0.00%|        return asanyarray(a).ravel(order=order)\n",
      "  1860|         0|            0|            0|  0.00%|\n",
      "  1861|         0|            0|            0|  0.00%|\n",
      "  1862|         0|            0|            0|  0.00%|def _nonzero_dispatcher(a):\n",
      "  1863|         0|            0|            0|  0.00%|    return (a,)\n",
      "  1864|         0|            0|            0|  0.00%|\n",
      "  1865|         0|            0|            0|  0.00%|\n",
      "  1866|         0|            0|            0|  0.00%|@array_function_dispatch(_nonzero_dispatcher)\n",
      "  1867|         0|            0|            0|  0.00%|def nonzero(a):\n",
      "  1868|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1869|         0|            0|            0|  0.00%|    Return the indices of the elements that are non-zero.\n",
      "  1870|         0|            0|            0|  0.00%|\n",
      "  1871|         0|            0|            0|  0.00%|    Returns a tuple of arrays, one for each dimension of `a`,\n",
      "  1872|         0|            0|            0|  0.00%|    containing the indices of the non-zero elements in that\n",
      "  1873|         0|            0|            0|  0.00%|    dimension. The values in `a` are always tested and returned in\n",
      "  1874|         0|            0|            0|  0.00%|    row-major, C-style order.\n",
      "  1875|         0|            0|            0|  0.00%|\n",
      "  1876|         0|            0|            0|  0.00%|    To group the indices by element, rather than dimension, use `argwhere`,\n",
      "  1877|         0|            0|            0|  0.00%|    which returns a row for each non-zero element.\n",
      "  1878|         0|            0|            0|  0.00%|\n",
      "  1879|         0|            0|            0|  0.00%|    .. note::\n",
      "  1880|         0|            0|            0|  0.00%|\n",
      "  1881|         0|            0|            0|  0.00%|       When called on a zero-d array or scalar, ``nonzero(a)`` is treated\n",
      "  1882|         0|            0|            0|  0.00%|       as ``nonzero(atleast_1d(a))``.\n",
      "  1883|         0|            0|            0|  0.00%|\n",
      "  1884|         0|            0|            0|  0.00%|       .. deprecated:: 1.17.0\n",
      "  1885|         0|            0|            0|  0.00%|\n",
      "  1886|         0|            0|            0|  0.00%|          Use `atleast_1d` explicitly if this behavior is deliberate.\n",
      "  1887|         0|            0|            0|  0.00%|\n",
      "  1888|         0|            0|            0|  0.00%|    Parameters\n",
      "  1889|         0|            0|            0|  0.00%|    ----------\n",
      "  1890|         0|            0|            0|  0.00%|    a : array_like\n",
      "  1891|         0|            0|            0|  0.00%|        Input array.\n",
      "  1892|         0|            0|            0|  0.00%|\n",
      "  1893|         0|            0|            0|  0.00%|    Returns\n",
      "  1894|         0|            0|            0|  0.00%|    -------\n",
      "  1895|         0|            0|            0|  0.00%|    tuple_of_arrays : tuple\n",
      "  1896|         0|            0|            0|  0.00%|        Indices of elements that are non-zero.\n",
      "  1897|         0|            0|            0|  0.00%|\n",
      "  1898|         0|            0|            0|  0.00%|    See Also\n",
      "  1899|         0|            0|            0|  0.00%|    --------\n",
      "  1900|         0|            0|            0|  0.00%|    flatnonzero :\n",
      "  1901|         0|            0|            0|  0.00%|        Return indices that are non-zero in the flattened version of the input\n",
      "  1902|         0|            0|            0|  0.00%|        array.\n",
      "  1903|         0|            0|            0|  0.00%|    ndarray.nonzero :\n",
      "  1904|         0|            0|            0|  0.00%|        Equivalent ndarray method.\n",
      "  1905|         0|            0|            0|  0.00%|    count_nonzero :\n",
      "  1906|         0|            0|            0|  0.00%|        Counts the number of non-zero elements in the input array.\n",
      "  1907|         0|            0|            0|  0.00%|\n",
      "  1908|         0|            0|            0|  0.00%|    Notes\n",
      "  1909|         0|            0|            0|  0.00%|    -----\n",
      "  1910|         0|            0|            0|  0.00%|    While the nonzero values can be obtained with ``a[nonzero(a)]``, it is\n",
      "  1911|         0|            0|            0|  0.00%|    recommended to use ``x[x.astype(bool)]`` or ``x[x != 0]`` instead, which\n",
      "  1912|         0|            0|            0|  0.00%|    will correctly handle 0-d arrays.\n",
      "  1913|         0|            0|            0|  0.00%|\n",
      "  1914|         0|            0|            0|  0.00%|    Examples\n",
      "  1915|         0|            0|            0|  0.00%|    --------\n",
      "  1916|         0|            0|            0|  0.00%|    >>> x = np.array([[3, 0, 0], [0, 4, 0], [5, 6, 0]])\n",
      "  1917|         0|            0|            0|  0.00%|    >>> x\n",
      "  1918|         0|            0|            0|  0.00%|    array([[3, 0, 0],\n",
      "  1919|         0|            0|            0|  0.00%|           [0, 4, 0],\n",
      "  1920|         0|            0|            0|  0.00%|           [5, 6, 0]])\n",
      "  1921|         0|            0|            0|  0.00%|    >>> np.nonzero(x)\n",
      "  1922|         0|            0|            0|  0.00%|    (array([0, 1, 2, 2]), array([0, 1, 0, 1]))\n",
      "  1923|         0|            0|            0|  0.00%|\n",
      "  1924|         0|            0|            0|  0.00%|    >>> x[np.nonzero(x)]\n",
      "  1925|         0|            0|            0|  0.00%|    array([3, 4, 5, 6])\n",
      "  1926|         0|            0|            0|  0.00%|    >>> np.transpose(np.nonzero(x))\n",
      "  1927|         0|            0|            0|  0.00%|    array([[0, 0],\n",
      "  1928|         0|            0|            0|  0.00%|           [1, 1],\n",
      "  1929|         0|            0|            0|  0.00%|           [2, 0],\n",
      "  1930|         0|            0|            0|  0.00%|           [2, 1]])\n",
      "  1931|         0|            0|            0|  0.00%|\n",
      "  1932|         0|            0|            0|  0.00%|    A common use for ``nonzero`` is to find the indices of an array, where\n",
      "  1933|         0|            0|            0|  0.00%|    a condition is True.  Given an array `a`, the condition `a` > 3 is a\n",
      "  1934|         0|            0|            0|  0.00%|    boolean array and since False is interpreted as 0, np.nonzero(a > 3)\n",
      "  1935|         0|            0|            0|  0.00%|    yields the indices of the `a` where the condition is true.\n",
      "  1936|         0|            0|            0|  0.00%|\n",
      "  1937|         0|            0|            0|  0.00%|    >>> a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
      "  1938|         0|            0|            0|  0.00%|    >>> a > 3\n",
      "  1939|         0|            0|            0|  0.00%|    array([[False, False, False],\n",
      "  1940|         0|            0|            0|  0.00%|           [ True,  True,  True],\n",
      "  1941|         0|            0|            0|  0.00%|           [ True,  True,  True]])\n",
      "  1942|         0|            0|            0|  0.00%|    >>> np.nonzero(a > 3)\n",
      "  1943|         0|            0|            0|  0.00%|    (array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))\n",
      "  1944|         0|            0|            0|  0.00%|\n",
      "  1945|         0|            0|            0|  0.00%|    Using this result to index `a` is equivalent to using the mask directly:\n",
      "  1946|         0|            0|            0|  0.00%|\n",
      "  1947|         0|            0|            0|  0.00%|    >>> a[np.nonzero(a > 3)]\n",
      "  1948|         0|            0|            0|  0.00%|    array([4, 5, 6, 7, 8, 9])\n",
      "  1949|         0|            0|            0|  0.00%|    >>> a[a > 3]  # prefer this spelling\n",
      "  1950|         0|            0|            0|  0.00%|    array([4, 5, 6, 7, 8, 9])\n",
      "  1951|         0|            0|            0|  0.00%|\n",
      "  1952|         0|            0|            0|  0.00%|    ``nonzero`` can also be called as a method of the array.\n",
      "  1953|         0|            0|            0|  0.00%|\n",
      "  1954|         0|            0|            0|  0.00%|    >>> (a > 3).nonzero()\n",
      "  1955|         0|            0|            0|  0.00%|    (array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))\n",
      "  1956|         0|            0|            0|  0.00%|\n",
      "  1957|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1958|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'nonzero')\n",
      "  1959|         0|            0|            0|  0.00%|\n",
      "  1960|         0|            0|            0|  0.00%|\n",
      "  1961|         0|            0|            0|  0.00%|def _shape_dispatcher(a):\n",
      "  1962|         0|            0|            0|  0.00%|    return (a,)\n",
      "  1963|         0|            0|            0|  0.00%|\n",
      "  1964|         0|            0|            0|  0.00%|\n",
      "  1965|         0|            0|            0|  0.00%|@array_function_dispatch(_shape_dispatcher)\n",
      "  1966|         0|            0|            0|  0.00%|def shape(a):\n",
      "  1967|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1968|         0|            0|            0|  0.00%|    Return the shape of an array.\n",
      "  1969|         0|            0|            0|  0.00%|\n",
      "  1970|         0|            0|            0|  0.00%|    Parameters\n",
      "  1971|         0|            0|            0|  0.00%|    ----------\n",
      "  1972|         0|            0|            0|  0.00%|    a : array_like\n",
      "  1973|         0|            0|            0|  0.00%|        Input array.\n",
      "  1974|         0|            0|            0|  0.00%|\n",
      "  1975|         0|            0|            0|  0.00%|    Returns\n",
      "  1976|         0|            0|            0|  0.00%|    -------\n",
      "  1977|         0|            0|            0|  0.00%|    shape : tuple of ints\n",
      "  1978|         0|            0|            0|  0.00%|        The elements of the shape tuple give the lengths of the\n",
      "  1979|         0|            0|            0|  0.00%|        corresponding array dimensions.\n",
      "  1980|         0|            0|            0|  0.00%|\n",
      "  1981|         0|            0|            0|  0.00%|    See Also\n",
      "  1982|         0|            0|            0|  0.00%|    --------\n",
      "  1983|         0|            0|            0|  0.00%|    len\n",
      "  1984|         0|            0|            0|  0.00%|    ndarray.shape : Equivalent array method.\n",
      "  1985|         0|            0|            0|  0.00%|\n",
      "  1986|         0|            0|            0|  0.00%|    Examples\n",
      "  1987|         0|            0|            0|  0.00%|    --------\n",
      "  1988|         0|            0|            0|  0.00%|    >>> np.shape(np.eye(3))\n",
      "  1989|         0|            0|            0|  0.00%|    (3, 3)\n",
      "  1990|         0|            0|            0|  0.00%|    >>> np.shape([[1, 2]])\n",
      "  1991|         0|            0|            0|  0.00%|    (1, 2)\n",
      "  1992|         0|            0|            0|  0.00%|    >>> np.shape([0])\n",
      "  1993|         0|            0|            0|  0.00%|    (1,)\n",
      "  1994|         0|            0|            0|  0.00%|    >>> np.shape(0)\n",
      "  1995|         0|            0|            0|  0.00%|    ()\n",
      "  1996|         0|            0|            0|  0.00%|\n",
      "  1997|         0|            0|            0|  0.00%|    >>> a = np.array([(1, 2), (3, 4)], dtype=[('x', 'i4'), ('y', 'i4')])\n",
      "  1998|         0|            0|            0|  0.00%|    >>> np.shape(a)\n",
      "  1999|         0|            0|            0|  0.00%|    (2,)\n",
      "  2000|         0|            0|            0|  0.00%|    >>> a.shape\n",
      "  2001|         0|            0|            0|  0.00%|    (2,)\n",
      "  2002|         0|            0|            0|  0.00%|\n",
      "  2003|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2004|         0|            0|            0|  0.00%|    try:\n",
      "  2005|         0|            0|            0|  0.00%|        result = a.shape\n",
      "  2006|         0|            0|            0|  0.00%|    except AttributeError:\n",
      "  2007|         0|            0|            0|  0.00%|        result = asarray(a).shape\n",
      "  2008|         0|            0|            0|  0.00%|    return result\n",
      "  2009|         0|            0|            0|  0.00%|\n",
      "  2010|         0|            0|            0|  0.00%|\n",
      "  2011|         0|            0|            0|  0.00%|def _compress_dispatcher(condition, a, axis=None, out=None):\n",
      "  2012|         0|            0|            0|  0.00%|    return (condition, a, out)\n",
      "  2013|         0|            0|            0|  0.00%|\n",
      "  2014|         0|            0|            0|  0.00%|\n",
      "  2015|         0|            0|            0|  0.00%|@array_function_dispatch(_compress_dispatcher)\n",
      "  2016|         0|            0|            0|  0.00%|def compress(condition, a, axis=None, out=None):\n",
      "  2017|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2018|         0|            0|            0|  0.00%|    Return selected slices of an array along given axis.\n",
      "  2019|         0|            0|            0|  0.00%|\n",
      "  2020|         0|            0|            0|  0.00%|    When working along a given axis, a slice along that axis is returned in\n",
      "  2021|         0|            0|            0|  0.00%|    `output` for each index where `condition` evaluates to True. When\n",
      "  2022|         0|            0|            0|  0.00%|    working on a 1-D array, `compress` is equivalent to `extract`.\n",
      "  2023|         0|            0|            0|  0.00%|\n",
      "  2024|         0|            0|            0|  0.00%|    Parameters\n",
      "  2025|         0|            0|            0|  0.00%|    ----------\n",
      "  2026|         0|            0|            0|  0.00%|    condition : 1-D array of bools\n",
      "  2027|         0|            0|            0|  0.00%|        Array that selects which entries to return. If len(condition)\n",
      "  2028|         0|            0|            0|  0.00%|        is less than the size of `a` along the given axis, then output is\n",
      "  2029|         0|            0|            0|  0.00%|        truncated to the length of the condition array.\n",
      "  2030|         0|            0|            0|  0.00%|    a : array_like\n",
      "  2031|         0|            0|            0|  0.00%|        Array from which to extract a part.\n",
      "  2032|         0|            0|            0|  0.00%|    axis : int, optional\n",
      "  2033|         0|            0|            0|  0.00%|        Axis along which to take slices. If None (default), work on the\n",
      "  2034|         0|            0|            0|  0.00%|        flattened array.\n",
      "  2035|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  2036|         0|            0|            0|  0.00%|        Output array.  Its type is preserved and it must be of the right\n",
      "  2037|         0|            0|            0|  0.00%|        shape to hold the output.\n",
      "  2038|         0|            0|            0|  0.00%|\n",
      "  2039|         0|            0|            0|  0.00%|    Returns\n",
      "  2040|         0|            0|            0|  0.00%|    -------\n",
      "  2041|         0|            0|            0|  0.00%|    compressed_array : ndarray\n",
      "  2042|         0|            0|            0|  0.00%|        A copy of `a` without the slices along axis for which `condition`\n",
      "  2043|         0|            0|            0|  0.00%|        is false.\n",
      "  2044|         0|            0|            0|  0.00%|\n",
      "  2045|         0|            0|            0|  0.00%|    See Also\n",
      "  2046|         0|            0|            0|  0.00%|    --------\n",
      "  2047|         0|            0|            0|  0.00%|    take, choose, diag, diagonal, select\n",
      "  2048|         0|            0|            0|  0.00%|    ndarray.compress : Equivalent method in ndarray\n",
      "  2049|         0|            0|            0|  0.00%|    extract : Equivalent method when working on 1-D arrays\n",
      "  2050|         0|            0|            0|  0.00%|    :ref:`ufuncs-output-type`\n",
      "  2051|         0|            0|            0|  0.00%|\n",
      "  2052|         0|            0|            0|  0.00%|    Examples\n",
      "  2053|         0|            0|            0|  0.00%|    --------\n",
      "  2054|         0|            0|            0|  0.00%|    >>> a = np.array([[1, 2], [3, 4], [5, 6]])\n",
      "  2055|         0|            0|            0|  0.00%|    >>> a\n",
      "  2056|         0|            0|            0|  0.00%|    array([[1, 2],\n",
      "  2057|         0|            0|            0|  0.00%|           [3, 4],\n",
      "  2058|         0|            0|            0|  0.00%|           [5, 6]])\n",
      "  2059|         0|            0|            0|  0.00%|    >>> np.compress([0, 1], a, axis=0)\n",
      "  2060|         0|            0|            0|  0.00%|    array([[3, 4]])\n",
      "  2061|         0|            0|            0|  0.00%|    >>> np.compress([False, True, True], a, axis=0)\n",
      "  2062|         0|            0|            0|  0.00%|    array([[3, 4],\n",
      "  2063|         0|            0|            0|  0.00%|           [5, 6]])\n",
      "  2064|         0|            0|            0|  0.00%|    >>> np.compress([False, True], a, axis=1)\n",
      "  2065|         0|            0|            0|  0.00%|    array([[2],\n",
      "  2066|         0|            0|            0|  0.00%|           [4],\n",
      "  2067|         0|            0|            0|  0.00%|           [6]])\n",
      "  2068|         0|            0|            0|  0.00%|\n",
      "  2069|         0|            0|            0|  0.00%|    Working on the flattened array does not return slices along an axis but\n",
      "  2070|         0|            0|            0|  0.00%|    selects elements.\n",
      "  2071|         0|            0|            0|  0.00%|\n",
      "  2072|         0|            0|            0|  0.00%|    >>> np.compress([False, True], a)\n",
      "  2073|         0|            0|            0|  0.00%|    array([2])\n",
      "  2074|         0|            0|            0|  0.00%|\n",
      "  2075|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2076|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'compress', condition, axis=axis, out=out)\n",
      "  2077|         0|            0|            0|  0.00%|\n",
      "  2078|         0|            0|            0|  0.00%|\n",
      "  2079|         0|            0|            0|  0.00%|def _clip_dispatcher(a, a_min, a_max, out=None, **kwargs):\n",
      "  2080|         0|            0|            0|  0.00%|    return (a, a_min, a_max)\n",
      "  2081|         0|            0|            0|  0.00%|\n",
      "  2082|         0|            0|            0|  0.00%|\n",
      "  2083|         0|            0|            0|  0.00%|@array_function_dispatch(_clip_dispatcher)\n",
      "  2084|         0|            0|            0|  0.00%|def clip(a, a_min, a_max, out=None, **kwargs):\n",
      "  2085|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2086|         0|            0|            0|  0.00%|    Clip (limit) the values in an array.\n",
      "  2087|         0|            0|            0|  0.00%|\n",
      "  2088|         0|            0|            0|  0.00%|    Given an interval, values outside the interval are clipped to\n",
      "  2089|         0|            0|            0|  0.00%|    the interval edges.  For example, if an interval of ``[0, 1]``\n",
      "  2090|         0|            0|            0|  0.00%|    is specified, values smaller than 0 become 0, and values larger\n",
      "  2091|         0|            0|            0|  0.00%|    than 1 become 1.\n",
      "  2092|         0|            0|            0|  0.00%|\n",
      "  2093|         0|            0|            0|  0.00%|    Equivalent to but faster than ``np.minimum(a_max, np.maximum(a, a_min))``.\n",
      "  2094|         0|            0|            0|  0.00%|\n",
      "  2095|         0|            0|            0|  0.00%|    No check is performed to ensure ``a_min < a_max``.\n",
      "  2096|         0|            0|            0|  0.00%|\n",
      "  2097|         0|            0|            0|  0.00%|    Parameters\n",
      "  2098|         0|            0|            0|  0.00%|    ----------\n",
      "  2099|         0|            0|            0|  0.00%|    a : array_like\n",
      "  2100|         0|            0|            0|  0.00%|        Array containing elements to clip.\n",
      "  2101|         0|            0|            0|  0.00%|    a_min, a_max : array_like or None\n",
      "  2102|         0|            0|            0|  0.00%|        Minimum and maximum value. If ``None``, clipping is not performed on\n",
      "  2103|         0|            0|            0|  0.00%|        the corresponding edge. Only one of `a_min` and `a_max` may be\n",
      "  2104|         0|            0|            0|  0.00%|        ``None``. Both are broadcast against `a`.\n",
      "  2105|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  2106|         0|            0|            0|  0.00%|        The results will be placed in this array. It may be the input\n",
      "  2107|         0|            0|            0|  0.00%|        array for in-place clipping.  `out` must be of the right shape\n",
      "  2108|         0|            0|            0|  0.00%|        to hold the output.  Its type is preserved.\n",
      "  2109|         0|            0|            0|  0.00%|    **kwargs\n",
      "  2110|         0|            0|            0|  0.00%|        For other keyword-only arguments, see the\n",
      "  2111|         0|            0|            0|  0.00%|        :ref:`ufunc docs <ufuncs.kwargs>`.\n",
      "  2112|         0|            0|            0|  0.00%|\n",
      "  2113|         0|            0|            0|  0.00%|        .. versionadded:: 1.17.0\n",
      "  2114|         0|            0|            0|  0.00%|\n",
      "  2115|         0|            0|            0|  0.00%|    Returns\n",
      "  2116|         0|            0|            0|  0.00%|    -------\n",
      "  2117|         0|            0|            0|  0.00%|    clipped_array : ndarray\n",
      "  2118|         0|            0|            0|  0.00%|        An array with the elements of `a`, but where values\n",
      "  2119|         0|            0|            0|  0.00%|        < `a_min` are replaced with `a_min`, and those > `a_max`\n",
      "  2120|         0|            0|            0|  0.00%|        with `a_max`.\n",
      "  2121|         0|            0|            0|  0.00%|\n",
      "  2122|         0|            0|            0|  0.00%|    See Also\n",
      "  2123|         0|            0|            0|  0.00%|    --------\n",
      "  2124|         0|            0|            0|  0.00%|    :ref:`ufuncs-output-type`\n",
      "  2125|         0|            0|            0|  0.00%|\n",
      "  2126|         0|            0|            0|  0.00%|    Notes\n",
      "  2127|         0|            0|            0|  0.00%|    -----\n",
      "  2128|         0|            0|            0|  0.00%|    When `a_min` is greater than `a_max`, `clip` returns an\n",
      "  2129|         0|            0|            0|  0.00%|    array in which all values are equal to `a_max`,\n",
      "  2130|         0|            0|            0|  0.00%|    as shown in the second example.\n",
      "  2131|         0|            0|            0|  0.00%|\n",
      "  2132|         0|            0|            0|  0.00%|    Examples\n",
      "  2133|         0|            0|            0|  0.00%|    --------\n",
      "  2134|         0|            0|            0|  0.00%|    >>> a = np.arange(10)\n",
      "  2135|         0|            0|            0|  0.00%|    >>> a\n",
      "  2136|         0|            0|            0|  0.00%|    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "  2137|         0|            0|            0|  0.00%|    >>> np.clip(a, 1, 8)\n",
      "  2138|         0|            0|            0|  0.00%|    array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8])\n",
      "  2139|         0|            0|            0|  0.00%|    >>> np.clip(a, 8, 1)\n",
      "  2140|         0|            0|            0|  0.00%|    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "  2141|         0|            0|            0|  0.00%|    >>> np.clip(a, 3, 6, out=a)\n",
      "  2142|         0|            0|            0|  0.00%|    array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6])\n",
      "  2143|         0|            0|            0|  0.00%|    >>> a\n",
      "  2144|         0|            0|            0|  0.00%|    array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6])\n",
      "  2145|         0|            0|            0|  0.00%|    >>> a = np.arange(10)\n",
      "  2146|         0|            0|            0|  0.00%|    >>> a\n",
      "  2147|         0|            0|            0|  0.00%|    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "  2148|         0|            0|            0|  0.00%|    >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8)\n",
      "  2149|         0|            0|            0|  0.00%|    array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8])\n",
      "  2150|         0|            0|            0|  0.00%|\n",
      "  2151|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2152|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)\n",
      "  2153|         0|            0|            0|  0.00%|\n",
      "  2154|         0|            0|            0|  0.00%|\n",
      "  2155|         0|            0|            0|  0.00%|def _sum_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None,\n",
      "  2156|         0|            0|            0|  0.00%|                    initial=None, where=None):\n",
      "  2157|         0|            0|            0|  0.00%|    return (a, out)\n",
      "  2158|         0|            0|            0|  0.00%|\n",
      "  2159|         0|            0|            0|  0.00%|\n",
      "  2160|         0|            0|            0|  0.00%|@array_function_dispatch(_sum_dispatcher)\n",
      "  2161|         0|            0|            0|  0.00%|def sum(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,\n",
      "  2162|         0|            0|            0|  0.00%|        initial=np._NoValue, where=np._NoValue):\n",
      "  2163|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2164|         0|            0|            0|  0.00%|    Sum of array elements over a given axis.\n",
      "  2165|         0|            0|            0|  0.00%|\n",
      "  2166|         0|            0|            0|  0.00%|    Parameters\n",
      "  2167|         0|            0|            0|  0.00%|    ----------\n",
      "  2168|         0|            0|            0|  0.00%|    a : array_like\n",
      "  2169|         0|            0|            0|  0.00%|        Elements to sum.\n",
      "  2170|         0|            0|            0|  0.00%|    axis : None or int or tuple of ints, optional\n",
      "  2171|         0|            0|            0|  0.00%|        Axis or axes along which a sum is performed.  The default,\n",
      "  2172|         0|            0|            0|  0.00%|        axis=None, will sum all of the elements of the input array.  If\n",
      "  2173|         0|            0|            0|  0.00%|        axis is negative it counts from the last to the first axis.\n",
      "  2174|         0|            0|            0|  0.00%|\n",
      "  2175|         0|            0|            0|  0.00%|        .. versionadded:: 1.7.0\n",
      "  2176|         0|            0|            0|  0.00%|\n",
      "  2177|         0|            0|            0|  0.00%|        If axis is a tuple of ints, a sum is performed on all of the axes\n",
      "  2178|         0|            0|            0|  0.00%|        specified in the tuple instead of a single axis or all the axes as\n",
      "  2179|         0|            0|            0|  0.00%|        before.\n",
      "  2180|         0|            0|            0|  0.00%|    dtype : dtype, optional\n",
      "  2181|         0|            0|            0|  0.00%|        The type of the returned array and of the accumulator in which the\n",
      "  2182|         0|            0|            0|  0.00%|        elements are summed.  The dtype of `a` is used by default unless `a`\n",
      "  2183|         0|            0|            0|  0.00%|        has an integer dtype of less precision than the default platform\n",
      "  2184|         0|            0|            0|  0.00%|        integer.  In that case, if `a` is signed then the platform integer\n",
      "  2185|         0|            0|            0|  0.00%|        is used while if `a` is unsigned then an unsigned integer of the\n",
      "  2186|         0|            0|            0|  0.00%|        same precision as the platform integer is used.\n",
      "  2187|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  2188|         0|            0|            0|  0.00%|        Alternative output array in which to place the result. It must have\n",
      "  2189|         0|            0|            0|  0.00%|        the same shape as the expected output, but the type of the output\n",
      "  2190|         0|            0|            0|  0.00%|        values will be cast if necessary.\n",
      "  2191|         0|            0|            0|  0.00%|    keepdims : bool, optional\n",
      "  2192|         0|            0|            0|  0.00%|        If this is set to True, the axes which are reduced are left\n",
      "  2193|         0|            0|            0|  0.00%|        in the result as dimensions with size one. With this option,\n",
      "  2194|         0|            0|            0|  0.00%|        the result will broadcast correctly against the input array.\n",
      "  2195|         0|            0|            0|  0.00%|\n",
      "  2196|         0|            0|            0|  0.00%|        If the default value is passed, then `keepdims` will not be\n",
      "  2197|         0|            0|            0|  0.00%|        passed through to the `sum` method of sub-classes of\n",
      "  2198|         0|            0|            0|  0.00%|        `ndarray`, however any non-default value will be.  If the\n",
      "  2199|         0|            0|            0|  0.00%|        sub-class' method does not implement `keepdims` any\n",
      "  2200|         0|            0|            0|  0.00%|        exceptions will be raised.\n",
      "  2201|         0|            0|            0|  0.00%|    initial : scalar, optional\n",
      "  2202|         0|            0|            0|  0.00%|        Starting value for the sum. See `~numpy.ufunc.reduce` for details.\n",
      "  2203|         0|            0|            0|  0.00%|\n",
      "  2204|         0|            0|            0|  0.00%|        .. versionadded:: 1.15.0\n",
      "  2205|         0|            0|            0|  0.00%|\n",
      "  2206|         0|            0|            0|  0.00%|    where : array_like of bool, optional\n",
      "  2207|         0|            0|            0|  0.00%|        Elements to include in the sum. See `~numpy.ufunc.reduce` for details.\n",
      "  2208|         0|            0|            0|  0.00%|\n",
      "  2209|         0|            0|            0|  0.00%|        .. versionadded:: 1.17.0\n",
      "  2210|         0|            0|            0|  0.00%|\n",
      "  2211|         0|            0|            0|  0.00%|    Returns\n",
      "  2212|         0|            0|            0|  0.00%|    -------\n",
      "  2213|         0|            0|            0|  0.00%|    sum_along_axis : ndarray\n",
      "  2214|         0|            0|            0|  0.00%|        An array with the same shape as `a`, with the specified\n",
      "  2215|         0|            0|            0|  0.00%|        axis removed.   If `a` is a 0-d array, or if `axis` is None, a scalar\n",
      "  2216|         0|            0|            0|  0.00%|        is returned.  If an output array is specified, a reference to\n",
      "  2217|         0|            0|            0|  0.00%|        `out` is returned.\n",
      "  2218|         0|            0|            0|  0.00%|\n",
      "  2219|         0|            0|            0|  0.00%|    See Also\n",
      "  2220|         0|            0|            0|  0.00%|    --------\n",
      "  2221|         0|            0|            0|  0.00%|    ndarray.sum : Equivalent method.\n",
      "  2222|         0|            0|            0|  0.00%|\n",
      "  2223|         0|            0|            0|  0.00%|    add.reduce : Equivalent functionality of `add`.\n",
      "  2224|         0|            0|            0|  0.00%|\n",
      "  2225|         0|            0|            0|  0.00%|    cumsum : Cumulative sum of array elements.\n",
      "  2226|         0|            0|            0|  0.00%|\n",
      "  2227|         0|            0|            0|  0.00%|    trapz : Integration of array values using the composite trapezoidal rule.\n",
      "  2228|         0|            0|            0|  0.00%|\n",
      "  2229|         0|            0|            0|  0.00%|    mean, average\n",
      "  2230|         0|            0|            0|  0.00%|\n",
      "  2231|         0|            0|            0|  0.00%|    Notes\n",
      "  2232|         0|            0|            0|  0.00%|    -----\n",
      "  2233|         0|            0|            0|  0.00%|    Arithmetic is modular when using integer types, and no error is\n",
      "  2234|         0|            0|            0|  0.00%|    raised on overflow.\n",
      "  2235|         0|            0|            0|  0.00%|\n",
      "  2236|         0|            0|            0|  0.00%|    The sum of an empty array is the neutral element 0:\n",
      "  2237|         0|            0|            0|  0.00%|\n",
      "  2238|         0|            0|            0|  0.00%|    >>> np.sum([])\n",
      "  2239|         0|            0|            0|  0.00%|    0.0\n",
      "  2240|         0|            0|            0|  0.00%|\n",
      "  2241|         0|            0|            0|  0.00%|    For floating point numbers the numerical precision of sum (and\n",
      "  2242|         0|            0|            0|  0.00%|    ``np.add.reduce``) is in general limited by directly adding each number\n",
      "  2243|         0|            0|            0|  0.00%|    individually to the result causing rounding errors in every step.\n",
      "  2244|         0|            0|            0|  0.00%|    However, often numpy will use a  numerically better approach (partial\n",
      "  2245|         0|            0|            0|  0.00%|    pairwise summation) leading to improved precision in many use-cases.\n",
      "  2246|         0|            0|            0|  0.00%|    This improved precision is always provided when no ``axis`` is given.\n",
      "  2247|         0|            0|            0|  0.00%|    When ``axis`` is given, it will depend on which axis is summed.\n",
      "  2248|         0|            0|            0|  0.00%|    Technically, to provide the best speed possible, the improved precision\n",
      "  2249|         0|            0|            0|  0.00%|    is only used when the summation is along the fast axis in memory.\n",
      "  2250|         0|            0|            0|  0.00%|    Note that the exact precision may vary depending on other parameters.\n",
      "  2251|         0|            0|            0|  0.00%|    In contrast to NumPy, Python's ``math.fsum`` function uses a slower but\n",
      "  2252|         0|            0|            0|  0.00%|    more precise approach to summation.\n",
      "  2253|         0|            0|            0|  0.00%|    Especially when summing a large number of lower precision floating point\n",
      "  2254|         0|            0|            0|  0.00%|    numbers, such as ``float32``, numerical errors can become significant.\n",
      "  2255|         0|            0|            0|  0.00%|    In such cases it can be advisable to use `dtype=\"float64\"` to use a higher\n",
      "  2256|         0|            0|            0|  0.00%|    precision for the output.\n",
      "  2257|         0|            0|            0|  0.00%|\n",
      "  2258|         0|            0|            0|  0.00%|    Examples\n",
      "  2259|         0|            0|            0|  0.00%|    --------\n",
      "  2260|         0|            0|            0|  0.00%|    >>> np.sum([0.5, 1.5])\n",
      "  2261|         0|            0|            0|  0.00%|    2.0\n",
      "  2262|         0|            0|            0|  0.00%|    >>> np.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32)\n",
      "  2263|         0|            0|            0|  0.00%|    1\n",
      "  2264|         0|            0|            0|  0.00%|    >>> np.sum([[0, 1], [0, 5]])\n",
      "  2265|         0|            0|            0|  0.00%|    6\n",
      "  2266|         0|            0|            0|  0.00%|    >>> np.sum([[0, 1], [0, 5]], axis=0)\n",
      "  2267|         0|            0|            0|  0.00%|    array([0, 6])\n",
      "  2268|         0|            0|            0|  0.00%|    >>> np.sum([[0, 1], [0, 5]], axis=1)\n",
      "  2269|         0|            0|            0|  0.00%|    array([1, 5])\n",
      "  2270|         0|            0|            0|  0.00%|    >>> np.sum([[0, 1], [np.nan, 5]], where=[False, True], axis=1)\n",
      "  2271|         0|            0|            0|  0.00%|    array([1., 5.])\n",
      "  2272|         0|            0|            0|  0.00%|\n",
      "  2273|         0|            0|            0|  0.00%|    If the accumulator is too small, overflow occurs:\n",
      "  2274|         0|            0|            0|  0.00%|\n",
      "  2275|         0|            0|            0|  0.00%|    >>> np.ones(128, dtype=np.int8).sum(dtype=np.int8)\n",
      "  2276|         0|            0|            0|  0.00%|    -128\n",
      "  2277|         0|            0|            0|  0.00%|\n",
      "  2278|         0|            0|            0|  0.00%|    You can also start the sum with a value other than zero:\n",
      "  2279|         0|            0|            0|  0.00%|\n",
      "  2280|         0|            0|            0|  0.00%|    >>> np.sum([10], initial=5)\n",
      "  2281|         0|            0|            0|  0.00%|    15\n",
      "  2282|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2283|         0|            0|            0|  0.00%|    if isinstance(a, _gentype):\n",
      "  2284|         0|            0|            0|  0.00%|        # 2018-02-25, 1.15.0\n",
      "  2285|         0|            0|            0|  0.00%|        warnings.warn(\n",
      "  2286|         0|            0|            0|  0.00%|            \"Calling np.sum(generator) is deprecated, and in the future will give a different result. \"\n",
      "  2287|         0|            0|            0|  0.00%|            \"Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\",\n",
      "  2288|         0|            0|            0|  0.00%|            DeprecationWarning, stacklevel=3)\n",
      "  2289|         0|            0|            0|  0.00%|\n",
      "  2290|         0|            0|            0|  0.00%|        res = _sum_(a)\n",
      "  2291|         0|            0|            0|  0.00%|        if out is not None:\n",
      "  2292|         0|            0|            0|  0.00%|            out[...] = res\n",
      "  2293|         0|            0|            0|  0.00%|            return out\n",
      "  2294|         0|            0|            0|  0.00%|        return res\n",
      "  2295|         0|            0|            0|  0.00%|\n",
      "  2296|         0|            0|            0|  0.00%|    return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n",
      "  2297|         0|            0|            0|  0.00%|                          initial=initial, where=where)\n",
      "  2298|         0|            0|            0|  0.00%|\n",
      "  2299|         0|            0|            0|  0.00%|\n",
      "  2300|         0|            0|            0|  0.00%|def _any_dispatcher(a, axis=None, out=None, keepdims=None, *,\n",
      "  2301|         0|            0|            0|  0.00%|                    where=np._NoValue):\n",
      "  2302|         0|            0|            0|  0.00%|    return (a, where, out)\n",
      "  2303|         0|            0|            0|  0.00%|\n",
      "  2304|         0|            0|            0|  0.00%|\n",
      "  2305|         0|            0|            0|  0.00%|@array_function_dispatch(_any_dispatcher)\n",
      "  2306|         0|            0|            0|  0.00%|def any(a, axis=None, out=None, keepdims=np._NoValue, *, where=np._NoValue):\n",
      "  2307|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2308|         0|            0|            0|  0.00%|    Test whether any array element along a given axis evaluates to True.\n",
      "  2309|         0|            0|            0|  0.00%|\n",
      "  2310|         0|            0|            0|  0.00%|    Returns single boolean unless `axis` is not ``None``\n",
      "  2311|         0|            0|            0|  0.00%|\n",
      "  2312|         0|            0|            0|  0.00%|    Parameters\n",
      "  2313|         0|            0|            0|  0.00%|    ----------\n",
      "  2314|         0|            0|            0|  0.00%|    a : array_like\n",
      "  2315|         0|            0|            0|  0.00%|        Input array or object that can be converted to an array.\n",
      "  2316|         0|            0|            0|  0.00%|    axis : None or int or tuple of ints, optional\n",
      "  2317|         0|            0|            0|  0.00%|        Axis or axes along which a logical OR reduction is performed.\n",
      "  2318|         0|            0|            0|  0.00%|        The default (``axis=None``) is to perform a logical OR over all\n",
      "  2319|         0|            0|            0|  0.00%|        the dimensions of the input array. `axis` may be negative, in\n",
      "  2320|         0|            0|            0|  0.00%|        which case it counts from the last to the first axis.\n",
      "  2321|         0|            0|            0|  0.00%|\n",
      "  2322|         0|            0|            0|  0.00%|        .. versionadded:: 1.7.0\n",
      "  2323|         0|            0|            0|  0.00%|\n",
      "  2324|         0|            0|            0|  0.00%|        If this is a tuple of ints, a reduction is performed on multiple\n",
      "  2325|         0|            0|            0|  0.00%|        axes, instead of a single axis or all the axes as before.\n",
      "  2326|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  2327|         0|            0|            0|  0.00%|        Alternate output array in which to place the result.  It must have\n",
      "  2328|         0|            0|            0|  0.00%|        the same shape as the expected output and its type is preserved\n",
      "  2329|         0|            0|            0|  0.00%|        (e.g., if it is of type float, then it will remain so, returning\n",
      "  2330|         0|            0|            0|  0.00%|        1.0 for True and 0.0 for False, regardless of the type of `a`).\n",
      "  2331|         0|            0|            0|  0.00%|        See :ref:`ufuncs-output-type` for more details.\n",
      "  2332|         0|            0|            0|  0.00%|\n",
      "  2333|         0|            0|            0|  0.00%|    keepdims : bool, optional\n",
      "  2334|         0|            0|            0|  0.00%|        If this is set to True, the axes which are reduced are left\n",
      "  2335|         0|            0|            0|  0.00%|        in the result as dimensions with size one. With this option,\n",
      "  2336|         0|            0|            0|  0.00%|        the result will broadcast correctly against the input array.\n",
      "  2337|         0|            0|            0|  0.00%|\n",
      "  2338|         0|            0|            0|  0.00%|        If the default value is passed, then `keepdims` will not be\n",
      "  2339|         0|            0|            0|  0.00%|        passed through to the `any` method of sub-classes of\n",
      "  2340|         0|            0|            0|  0.00%|        `ndarray`, however any non-default value will be.  If the\n",
      "  2341|         0|            0|            0|  0.00%|        sub-class' method does not implement `keepdims` any\n",
      "  2342|         0|            0|            0|  0.00%|        exceptions will be raised.\n",
      "  2343|         0|            0|            0|  0.00%|\n",
      "  2344|         0|            0|            0|  0.00%|    where : array_like of bool, optional\n",
      "  2345|         0|            0|            0|  0.00%|        Elements to include in checking for any `True` values.\n",
      "  2346|         0|            0|            0|  0.00%|        See `~numpy.ufunc.reduce` for details.\n",
      "  2347|         0|            0|            0|  0.00%|\n",
      "  2348|         0|            0|            0|  0.00%|        .. versionadded:: 1.20.0\n",
      "  2349|         0|            0|            0|  0.00%|\n",
      "  2350|         0|            0|            0|  0.00%|    Returns\n",
      "  2351|         0|            0|            0|  0.00%|    -------\n",
      "  2352|         0|            0|            0|  0.00%|    any : bool or ndarray\n",
      "  2353|         0|            0|            0|  0.00%|        A new boolean or `ndarray` is returned unless `out` is specified,\n",
      "  2354|         0|            0|            0|  0.00%|        in which case a reference to `out` is returned.\n",
      "  2355|         0|            0|            0|  0.00%|\n",
      "  2356|         0|            0|            0|  0.00%|    See Also\n",
      "  2357|         0|            0|            0|  0.00%|    --------\n",
      "  2358|         0|            0|            0|  0.00%|    ndarray.any : equivalent method\n",
      "  2359|         0|            0|            0|  0.00%|\n",
      "  2360|         0|            0|            0|  0.00%|    all : Test whether all elements along a given axis evaluate to True.\n",
      "  2361|         0|            0|            0|  0.00%|\n",
      "  2362|         0|            0|            0|  0.00%|    Notes\n",
      "  2363|         0|            0|            0|  0.00%|    -----\n",
      "  2364|         0|            0|            0|  0.00%|    Not a Number (NaN), positive infinity and negative infinity evaluate\n",
      "  2365|         0|            0|            0|  0.00%|    to `True` because these are not equal to zero.\n",
      "  2366|         0|            0|            0|  0.00%|\n",
      "  2367|         0|            0|            0|  0.00%|    Examples\n",
      "  2368|         0|            0|            0|  0.00%|    --------\n",
      "  2369|         0|            0|            0|  0.00%|    >>> np.any([[True, False], [True, True]])\n",
      "  2370|         0|            0|            0|  0.00%|    True\n",
      "  2371|         0|            0|            0|  0.00%|\n",
      "  2372|         0|            0|            0|  0.00%|    >>> np.any([[True, False], [False, False]], axis=0)\n",
      "  2373|         0|            0|            0|  0.00%|    array([ True, False])\n",
      "  2374|         0|            0|            0|  0.00%|\n",
      "  2375|         0|            0|            0|  0.00%|    >>> np.any([-1, 0, 5])\n",
      "  2376|         0|            0|            0|  0.00%|    True\n",
      "  2377|         0|            0|            0|  0.00%|\n",
      "  2378|         0|            0|            0|  0.00%|    >>> np.any(np.nan)\n",
      "  2379|         0|            0|            0|  0.00%|    True\n",
      "  2380|         0|            0|            0|  0.00%|\n",
      "  2381|         0|            0|            0|  0.00%|    >>> np.any([[True, False], [False, False]], where=[[False], [True]])\n",
      "  2382|         0|            0|            0|  0.00%|    False\n",
      "  2383|         0|            0|            0|  0.00%|\n",
      "  2384|         0|            0|            0|  0.00%|    >>> o=np.array(False)\n",
      "  2385|         0|            0|            0|  0.00%|    >>> z=np.any([-1, 4, 5], out=o)\n",
      "  2386|         0|            0|            0|  0.00%|    >>> z, o\n",
      "  2387|         0|            0|            0|  0.00%|    (array(True), array(True))\n",
      "  2388|         0|            0|            0|  0.00%|    >>> # Check now that z is a reference to o\n",
      "  2389|         0|            0|            0|  0.00%|    >>> z is o\n",
      "  2390|         0|            0|            0|  0.00%|    True\n",
      "  2391|         0|            0|            0|  0.00%|    >>> id(z), id(o) # identity of z and o              # doctest: +SKIP\n",
      "  2392|         0|            0|            0|  0.00%|    (191614240, 191614240)\n",
      "  2393|         0|            0|            0|  0.00%|\n",
      "  2394|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2395|         0|            0|            0|  0.00%|    return _wrapreduction(a, np.logical_or, 'any', axis, None, out,\n",
      "  2396|         0|            0|            0|  0.00%|                          keepdims=keepdims, where=where)\n",
      "  2397|         0|            0|            0|  0.00%|\n",
      "  2398|         0|            0|            0|  0.00%|\n",
      "  2399|         0|            0|            0|  0.00%|def _all_dispatcher(a, axis=None, out=None, keepdims=None, *,\n",
      "  2400|         0|            0|            0|  0.00%|                    where=None):\n",
      "  2401|         0|            0|            0|  0.00%|    return (a, where, out)\n",
      "  2402|         0|            0|            0|  0.00%|\n",
      "  2403|         0|            0|            0|  0.00%|\n",
      "  2404|         0|            0|            0|  0.00%|@array_function_dispatch(_all_dispatcher)\n",
      "  2405|         0|            0|            0|  0.00%|def all(a, axis=None, out=None, keepdims=np._NoValue, *, where=np._NoValue):\n",
      "  2406|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2407|         0|            0|            0|  0.00%|    Test whether all array elements along a given axis evaluate to True.\n",
      "  2408|         0|            0|            0|  0.00%|\n",
      "  2409|         0|            0|            0|  0.00%|    Parameters\n",
      "  2410|         0|            0|            0|  0.00%|    ----------\n",
      "  2411|         0|            0|            0|  0.00%|    a : array_like\n",
      "  2412|         0|            0|            0|  0.00%|        Input array or object that can be converted to an array.\n",
      "  2413|         0|            0|            0|  0.00%|    axis : None or int or tuple of ints, optional\n",
      "  2414|         0|            0|            0|  0.00%|        Axis or axes along which a logical AND reduction is performed.\n",
      "  2415|         0|            0|            0|  0.00%|        The default (``axis=None``) is to perform a logical AND over all\n",
      "  2416|         0|            0|            0|  0.00%|        the dimensions of the input array. `axis` may be negative, in\n",
      "  2417|         0|            0|            0|  0.00%|        which case it counts from the last to the first axis.\n",
      "  2418|         0|            0|            0|  0.00%|\n",
      "  2419|         0|            0|            0|  0.00%|        .. versionadded:: 1.7.0\n",
      "  2420|         0|            0|            0|  0.00%|\n",
      "  2421|         0|            0|            0|  0.00%|        If this is a tuple of ints, a reduction is performed on multiple\n",
      "  2422|         0|            0|            0|  0.00%|        axes, instead of a single axis or all the axes as before.\n",
      "  2423|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  2424|         0|            0|            0|  0.00%|        Alternate output array in which to place the result.\n",
      "  2425|         0|            0|            0|  0.00%|        It must have the same shape as the expected output and its\n",
      "  2426|         0|            0|            0|  0.00%|        type is preserved (e.g., if ``dtype(out)`` is float, the result\n",
      "  2427|         0|            0|            0|  0.00%|        will consist of 0.0's and 1.0's). See :ref:`ufuncs-output-type` for more\n",
      "  2428|         0|            0|            0|  0.00%|        details.\n",
      "  2429|         0|            0|            0|  0.00%|\n",
      "  2430|         0|            0|            0|  0.00%|    keepdims : bool, optional\n",
      "  2431|         0|            0|            0|  0.00%|        If this is set to True, the axes which are reduced are left\n",
      "  2432|         0|            0|            0|  0.00%|        in the result as dimensions with size one. With this option,\n",
      "  2433|         0|            0|            0|  0.00%|        the result will broadcast correctly against the input array.\n",
      "  2434|         0|            0|            0|  0.00%|\n",
      "  2435|         0|            0|            0|  0.00%|        If the default value is passed, then `keepdims` will not be\n",
      "  2436|         0|            0|            0|  0.00%|        passed through to the `all` method of sub-classes of\n",
      "  2437|         0|            0|            0|  0.00%|        `ndarray`, however any non-default value will be.  If the\n",
      "  2438|         0|            0|            0|  0.00%|        sub-class' method does not implement `keepdims` any\n",
      "  2439|         0|            0|            0|  0.00%|        exceptions will be raised.\n",
      "  2440|         0|            0|            0|  0.00%|\n",
      "  2441|         0|            0|            0|  0.00%|    where : array_like of bool, optional\n",
      "  2442|         0|            0|            0|  0.00%|        Elements to include in checking for all `True` values.\n",
      "  2443|         0|            0|            0|  0.00%|        See `~numpy.ufunc.reduce` for details.\n",
      "  2444|         0|            0|            0|  0.00%|\n",
      "  2445|         0|            0|            0|  0.00%|        .. versionadded:: 1.20.0\n",
      "  2446|         0|            0|            0|  0.00%|\n",
      "  2447|         0|            0|            0|  0.00%|    Returns\n",
      "  2448|         0|            0|            0|  0.00%|    -------\n",
      "  2449|         0|            0|            0|  0.00%|    all : ndarray, bool\n",
      "  2450|         0|            0|            0|  0.00%|        A new boolean or array is returned unless `out` is specified,\n",
      "  2451|         0|            0|            0|  0.00%|        in which case a reference to `out` is returned.\n",
      "  2452|         0|            0|            0|  0.00%|\n",
      "  2453|         0|            0|            0|  0.00%|    See Also\n",
      "  2454|         0|            0|            0|  0.00%|    --------\n",
      "  2455|         0|            0|            0|  0.00%|    ndarray.all : equivalent method\n",
      "  2456|         0|            0|            0|  0.00%|\n",
      "  2457|         0|            0|            0|  0.00%|    any : Test whether any element along a given axis evaluates to True.\n",
      "  2458|         0|            0|            0|  0.00%|\n",
      "  2459|         0|            0|            0|  0.00%|    Notes\n",
      "  2460|         0|            0|            0|  0.00%|    -----\n",
      "  2461|         0|            0|            0|  0.00%|    Not a Number (NaN), positive infinity and negative infinity\n",
      "  2462|         0|            0|            0|  0.00%|    evaluate to `True` because these are not equal to zero.\n",
      "  2463|         0|            0|            0|  0.00%|\n",
      "  2464|         0|            0|            0|  0.00%|    Examples\n",
      "  2465|         0|            0|            0|  0.00%|    --------\n",
      "  2466|         0|            0|            0|  0.00%|    >>> np.all([[True,False],[True,True]])\n",
      "  2467|         0|            0|            0|  0.00%|    False\n",
      "  2468|         0|            0|            0|  0.00%|\n",
      "  2469|         0|            0|            0|  0.00%|    >>> np.all([[True,False],[True,True]], axis=0)\n",
      "  2470|         0|            0|            0|  0.00%|    array([ True, False])\n",
      "  2471|         0|            0|            0|  0.00%|\n",
      "  2472|         0|            0|            0|  0.00%|    >>> np.all([-1, 4, 5])\n",
      "  2473|         0|            0|            0|  0.00%|    True\n",
      "  2474|         0|            0|            0|  0.00%|\n",
      "  2475|         0|            0|            0|  0.00%|    >>> np.all([1.0, np.nan])\n",
      "  2476|         0|            0|            0|  0.00%|    True\n",
      "  2477|         0|            0|            0|  0.00%|\n",
      "  2478|         0|            0|            0|  0.00%|    >>> np.all([[True, True], [False, True]], where=[[True], [False]])\n",
      "  2479|         0|            0|            0|  0.00%|    True\n",
      "  2480|         0|            0|            0|  0.00%|\n",
      "  2481|         0|            0|            0|  0.00%|    >>> o=np.array(False)\n",
      "  2482|         0|            0|            0|  0.00%|    >>> z=np.all([-1, 4, 5], out=o)\n",
      "  2483|         0|            0|            0|  0.00%|    >>> id(z), id(o), z\n",
      "  2484|         0|            0|            0|  0.00%|    (28293632, 28293632, array(True)) # may vary\n",
      "  2485|         0|            0|            0|  0.00%|\n",
      "  2486|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2487|         0|            0|            0|  0.00%|    return _wrapreduction(a, np.logical_and, 'all', axis, None, out,\n",
      "  2488|         0|            0|            0|  0.00%|                          keepdims=keepdims, where=where)\n",
      "  2489|         0|            0|            0|  0.00%|\n",
      "  2490|         0|            0|            0|  0.00%|\n",
      "  2491|     74880|      0.10704|  1.42949e-06|  0.02%|def _cumsum_dispatcher(a, axis=None, dtype=None, out=None):\n",
      "  2492|     74880|     0.141059|   1.8838e-06|  0.02%|    return (a, out)\n",
      "  2493|         0|            0|            0|  0.00%|\n",
      "  2494|         0|            0|            0|  0.00%|\n",
      "  2495|     74880|     0.109867|  1.46724e-06|  0.02%|@array_function_dispatch(_cumsum_dispatcher)\n",
      "  2496|         0|            0|            0|  0.00%|def cumsum(a, axis=None, dtype=None, out=None):\n",
      "  2497|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2498|         0|            0|            0|  0.00%|    Return the cumulative sum of the elements along a given axis.\n",
      "  2499|         0|            0|            0|  0.00%|\n",
      "  2500|         0|            0|            0|  0.00%|    Parameters\n",
      "  2501|         0|            0|            0|  0.00%|    ----------\n",
      "  2502|         0|            0|            0|  0.00%|    a : array_like\n",
      "  2503|         0|            0|            0|  0.00%|        Input array.\n",
      "  2504|         0|            0|            0|  0.00%|    axis : int, optional\n",
      "  2505|         0|            0|            0|  0.00%|        Axis along which the cumulative sum is computed. The default\n",
      "  2506|         0|            0|            0|  0.00%|        (None) is to compute the cumsum over the flattened array.\n",
      "  2507|         0|            0|            0|  0.00%|    dtype : dtype, optional\n",
      "  2508|         0|            0|            0|  0.00%|        Type of the returned array and of the accumulator in which the\n",
      "  2509|         0|            0|            0|  0.00%|        elements are summed.  If `dtype` is not specified, it defaults\n",
      "  2510|         0|            0|            0|  0.00%|        to the dtype of `a`, unless `a` has an integer dtype with a\n",
      "  2511|         0|            0|            0|  0.00%|        precision less than that of the default platform integer.  In\n",
      "  2512|         0|            0|            0|  0.00%|        that case, the default platform integer is used.\n",
      "  2513|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  2514|         0|            0|            0|  0.00%|        Alternative output array in which to place the result. It must\n",
      "  2515|         0|            0|            0|  0.00%|        have the same shape and buffer length as the expected output\n",
      "  2516|         0|            0|            0|  0.00%|        but the type will be cast if necessary. See :ref:`ufuncs-output-type` for\n",
      "  2517|         0|            0|            0|  0.00%|        more details.\n",
      "  2518|         0|            0|            0|  0.00%|\n",
      "  2519|         0|            0|            0|  0.00%|    Returns\n",
      "  2520|         0|            0|            0|  0.00%|    -------\n",
      "  2521|         0|            0|            0|  0.00%|    cumsum_along_axis : ndarray.\n",
      "  2522|         0|            0|            0|  0.00%|        A new array holding the result is returned unless `out` is\n",
      "  2523|         0|            0|            0|  0.00%|        specified, in which case a reference to `out` is returned. The\n",
      "  2524|         0|            0|            0|  0.00%|        result has the same size as `a`, and the same shape as `a` if\n",
      "  2525|         0|            0|            0|  0.00%|        `axis` is not None or `a` is a 1-d array.\n",
      "  2526|         0|            0|            0|  0.00%|\n",
      "  2527|         0|            0|            0|  0.00%|    See Also\n",
      "  2528|         0|            0|            0|  0.00%|    --------\n",
      "  2529|         0|            0|            0|  0.00%|    sum : Sum array elements.\n",
      "  2530|         0|            0|            0|  0.00%|    trapz : Integration of array values using the composite trapezoidal rule.\n",
      "  2531|         0|            0|            0|  0.00%|    diff : Calculate the n-th discrete difference along given axis.\n",
      "  2532|         0|            0|            0|  0.00%|\n",
      "  2533|         0|            0|            0|  0.00%|    Notes\n",
      "  2534|         0|            0|            0|  0.00%|    -----\n",
      "  2535|         0|            0|            0|  0.00%|    Arithmetic is modular when using integer types, and no error is\n",
      "  2536|         0|            0|            0|  0.00%|    raised on overflow.\n",
      "  2537|         0|            0|            0|  0.00%|\n",
      "  2538|         0|            0|            0|  0.00%|    ``cumsum(a)[-1]`` may not be equal to ``sum(a)`` for floating-point\n",
      "  2539|         0|            0|            0|  0.00%|    values since ``sum`` may use a pairwise summation routine, reducing\n",
      "  2540|         0|            0|            0|  0.00%|    the roundoff-error. See `sum` for more information.\n",
      "  2541|         0|            0|            0|  0.00%|\n",
      "  2542|         0|            0|            0|  0.00%|    Examples\n",
      "  2543|         0|            0|            0|  0.00%|    --------\n",
      "  2544|         0|            0|            0|  0.00%|    >>> a = np.array([[1,2,3], [4,5,6]])\n",
      "  2545|         0|            0|            0|  0.00%|    >>> a\n",
      "  2546|         0|            0|            0|  0.00%|    array([[1, 2, 3],\n",
      "  2547|         0|            0|            0|  0.00%|           [4, 5, 6]])\n",
      "  2548|         0|            0|            0|  0.00%|    >>> np.cumsum(a)\n",
      "  2549|         0|            0|            0|  0.00%|    array([ 1,  3,  6, 10, 15, 21])\n",
      "  2550|         0|            0|            0|  0.00%|    >>> np.cumsum(a, dtype=float)     # specifies type of output value(s)\n",
      "  2551|         0|            0|            0|  0.00%|    array([  1.,   3.,   6.,  10.,  15.,  21.])\n",
      "  2552|         0|            0|            0|  0.00%|\n",
      "  2553|         0|            0|            0|  0.00%|    >>> np.cumsum(a,axis=0)      # sum over rows for each of the 3 columns\n",
      "  2554|         0|            0|            0|  0.00%|    array([[1, 2, 3],\n",
      "  2555|         0|            0|            0|  0.00%|           [5, 7, 9]])\n",
      "  2556|         0|            0|            0|  0.00%|    >>> np.cumsum(a,axis=1)      # sum over columns for each of the 2 rows\n",
      "  2557|         0|            0|            0|  0.00%|    array([[ 1,  3,  6],\n",
      "  2558|         0|            0|            0|  0.00%|           [ 4,  9, 15]])\n",
      "  2559|         0|            0|            0|  0.00%|\n",
      "  2560|         0|            0|            0|  0.00%|    ``cumsum(b)[-1]`` may not be equal to ``sum(b)``\n",
      "  2561|         0|            0|            0|  0.00%|\n",
      "  2562|         0|            0|            0|  0.00%|    >>> b = np.array([1, 2e-9, 3e-9] * 1000000)\n",
      "  2563|         0|            0|            0|  0.00%|    >>> b.cumsum()[-1]\n",
      "  2564|         0|            0|            0|  0.00%|    1000000.0050045159\n",
      "  2565|         0|            0|            0|  0.00%|    >>> b.sum()\n",
      "  2566|         0|            0|            0|  0.00%|    1000000.0050000029\n",
      "  2567|         0|            0|            0|  0.00%|\n",
      "  2568|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2569|     74880|     0.469344|  6.26795e-06|  0.08%|    return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out)\n",
      "(call)|     74880|      3.15946|  4.21937e-05|  0.51%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:51 _wrapfunc\n",
      "  2570|         0|            0|            0|  0.00%|\n",
      "  2571|         0|            0|            0|  0.00%|\n",
      "  2572|         0|            0|            0|  0.00%|def _ptp_dispatcher(a, axis=None, out=None, keepdims=None):\n",
      "  2573|         0|            0|            0|  0.00%|    return (a, out)\n",
      "  2574|         0|            0|            0|  0.00%|\n",
      "  2575|         0|            0|            0|  0.00%|\n",
      "  2576|         0|            0|            0|  0.00%|@array_function_dispatch(_ptp_dispatcher)\n",
      "  2577|         0|            0|            0|  0.00%|def ptp(a, axis=None, out=None, keepdims=np._NoValue):\n",
      "  2578|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2579|         0|            0|            0|  0.00%|    Range of values (maximum - minimum) along an axis.\n",
      "  2580|         0|            0|            0|  0.00%|\n",
      "  2581|         0|            0|            0|  0.00%|    The name of the function comes from the acronym for 'peak to peak'.\n",
      "  2582|         0|            0|            0|  0.00%|\n",
      "  2583|         0|            0|            0|  0.00%|    .. warning::\n",
      "  2584|         0|            0|            0|  0.00%|        `ptp` preserves the data type of the array. This means the\n",
      "  2585|         0|            0|            0|  0.00%|        return value for an input of signed integers with n bits\n",
      "  2586|         0|            0|            0|  0.00%|        (e.g. `np.int8`, `np.int16`, etc) is also a signed integer\n",
      "  2587|         0|            0|            0|  0.00%|        with n bits.  In that case, peak-to-peak values greater than\n",
      "  2588|         0|            0|            0|  0.00%|        ``2**(n-1)-1`` will be returned as negative values. An example\n",
      "  2589|         0|            0|            0|  0.00%|        with a work-around is shown below.\n",
      "  2590|         0|            0|            0|  0.00%|\n",
      "  2591|         0|            0|            0|  0.00%|    Parameters\n",
      "  2592|         0|            0|            0|  0.00%|    ----------\n",
      "  2593|         0|            0|            0|  0.00%|    a : array_like\n",
      "  2594|         0|            0|            0|  0.00%|        Input values.\n",
      "  2595|         0|            0|            0|  0.00%|    axis : None or int or tuple of ints, optional\n",
      "  2596|         0|            0|            0|  0.00%|        Axis along which to find the peaks.  By default, flatten the\n",
      "  2597|         0|            0|            0|  0.00%|        array.  `axis` may be negative, in\n",
      "  2598|         0|            0|            0|  0.00%|        which case it counts from the last to the first axis.\n",
      "  2599|         0|            0|            0|  0.00%|\n",
      "  2600|         0|            0|            0|  0.00%|        .. versionadded:: 1.15.0\n",
      "  2601|         0|            0|            0|  0.00%|\n",
      "  2602|         0|            0|            0|  0.00%|        If this is a tuple of ints, a reduction is performed on multiple\n",
      "  2603|         0|            0|            0|  0.00%|        axes, instead of a single axis or all the axes as before.\n",
      "  2604|         0|            0|            0|  0.00%|    out : array_like\n",
      "  2605|         0|            0|            0|  0.00%|        Alternative output array in which to place the result. It must\n",
      "  2606|         0|            0|            0|  0.00%|        have the same shape and buffer length as the expected output,\n",
      "  2607|         0|            0|            0|  0.00%|        but the type of the output values will be cast if necessary.\n",
      "  2608|         0|            0|            0|  0.00%|\n",
      "  2609|         0|            0|            0|  0.00%|    keepdims : bool, optional\n",
      "  2610|         0|            0|            0|  0.00%|        If this is set to True, the axes which are reduced are left\n",
      "  2611|         0|            0|            0|  0.00%|        in the result as dimensions with size one. With this option,\n",
      "  2612|         0|            0|            0|  0.00%|        the result will broadcast correctly against the input array.\n",
      "  2613|         0|            0|            0|  0.00%|\n",
      "  2614|         0|            0|            0|  0.00%|        If the default value is passed, then `keepdims` will not be\n",
      "  2615|         0|            0|            0|  0.00%|        passed through to the `ptp` method of sub-classes of\n",
      "  2616|         0|            0|            0|  0.00%|        `ndarray`, however any non-default value will be.  If the\n",
      "  2617|         0|            0|            0|  0.00%|        sub-class' method does not implement `keepdims` any\n",
      "  2618|         0|            0|            0|  0.00%|        exceptions will be raised.\n",
      "  2619|         0|            0|            0|  0.00%|\n",
      "  2620|         0|            0|            0|  0.00%|    Returns\n",
      "  2621|         0|            0|            0|  0.00%|    -------\n",
      "  2622|         0|            0|            0|  0.00%|    ptp : ndarray\n",
      "  2623|         0|            0|            0|  0.00%|        A new array holding the result, unless `out` was\n",
      "  2624|         0|            0|            0|  0.00%|        specified, in which case a reference to `out` is returned.\n",
      "  2625|         0|            0|            0|  0.00%|\n",
      "  2626|         0|            0|            0|  0.00%|    Examples\n",
      "  2627|         0|            0|            0|  0.00%|    --------\n",
      "  2628|         0|            0|            0|  0.00%|    >>> x = np.array([[4, 9, 2, 10],\n",
      "  2629|         0|            0|            0|  0.00%|    ...               [6, 9, 7, 12]])\n",
      "  2630|         0|            0|            0|  0.00%|\n",
      "  2631|         0|            0|            0|  0.00%|    >>> np.ptp(x, axis=1)\n",
      "  2632|         0|            0|            0|  0.00%|    array([8, 6])\n",
      "  2633|         0|            0|            0|  0.00%|\n",
      "  2634|         0|            0|            0|  0.00%|    >>> np.ptp(x, axis=0)\n",
      "  2635|         0|            0|            0|  0.00%|    array([2, 0, 5, 2])\n",
      "  2636|         0|            0|            0|  0.00%|\n",
      "  2637|         0|            0|            0|  0.00%|    >>> np.ptp(x)\n",
      "  2638|         0|            0|            0|  0.00%|    10\n",
      "  2639|         0|            0|            0|  0.00%|\n",
      "  2640|         0|            0|            0|  0.00%|    This example shows that a negative value can be returned when\n",
      "  2641|         0|            0|            0|  0.00%|    the input is an array of signed integers.\n",
      "  2642|         0|            0|            0|  0.00%|\n",
      "  2643|         0|            0|            0|  0.00%|    >>> y = np.array([[1, 127],\n",
      "  2644|         0|            0|            0|  0.00%|    ...               [0, 127],\n",
      "  2645|         0|            0|            0|  0.00%|    ...               [-1, 127],\n",
      "  2646|         0|            0|            0|  0.00%|    ...               [-2, 127]], dtype=np.int8)\n",
      "  2647|         0|            0|            0|  0.00%|    >>> np.ptp(y, axis=1)\n",
      "  2648|         0|            0|            0|  0.00%|    array([ 126,  127, -128, -127], dtype=int8)\n",
      "  2649|         0|            0|            0|  0.00%|\n",
      "  2650|         0|            0|            0|  0.00%|    A work-around is to use the `view()` method to view the result as\n",
      "  2651|         0|            0|            0|  0.00%|    unsigned integers with the same bit width:\n",
      "  2652|         0|            0|            0|  0.00%|\n",
      "  2653|         0|            0|            0|  0.00%|    >>> np.ptp(y, axis=1).view(np.uint8)\n",
      "  2654|         0|            0|            0|  0.00%|    array([126, 127, 128, 129], dtype=uint8)\n",
      "  2655|         0|            0|            0|  0.00%|\n",
      "  2656|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2657|         0|            0|            0|  0.00%|    kwargs = {}\n",
      "  2658|         0|            0|            0|  0.00%|    if keepdims is not np._NoValue:\n",
      "  2659|         0|            0|            0|  0.00%|        kwargs['keepdims'] = keepdims\n",
      "  2660|         0|            0|            0|  0.00%|    if type(a) is not mu.ndarray:\n",
      "  2661|         0|            0|            0|  0.00%|        try:\n",
      "  2662|         0|            0|            0|  0.00%|            ptp = a.ptp\n",
      "  2663|         0|            0|            0|  0.00%|        except AttributeError:\n",
      "  2664|         0|            0|            0|  0.00%|            pass\n",
      "  2665|         0|            0|            0|  0.00%|        else:\n",
      "  2666|         0|            0|            0|  0.00%|            return ptp(axis=axis, out=out, **kwargs)\n",
      "  2667|         0|            0|            0|  0.00%|    return _methods._ptp(a, axis=axis, out=out, **kwargs)\n",
      "  2668|         0|            0|            0|  0.00%|\n",
      "  2669|         0|            0|            0|  0.00%|\n",
      "  2670|         0|            0|            0|  0.00%|def _amax_dispatcher(a, axis=None, out=None, keepdims=None, initial=None,\n",
      "  2671|         0|            0|            0|  0.00%|                     where=None):\n",
      "  2672|         0|            0|            0|  0.00%|    return (a, out)\n",
      "  2673|         0|            0|            0|  0.00%|\n",
      "  2674|         0|            0|            0|  0.00%|\n",
      "  2675|         0|            0|            0|  0.00%|@array_function_dispatch(_amax_dispatcher)\n",
      "  2676|         0|            0|            0|  0.00%|def amax(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n",
      "  2677|         0|            0|            0|  0.00%|         where=np._NoValue):\n",
      "  2678|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2679|         0|            0|            0|  0.00%|    Return the maximum of an array or maximum along an axis.\n",
      "  2680|         0|            0|            0|  0.00%|\n",
      "  2681|         0|            0|            0|  0.00%|    Parameters\n",
      "  2682|         0|            0|            0|  0.00%|    ----------\n",
      "  2683|         0|            0|            0|  0.00%|    a : array_like\n",
      "  2684|         0|            0|            0|  0.00%|        Input data.\n",
      "  2685|         0|            0|            0|  0.00%|    axis : None or int or tuple of ints, optional\n",
      "  2686|         0|            0|            0|  0.00%|        Axis or axes along which to operate.  By default, flattened input is\n",
      "  2687|         0|            0|            0|  0.00%|        used.\n",
      "  2688|         0|            0|            0|  0.00%|\n",
      "  2689|         0|            0|            0|  0.00%|        .. versionadded:: 1.7.0\n",
      "  2690|         0|            0|            0|  0.00%|\n",
      "  2691|         0|            0|            0|  0.00%|        If this is a tuple of ints, the maximum is selected over multiple axes,\n",
      "  2692|         0|            0|            0|  0.00%|        instead of a single axis or all the axes as before.\n",
      "  2693|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  2694|         0|            0|            0|  0.00%|        Alternative output array in which to place the result.  Must\n",
      "  2695|         0|            0|            0|  0.00%|        be of the same shape and buffer length as the expected output.\n",
      "  2696|         0|            0|            0|  0.00%|        See :ref:`ufuncs-output-type` for more details.\n",
      "  2697|         0|            0|            0|  0.00%|\n",
      "  2698|         0|            0|            0|  0.00%|    keepdims : bool, optional\n",
      "  2699|         0|            0|            0|  0.00%|        If this is set to True, the axes which are reduced are left\n",
      "  2700|         0|            0|            0|  0.00%|        in the result as dimensions with size one. With this option,\n",
      "  2701|         0|            0|            0|  0.00%|        the result will broadcast correctly against the input array.\n",
      "  2702|         0|            0|            0|  0.00%|\n",
      "  2703|         0|            0|            0|  0.00%|        If the default value is passed, then `keepdims` will not be\n",
      "  2704|         0|            0|            0|  0.00%|        passed through to the `amax` method of sub-classes of\n",
      "  2705|         0|            0|            0|  0.00%|        `ndarray`, however any non-default value will be.  If the\n",
      "  2706|         0|            0|            0|  0.00%|        sub-class' method does not implement `keepdims` any\n",
      "  2707|         0|            0|            0|  0.00%|        exceptions will be raised.\n",
      "  2708|         0|            0|            0|  0.00%|\n",
      "  2709|         0|            0|            0|  0.00%|    initial : scalar, optional\n",
      "  2710|         0|            0|            0|  0.00%|        The minimum value of an output element. Must be present to allow\n",
      "  2711|         0|            0|            0|  0.00%|        computation on empty slice. See `~numpy.ufunc.reduce` for details.\n",
      "  2712|         0|            0|            0|  0.00%|\n",
      "  2713|         0|            0|            0|  0.00%|        .. versionadded:: 1.15.0\n",
      "  2714|         0|            0|            0|  0.00%|\n",
      "  2715|         0|            0|            0|  0.00%|    where : array_like of bool, optional\n",
      "  2716|         0|            0|            0|  0.00%|        Elements to compare for the maximum. See `~numpy.ufunc.reduce`\n",
      "  2717|         0|            0|            0|  0.00%|        for details.\n",
      "  2718|         0|            0|            0|  0.00%|\n",
      "  2719|         0|            0|            0|  0.00%|        .. versionadded:: 1.17.0\n",
      "  2720|         0|            0|            0|  0.00%|\n",
      "  2721|         0|            0|            0|  0.00%|    Returns\n",
      "  2722|         0|            0|            0|  0.00%|    -------\n",
      "  2723|         0|            0|            0|  0.00%|    amax : ndarray or scalar\n",
      "  2724|         0|            0|            0|  0.00%|        Maximum of `a`. If `axis` is None, the result is a scalar value.\n",
      "  2725|         0|            0|            0|  0.00%|        If `axis` is given, the result is an array of dimension\n",
      "  2726|         0|            0|            0|  0.00%|        ``a.ndim - 1``.\n",
      "  2727|         0|            0|            0|  0.00%|\n",
      "  2728|         0|            0|            0|  0.00%|    See Also\n",
      "  2729|         0|            0|            0|  0.00%|    --------\n",
      "  2730|         0|            0|            0|  0.00%|    amin :\n",
      "  2731|         0|            0|            0|  0.00%|        The minimum value of an array along a given axis, propagating any NaNs.\n",
      "  2732|         0|            0|            0|  0.00%|    nanmax :\n",
      "  2733|         0|            0|            0|  0.00%|        The maximum value of an array along a given axis, ignoring any NaNs.\n",
      "  2734|         0|            0|            0|  0.00%|    maximum :\n",
      "  2735|         0|            0|            0|  0.00%|        Element-wise maximum of two arrays, propagating any NaNs.\n",
      "  2736|         0|            0|            0|  0.00%|    fmax :\n",
      "  2737|         0|            0|            0|  0.00%|        Element-wise maximum of two arrays, ignoring any NaNs.\n",
      "  2738|         0|            0|            0|  0.00%|    argmax :\n",
      "  2739|         0|            0|            0|  0.00%|        Return the indices of the maximum values.\n",
      "  2740|         0|            0|            0|  0.00%|\n",
      "  2741|         0|            0|            0|  0.00%|    nanmin, minimum, fmin\n",
      "  2742|         0|            0|            0|  0.00%|\n",
      "  2743|         0|            0|            0|  0.00%|    Notes\n",
      "  2744|         0|            0|            0|  0.00%|    -----\n",
      "  2745|         0|            0|            0|  0.00%|    NaN values are propagated, that is if at least one item is NaN, the\n",
      "  2746|         0|            0|            0|  0.00%|    corresponding max value will be NaN as well. To ignore NaN values\n",
      "  2747|         0|            0|            0|  0.00%|    (MATLAB behavior), please use nanmax.\n",
      "  2748|         0|            0|            0|  0.00%|\n",
      "  2749|         0|            0|            0|  0.00%|    Don't use `amax` for element-wise comparison of 2 arrays; when\n",
      "  2750|         0|            0|            0|  0.00%|    ``a.shape[0]`` is 2, ``maximum(a[0], a[1])`` is faster than\n",
      "  2751|         0|            0|            0|  0.00%|    ``amax(a, axis=0)``.\n",
      "  2752|         0|            0|            0|  0.00%|\n",
      "  2753|         0|            0|            0|  0.00%|    Examples\n",
      "  2754|         0|            0|            0|  0.00%|    --------\n",
      "  2755|         0|            0|            0|  0.00%|    >>> a = np.arange(4).reshape((2,2))\n",
      "  2756|         0|            0|            0|  0.00%|    >>> a\n",
      "  2757|         0|            0|            0|  0.00%|    array([[0, 1],\n",
      "  2758|         0|            0|            0|  0.00%|           [2, 3]])\n",
      "  2759|         0|            0|            0|  0.00%|    >>> np.amax(a)           # Maximum of the flattened array\n",
      "  2760|         0|            0|            0|  0.00%|    3\n",
      "  2761|         0|            0|            0|  0.00%|    >>> np.amax(a, axis=0)   # Maxima along the first axis\n",
      "  2762|         0|            0|            0|  0.00%|    array([2, 3])\n",
      "  2763|         0|            0|            0|  0.00%|    >>> np.amax(a, axis=1)   # Maxima along the second axis\n",
      "  2764|         0|            0|            0|  0.00%|    array([1, 3])\n",
      "  2765|         0|            0|            0|  0.00%|    >>> np.amax(a, where=[False, True], initial=-1, axis=0)\n",
      "  2766|         0|            0|            0|  0.00%|    array([-1,  3])\n",
      "  2767|         0|            0|            0|  0.00%|    >>> b = np.arange(5, dtype=float)\n",
      "  2768|         0|            0|            0|  0.00%|    >>> b[2] = np.NaN\n",
      "  2769|         0|            0|            0|  0.00%|    >>> np.amax(b)\n",
      "  2770|         0|            0|            0|  0.00%|    nan\n",
      "  2771|         0|            0|            0|  0.00%|    >>> np.amax(b, where=~np.isnan(b), initial=-1)\n",
      "  2772|         0|            0|            0|  0.00%|    4.0\n",
      "  2773|         0|            0|            0|  0.00%|    >>> np.nanmax(b)\n",
      "  2774|         0|            0|            0|  0.00%|    4.0\n",
      "  2775|         0|            0|            0|  0.00%|\n",
      "  2776|         0|            0|            0|  0.00%|    You can use an initial value to compute the maximum of an empty slice, or\n",
      "  2777|         0|            0|            0|  0.00%|    to initialize it to a different value:\n",
      "  2778|         0|            0|            0|  0.00%|\n",
      "  2779|         0|            0|            0|  0.00%|    >>> np.amax([[-50], [10]], axis=-1, initial=0)\n",
      "  2780|         0|            0|            0|  0.00%|    array([ 0, 10])\n",
      "  2781|         0|            0|            0|  0.00%|\n",
      "  2782|         0|            0|            0|  0.00%|    Notice that the initial value is used as one of the elements for which the\n",
      "  2783|         0|            0|            0|  0.00%|    maximum is determined, unlike for the default argument Python's max\n",
      "  2784|         0|            0|            0|  0.00%|    function, which is only used for empty iterables.\n",
      "  2785|         0|            0|            0|  0.00%|\n",
      "  2786|         0|            0|            0|  0.00%|    >>> np.amax([5], initial=6)\n",
      "  2787|         0|            0|            0|  0.00%|    6\n",
      "  2788|         0|            0|            0|  0.00%|    >>> max([5], default=6)\n",
      "  2789|         0|            0|            0|  0.00%|    5\n",
      "  2790|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2791|         0|            0|            0|  0.00%|    return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n",
      "  2792|         0|            0|            0|  0.00%|                          keepdims=keepdims, initial=initial, where=where)\n",
      "  2793|         0|            0|            0|  0.00%|\n",
      "  2794|         0|            0|            0|  0.00%|\n",
      "  2795|         0|            0|            0|  0.00%|def _amin_dispatcher(a, axis=None, out=None, keepdims=None, initial=None,\n",
      "  2796|         0|            0|            0|  0.00%|                     where=None):\n",
      "  2797|         0|            0|            0|  0.00%|    return (a, out)\n",
      "  2798|         0|            0|            0|  0.00%|\n",
      "  2799|         0|            0|            0|  0.00%|\n",
      "  2800|         0|            0|            0|  0.00%|@array_function_dispatch(_amin_dispatcher)\n",
      "  2801|         0|            0|            0|  0.00%|def amin(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n",
      "  2802|         0|            0|            0|  0.00%|         where=np._NoValue):\n",
      "  2803|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2804|         0|            0|            0|  0.00%|    Return the minimum of an array or minimum along an axis.\n",
      "  2805|         0|            0|            0|  0.00%|\n",
      "  2806|         0|            0|            0|  0.00%|    Parameters\n",
      "  2807|         0|            0|            0|  0.00%|    ----------\n",
      "  2808|         0|            0|            0|  0.00%|    a : array_like\n",
      "  2809|         0|            0|            0|  0.00%|        Input data.\n",
      "  2810|         0|            0|            0|  0.00%|    axis : None or int or tuple of ints, optional\n",
      "  2811|         0|            0|            0|  0.00%|        Axis or axes along which to operate.  By default, flattened input is\n",
      "  2812|         0|            0|            0|  0.00%|        used.\n",
      "  2813|         0|            0|            0|  0.00%|\n",
      "  2814|         0|            0|            0|  0.00%|        .. versionadded:: 1.7.0\n",
      "  2815|         0|            0|            0|  0.00%|\n",
      "  2816|         0|            0|            0|  0.00%|        If this is a tuple of ints, the minimum is selected over multiple axes,\n",
      "  2817|         0|            0|            0|  0.00%|        instead of a single axis or all the axes as before.\n",
      "  2818|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  2819|         0|            0|            0|  0.00%|        Alternative output array in which to place the result.  Must\n",
      "  2820|         0|            0|            0|  0.00%|        be of the same shape and buffer length as the expected output.\n",
      "  2821|         0|            0|            0|  0.00%|        See :ref:`ufuncs-output-type` for more details.\n",
      "  2822|         0|            0|            0|  0.00%|\n",
      "  2823|         0|            0|            0|  0.00%|    keepdims : bool, optional\n",
      "  2824|         0|            0|            0|  0.00%|        If this is set to True, the axes which are reduced are left\n",
      "  2825|         0|            0|            0|  0.00%|        in the result as dimensions with size one. With this option,\n",
      "  2826|         0|            0|            0|  0.00%|        the result will broadcast correctly against the input array.\n",
      "  2827|         0|            0|            0|  0.00%|\n",
      "  2828|         0|            0|            0|  0.00%|        If the default value is passed, then `keepdims` will not be\n",
      "  2829|         0|            0|            0|  0.00%|        passed through to the `amin` method of sub-classes of\n",
      "  2830|         0|            0|            0|  0.00%|        `ndarray`, however any non-default value will be.  If the\n",
      "  2831|         0|            0|            0|  0.00%|        sub-class' method does not implement `keepdims` any\n",
      "  2832|         0|            0|            0|  0.00%|        exceptions will be raised.\n",
      "  2833|         0|            0|            0|  0.00%|\n",
      "  2834|         0|            0|            0|  0.00%|    initial : scalar, optional\n",
      "  2835|         0|            0|            0|  0.00%|        The maximum value of an output element. Must be present to allow\n",
      "  2836|         0|            0|            0|  0.00%|        computation on empty slice. See `~numpy.ufunc.reduce` for details.\n",
      "  2837|         0|            0|            0|  0.00%|\n",
      "  2838|         0|            0|            0|  0.00%|        .. versionadded:: 1.15.0\n",
      "  2839|         0|            0|            0|  0.00%|\n",
      "  2840|         0|            0|            0|  0.00%|    where : array_like of bool, optional\n",
      "  2841|         0|            0|            0|  0.00%|        Elements to compare for the minimum. See `~numpy.ufunc.reduce`\n",
      "  2842|         0|            0|            0|  0.00%|        for details.\n",
      "  2843|         0|            0|            0|  0.00%|\n",
      "  2844|         0|            0|            0|  0.00%|        .. versionadded:: 1.17.0\n",
      "  2845|         0|            0|            0|  0.00%|\n",
      "  2846|         0|            0|            0|  0.00%|    Returns\n",
      "  2847|         0|            0|            0|  0.00%|    -------\n",
      "  2848|         0|            0|            0|  0.00%|    amin : ndarray or scalar\n",
      "  2849|         0|            0|            0|  0.00%|        Minimum of `a`. If `axis` is None, the result is a scalar value.\n",
      "  2850|         0|            0|            0|  0.00%|        If `axis` is given, the result is an array of dimension\n",
      "  2851|         0|            0|            0|  0.00%|        ``a.ndim - 1``.\n",
      "  2852|         0|            0|            0|  0.00%|\n",
      "  2853|         0|            0|            0|  0.00%|    See Also\n",
      "  2854|         0|            0|            0|  0.00%|    --------\n",
      "  2855|         0|            0|            0|  0.00%|    amax :\n",
      "  2856|         0|            0|            0|  0.00%|        The maximum value of an array along a given axis, propagating any NaNs.\n",
      "  2857|         0|            0|            0|  0.00%|    nanmin :\n",
      "  2858|         0|            0|            0|  0.00%|        The minimum value of an array along a given axis, ignoring any NaNs.\n",
      "  2859|         0|            0|            0|  0.00%|    minimum :\n",
      "  2860|         0|            0|            0|  0.00%|        Element-wise minimum of two arrays, propagating any NaNs.\n",
      "  2861|         0|            0|            0|  0.00%|    fmin :\n",
      "  2862|         0|            0|            0|  0.00%|        Element-wise minimum of two arrays, ignoring any NaNs.\n",
      "  2863|         0|            0|            0|  0.00%|    argmin :\n",
      "  2864|         0|            0|            0|  0.00%|        Return the indices of the minimum values.\n",
      "  2865|         0|            0|            0|  0.00%|\n",
      "  2866|         0|            0|            0|  0.00%|    nanmax, maximum, fmax\n",
      "  2867|         0|            0|            0|  0.00%|\n",
      "  2868|         0|            0|            0|  0.00%|    Notes\n",
      "  2869|         0|            0|            0|  0.00%|    -----\n",
      "  2870|         0|            0|            0|  0.00%|    NaN values are propagated, that is if at least one item is NaN, the\n",
      "  2871|         0|            0|            0|  0.00%|    corresponding min value will be NaN as well. To ignore NaN values\n",
      "  2872|         0|            0|            0|  0.00%|    (MATLAB behavior), please use nanmin.\n",
      "  2873|         0|            0|            0|  0.00%|\n",
      "  2874|         0|            0|            0|  0.00%|    Don't use `amin` for element-wise comparison of 2 arrays; when\n",
      "  2875|         0|            0|            0|  0.00%|    ``a.shape[0]`` is 2, ``minimum(a[0], a[1])`` is faster than\n",
      "  2876|         0|            0|            0|  0.00%|    ``amin(a, axis=0)``.\n",
      "  2877|         0|            0|            0|  0.00%|\n",
      "  2878|         0|            0|            0|  0.00%|    Examples\n",
      "  2879|         0|            0|            0|  0.00%|    --------\n",
      "  2880|         0|            0|            0|  0.00%|    >>> a = np.arange(4).reshape((2,2))\n",
      "  2881|         0|            0|            0|  0.00%|    >>> a\n",
      "  2882|         0|            0|            0|  0.00%|    array([[0, 1],\n",
      "  2883|         0|            0|            0|  0.00%|           [2, 3]])\n",
      "  2884|         0|            0|            0|  0.00%|    >>> np.amin(a)           # Minimum of the flattened array\n",
      "  2885|         0|            0|            0|  0.00%|    0\n",
      "  2886|         0|            0|            0|  0.00%|    >>> np.amin(a, axis=0)   # Minima along the first axis\n",
      "  2887|         0|            0|            0|  0.00%|    array([0, 1])\n",
      "  2888|         0|            0|            0|  0.00%|    >>> np.amin(a, axis=1)   # Minima along the second axis\n",
      "  2889|         0|            0|            0|  0.00%|    array([0, 2])\n",
      "  2890|         0|            0|            0|  0.00%|    >>> np.amin(a, where=[False, True], initial=10, axis=0)\n",
      "  2891|         0|            0|            0|  0.00%|    array([10,  1])\n",
      "  2892|         0|            0|            0|  0.00%|\n",
      "  2893|         0|            0|            0|  0.00%|    >>> b = np.arange(5, dtype=float)\n",
      "  2894|         0|            0|            0|  0.00%|    >>> b[2] = np.NaN\n",
      "  2895|         0|            0|            0|  0.00%|    >>> np.amin(b)\n",
      "  2896|         0|            0|            0|  0.00%|    nan\n",
      "  2897|         0|            0|            0|  0.00%|    >>> np.amin(b, where=~np.isnan(b), initial=10)\n",
      "  2898|         0|            0|            0|  0.00%|    0.0\n",
      "  2899|         0|            0|            0|  0.00%|    >>> np.nanmin(b)\n",
      "  2900|         0|            0|            0|  0.00%|    0.0\n",
      "  2901|         0|            0|            0|  0.00%|\n",
      "  2902|         0|            0|            0|  0.00%|    >>> np.amin([[-50], [10]], axis=-1, initial=0)\n",
      "  2903|         0|            0|            0|  0.00%|    array([-50,   0])\n",
      "  2904|         0|            0|            0|  0.00%|\n",
      "  2905|         0|            0|            0|  0.00%|    Notice that the initial value is used as one of the elements for which the\n",
      "  2906|         0|            0|            0|  0.00%|    minimum is determined, unlike for the default argument Python's max\n",
      "  2907|         0|            0|            0|  0.00%|    function, which is only used for empty iterables.\n",
      "  2908|         0|            0|            0|  0.00%|\n",
      "  2909|         0|            0|            0|  0.00%|    Notice that this isn't the same as Python's ``default`` argument.\n",
      "  2910|         0|            0|            0|  0.00%|\n",
      "  2911|         0|            0|            0|  0.00%|    >>> np.amin([6], initial=5)\n",
      "  2912|         0|            0|            0|  0.00%|    5\n",
      "  2913|         0|            0|            0|  0.00%|    >>> min([6], default=5)\n",
      "  2914|         0|            0|            0|  0.00%|    6\n",
      "  2915|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2916|         0|            0|            0|  0.00%|    return _wrapreduction(a, np.minimum, 'min', axis, None, out,\n",
      "  2917|         0|            0|            0|  0.00%|                          keepdims=keepdims, initial=initial, where=where)\n",
      "  2918|         0|            0|            0|  0.00%|\n",
      "  2919|         0|            0|            0|  0.00%|\n",
      "  2920|         0|            0|            0|  0.00%|def _alen_dispathcer(a):\n",
      "  2921|         0|            0|            0|  0.00%|    return (a,)\n",
      "  2922|         0|            0|            0|  0.00%|\n",
      "  2923|         0|            0|            0|  0.00%|\n",
      "  2924|         0|            0|            0|  0.00%|@array_function_dispatch(_alen_dispathcer)\n",
      "  2925|         0|            0|            0|  0.00%|def alen(a):\n",
      "  2926|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2927|         0|            0|            0|  0.00%|    Return the length of the first dimension of the input array.\n",
      "  2928|         0|            0|            0|  0.00%|\n",
      "  2929|         0|            0|            0|  0.00%|    .. deprecated:: 1.18\n",
      "  2930|         0|            0|            0|  0.00%|       `numpy.alen` is deprecated, use `len` instead.\n",
      "  2931|         0|            0|            0|  0.00%|\n",
      "  2932|         0|            0|            0|  0.00%|    Parameters\n",
      "  2933|         0|            0|            0|  0.00%|    ----------\n",
      "  2934|         0|            0|            0|  0.00%|    a : array_like\n",
      "  2935|         0|            0|            0|  0.00%|       Input array.\n",
      "  2936|         0|            0|            0|  0.00%|\n",
      "  2937|         0|            0|            0|  0.00%|    Returns\n",
      "  2938|         0|            0|            0|  0.00%|    -------\n",
      "  2939|         0|            0|            0|  0.00%|    alen : int\n",
      "  2940|         0|            0|            0|  0.00%|       Length of the first dimension of `a`.\n",
      "  2941|         0|            0|            0|  0.00%|\n",
      "  2942|         0|            0|            0|  0.00%|    See Also\n",
      "  2943|         0|            0|            0|  0.00%|    --------\n",
      "  2944|         0|            0|            0|  0.00%|    shape, size\n",
      "  2945|         0|            0|            0|  0.00%|\n",
      "  2946|         0|            0|            0|  0.00%|    Examples\n",
      "  2947|         0|            0|            0|  0.00%|    --------\n",
      "  2948|         0|            0|            0|  0.00%|    >>> a = np.zeros((7,4,5))\n",
      "  2949|         0|            0|            0|  0.00%|    >>> a.shape[0]\n",
      "  2950|         0|            0|            0|  0.00%|    7\n",
      "  2951|         0|            0|            0|  0.00%|    >>> np.alen(a)\n",
      "  2952|         0|            0|            0|  0.00%|    7\n",
      "  2953|         0|            0|            0|  0.00%|\n",
      "  2954|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2955|         0|            0|            0|  0.00%|    # NumPy 1.18.0, 2019-08-02\n",
      "  2956|         0|            0|            0|  0.00%|    warnings.warn(\n",
      "  2957|         0|            0|            0|  0.00%|        \"`np.alen` is deprecated, use `len` instead\",\n",
      "  2958|         0|            0|            0|  0.00%|        DeprecationWarning, stacklevel=2)\n",
      "  2959|         0|            0|            0|  0.00%|    try:\n",
      "  2960|         0|            0|            0|  0.00%|        return len(a)\n",
      "  2961|         0|            0|            0|  0.00%|    except TypeError:\n",
      "  2962|         0|            0|            0|  0.00%|        return len(array(a, ndmin=1))\n",
      "  2963|         0|            0|            0|  0.00%|\n",
      "  2964|         0|            0|            0|  0.00%|\n",
      "  2965|         0|            0|            0|  0.00%|def _prod_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None,\n",
      "  2966|         0|            0|            0|  0.00%|                     initial=None, where=None):\n",
      "  2967|         0|            0|            0|  0.00%|    return (a, out)\n",
      "  2968|         0|            0|            0|  0.00%|\n",
      "  2969|         0|            0|            0|  0.00%|\n",
      "  2970|         0|            0|            0|  0.00%|@array_function_dispatch(_prod_dispatcher)\n",
      "  2971|         0|            0|            0|  0.00%|def prod(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,\n",
      "  2972|         0|            0|            0|  0.00%|         initial=np._NoValue, where=np._NoValue):\n",
      "  2973|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2974|         0|            0|            0|  0.00%|    Return the product of array elements over a given axis.\n",
      "  2975|         0|            0|            0|  0.00%|\n",
      "  2976|         0|            0|            0|  0.00%|    Parameters\n",
      "  2977|         0|            0|            0|  0.00%|    ----------\n",
      "  2978|         0|            0|            0|  0.00%|    a : array_like\n",
      "  2979|         0|            0|            0|  0.00%|        Input data.\n",
      "  2980|         0|            0|            0|  0.00%|    axis : None or int or tuple of ints, optional\n",
      "  2981|         0|            0|            0|  0.00%|        Axis or axes along which a product is performed.  The default,\n",
      "  2982|         0|            0|            0|  0.00%|        axis=None, will calculate the product of all the elements in the\n",
      "  2983|         0|            0|            0|  0.00%|        input array. If axis is negative it counts from the last to the\n",
      "  2984|         0|            0|            0|  0.00%|        first axis.\n",
      "  2985|         0|            0|            0|  0.00%|\n",
      "  2986|         0|            0|            0|  0.00%|        .. versionadded:: 1.7.0\n",
      "  2987|         0|            0|            0|  0.00%|\n",
      "  2988|         0|            0|            0|  0.00%|        If axis is a tuple of ints, a product is performed on all of the\n",
      "  2989|         0|            0|            0|  0.00%|        axes specified in the tuple instead of a single axis or all the\n",
      "  2990|         0|            0|            0|  0.00%|        axes as before.\n",
      "  2991|         0|            0|            0|  0.00%|    dtype : dtype, optional\n",
      "  2992|         0|            0|            0|  0.00%|        The type of the returned array, as well as of the accumulator in\n",
      "  2993|         0|            0|            0|  0.00%|        which the elements are multiplied.  The dtype of `a` is used by\n",
      "  2994|         0|            0|            0|  0.00%|        default unless `a` has an integer dtype of less precision than the\n",
      "  2995|         0|            0|            0|  0.00%|        default platform integer.  In that case, if `a` is signed then the\n",
      "  2996|         0|            0|            0|  0.00%|        platform integer is used while if `a` is unsigned then an unsigned\n",
      "  2997|         0|            0|            0|  0.00%|        integer of the same precision as the platform integer is used.\n",
      "  2998|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  2999|         0|            0|            0|  0.00%|        Alternative output array in which to place the result. It must have\n",
      "  3000|         0|            0|            0|  0.00%|        the same shape as the expected output, but the type of the output\n",
      "  3001|         0|            0|            0|  0.00%|        values will be cast if necessary.\n",
      "  3002|         0|            0|            0|  0.00%|    keepdims : bool, optional\n",
      "  3003|         0|            0|            0|  0.00%|        If this is set to True, the axes which are reduced are left in the\n",
      "  3004|         0|            0|            0|  0.00%|        result as dimensions with size one. With this option, the result\n",
      "  3005|         0|            0|            0|  0.00%|        will broadcast correctly against the input array.\n",
      "  3006|         0|            0|            0|  0.00%|\n",
      "  3007|         0|            0|            0|  0.00%|        If the default value is passed, then `keepdims` will not be\n",
      "  3008|         0|            0|            0|  0.00%|        passed through to the `prod` method of sub-classes of\n",
      "  3009|         0|            0|            0|  0.00%|        `ndarray`, however any non-default value will be.  If the\n",
      "  3010|         0|            0|            0|  0.00%|        sub-class' method does not implement `keepdims` any\n",
      "  3011|         0|            0|            0|  0.00%|        exceptions will be raised.\n",
      "  3012|         0|            0|            0|  0.00%|    initial : scalar, optional\n",
      "  3013|         0|            0|            0|  0.00%|        The starting value for this product. See `~numpy.ufunc.reduce` for details.\n",
      "  3014|         0|            0|            0|  0.00%|\n",
      "  3015|         0|            0|            0|  0.00%|        .. versionadded:: 1.15.0\n",
      "  3016|         0|            0|            0|  0.00%|\n",
      "  3017|         0|            0|            0|  0.00%|    where : array_like of bool, optional\n",
      "  3018|         0|            0|            0|  0.00%|        Elements to include in the product. See `~numpy.ufunc.reduce` for details.\n",
      "  3019|         0|            0|            0|  0.00%|\n",
      "  3020|         0|            0|            0|  0.00%|        .. versionadded:: 1.17.0\n",
      "  3021|         0|            0|            0|  0.00%|\n",
      "  3022|         0|            0|            0|  0.00%|    Returns\n",
      "  3023|         0|            0|            0|  0.00%|    -------\n",
      "  3024|         0|            0|            0|  0.00%|    product_along_axis : ndarray, see `dtype` parameter above.\n",
      "  3025|         0|            0|            0|  0.00%|        An array shaped as `a` but with the specified axis removed.\n",
      "  3026|         0|            0|            0|  0.00%|        Returns a reference to `out` if specified.\n",
      "  3027|         0|            0|            0|  0.00%|\n",
      "  3028|         0|            0|            0|  0.00%|    See Also\n",
      "  3029|         0|            0|            0|  0.00%|    --------\n",
      "  3030|         0|            0|            0|  0.00%|    ndarray.prod : equivalent method\n",
      "  3031|         0|            0|            0|  0.00%|    :ref:`ufuncs-output-type`\n",
      "  3032|         0|            0|            0|  0.00%|\n",
      "  3033|         0|            0|            0|  0.00%|    Notes\n",
      "  3034|         0|            0|            0|  0.00%|    -----\n",
      "  3035|         0|            0|            0|  0.00%|    Arithmetic is modular when using integer types, and no error is\n",
      "  3036|         0|            0|            0|  0.00%|    raised on overflow.  That means that, on a 32-bit platform:\n",
      "  3037|         0|            0|            0|  0.00%|\n",
      "  3038|         0|            0|            0|  0.00%|    >>> x = np.array([536870910, 536870910, 536870910, 536870910])\n",
      "  3039|         0|            0|            0|  0.00%|    >>> np.prod(x)\n",
      "  3040|         0|            0|            0|  0.00%|    16 # may vary\n",
      "  3041|         0|            0|            0|  0.00%|\n",
      "  3042|         0|            0|            0|  0.00%|    The product of an empty array is the neutral element 1:\n",
      "  3043|         0|            0|            0|  0.00%|\n",
      "  3044|         0|            0|            0|  0.00%|    >>> np.prod([])\n",
      "  3045|         0|            0|            0|  0.00%|    1.0\n",
      "  3046|         0|            0|            0|  0.00%|\n",
      "  3047|         0|            0|            0|  0.00%|    Examples\n",
      "  3048|         0|            0|            0|  0.00%|    --------\n",
      "  3049|         0|            0|            0|  0.00%|    By default, calculate the product of all elements:\n",
      "  3050|         0|            0|            0|  0.00%|\n",
      "  3051|         0|            0|            0|  0.00%|    >>> np.prod([1.,2.])\n",
      "  3052|         0|            0|            0|  0.00%|    2.0\n",
      "  3053|         0|            0|            0|  0.00%|\n",
      "  3054|         0|            0|            0|  0.00%|    Even when the input array is two-dimensional:\n",
      "  3055|         0|            0|            0|  0.00%|\n",
      "  3056|         0|            0|            0|  0.00%|    >>> np.prod([[1.,2.],[3.,4.]])\n",
      "  3057|         0|            0|            0|  0.00%|    24.0\n",
      "  3058|         0|            0|            0|  0.00%|\n",
      "  3059|         0|            0|            0|  0.00%|    But we can also specify the axis over which to multiply:\n",
      "  3060|         0|            0|            0|  0.00%|\n",
      "  3061|         0|            0|            0|  0.00%|    >>> np.prod([[1.,2.],[3.,4.]], axis=1)\n",
      "  3062|         0|            0|            0|  0.00%|    array([  2.,  12.])\n",
      "  3063|         0|            0|            0|  0.00%|\n",
      "  3064|         0|            0|            0|  0.00%|    Or select specific elements to include:\n",
      "  3065|         0|            0|            0|  0.00%|\n",
      "  3066|         0|            0|            0|  0.00%|    >>> np.prod([1., np.nan, 3.], where=[True, False, True])\n",
      "  3067|         0|            0|            0|  0.00%|    3.0\n",
      "  3068|         0|            0|            0|  0.00%|\n",
      "  3069|         0|            0|            0|  0.00%|    If the type of `x` is unsigned, then the output type is\n",
      "  3070|         0|            0|            0|  0.00%|    the unsigned platform integer:\n",
      "  3071|         0|            0|            0|  0.00%|\n",
      "  3072|         0|            0|            0|  0.00%|    >>> x = np.array([1, 2, 3], dtype=np.uint8)\n",
      "  3073|         0|            0|            0|  0.00%|    >>> np.prod(x).dtype == np.uint\n",
      "  3074|         0|            0|            0|  0.00%|    True\n",
      "  3075|         0|            0|            0|  0.00%|\n",
      "  3076|         0|            0|            0|  0.00%|    If `x` is of a signed integer type, then the output type\n",
      "  3077|         0|            0|            0|  0.00%|    is the default platform integer:\n",
      "  3078|         0|            0|            0|  0.00%|\n",
      "  3079|         0|            0|            0|  0.00%|    >>> x = np.array([1, 2, 3], dtype=np.int8)\n",
      "  3080|         0|            0|            0|  0.00%|    >>> np.prod(x).dtype == int\n",
      "  3081|         0|            0|            0|  0.00%|    True\n",
      "  3082|         0|            0|            0|  0.00%|\n",
      "  3083|         0|            0|            0|  0.00%|    You can also start the product with a value other than one:\n",
      "  3084|         0|            0|            0|  0.00%|\n",
      "  3085|         0|            0|            0|  0.00%|    >>> np.prod([1, 2], initial=5)\n",
      "  3086|         0|            0|            0|  0.00%|    10\n",
      "  3087|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3088|         0|            0|            0|  0.00%|    return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n",
      "  3089|         0|            0|            0|  0.00%|                          keepdims=keepdims, initial=initial, where=where)\n",
      "  3090|         0|            0|            0|  0.00%|\n",
      "  3091|         0|            0|            0|  0.00%|\n",
      "  3092|         0|            0|            0|  0.00%|def _cumprod_dispatcher(a, axis=None, dtype=None, out=None):\n",
      "  3093|         0|            0|            0|  0.00%|    return (a, out)\n",
      "  3094|         0|            0|            0|  0.00%|\n",
      "  3095|         0|            0|            0|  0.00%|\n",
      "  3096|         0|            0|            0|  0.00%|@array_function_dispatch(_cumprod_dispatcher)\n",
      "  3097|         0|            0|            0|  0.00%|def cumprod(a, axis=None, dtype=None, out=None):\n",
      "  3098|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3099|         0|            0|            0|  0.00%|    Return the cumulative product of elements along a given axis.\n",
      "  3100|         0|            0|            0|  0.00%|\n",
      "  3101|         0|            0|            0|  0.00%|    Parameters\n",
      "  3102|         0|            0|            0|  0.00%|    ----------\n",
      "  3103|         0|            0|            0|  0.00%|    a : array_like\n",
      "  3104|         0|            0|            0|  0.00%|        Input array.\n",
      "  3105|         0|            0|            0|  0.00%|    axis : int, optional\n",
      "  3106|         0|            0|            0|  0.00%|        Axis along which the cumulative product is computed.  By default\n",
      "  3107|         0|            0|            0|  0.00%|        the input is flattened.\n",
      "  3108|         0|            0|            0|  0.00%|    dtype : dtype, optional\n",
      "  3109|         0|            0|            0|  0.00%|        Type of the returned array, as well as of the accumulator in which\n",
      "  3110|         0|            0|            0|  0.00%|        the elements are multiplied.  If *dtype* is not specified, it\n",
      "  3111|         0|            0|            0|  0.00%|        defaults to the dtype of `a`, unless `a` has an integer dtype with\n",
      "  3112|         0|            0|            0|  0.00%|        a precision less than that of the default platform integer.  In\n",
      "  3113|         0|            0|            0|  0.00%|        that case, the default platform integer is used instead.\n",
      "  3114|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  3115|         0|            0|            0|  0.00%|        Alternative output array in which to place the result. It must\n",
      "  3116|         0|            0|            0|  0.00%|        have the same shape and buffer length as the expected output\n",
      "  3117|         0|            0|            0|  0.00%|        but the type of the resulting values will be cast if necessary.\n",
      "  3118|         0|            0|            0|  0.00%|\n",
      "  3119|         0|            0|            0|  0.00%|    Returns\n",
      "  3120|         0|            0|            0|  0.00%|    -------\n",
      "  3121|         0|            0|            0|  0.00%|    cumprod : ndarray\n",
      "  3122|         0|            0|            0|  0.00%|        A new array holding the result is returned unless `out` is\n",
      "  3123|         0|            0|            0|  0.00%|        specified, in which case a reference to out is returned.\n",
      "  3124|         0|            0|            0|  0.00%|\n",
      "  3125|         0|            0|            0|  0.00%|    See Also\n",
      "  3126|         0|            0|            0|  0.00%|    --------\n",
      "  3127|         0|            0|            0|  0.00%|    :ref:`ufuncs-output-type`\n",
      "  3128|         0|            0|            0|  0.00%|\n",
      "  3129|         0|            0|            0|  0.00%|    Notes\n",
      "  3130|         0|            0|            0|  0.00%|    -----\n",
      "  3131|         0|            0|            0|  0.00%|    Arithmetic is modular when using integer types, and no error is\n",
      "  3132|         0|            0|            0|  0.00%|    raised on overflow.\n",
      "  3133|         0|            0|            0|  0.00%|\n",
      "  3134|         0|            0|            0|  0.00%|    Examples\n",
      "  3135|         0|            0|            0|  0.00%|    --------\n",
      "  3136|         0|            0|            0|  0.00%|    >>> a = np.array([1,2,3])\n",
      "  3137|         0|            0|            0|  0.00%|    >>> np.cumprod(a) # intermediate results 1, 1*2\n",
      "  3138|         0|            0|            0|  0.00%|    ...               # total product 1*2*3 = 6\n",
      "  3139|         0|            0|            0|  0.00%|    array([1, 2, 6])\n",
      "  3140|         0|            0|            0|  0.00%|    >>> a = np.array([[1, 2, 3], [4, 5, 6]])\n",
      "  3141|         0|            0|            0|  0.00%|    >>> np.cumprod(a, dtype=float) # specify type of output\n",
      "  3142|         0|            0|            0|  0.00%|    array([   1.,    2.,    6.,   24.,  120.,  720.])\n",
      "  3143|         0|            0|            0|  0.00%|\n",
      "  3144|         0|            0|            0|  0.00%|    The cumulative product for each column (i.e., over the rows) of `a`:\n",
      "  3145|         0|            0|            0|  0.00%|\n",
      "  3146|         0|            0|            0|  0.00%|    >>> np.cumprod(a, axis=0)\n",
      "  3147|         0|            0|            0|  0.00%|    array([[ 1,  2,  3],\n",
      "  3148|         0|            0|            0|  0.00%|           [ 4, 10, 18]])\n",
      "  3149|         0|            0|            0|  0.00%|\n",
      "  3150|         0|            0|            0|  0.00%|    The cumulative product for each row (i.e. over the columns) of `a`:\n",
      "  3151|         0|            0|            0|  0.00%|\n",
      "  3152|         0|            0|            0|  0.00%|    >>> np.cumprod(a,axis=1)\n",
      "  3153|         0|            0|            0|  0.00%|    array([[  1,   2,   6],\n",
      "  3154|         0|            0|            0|  0.00%|           [  4,  20, 120]])\n",
      "  3155|         0|            0|            0|  0.00%|\n",
      "  3156|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3157|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'cumprod', axis=axis, dtype=dtype, out=out)\n",
      "  3158|         0|            0|            0|  0.00%|\n",
      "  3159|         0|            0|            0|  0.00%|\n",
      "  3160|         0|            0|            0|  0.00%|def _ndim_dispatcher(a):\n",
      "  3161|         0|            0|            0|  0.00%|    return (a,)\n",
      "  3162|         0|            0|            0|  0.00%|\n",
      "  3163|         0|            0|            0|  0.00%|\n",
      "  3164|         0|            0|            0|  0.00%|@array_function_dispatch(_ndim_dispatcher)\n",
      "  3165|         0|            0|            0|  0.00%|def ndim(a):\n",
      "  3166|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3167|         0|            0|            0|  0.00%|    Return the number of dimensions of an array.\n",
      "  3168|         0|            0|            0|  0.00%|\n",
      "  3169|         0|            0|            0|  0.00%|    Parameters\n",
      "  3170|         0|            0|            0|  0.00%|    ----------\n",
      "  3171|         0|            0|            0|  0.00%|    a : array_like\n",
      "  3172|         0|            0|            0|  0.00%|        Input array.  If it is not already an ndarray, a conversion is\n",
      "  3173|         0|            0|            0|  0.00%|        attempted.\n",
      "  3174|         0|            0|            0|  0.00%|\n",
      "  3175|         0|            0|            0|  0.00%|    Returns\n",
      "  3176|         0|            0|            0|  0.00%|    -------\n",
      "  3177|         0|            0|            0|  0.00%|    number_of_dimensions : int\n",
      "  3178|         0|            0|            0|  0.00%|        The number of dimensions in `a`.  Scalars are zero-dimensional.\n",
      "  3179|         0|            0|            0|  0.00%|\n",
      "  3180|         0|            0|            0|  0.00%|    See Also\n",
      "  3181|         0|            0|            0|  0.00%|    --------\n",
      "  3182|         0|            0|            0|  0.00%|    ndarray.ndim : equivalent method\n",
      "  3183|         0|            0|            0|  0.00%|    shape : dimensions of array\n",
      "  3184|         0|            0|            0|  0.00%|    ndarray.shape : dimensions of array\n",
      "  3185|         0|            0|            0|  0.00%|\n",
      "  3186|         0|            0|            0|  0.00%|    Examples\n",
      "  3187|         0|            0|            0|  0.00%|    --------\n",
      "  3188|         0|            0|            0|  0.00%|    >>> np.ndim([[1,2,3],[4,5,6]])\n",
      "  3189|         0|            0|            0|  0.00%|    2\n",
      "  3190|         0|            0|            0|  0.00%|    >>> np.ndim(np.array([[1,2,3],[4,5,6]]))\n",
      "  3191|         0|            0|            0|  0.00%|    2\n",
      "  3192|         0|            0|            0|  0.00%|    >>> np.ndim(1)\n",
      "  3193|         0|            0|            0|  0.00%|    0\n",
      "  3194|         0|            0|            0|  0.00%|\n",
      "  3195|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3196|         0|            0|            0|  0.00%|    try:\n",
      "  3197|         0|            0|            0|  0.00%|        return a.ndim\n",
      "  3198|         0|            0|            0|  0.00%|    except AttributeError:\n",
      "  3199|         0|            0|            0|  0.00%|        return asarray(a).ndim\n",
      "  3200|         0|            0|            0|  0.00%|\n",
      "  3201|         0|            0|            0|  0.00%|\n",
      "  3202|         0|            0|            0|  0.00%|def _size_dispatcher(a, axis=None):\n",
      "  3203|         0|            0|            0|  0.00%|    return (a,)\n",
      "  3204|         0|            0|            0|  0.00%|\n",
      "  3205|         0|            0|            0|  0.00%|\n",
      "  3206|         0|            0|            0|  0.00%|@array_function_dispatch(_size_dispatcher)\n",
      "  3207|         0|            0|            0|  0.00%|def size(a, axis=None):\n",
      "  3208|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3209|         0|            0|            0|  0.00%|    Return the number of elements along a given axis.\n",
      "  3210|         0|            0|            0|  0.00%|\n",
      "  3211|         0|            0|            0|  0.00%|    Parameters\n",
      "  3212|         0|            0|            0|  0.00%|    ----------\n",
      "  3213|         0|            0|            0|  0.00%|    a : array_like\n",
      "  3214|         0|            0|            0|  0.00%|        Input data.\n",
      "  3215|         0|            0|            0|  0.00%|    axis : int, optional\n",
      "  3216|         0|            0|            0|  0.00%|        Axis along which the elements are counted.  By default, give\n",
      "  3217|         0|            0|            0|  0.00%|        the total number of elements.\n",
      "  3218|         0|            0|            0|  0.00%|\n",
      "  3219|         0|            0|            0|  0.00%|    Returns\n",
      "  3220|         0|            0|            0|  0.00%|    -------\n",
      "  3221|         0|            0|            0|  0.00%|    element_count : int\n",
      "  3222|         0|            0|            0|  0.00%|        Number of elements along the specified axis.\n",
      "  3223|         0|            0|            0|  0.00%|\n",
      "  3224|         0|            0|            0|  0.00%|    See Also\n",
      "  3225|         0|            0|            0|  0.00%|    --------\n",
      "  3226|         0|            0|            0|  0.00%|    shape : dimensions of array\n",
      "  3227|         0|            0|            0|  0.00%|    ndarray.shape : dimensions of array\n",
      "  3228|         0|            0|            0|  0.00%|    ndarray.size : number of elements in array\n",
      "  3229|         0|            0|            0|  0.00%|\n",
      "  3230|         0|            0|            0|  0.00%|    Examples\n",
      "  3231|         0|            0|            0|  0.00%|    --------\n",
      "  3232|         0|            0|            0|  0.00%|    >>> a = np.array([[1,2,3],[4,5,6]])\n",
      "  3233|         0|            0|            0|  0.00%|    >>> np.size(a)\n",
      "  3234|         0|            0|            0|  0.00%|    6\n",
      "  3235|         0|            0|            0|  0.00%|    >>> np.size(a,1)\n",
      "  3236|         0|            0|            0|  0.00%|    3\n",
      "  3237|         0|            0|            0|  0.00%|    >>> np.size(a,0)\n",
      "  3238|         0|            0|            0|  0.00%|    2\n",
      "  3239|         0|            0|            0|  0.00%|\n",
      "  3240|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3241|         0|            0|            0|  0.00%|    if axis is None:\n",
      "  3242|         0|            0|            0|  0.00%|        try:\n",
      "  3243|         0|            0|            0|  0.00%|            return a.size\n",
      "  3244|         0|            0|            0|  0.00%|        except AttributeError:\n",
      "  3245|         0|            0|            0|  0.00%|            return asarray(a).size\n",
      "  3246|         0|            0|            0|  0.00%|    else:\n",
      "  3247|         0|            0|            0|  0.00%|        try:\n",
      "  3248|         0|            0|            0|  0.00%|            return a.shape[axis]\n",
      "  3249|         0|            0|            0|  0.00%|        except AttributeError:\n",
      "  3250|         0|            0|            0|  0.00%|            return asarray(a).shape[axis]\n",
      "  3251|         0|            0|            0|  0.00%|\n",
      "  3252|         0|            0|            0|  0.00%|\n",
      "  3253|         0|            0|            0|  0.00%|def _around_dispatcher(a, decimals=None, out=None):\n",
      "  3254|         0|            0|            0|  0.00%|    return (a, out)\n",
      "  3255|         0|            0|            0|  0.00%|\n",
      "  3256|         0|            0|            0|  0.00%|\n",
      "  3257|         0|            0|            0|  0.00%|@array_function_dispatch(_around_dispatcher)\n",
      "  3258|         0|            0|            0|  0.00%|def around(a, decimals=0, out=None):\n",
      "  3259|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3260|         0|            0|            0|  0.00%|    Evenly round to the given number of decimals.\n",
      "  3261|         0|            0|            0|  0.00%|\n",
      "  3262|         0|            0|            0|  0.00%|    Parameters\n",
      "  3263|         0|            0|            0|  0.00%|    ----------\n",
      "  3264|         0|            0|            0|  0.00%|    a : array_like\n",
      "  3265|         0|            0|            0|  0.00%|        Input data.\n",
      "  3266|         0|            0|            0|  0.00%|    decimals : int, optional\n",
      "  3267|         0|            0|            0|  0.00%|        Number of decimal places to round to (default: 0).  If\n",
      "  3268|         0|            0|            0|  0.00%|        decimals is negative, it specifies the number of positions to\n",
      "  3269|         0|            0|            0|  0.00%|        the left of the decimal point.\n",
      "  3270|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  3271|         0|            0|            0|  0.00%|        Alternative output array in which to place the result. It must have\n",
      "  3272|         0|            0|            0|  0.00%|        the same shape as the expected output, but the type of the output\n",
      "  3273|         0|            0|            0|  0.00%|        values will be cast if necessary. See :ref:`ufuncs-output-type` for more\n",
      "  3274|         0|            0|            0|  0.00%|        details.\n",
      "  3275|         0|            0|            0|  0.00%|\n",
      "  3276|         0|            0|            0|  0.00%|    Returns\n",
      "  3277|         0|            0|            0|  0.00%|    -------\n",
      "  3278|         0|            0|            0|  0.00%|    rounded_array : ndarray\n",
      "  3279|         0|            0|            0|  0.00%|        An array of the same type as `a`, containing the rounded values.\n",
      "  3280|         0|            0|            0|  0.00%|        Unless `out` was specified, a new array is created.  A reference to\n",
      "  3281|         0|            0|            0|  0.00%|        the result is returned.\n",
      "  3282|         0|            0|            0|  0.00%|\n",
      "  3283|         0|            0|            0|  0.00%|        The real and imaginary parts of complex numbers are rounded\n",
      "  3284|         0|            0|            0|  0.00%|        separately.  The result of rounding a float is a float.\n",
      "  3285|         0|            0|            0|  0.00%|\n",
      "  3286|         0|            0|            0|  0.00%|    See Also\n",
      "  3287|         0|            0|            0|  0.00%|    --------\n",
      "  3288|         0|            0|            0|  0.00%|    ndarray.round : equivalent method\n",
      "  3289|         0|            0|            0|  0.00%|\n",
      "  3290|         0|            0|            0|  0.00%|    ceil, fix, floor, rint, trunc\n",
      "  3291|         0|            0|            0|  0.00%|\n",
      "  3292|         0|            0|            0|  0.00%|\n",
      "  3293|         0|            0|            0|  0.00%|    Notes\n",
      "  3294|         0|            0|            0|  0.00%|    -----\n",
      "  3295|         0|            0|            0|  0.00%|    For values exactly halfway between rounded decimal values, NumPy\n",
      "  3296|         0|            0|            0|  0.00%|    rounds to the nearest even value. Thus 1.5 and 2.5 round to 2.0,\n",
      "  3297|         0|            0|            0|  0.00%|    -0.5 and 0.5 round to 0.0, etc.\n",
      "  3298|         0|            0|            0|  0.00%|\n",
      "  3299|         0|            0|            0|  0.00%|    ``np.around`` uses a fast but sometimes inexact algorithm to round\n",
      "  3300|         0|            0|            0|  0.00%|    floating-point datatypes. For positive `decimals` it is equivalent to\n",
      "  3301|         0|            0|            0|  0.00%|    ``np.true_divide(np.rint(a * 10**decimals), 10**decimals)``, which has\n",
      "  3302|         0|            0|            0|  0.00%|    error due to the inexact representation of decimal fractions in the IEEE\n",
      "  3303|         0|            0|            0|  0.00%|    floating point standard [1]_ and errors introduced when scaling by powers\n",
      "  3304|         0|            0|            0|  0.00%|    of ten. For instance, note the extra \"1\" in the following:\n",
      "  3305|         0|            0|            0|  0.00%|\n",
      "  3306|         0|            0|            0|  0.00%|        >>> np.round(56294995342131.5, 3)\n",
      "  3307|         0|            0|            0|  0.00%|        56294995342131.51\n",
      "  3308|         0|            0|            0|  0.00%|\n",
      "  3309|         0|            0|            0|  0.00%|    If your goal is to print such values with a fixed number of decimals, it is\n",
      "  3310|         0|            0|            0|  0.00%|    preferable to use numpy's float printing routines to limit the number of\n",
      "  3311|         0|            0|            0|  0.00%|    printed decimals:\n",
      "  3312|         0|            0|            0|  0.00%|\n",
      "  3313|         0|            0|            0|  0.00%|        >>> np.format_float_positional(56294995342131.5, precision=3)\n",
      "  3314|         0|            0|            0|  0.00%|        '56294995342131.5'\n",
      "  3315|         0|            0|            0|  0.00%|\n",
      "  3316|         0|            0|            0|  0.00%|    The float printing routines use an accurate but much more computationally\n",
      "  3317|         0|            0|            0|  0.00%|    demanding algorithm to compute the number of digits after the decimal\n",
      "  3318|         0|            0|            0|  0.00%|    point.\n",
      "  3319|         0|            0|            0|  0.00%|\n",
      "  3320|         0|            0|            0|  0.00%|    Alternatively, Python's builtin `round` function uses a more accurate\n",
      "  3321|         0|            0|            0|  0.00%|    but slower algorithm for 64-bit floating point values:\n",
      "  3322|         0|            0|            0|  0.00%|\n",
      "  3323|         0|            0|            0|  0.00%|        >>> round(56294995342131.5, 3)\n",
      "  3324|         0|            0|            0|  0.00%|        56294995342131.5\n",
      "  3325|         0|            0|            0|  0.00%|        >>> np.round(16.055, 2), round(16.055, 2)  # equals 16.0549999999999997\n",
      "  3326|         0|            0|            0|  0.00%|        (16.06, 16.05)\n",
      "  3327|         0|            0|            0|  0.00%|\n",
      "  3328|         0|            0|            0|  0.00%|\n",
      "  3329|         0|            0|            0|  0.00%|    References\n",
      "  3330|         0|            0|            0|  0.00%|    ----------\n",
      "  3331|         0|            0|            0|  0.00%|    .. [1] \"Lecture Notes on the Status of IEEE 754\", William Kahan,\n",
      "  3332|         0|            0|            0|  0.00%|           https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF\n",
      "  3333|         0|            0|            0|  0.00%|\n",
      "  3334|         0|            0|            0|  0.00%|    Examples\n",
      "  3335|         0|            0|            0|  0.00%|    --------\n",
      "  3336|         0|            0|            0|  0.00%|    >>> np.around([0.37, 1.64])\n",
      "  3337|         0|            0|            0|  0.00%|    array([0., 2.])\n",
      "  3338|         0|            0|            0|  0.00%|    >>> np.around([0.37, 1.64], decimals=1)\n",
      "  3339|         0|            0|            0|  0.00%|    array([0.4, 1.6])\n",
      "  3340|         0|            0|            0|  0.00%|    >>> np.around([.5, 1.5, 2.5, 3.5, 4.5]) # rounds to nearest even value\n",
      "  3341|         0|            0|            0|  0.00%|    array([0., 2., 2., 4., 4.])\n",
      "  3342|         0|            0|            0|  0.00%|    >>> np.around([1,2,3,11], decimals=1) # ndarray of ints is returned\n",
      "  3343|         0|            0|            0|  0.00%|    array([ 1,  2,  3, 11])\n",
      "  3344|         0|            0|            0|  0.00%|    >>> np.around([1,2,3,11], decimals=-1)\n",
      "  3345|         0|            0|            0|  0.00%|    array([ 0,  0,  0, 10])\n",
      "  3346|         0|            0|            0|  0.00%|\n",
      "  3347|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3348|         0|            0|            0|  0.00%|    return _wrapfunc(a, 'round', decimals=decimals, out=out)\n",
      "  3349|         0|            0|            0|  0.00%|\n",
      "  3350|         0|            0|            0|  0.00%|\n",
      "  3351|         0|            0|            0|  0.00%|def _mean_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None, *,\n",
      "  3352|         0|            0|            0|  0.00%|                     where=None):\n",
      "  3353|         0|            0|            0|  0.00%|    return (a, where, out)\n",
      "  3354|         0|            0|            0|  0.00%|\n",
      "  3355|         0|            0|            0|  0.00%|\n",
      "  3356|         0|            0|            0|  0.00%|@array_function_dispatch(_mean_dispatcher)\n",
      "  3357|         0|            0|            0|  0.00%|def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,\n",
      "  3358|         0|            0|            0|  0.00%|         where=np._NoValue):\n",
      "  3359|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3360|         0|            0|            0|  0.00%|    Compute the arithmetic mean along the specified axis.\n",
      "  3361|         0|            0|            0|  0.00%|\n",
      "  3362|         0|            0|            0|  0.00%|    Returns the average of the array elements.  The average is taken over\n",
      "  3363|         0|            0|            0|  0.00%|    the flattened array by default, otherwise over the specified axis.\n",
      "  3364|         0|            0|            0|  0.00%|    `float64` intermediate and return values are used for integer inputs.\n",
      "  3365|         0|            0|            0|  0.00%|\n",
      "  3366|         0|            0|            0|  0.00%|    Parameters\n",
      "  3367|         0|            0|            0|  0.00%|    ----------\n",
      "  3368|         0|            0|            0|  0.00%|    a : array_like\n",
      "  3369|         0|            0|            0|  0.00%|        Array containing numbers whose mean is desired. If `a` is not an\n",
      "  3370|         0|            0|            0|  0.00%|        array, a conversion is attempted.\n",
      "  3371|         0|            0|            0|  0.00%|    axis : None or int or tuple of ints, optional\n",
      "  3372|         0|            0|            0|  0.00%|        Axis or axes along which the means are computed. The default is to\n",
      "  3373|         0|            0|            0|  0.00%|        compute the mean of the flattened array.\n",
      "  3374|         0|            0|            0|  0.00%|\n",
      "  3375|         0|            0|            0|  0.00%|        .. versionadded:: 1.7.0\n",
      "  3376|         0|            0|            0|  0.00%|\n",
      "  3377|         0|            0|            0|  0.00%|        If this is a tuple of ints, a mean is performed over multiple axes,\n",
      "  3378|         0|            0|            0|  0.00%|        instead of a single axis or all the axes as before.\n",
      "  3379|         0|            0|            0|  0.00%|    dtype : data-type, optional\n",
      "  3380|         0|            0|            0|  0.00%|        Type to use in computing the mean.  For integer inputs, the default\n",
      "  3381|         0|            0|            0|  0.00%|        is `float64`; for floating point inputs, it is the same as the\n",
      "  3382|         0|            0|            0|  0.00%|        input dtype.\n",
      "  3383|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  3384|         0|            0|            0|  0.00%|        Alternate output array in which to place the result.  The default\n",
      "  3385|         0|            0|            0|  0.00%|        is ``None``; if provided, it must have the same shape as the\n",
      "  3386|         0|            0|            0|  0.00%|        expected output, but the type will be cast if necessary.\n",
      "  3387|         0|            0|            0|  0.00%|        See :ref:`ufuncs-output-type` for more details.\n",
      "  3388|         0|            0|            0|  0.00%|\n",
      "  3389|         0|            0|            0|  0.00%|    keepdims : bool, optional\n",
      "  3390|         0|            0|            0|  0.00%|        If this is set to True, the axes which are reduced are left\n",
      "  3391|         0|            0|            0|  0.00%|        in the result as dimensions with size one. With this option,\n",
      "  3392|         0|            0|            0|  0.00%|        the result will broadcast correctly against the input array.\n",
      "  3393|         0|            0|            0|  0.00%|\n",
      "  3394|         0|            0|            0|  0.00%|        If the default value is passed, then `keepdims` will not be\n",
      "  3395|         0|            0|            0|  0.00%|        passed through to the `mean` method of sub-classes of\n",
      "  3396|         0|            0|            0|  0.00%|        `ndarray`, however any non-default value will be.  If the\n",
      "  3397|         0|            0|            0|  0.00%|        sub-class' method does not implement `keepdims` any\n",
      "  3398|         0|            0|            0|  0.00%|        exceptions will be raised.\n",
      "  3399|         0|            0|            0|  0.00%|\n",
      "  3400|         0|            0|            0|  0.00%|    where : array_like of bool, optional\n",
      "  3401|         0|            0|            0|  0.00%|        Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n",
      "  3402|         0|            0|            0|  0.00%|\n",
      "  3403|         0|            0|            0|  0.00%|        .. versionadded:: 1.20.0\n",
      "  3404|         0|            0|            0|  0.00%|\n",
      "  3405|         0|            0|            0|  0.00%|    Returns\n",
      "  3406|         0|            0|            0|  0.00%|    -------\n",
      "  3407|         0|            0|            0|  0.00%|    m : ndarray, see dtype parameter above\n",
      "  3408|         0|            0|            0|  0.00%|        If `out=None`, returns a new array containing the mean values,\n",
      "  3409|         0|            0|            0|  0.00%|        otherwise a reference to the output array is returned.\n",
      "  3410|         0|            0|            0|  0.00%|\n",
      "  3411|         0|            0|            0|  0.00%|    See Also\n",
      "  3412|         0|            0|            0|  0.00%|    --------\n",
      "  3413|         0|            0|            0|  0.00%|    average : Weighted average\n",
      "  3414|         0|            0|            0|  0.00%|    std, var, nanmean, nanstd, nanvar\n",
      "  3415|         0|            0|            0|  0.00%|\n",
      "  3416|         0|            0|            0|  0.00%|    Notes\n",
      "  3417|         0|            0|            0|  0.00%|    -----\n",
      "  3418|         0|            0|            0|  0.00%|    The arithmetic mean is the sum of the elements along the axis divided\n",
      "  3419|         0|            0|            0|  0.00%|    by the number of elements.\n",
      "  3420|         0|            0|            0|  0.00%|\n",
      "  3421|         0|            0|            0|  0.00%|    Note that for floating-point input, the mean is computed using the\n",
      "  3422|         0|            0|            0|  0.00%|    same precision the input has.  Depending on the input data, this can\n",
      "  3423|         0|            0|            0|  0.00%|    cause the results to be inaccurate, especially for `float32` (see\n",
      "  3424|         0|            0|            0|  0.00%|    example below).  Specifying a higher-precision accumulator using the\n",
      "  3425|         0|            0|            0|  0.00%|    `dtype` keyword can alleviate this issue.\n",
      "  3426|         0|            0|            0|  0.00%|\n",
      "  3427|         0|            0|            0|  0.00%|    By default, `float16` results are computed using `float32` intermediates\n",
      "  3428|         0|            0|            0|  0.00%|    for extra precision.\n",
      "  3429|         0|            0|            0|  0.00%|\n",
      "  3430|         0|            0|            0|  0.00%|    Examples\n",
      "  3431|         0|            0|            0|  0.00%|    --------\n",
      "  3432|         0|            0|            0|  0.00%|    >>> a = np.array([[1, 2], [3, 4]])\n",
      "  3433|         0|            0|            0|  0.00%|    >>> np.mean(a)\n",
      "  3434|         0|            0|            0|  0.00%|    2.5\n",
      "  3435|         0|            0|            0|  0.00%|    >>> np.mean(a, axis=0)\n",
      "  3436|         0|            0|            0|  0.00%|    array([2., 3.])\n",
      "  3437|         0|            0|            0|  0.00%|    >>> np.mean(a, axis=1)\n",
      "  3438|         0|            0|            0|  0.00%|    array([1.5, 3.5])\n",
      "  3439|         0|            0|            0|  0.00%|\n",
      "  3440|         0|            0|            0|  0.00%|    In single precision, `mean` can be inaccurate:\n",
      "  3441|         0|            0|            0|  0.00%|\n",
      "  3442|         0|            0|            0|  0.00%|    >>> a = np.zeros((2, 512*512), dtype=np.float32)\n",
      "  3443|         0|            0|            0|  0.00%|    >>> a[0, :] = 1.0\n",
      "  3444|         0|            0|            0|  0.00%|    >>> a[1, :] = 0.1\n",
      "  3445|         0|            0|            0|  0.00%|    >>> np.mean(a)\n",
      "  3446|         0|            0|            0|  0.00%|    0.54999924\n",
      "  3447|         0|            0|            0|  0.00%|\n",
      "  3448|         0|            0|            0|  0.00%|    Computing the mean in float64 is more accurate:\n",
      "  3449|         0|            0|            0|  0.00%|\n",
      "  3450|         0|            0|            0|  0.00%|    >>> np.mean(a, dtype=np.float64)\n",
      "  3451|         0|            0|            0|  0.00%|    0.55000000074505806 # may vary\n",
      "  3452|         0|            0|            0|  0.00%|\n",
      "  3453|         0|            0|            0|  0.00%|    Specifying a where argument:\n",
      "  3454|         0|            0|            0|  0.00%|    >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])\n",
      "  3455|         0|            0|            0|  0.00%|    >>> np.mean(a)\n",
      "  3456|         0|            0|            0|  0.00%|    12.0\n",
      "  3457|         0|            0|            0|  0.00%|    >>> np.mean(a, where=[[True], [False], [False]])\n",
      "  3458|         0|            0|            0|  0.00%|    9.0\n",
      "  3459|         0|            0|            0|  0.00%|\n",
      "  3460|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3461|         0|            0|            0|  0.00%|    kwargs = {}\n",
      "  3462|         0|            0|            0|  0.00%|    if keepdims is not np._NoValue:\n",
      "  3463|         0|            0|            0|  0.00%|        kwargs['keepdims'] = keepdims\n",
      "  3464|         0|            0|            0|  0.00%|    if where is not np._NoValue:\n",
      "  3465|         0|            0|            0|  0.00%|        kwargs['where'] = where\n",
      "  3466|         0|            0|            0|  0.00%|    if type(a) is not mu.ndarray:\n",
      "  3467|         0|            0|            0|  0.00%|        try:\n",
      "  3468|         0|            0|            0|  0.00%|            mean = a.mean\n",
      "  3469|         0|            0|            0|  0.00%|        except AttributeError:\n",
      "  3470|         0|            0|            0|  0.00%|            pass\n",
      "  3471|         0|            0|            0|  0.00%|        else:\n",
      "  3472|         0|            0|            0|  0.00%|            return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "  3473|         0|            0|            0|  0.00%|\n",
      "  3474|         0|            0|            0|  0.00%|    return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "  3475|         0|            0|            0|  0.00%|                          out=out, **kwargs)\n",
      "  3476|         0|            0|            0|  0.00%|\n",
      "  3477|         0|            0|            0|  0.00%|\n",
      "  3478|         0|            0|            0|  0.00%|def _std_dispatcher(a, axis=None, dtype=None, out=None, ddof=None,\n",
      "  3479|         0|            0|            0|  0.00%|                    keepdims=None, *, where=None):\n",
      "  3480|         0|            0|            0|  0.00%|    return (a, where, out)\n",
      "  3481|         0|            0|            0|  0.00%|\n",
      "  3482|         0|            0|            0|  0.00%|\n",
      "  3483|         0|            0|            0|  0.00%|@array_function_dispatch(_std_dispatcher)\n",
      "  3484|         0|            0|            0|  0.00%|def std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n",
      "  3485|         0|            0|            0|  0.00%|        where=np._NoValue):\n",
      "  3486|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3487|         0|            0|            0|  0.00%|    Compute the standard deviation along the specified axis.\n",
      "  3488|         0|            0|            0|  0.00%|\n",
      "  3489|         0|            0|            0|  0.00%|    Returns the standard deviation, a measure of the spread of a distribution,\n",
      "  3490|         0|            0|            0|  0.00%|    of the array elements. The standard deviation is computed for the\n",
      "  3491|         0|            0|            0|  0.00%|    flattened array by default, otherwise over the specified axis.\n",
      "  3492|         0|            0|            0|  0.00%|\n",
      "  3493|         0|            0|            0|  0.00%|    Parameters\n",
      "  3494|         0|            0|            0|  0.00%|    ----------\n",
      "  3495|         0|            0|            0|  0.00%|    a : array_like\n",
      "  3496|         0|            0|            0|  0.00%|        Calculate the standard deviation of these values.\n",
      "  3497|         0|            0|            0|  0.00%|    axis : None or int or tuple of ints, optional\n",
      "  3498|         0|            0|            0|  0.00%|        Axis or axes along which the standard deviation is computed. The\n",
      "  3499|         0|            0|            0|  0.00%|        default is to compute the standard deviation of the flattened array.\n",
      "  3500|         0|            0|            0|  0.00%|\n",
      "  3501|         0|            0|            0|  0.00%|        .. versionadded:: 1.7.0\n",
      "  3502|         0|            0|            0|  0.00%|\n",
      "  3503|         0|            0|            0|  0.00%|        If this is a tuple of ints, a standard deviation is performed over\n",
      "  3504|         0|            0|            0|  0.00%|        multiple axes, instead of a single axis or all the axes as before.\n",
      "  3505|         0|            0|            0|  0.00%|    dtype : dtype, optional\n",
      "  3506|         0|            0|            0|  0.00%|        Type to use in computing the standard deviation. For arrays of\n",
      "  3507|         0|            0|            0|  0.00%|        integer type the default is float64, for arrays of float types it is\n",
      "  3508|         0|            0|            0|  0.00%|        the same as the array type.\n",
      "  3509|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  3510|         0|            0|            0|  0.00%|        Alternative output array in which to place the result. It must have\n",
      "  3511|         0|            0|            0|  0.00%|        the same shape as the expected output but the type (of the calculated\n",
      "  3512|         0|            0|            0|  0.00%|        values) will be cast if necessary.\n",
      "  3513|         0|            0|            0|  0.00%|    ddof : int, optional\n",
      "  3514|         0|            0|            0|  0.00%|        Means Delta Degrees of Freedom.  The divisor used in calculations\n",
      "  3515|         0|            0|            0|  0.00%|        is ``N - ddof``, where ``N`` represents the number of elements.\n",
      "  3516|         0|            0|            0|  0.00%|        By default `ddof` is zero.\n",
      "  3517|         0|            0|            0|  0.00%|    keepdims : bool, optional\n",
      "  3518|         0|            0|            0|  0.00%|        If this is set to True, the axes which are reduced are left\n",
      "  3519|         0|            0|            0|  0.00%|        in the result as dimensions with size one. With this option,\n",
      "  3520|         0|            0|            0|  0.00%|        the result will broadcast correctly against the input array.\n",
      "  3521|         0|            0|            0|  0.00%|\n",
      "  3522|         0|            0|            0|  0.00%|        If the default value is passed, then `keepdims` will not be\n",
      "  3523|         0|            0|            0|  0.00%|        passed through to the `std` method of sub-classes of\n",
      "  3524|         0|            0|            0|  0.00%|        `ndarray`, however any non-default value will be.  If the\n",
      "  3525|         0|            0|            0|  0.00%|        sub-class' method does not implement `keepdims` any\n",
      "  3526|         0|            0|            0|  0.00%|        exceptions will be raised.\n",
      "  3527|         0|            0|            0|  0.00%|\n",
      "  3528|         0|            0|            0|  0.00%|    where : array_like of bool, optional\n",
      "  3529|         0|            0|            0|  0.00%|        Elements to include in the standard deviation.\n",
      "  3530|         0|            0|            0|  0.00%|        See `~numpy.ufunc.reduce` for details.\n",
      "  3531|         0|            0|            0|  0.00%|\n",
      "  3532|         0|            0|            0|  0.00%|        .. versionadded:: 1.20.0\n",
      "  3533|         0|            0|            0|  0.00%|\n",
      "  3534|         0|            0|            0|  0.00%|    Returns\n",
      "  3535|         0|            0|            0|  0.00%|    -------\n",
      "  3536|         0|            0|            0|  0.00%|    standard_deviation : ndarray, see dtype parameter above.\n",
      "  3537|         0|            0|            0|  0.00%|        If `out` is None, return a new array containing the standard deviation,\n",
      "  3538|         0|            0|            0|  0.00%|        otherwise return a reference to the output array.\n",
      "  3539|         0|            0|            0|  0.00%|\n",
      "  3540|         0|            0|            0|  0.00%|    See Also\n",
      "  3541|         0|            0|            0|  0.00%|    --------\n",
      "  3542|         0|            0|            0|  0.00%|    var, mean, nanmean, nanstd, nanvar\n",
      "  3543|         0|            0|            0|  0.00%|    :ref:`ufuncs-output-type`\n",
      "  3544|         0|            0|            0|  0.00%|\n",
      "  3545|         0|            0|            0|  0.00%|    Notes\n",
      "  3546|         0|            0|            0|  0.00%|    -----\n",
      "  3547|         0|            0|            0|  0.00%|    The standard deviation is the square root of the average of the squared\n",
      "  3548|         0|            0|            0|  0.00%|    deviations from the mean, i.e., ``std = sqrt(mean(x))``, where\n",
      "  3549|         0|            0|            0|  0.00%|    ``x = abs(a - a.mean())**2``.\n",
      "  3550|         0|            0|            0|  0.00%|\n",
      "  3551|         0|            0|            0|  0.00%|    The average squared deviation is typically calculated as ``x.sum() / N``,\n",
      "  3552|         0|            0|            0|  0.00%|    where ``N = len(x)``. If, however, `ddof` is specified, the divisor\n",
      "  3553|         0|            0|            0|  0.00%|    ``N - ddof`` is used instead. In standard statistical practice, ``ddof=1``\n",
      "  3554|         0|            0|            0|  0.00%|    provides an unbiased estimator of the variance of the infinite population.\n",
      "  3555|         0|            0|            0|  0.00%|    ``ddof=0`` provides a maximum likelihood estimate of the variance for\n",
      "  3556|         0|            0|            0|  0.00%|    normally distributed variables. The standard deviation computed in this\n",
      "  3557|         0|            0|            0|  0.00%|    function is the square root of the estimated variance, so even with\n",
      "  3558|         0|            0|            0|  0.00%|    ``ddof=1``, it will not be an unbiased estimate of the standard deviation\n",
      "  3559|         0|            0|            0|  0.00%|    per se.\n",
      "  3560|         0|            0|            0|  0.00%|\n",
      "  3561|         0|            0|            0|  0.00%|    Note that, for complex numbers, `std` takes the absolute\n",
      "  3562|         0|            0|            0|  0.00%|    value before squaring, so that the result is always real and nonnegative.\n",
      "  3563|         0|            0|            0|  0.00%|\n",
      "  3564|         0|            0|            0|  0.00%|    For floating-point input, the *std* is computed using the same\n",
      "  3565|         0|            0|            0|  0.00%|    precision the input has. Depending on the input data, this can cause\n",
      "  3566|         0|            0|            0|  0.00%|    the results to be inaccurate, especially for float32 (see example below).\n",
      "  3567|         0|            0|            0|  0.00%|    Specifying a higher-accuracy accumulator using the `dtype` keyword can\n",
      "  3568|         0|            0|            0|  0.00%|    alleviate this issue.\n",
      "  3569|         0|            0|            0|  0.00%|\n",
      "  3570|         0|            0|            0|  0.00%|    Examples\n",
      "  3571|         0|            0|            0|  0.00%|    --------\n",
      "  3572|         0|            0|            0|  0.00%|    >>> a = np.array([[1, 2], [3, 4]])\n",
      "  3573|         0|            0|            0|  0.00%|    >>> np.std(a)\n",
      "  3574|         0|            0|            0|  0.00%|    1.1180339887498949 # may vary\n",
      "  3575|         0|            0|            0|  0.00%|    >>> np.std(a, axis=0)\n",
      "  3576|         0|            0|            0|  0.00%|    array([1.,  1.])\n",
      "  3577|         0|            0|            0|  0.00%|    >>> np.std(a, axis=1)\n",
      "  3578|         0|            0|            0|  0.00%|    array([0.5,  0.5])\n",
      "  3579|         0|            0|            0|  0.00%|\n",
      "  3580|         0|            0|            0|  0.00%|    In single precision, std() can be inaccurate:\n",
      "  3581|         0|            0|            0|  0.00%|\n",
      "  3582|         0|            0|            0|  0.00%|    >>> a = np.zeros((2, 512*512), dtype=np.float32)\n",
      "  3583|         0|            0|            0|  0.00%|    >>> a[0, :] = 1.0\n",
      "  3584|         0|            0|            0|  0.00%|    >>> a[1, :] = 0.1\n",
      "  3585|         0|            0|            0|  0.00%|    >>> np.std(a)\n",
      "  3586|         0|            0|            0|  0.00%|    0.45000005\n",
      "  3587|         0|            0|            0|  0.00%|\n",
      "  3588|         0|            0|            0|  0.00%|    Computing the standard deviation in float64 is more accurate:\n",
      "  3589|         0|            0|            0|  0.00%|\n",
      "  3590|         0|            0|            0|  0.00%|    >>> np.std(a, dtype=np.float64)\n",
      "  3591|         0|            0|            0|  0.00%|    0.44999999925494177 # may vary\n",
      "  3592|         0|            0|            0|  0.00%|\n",
      "  3593|         0|            0|            0|  0.00%|    Specifying a where argument:\n",
      "  3594|         0|            0|            0|  0.00%|\n",
      "  3595|         0|            0|            0|  0.00%|    >>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])\n",
      "  3596|         0|            0|            0|  0.00%|    >>> np.std(a)\n",
      "  3597|         0|            0|            0|  0.00%|    2.614064523559687 # may vary\n",
      "  3598|         0|            0|            0|  0.00%|    >>> np.std(a, where=[[True], [True], [False]])\n",
      "  3599|         0|            0|            0|  0.00%|    2.0\n",
      "  3600|         0|            0|            0|  0.00%|\n",
      "  3601|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3602|         0|            0|            0|  0.00%|    kwargs = {}\n",
      "  3603|         0|            0|            0|  0.00%|    if keepdims is not np._NoValue:\n",
      "  3604|         0|            0|            0|  0.00%|        kwargs['keepdims'] = keepdims\n",
      "  3605|         0|            0|            0|  0.00%|    if where is not np._NoValue:\n",
      "  3606|         0|            0|            0|  0.00%|        kwargs['where'] = where\n",
      "  3607|         0|            0|            0|  0.00%|    if type(a) is not mu.ndarray:\n",
      "  3608|         0|            0|            0|  0.00%|        try:\n",
      "  3609|         0|            0|            0|  0.00%|            std = a.std\n",
      "  3610|         0|            0|            0|  0.00%|        except AttributeError:\n",
      "  3611|         0|            0|            0|  0.00%|            pass\n",
      "  3612|         0|            0|            0|  0.00%|        else:\n",
      "  3613|         0|            0|            0|  0.00%|            return std(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n",
      "  3614|         0|            0|            0|  0.00%|\n",
      "  3615|         0|            0|            0|  0.00%|    return _methods._std(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "  3616|         0|            0|            0|  0.00%|                         **kwargs)\n",
      "  3617|         0|            0|            0|  0.00%|\n",
      "  3618|         0|            0|            0|  0.00%|\n",
      "  3619|       780|   0.00137711|  1.76552e-06|  0.00%|def _var_dispatcher(a, axis=None, dtype=None, out=None, ddof=None,\n",
      "  3620|         0|            0|            0|  0.00%|                    keepdims=None, *, where=None):\n",
      "  3621|       780|   0.00170779|  2.18948e-06|  0.00%|    return (a, where, out)\n",
      "  3622|         0|            0|            0|  0.00%|\n",
      "  3623|         0|            0|            0|  0.00%|\n",
      "  3624|       780|   0.00201201|  2.57951e-06|  0.00%|@array_function_dispatch(_var_dispatcher)\n",
      "  3625|         0|            0|            0|  0.00%|def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n",
      "  3626|         0|            0|            0|  0.00%|        where=np._NoValue):\n",
      "  3627|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3628|         0|            0|            0|  0.00%|    Compute the variance along the specified axis.\n",
      "  3629|         0|            0|            0|  0.00%|\n",
      "  3630|         0|            0|            0|  0.00%|    Returns the variance of the array elements, a measure of the spread of a\n",
      "  3631|         0|            0|            0|  0.00%|    distribution.  The variance is computed for the flattened array by\n",
      "  3632|         0|            0|            0|  0.00%|    default, otherwise over the specified axis.\n",
      "  3633|         0|            0|            0|  0.00%|\n",
      "  3634|         0|            0|            0|  0.00%|    Parameters\n",
      "  3635|         0|            0|            0|  0.00%|    ----------\n",
      "  3636|         0|            0|            0|  0.00%|    a : array_like\n",
      "  3637|         0|            0|            0|  0.00%|        Array containing numbers whose variance is desired.  If `a` is not an\n",
      "  3638|         0|            0|            0|  0.00%|        array, a conversion is attempted.\n",
      "  3639|         0|            0|            0|  0.00%|    axis : None or int or tuple of ints, optional\n",
      "  3640|         0|            0|            0|  0.00%|        Axis or axes along which the variance is computed.  The default is to\n",
      "  3641|         0|            0|            0|  0.00%|        compute the variance of the flattened array.\n",
      "  3642|         0|            0|            0|  0.00%|\n",
      "  3643|         0|            0|            0|  0.00%|        .. versionadded:: 1.7.0\n",
      "  3644|         0|            0|            0|  0.00%|\n",
      "  3645|         0|            0|            0|  0.00%|        If this is a tuple of ints, a variance is performed over multiple axes,\n",
      "  3646|         0|            0|            0|  0.00%|        instead of a single axis or all the axes as before.\n",
      "  3647|         0|            0|            0|  0.00%|    dtype : data-type, optional\n",
      "  3648|         0|            0|            0|  0.00%|        Type to use in computing the variance.  For arrays of integer type\n",
      "  3649|         0|            0|            0|  0.00%|        the default is `float64`; for arrays of float types it is the same as\n",
      "  3650|         0|            0|            0|  0.00%|        the array type.\n",
      "  3651|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "  3652|         0|            0|            0|  0.00%|        Alternate output array in which to place the result.  It must have\n",
      "  3653|         0|            0|            0|  0.00%|        the same shape as the expected output, but the type is cast if\n",
      "  3654|         0|            0|            0|  0.00%|        necessary.\n",
      "  3655|         0|            0|            0|  0.00%|    ddof : int, optional\n",
      "  3656|         0|            0|            0|  0.00%|        \"Delta Degrees of Freedom\": the divisor used in the calculation is\n",
      "  3657|         0|            0|            0|  0.00%|        ``N - ddof``, where ``N`` represents the number of elements. By\n",
      "  3658|         0|            0|            0|  0.00%|        default `ddof` is zero.\n",
      "  3659|         0|            0|            0|  0.00%|    keepdims : bool, optional\n",
      "  3660|         0|            0|            0|  0.00%|        If this is set to True, the axes which are reduced are left\n",
      "  3661|         0|            0|            0|  0.00%|        in the result as dimensions with size one. With this option,\n",
      "  3662|         0|            0|            0|  0.00%|        the result will broadcast correctly against the input array.\n",
      "  3663|         0|            0|            0|  0.00%|\n",
      "  3664|         0|            0|            0|  0.00%|        If the default value is passed, then `keepdims` will not be\n",
      "  3665|         0|            0|            0|  0.00%|        passed through to the `var` method of sub-classes of\n",
      "  3666|         0|            0|            0|  0.00%|        `ndarray`, however any non-default value will be.  If the\n",
      "  3667|         0|            0|            0|  0.00%|        sub-class' method does not implement `keepdims` any\n",
      "  3668|         0|            0|            0|  0.00%|        exceptions will be raised.\n",
      "  3669|         0|            0|            0|  0.00%|\n",
      "  3670|         0|            0|            0|  0.00%|    where : array_like of bool, optional\n",
      "  3671|         0|            0|            0|  0.00%|        Elements to include in the variance. See `~numpy.ufunc.reduce` for\n",
      "  3672|         0|            0|            0|  0.00%|        details.\n",
      "  3673|         0|            0|            0|  0.00%|\n",
      "  3674|         0|            0|            0|  0.00%|        .. versionadded:: 1.20.0\n",
      "  3675|         0|            0|            0|  0.00%|\n",
      "  3676|         0|            0|            0|  0.00%|    Returns\n",
      "  3677|         0|            0|            0|  0.00%|    -------\n",
      "  3678|         0|            0|            0|  0.00%|    variance : ndarray, see dtype parameter above\n",
      "  3679|         0|            0|            0|  0.00%|        If ``out=None``, returns a new array containing the variance;\n",
      "  3680|         0|            0|            0|  0.00%|        otherwise, a reference to the output array is returned.\n",
      "  3681|         0|            0|            0|  0.00%|\n",
      "  3682|         0|            0|            0|  0.00%|    See Also\n",
      "  3683|         0|            0|            0|  0.00%|    --------\n",
      "  3684|         0|            0|            0|  0.00%|    std, mean, nanmean, nanstd, nanvar\n",
      "  3685|         0|            0|            0|  0.00%|    :ref:`ufuncs-output-type`\n",
      "  3686|         0|            0|            0|  0.00%|\n",
      "  3687|         0|            0|            0|  0.00%|    Notes\n",
      "  3688|         0|            0|            0|  0.00%|    -----\n",
      "  3689|         0|            0|            0|  0.00%|    The variance is the average of the squared deviations from the mean,\n",
      "  3690|         0|            0|            0|  0.00%|    i.e.,  ``var = mean(x)``, where ``x = abs(a - a.mean())**2``.\n",
      "  3691|         0|            0|            0|  0.00%|\n",
      "  3692|         0|            0|            0|  0.00%|    The mean is typically calculated as ``x.sum() / N``, where ``N = len(x)``.\n",
      "  3693|         0|            0|            0|  0.00%|    If, however, `ddof` is specified, the divisor ``N - ddof`` is used\n",
      "  3694|         0|            0|            0|  0.00%|    instead.  In standard statistical practice, ``ddof=1`` provides an\n",
      "  3695|         0|            0|            0|  0.00%|    unbiased estimator of the variance of a hypothetical infinite population.\n",
      "  3696|         0|            0|            0|  0.00%|    ``ddof=0`` provides a maximum likelihood estimate of the variance for\n",
      "  3697|         0|            0|            0|  0.00%|    normally distributed variables.\n",
      "  3698|         0|            0|            0|  0.00%|\n",
      "  3699|         0|            0|            0|  0.00%|    Note that for complex numbers, the absolute value is taken before\n",
      "  3700|         0|            0|            0|  0.00%|    squaring, so that the result is always real and nonnegative.\n",
      "  3701|         0|            0|            0|  0.00%|\n",
      "  3702|         0|            0|            0|  0.00%|    For floating-point input, the variance is computed using the same\n",
      "  3703|         0|            0|            0|  0.00%|    precision the input has.  Depending on the input data, this can cause\n",
      "  3704|         0|            0|            0|  0.00%|    the results to be inaccurate, especially for `float32` (see example\n",
      "  3705|         0|            0|            0|  0.00%|    below).  Specifying a higher-accuracy accumulator using the ``dtype``\n",
      "  3706|         0|            0|            0|  0.00%|    keyword can alleviate this issue.\n",
      "  3707|         0|            0|            0|  0.00%|\n",
      "  3708|         0|            0|            0|  0.00%|    Examples\n",
      "  3709|         0|            0|            0|  0.00%|    --------\n",
      "  3710|         0|            0|            0|  0.00%|    >>> a = np.array([[1, 2], [3, 4]])\n",
      "  3711|         0|            0|            0|  0.00%|    >>> np.var(a)\n",
      "  3712|         0|            0|            0|  0.00%|    1.25\n",
      "  3713|         0|            0|            0|  0.00%|    >>> np.var(a, axis=0)\n",
      "  3714|         0|            0|            0|  0.00%|    array([1.,  1.])\n",
      "  3715|         0|            0|            0|  0.00%|    >>> np.var(a, axis=1)\n",
      "  3716|         0|            0|            0|  0.00%|    array([0.25,  0.25])\n",
      "  3717|         0|            0|            0|  0.00%|\n",
      "  3718|         0|            0|            0|  0.00%|    In single precision, var() can be inaccurate:\n",
      "  3719|         0|            0|            0|  0.00%|\n",
      "  3720|         0|            0|            0|  0.00%|    >>> a = np.zeros((2, 512*512), dtype=np.float32)\n",
      "  3721|         0|            0|            0|  0.00%|    >>> a[0, :] = 1.0\n",
      "  3722|         0|            0|            0|  0.00%|    >>> a[1, :] = 0.1\n",
      "  3723|         0|            0|            0|  0.00%|    >>> np.var(a)\n",
      "  3724|         0|            0|            0|  0.00%|    0.20250003\n",
      "  3725|         0|            0|            0|  0.00%|\n",
      "  3726|         0|            0|            0|  0.00%|    Computing the variance in float64 is more accurate:\n",
      "  3727|         0|            0|            0|  0.00%|\n",
      "  3728|         0|            0|            0|  0.00%|    >>> np.var(a, dtype=np.float64)\n",
      "  3729|         0|            0|            0|  0.00%|    0.20249999932944759 # may vary\n",
      "  3730|         0|            0|            0|  0.00%|    >>> ((1-0.55)**2 + (0.1-0.55)**2)/2\n",
      "  3731|         0|            0|            0|  0.00%|    0.2025\n",
      "  3732|         0|            0|            0|  0.00%|\n",
      "  3733|         0|            0|            0|  0.00%|    Specifying a where argument:\n",
      "  3734|         0|            0|            0|  0.00%|\n",
      "  3735|         0|            0|            0|  0.00%|    >>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])\n",
      "  3736|         0|            0|            0|  0.00%|    >>> np.var(a)\n",
      "  3737|         0|            0|            0|  0.00%|    6.833333333333333 # may vary\n",
      "  3738|         0|            0|            0|  0.00%|    >>> np.var(a, where=[[True], [True], [False]])\n",
      "  3739|         0|            0|            0|  0.00%|    4.0\n",
      "  3740|         0|            0|            0|  0.00%|\n",
      "  3741|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3742|       780|    0.0019176|  2.45846e-06|  0.00%|    kwargs = {}\n",
      "  3743|       780|   0.00200057|  2.56483e-06|  0.00%|    if keepdims is not np._NoValue:\n",
      "  3744|         0|            0|            0|  0.00%|        kwargs['keepdims'] = keepdims\n",
      "  3745|       780|   0.00161505|  2.07057e-06|  0.00%|    if where is not np._NoValue:\n",
      "  3746|         0|            0|            0|  0.00%|        kwargs['where'] = where\n",
      "  3747|         0|            0|            0|  0.00%|\n",
      "  3748|       780|   0.00218225|  2.79775e-06|  0.00%|    if type(a) is not mu.ndarray:\n",
      "  3749|         0|            0|            0|  0.00%|        try:\n",
      "  3750|         0|            0|            0|  0.00%|            var = a.var\n",
      "  3751|         0|            0|            0|  0.00%|\n",
      "  3752|         0|            0|            0|  0.00%|        except AttributeError:\n",
      "  3753|         0|            0|            0|  0.00%|            pass\n",
      "  3754|         0|            0|            0|  0.00%|        else:\n",
      "  3755|         0|            0|            0|  0.00%|            return var(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n",
      "  3756|         0|            0|            0|  0.00%|\n",
      "  3757|      1560|    0.0109107|  6.99407e-06|  0.00%|    return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "(call)|       780|     0.111461|  0.000142899|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/_methods.py:195 _var\n",
      "  3758|       780|   0.00161171|  2.06629e-06|  0.00%|                         **kwargs)\n",
      "  3759|         0|            0|            0|  0.00%|\n",
      "  3760|         0|            0|            0|  0.00%|\n",
      "  3761|         0|            0|            0|  0.00%|# Aliases of other functions. These have their own definitions only so that\n",
      "  3762|         0|            0|            0|  0.00%|# they can have unique docstrings.\n",
      "  3763|         0|            0|            0|  0.00%|\n",
      "  3764|         0|            0|            0|  0.00%|@array_function_dispatch(_around_dispatcher)\n",
      "  3765|         0|            0|            0|  0.00%|def round_(a, decimals=0, out=None):\n",
      "  3766|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3767|         0|            0|            0|  0.00%|    Round an array to the given number of decimals.\n",
      "  3768|         0|            0|            0|  0.00%|\n",
      "  3769|         0|            0|            0|  0.00%|    See Also\n",
      "  3770|         0|            0|            0|  0.00%|    --------\n",
      "  3771|         0|            0|            0|  0.00%|    around : equivalent function; see for details.\n",
      "  3772|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3773|         0|            0|            0|  0.00%|    return around(a, decimals=decimals, out=out)\n",
      "  3774|         0|            0|            0|  0.00%|\n",
      "  3775|         0|            0|            0|  0.00%|\n",
      "  3776|         0|            0|            0|  0.00%|@array_function_dispatch(_prod_dispatcher, verify=False)\n",
      "  3777|         0|            0|            0|  0.00%|def product(*args, **kwargs):\n",
      "  3778|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3779|         0|            0|            0|  0.00%|    Return the product of array elements over a given axis.\n",
      "  3780|         0|            0|            0|  0.00%|\n",
      "  3781|         0|            0|            0|  0.00%|    See Also\n",
      "  3782|         0|            0|            0|  0.00%|    --------\n",
      "  3783|         0|            0|            0|  0.00%|    prod : equivalent function; see for details.\n",
      "  3784|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3785|         0|            0|            0|  0.00%|    return prod(*args, **kwargs)\n",
      "  3786|         0|            0|            0|  0.00%|\n",
      "  3787|         0|            0|            0|  0.00%|\n",
      "  3788|         0|            0|            0|  0.00%|@array_function_dispatch(_cumprod_dispatcher, verify=False)\n",
      "  3789|         0|            0|            0|  0.00%|def cumproduct(*args, **kwargs):\n",
      "  3790|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3791|         0|            0|            0|  0.00%|    Return the cumulative product over the given axis.\n",
      "  3792|         0|            0|            0|  0.00%|\n",
      "  3793|         0|            0|            0|  0.00%|    See Also\n",
      "  3794|         0|            0|            0|  0.00%|    --------\n",
      "  3795|         0|            0|            0|  0.00%|    cumprod : equivalent function; see for details.\n",
      "  3796|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3797|         0|            0|            0|  0.00%|    return cumprod(*args, **kwargs)\n",
      "  3798|         0|            0|            0|  0.00%|\n",
      "  3799|         0|            0|            0|  0.00%|\n",
      "  3800|         0|            0|            0|  0.00%|@array_function_dispatch(_any_dispatcher, verify=False)\n",
      "  3801|         0|            0|            0|  0.00%|def sometrue(*args, **kwargs):\n",
      "  3802|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3803|         0|            0|            0|  0.00%|    Check whether some values are true.\n",
      "  3804|         0|            0|            0|  0.00%|\n",
      "  3805|         0|            0|            0|  0.00%|    Refer to `any` for full documentation.\n",
      "  3806|         0|            0|            0|  0.00%|\n",
      "  3807|         0|            0|            0|  0.00%|    See Also\n",
      "  3808|         0|            0|            0|  0.00%|    --------\n",
      "  3809|         0|            0|            0|  0.00%|    any : equivalent function; see for details.\n",
      "  3810|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3811|         0|            0|            0|  0.00%|    return any(*args, **kwargs)\n",
      "  3812|         0|            0|            0|  0.00%|\n",
      "  3813|         0|            0|            0|  0.00%|\n",
      "  3814|         0|            0|            0|  0.00%|@array_function_dispatch(_all_dispatcher, verify=False)\n",
      "  3815|         0|            0|            0|  0.00%|def alltrue(*args, **kwargs):\n",
      "  3816|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3817|         0|            0|            0|  0.00%|    Check if all elements of input array are true.\n",
      "  3818|         0|            0|            0|  0.00%|\n",
      "  3819|         0|            0|            0|  0.00%|    See Also\n",
      "  3820|         0|            0|            0|  0.00%|    --------\n",
      "  3821|         0|            0|            0|  0.00%|    numpy.all : Equivalent function; see for details.\n",
      "  3822|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3823|         0|            0|            0|  0.00%|    return all(*args, **kwargs)\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/__init__.py\n",
      "File duration: 5.61016s (0.90%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|\"\"\"\n",
      "     2|         0|            0|            0|  0.00%|``torch.autograd`` provides classes and functions implementing automatic\n",
      "     3|         0|            0|            0|  0.00%|differentiation of arbitrary scalar valued functions. It requires minimal\n",
      "     4|         0|            0|            0|  0.00%|changes to the existing code - you only need to declare :class:`Tensor` s\n",
      "     5|         0|            0|            0|  0.00%|for which gradients should be computed with the ``requires_grad=True`` keyword.\n",
      "     6|         0|            0|            0|  0.00%|As of now, we only support autograd for floating point :class:`Tensor` types (\n",
      "     7|         0|            0|            0|  0.00%|half, float, double and bfloat16) and complex :class:`Tensor` types (cfloat, cdouble).\n",
      "     8|         0|            0|            0|  0.00%|\"\"\"\n",
      "     9|         0|            0|            0|  0.00%|import torch\n",
      "    10|         0|            0|            0|  0.00%|import warnings\n",
      "    11|         0|            0|            0|  0.00%|\n",
      "    12|         0|            0|            0|  0.00%|from torch.types import _TensorOrTensors\n",
      "    13|         0|            0|            0|  0.00%|from typing import Any, Callable, List, Optional, Sequence, Tuple, Union, cast\n",
      "    14|         0|            0|            0|  0.00%|\n",
      "    15|         0|            0|            0|  0.00%|from .variable import Variable\n",
      "    16|         0|            0|            0|  0.00%|from .function import Function, NestedIOFunction\n",
      "    17|         0|            0|            0|  0.00%|from .gradcheck import gradcheck, gradgradcheck\n",
      "    18|         0|            0|            0|  0.00%|from .grad_mode import no_grad, enable_grad, set_grad_enabled, inference_mode\n",
      "    19|         0|            0|            0|  0.00%|from .anomaly_mode import detect_anomaly, set_detect_anomaly\n",
      "    20|         0|            0|            0|  0.00%|from ..overrides import has_torch_function, handle_torch_function, is_tensor_like\n",
      "    21|         0|            0|            0|  0.00%|from . import functional\n",
      "    22|         0|            0|            0|  0.00%|from . import forward_ad\n",
      "    23|         0|            0|            0|  0.00%|from . import graph\n",
      "    24|         0|            0|            0|  0.00%|from .. import _vmap_internals\n",
      "    25|         0|            0|            0|  0.00%|\n",
      "    26|         0|            0|            0|  0.00%|__all__ = ['Variable', 'Function', 'backward', 'grad_mode']\n",
      "    27|         0|            0|            0|  0.00%|\n",
      "    28|         0|            0|            0|  0.00%|_OptionalTensor = Optional[torch.Tensor]\n",
      "    29|         0|            0|            0|  0.00%|\n",
      "    30|      6240|    0.0150919|  2.41857e-06|  0.00%|def _make_grads(outputs: Sequence[torch.Tensor], grads: Sequence[_OptionalTensor],\n",
      "    31|         0|            0|            0|  0.00%|                is_grads_batched: bool) -> Tuple[_OptionalTensor, ...]:\n",
      "    32|      6240|    0.0152929|  2.45078e-06|  0.00%|    new_grads: List[_OptionalTensor] = []\n",
      "    33|     12480|    0.0362298|  2.90303e-06|  0.01%|    for out, grad in zip(outputs, grads):\n",
      "    34|      6240|    0.0155458|  2.49132e-06|  0.00%|        if isinstance(grad, torch.Tensor):\n",
      "    35|         0|            0|            0|  0.00%|            grad_shape = grad.shape if not is_grads_batched else grad.shape[1:]\n",
      "    36|         0|            0|            0|  0.00%|            if not out.shape == grad_shape:\n",
      "    37|         0|            0|            0|  0.00%|                if is_grads_batched:\n",
      "    38|         0|            0|            0|  0.00%|                    raise RuntimeError(\"If `is_grads_batched=True`, we interpret the first \"\n",
      "    39|         0|            0|            0|  0.00%|                                       \"dimension of each grad_output as the batch dimension. \"\n",
      "    40|         0|            0|            0|  0.00%|                                       \"The sizes of the remaining dimensions are expected to match \"\n",
      "    41|         0|            0|            0|  0.00%|                                       \"the shape of corresponding output, but a mismatch \"\n",
      "    42|         0|            0|            0|  0.00%|                                       \"was detected: grad_output[\"\n",
      "    43|         0|            0|            0|  0.00%|                                       + str(grads.index(grad)) + \"] has a shape of \"\n",
      "    44|         0|            0|            0|  0.00%|                                       + str(grad.shape) + \" and output[\"\n",
      "    45|         0|            0|            0|  0.00%|                                       + str(outputs.index(out)) + \"] has a shape of \"\n",
      "    46|         0|            0|            0|  0.00%|                                       + str(out.shape) + \". \"\n",
      "    47|         0|            0|            0|  0.00%|                                       \"If you only want some tensors in `grad_output` to be considered \"\n",
      "    48|         0|            0|            0|  0.00%|                                       \"batched, consider using vmap.\")\n",
      "    49|         0|            0|            0|  0.00%|                else:\n",
      "    50|         0|            0|            0|  0.00%|                    raise RuntimeError(\"Mismatch in shape: grad_output[\"\n",
      "    51|         0|            0|            0|  0.00%|                                       + str(grads.index(grad)) + \"] has a shape of \"\n",
      "    52|         0|            0|            0|  0.00%|                                       + str(grad.shape) + \" and output[\"\n",
      "    53|         0|            0|            0|  0.00%|                                       + str(outputs.index(out)) + \"] has a shape of \"\n",
      "    54|         0|            0|            0|  0.00%|                                       + str(out.shape) + \".\")\n",
      "    55|         0|            0|            0|  0.00%|            if out.dtype.is_complex != grad.dtype.is_complex:\n",
      "    56|         0|            0|            0|  0.00%|                raise RuntimeError(\"For complex Tensors, both grad_output and output\"\n",
      "    57|         0|            0|            0|  0.00%|                                   \" are required to have the same dtype.\"\n",
      "    58|         0|            0|            0|  0.00%|                                   \" Mismatch in dtype: grad_output[\"\n",
      "    59|         0|            0|            0|  0.00%|                                   + str(grads.index(grad)) + \"] has a dtype of \"\n",
      "    60|         0|            0|            0|  0.00%|                                   + str(grad.dtype) + \" and output[\"\n",
      "    61|         0|            0|            0|  0.00%|                                   + str(outputs.index(out)) + \"] has a dtype of \"\n",
      "    62|         0|            0|            0|  0.00%|                                   + str(out.dtype) + \".\")\n",
      "    63|         0|            0|            0|  0.00%|            new_grads.append(grad)\n",
      "    64|      6240|    0.0130403|  2.08979e-06|  0.00%|        elif grad is None:\n",
      "    65|      6240|    0.0149174|   2.3906e-06|  0.00%|            if out.requires_grad:\n",
      "    66|      6240|    0.0158908|  2.54661e-06|  0.00%|                if out.numel() != 1:\n",
      "    67|         0|            0|            0|  0.00%|                    raise RuntimeError(\"grad can be implicitly created only for scalar outputs\")\n",
      "    68|      6240|    0.0619357|  9.92559e-06|  0.01%|                new_grads.append(torch.ones_like(out, memory_format=torch.preserve_format))\n",
      "    69|         0|            0|            0|  0.00%|            else:\n",
      "    70|         0|            0|            0|  0.00%|                new_grads.append(None)\n",
      "    71|         0|            0|            0|  0.00%|        else:\n",
      "    72|         0|            0|            0|  0.00%|            raise TypeError(\"gradients can be either Tensors or None, but got \" +\n",
      "    73|         0|            0|            0|  0.00%|                            type(grad).__name__)\n",
      "    74|      6240|    0.0135949|  2.17866e-06|  0.00%|    return tuple(new_grads)\n",
      "    75|         0|            0|            0|  0.00%|\n",
      "    76|         0|            0|            0|  0.00%|\n",
      "    77|      6240|   0.00994468|   1.5937e-06|  0.00%|def _tensor_or_tensors_to_tuple(tensors: Optional[_TensorOrTensors], length: int) -> Tuple[_OptionalTensor, ...]:\n",
      "    78|      6240|   0.00991631|  1.58915e-06|  0.00%|    if tensors is None:\n",
      "    79|      6240|    0.0105054|  1.68356e-06|  0.00%|        return (None, ) * length\n",
      "    80|         0|            0|            0|  0.00%|    if isinstance(tensors, torch.Tensor):\n",
      "    81|         0|            0|            0|  0.00%|        return (tensors, )\n",
      "    82|         0|            0|            0|  0.00%|    return tuple(tensors)\n",
      "    83|         0|            0|            0|  0.00%|\n",
      "    84|         0|            0|            0|  0.00%|\n",
      "    85|      6240|    0.0149338|  2.39324e-06|  0.00%|def backward(\n",
      "    86|         0|            0|            0|  0.00%|    tensors: _TensorOrTensors,\n",
      "    87|         0|            0|            0|  0.00%|    grad_tensors: Optional[_TensorOrTensors] = None,\n",
      "    88|         0|            0|            0|  0.00%|    retain_graph: Optional[bool] = None,\n",
      "    89|         0|            0|            0|  0.00%|    create_graph: bool = False,\n",
      "    90|         0|            0|            0|  0.00%|    grad_variables: Optional[_TensorOrTensors] = None,\n",
      "    91|         0|            0|            0|  0.00%|    inputs: Optional[_TensorOrTensors] = None,\n",
      "    92|         0|            0|            0|  0.00%|) -> None:\n",
      "    93|         0|            0|            0|  0.00%|    r\"\"\"Computes the sum of gradients of given tensors with respect to graph\n",
      "    94|         0|            0|            0|  0.00%|    leaves.\n",
      "    95|         0|            0|            0|  0.00%|\n",
      "    96|         0|            0|            0|  0.00%|    The graph is differentiated using the chain rule. If any of ``tensors``\n",
      "    97|         0|            0|            0|  0.00%|    are non-scalar (i.e. their data has more than one element) and require\n",
      "    98|         0|            0|            0|  0.00%|    gradient, then the Jacobian-vector product would be computed, in this\n",
      "    99|         0|            0|            0|  0.00%|    case the function additionally requires specifying ``grad_tensors``.\n",
      "   100|         0|            0|            0|  0.00%|    It should be a sequence of matching length, that contains the \"vector\"\n",
      "   101|         0|            0|            0|  0.00%|    in the Jacobian-vector product, usually the gradient of the differentiated\n",
      "   102|         0|            0|            0|  0.00%|    function w.r.t. corresponding tensors (``None`` is an acceptable value for\n",
      "   103|         0|            0|            0|  0.00%|    all tensors that don't need gradient tensors).\n",
      "   104|         0|            0|            0|  0.00%|\n",
      "   105|         0|            0|            0|  0.00%|    This function accumulates gradients in the leaves - you might need to zero\n",
      "   106|         0|            0|            0|  0.00%|    ``.grad`` attributes or set them to ``None`` before calling it.\n",
      "   107|         0|            0|            0|  0.00%|    See :ref:`Default gradient layouts<default-grad-layouts>`\n",
      "   108|         0|            0|            0|  0.00%|    for details on the memory layout of accumulated gradients.\n",
      "   109|         0|            0|            0|  0.00%|\n",
      "   110|         0|            0|            0|  0.00%|    .. note::\n",
      "   111|         0|            0|            0|  0.00%|        Using this method with ``create_graph=True`` will create a reference cycle\n",
      "   112|         0|            0|            0|  0.00%|        between the parameter and its gradient which can cause a memory leak.\n",
      "   113|         0|            0|            0|  0.00%|        We recommend using ``autograd.grad`` when creating the graph to avoid this.\n",
      "   114|         0|            0|            0|  0.00%|        If you have to use this function, make sure to reset the ``.grad`` fields of your\n",
      "   115|         0|            0|            0|  0.00%|        parameters to ``None`` after use to break the cycle and avoid the leak.\n",
      "   116|         0|            0|            0|  0.00%|\n",
      "   117|         0|            0|            0|  0.00%|    .. note::\n",
      "   118|         0|            0|            0|  0.00%|\n",
      "   119|         0|            0|            0|  0.00%|        If you run any forward ops, create ``grad_tensors``, and/or call ``backward``\n",
      "   120|         0|            0|            0|  0.00%|        in a user-specified CUDA stream context, see\n",
      "   121|         0|            0|            0|  0.00%|        :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n",
      "   122|         0|            0|            0|  0.00%|\n",
      "   123|         0|            0|            0|  0.00%|    .. note::\n",
      "   124|         0|            0|            0|  0.00%|\n",
      "   125|         0|            0|            0|  0.00%|        When ``inputs`` are provided and a given input is not a leaf,\n",
      "   126|         0|            0|            0|  0.00%|        the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).\n",
      "   127|         0|            0|            0|  0.00%|        It is an implementation detail on which the user should not rely.\n",
      "   128|         0|            0|            0|  0.00%|        See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.\n",
      "   129|         0|            0|            0|  0.00%|\n",
      "   130|         0|            0|            0|  0.00%|    Args:\n",
      "   131|         0|            0|            0|  0.00%|        tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be\n",
      "   132|         0|            0|            0|  0.00%|            computed.\n",
      "   133|         0|            0|            0|  0.00%|        grad_tensors (Sequence[Tensor or None] or Tensor, optional): The \"vector\" in\n",
      "   134|         0|            0|            0|  0.00%|            the Jacobian-vector product, usually gradients w.r.t. each element of\n",
      "   135|         0|            0|            0|  0.00%|            corresponding tensors. None values can be specified for scalar Tensors or\n",
      "   136|         0|            0|            0|  0.00%|            ones that don't require grad. If a None value would be acceptable for all\n",
      "   137|         0|            0|            0|  0.00%|            grad_tensors, then this argument is optional.\n",
      "   138|         0|            0|            0|  0.00%|        retain_graph (bool, optional): If ``False``, the graph used to compute the grad\n",
      "   139|         0|            0|            0|  0.00%|            will be freed. Note that in nearly all cases setting this option to ``True``\n",
      "   140|         0|            0|            0|  0.00%|            is not needed and often can be worked around in a much more efficient\n",
      "   141|         0|            0|            0|  0.00%|            way. Defaults to the value of ``create_graph``.\n",
      "   142|         0|            0|            0|  0.00%|        create_graph (bool, optional): If ``True``, graph of the derivative will\n",
      "   143|         0|            0|            0|  0.00%|            be constructed, allowing to compute higher order derivative products.\n",
      "   144|         0|            0|            0|  0.00%|            Defaults to ``False``.\n",
      "   145|         0|            0|            0|  0.00%|        inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient\n",
      "   146|         0|            0|            0|  0.00%|            be will accumulated into ``.grad``. All other Tensors will be ignored. If\n",
      "   147|         0|            0|            0|  0.00%|            not provided, the gradient is accumulated into all the leaf Tensors that\n",
      "   148|         0|            0|            0|  0.00%|            were used to compute the attr::tensors.\n",
      "   149|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   150|      6240|    0.0139897|  2.24194e-06|  0.00%|    if grad_variables is not None:\n",
      "   151|         0|            0|            0|  0.00%|        warnings.warn(\"'grad_variables' is deprecated. Use 'grad_tensors' instead.\")\n",
      "   152|         0|            0|            0|  0.00%|        if grad_tensors is None:\n",
      "   153|         0|            0|            0|  0.00%|            grad_tensors = grad_variables\n",
      "   154|         0|            0|            0|  0.00%|        else:\n",
      "   155|         0|            0|            0|  0.00%|            raise RuntimeError(\"'grad_tensors' and 'grad_variables' (deprecated) \"\n",
      "   156|         0|            0|            0|  0.00%|                               \"arguments both passed to backward(). Please only \"\n",
      "   157|         0|            0|            0|  0.00%|                               \"use 'grad_tensors'.\")\n",
      "   158|      6240|    0.0120339|  1.92852e-06|  0.00%|    if inputs is not None and len(inputs) == 0:\n",
      "   159|         0|            0|            0|  0.00%|        raise RuntimeError(\"'inputs' argument to backward() cannot be empty.\")\n",
      "   160|         0|            0|            0|  0.00%|\n",
      "   161|      6240|    0.0155752|  2.49602e-06|  0.00%|    tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)\n",
      "   162|     12480|    0.0255086|  2.04396e-06|  0.00%|    inputs = (inputs,) if isinstance(inputs, torch.Tensor) else \\\n",
      "   163|      6240|    0.0134037|  2.14802e-06|  0.00%|        tuple(inputs) if inputs is not None else tuple()\n",
      "   164|         0|            0|            0|  0.00%|\n",
      "   165|      6240|    0.0367303|  5.88626e-06|  0.01%|    grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))\n",
      "(call)|      6240|    0.0303664|  4.86641e-06|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:77 _tensor_or_tensors_to_tuple\n",
      "   166|      6240|    0.0434859|  6.96889e-06|  0.01%|    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)\n",
      "(call)|      6240|      0.20154|   3.2298e-05|  0.03%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:30 _make_grads\n",
      "   167|      6240|    0.0122383|  1.96126e-06|  0.00%|    if retain_graph is None:\n",
      "   168|      6240|    0.0113633|  1.82104e-06|  0.00%|        retain_graph = create_graph\n",
      "   169|         0|            0|            0|  0.00%|\n",
      "   170|         0|            0|            0|  0.00%|    # The reason we repeat same the comment below is that\n",
      "   171|         0|            0|            0|  0.00%|    # some Python versions print out the first line of a multi-line function\n",
      "   172|         0|            0|            0|  0.00%|    # calls in the traceback and some print out the last line\n",
      "   173|     12480|      5.15653|  0.000413183|  0.83%|    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "   174|      6240|    0.0111938|  1.79387e-06|  0.00%|        tensors, grad_tensors_, retain_graph, create_graph, inputs,\n",
      "   175|      6240|    0.0112767|  1.80717e-06|  0.00%|        allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "   176|         0|            0|            0|  0.00%|\n",
      "   177|         0|            0|            0|  0.00%|def grad(\n",
      "   178|         0|            0|            0|  0.00%|    outputs: _TensorOrTensors,\n",
      "   179|         0|            0|            0|  0.00%|    inputs: _TensorOrTensors,\n",
      "   180|         0|            0|            0|  0.00%|    grad_outputs: Optional[_TensorOrTensors] = None,\n",
      "   181|         0|            0|            0|  0.00%|    retain_graph: Optional[bool] = None,\n",
      "   182|         0|            0|            0|  0.00%|    create_graph: bool = False,\n",
      "   183|         0|            0|            0|  0.00%|    only_inputs: bool = True,\n",
      "   184|         0|            0|            0|  0.00%|    allow_unused: bool = False,\n",
      "   185|         0|            0|            0|  0.00%|    is_grads_batched: bool = False\n",
      "   186|         0|            0|            0|  0.00%|) -> Tuple[torch.Tensor, ...]:\n",
      "   187|         0|            0|            0|  0.00%|    r\"\"\"Computes and returns the sum of gradients of outputs with respect to\n",
      "   188|         0|            0|            0|  0.00%|    the inputs.\n",
      "   189|         0|            0|            0|  0.00%|\n",
      "   190|         0|            0|            0|  0.00%|    ``grad_outputs`` should be a sequence of length matching ``output``\n",
      "   191|         0|            0|            0|  0.00%|    containing the \"vector\" in vector-Jacobian product, usually the pre-computed\n",
      "   192|         0|            0|            0|  0.00%|    gradients w.r.t. each of the outputs. If an output doesn't require_grad,\n",
      "   193|         0|            0|            0|  0.00%|    then the gradient can be ``None``).\n",
      "   194|         0|            0|            0|  0.00%|\n",
      "   195|         0|            0|            0|  0.00%|    .. note::\n",
      "   196|         0|            0|            0|  0.00%|\n",
      "   197|         0|            0|            0|  0.00%|        If you run any forward ops, create ``grad_outputs``, and/or call ``grad``\n",
      "   198|         0|            0|            0|  0.00%|        in a user-specified CUDA stream context, see\n",
      "   199|         0|            0|            0|  0.00%|        :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n",
      "   200|         0|            0|            0|  0.00%|\n",
      "   201|         0|            0|            0|  0.00%|    .. note::\n",
      "   202|         0|            0|            0|  0.00%|\n",
      "   203|         0|            0|            0|  0.00%|        ``only_inputs`` argument is deprecated and is ignored now (defaults to ``True``).\n",
      "   204|         0|            0|            0|  0.00%|        To accumulate gradient for other parts of the graph, please use\n",
      "   205|         0|            0|            0|  0.00%|        ``torch.autograd.backward``.\n",
      "   206|         0|            0|            0|  0.00%|\n",
      "   207|         0|            0|            0|  0.00%|    Args:\n",
      "   208|         0|            0|            0|  0.00%|        outputs (sequence of Tensor): outputs of the differentiated function.\n",
      "   209|         0|            0|            0|  0.00%|        inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be\n",
      "   210|         0|            0|            0|  0.00%|            returned (and not accumulated into ``.grad``).\n",
      "   211|         0|            0|            0|  0.00%|        grad_outputs (sequence of Tensor): The \"vector\" in the vector-Jacobian product.\n",
      "   212|         0|            0|            0|  0.00%|            Usually gradients w.r.t. each output. None values can be specified for scalar\n",
      "   213|         0|            0|            0|  0.00%|            Tensors or ones that don't require grad. If a None value would be acceptable\n",
      "   214|         0|            0|            0|  0.00%|            for all grad_tensors, then this argument is optional. Default: None.\n",
      "   215|         0|            0|            0|  0.00%|        retain_graph (bool, optional): If ``False``, the graph used to compute the grad\n",
      "   216|         0|            0|            0|  0.00%|            will be freed. Note that in nearly all cases setting this option to ``True``\n",
      "   217|         0|            0|            0|  0.00%|            is not needed and often can be worked around in a much more efficient\n",
      "   218|         0|            0|            0|  0.00%|            way. Defaults to the value of ``create_graph``.\n",
      "   219|         0|            0|            0|  0.00%|        create_graph (bool, optional): If ``True``, graph of the derivative will\n",
      "   220|         0|            0|            0|  0.00%|            be constructed, allowing to compute higher order derivative products.\n",
      "   221|         0|            0|            0|  0.00%|            Default: ``False``.\n",
      "   222|         0|            0|            0|  0.00%|        allow_unused (bool, optional): If ``False``, specifying inputs that were not\n",
      "   223|         0|            0|            0|  0.00%|            used when computing outputs (and therefore their grad is always zero)\n",
      "   224|         0|            0|            0|  0.00%|            is an error. Defaults to ``False``.\n",
      "   225|         0|            0|            0|  0.00%|        is_grads_batched (bool, optional): If ``True``, the first dimension of each\n",
      "   226|         0|            0|            0|  0.00%|            tensor in ``grad_outputs`` will be interpreted as the batch dimension.\n",
      "   227|         0|            0|            0|  0.00%|            Instead of computing a single vector-Jacobian product, we compute a\n",
      "   228|         0|            0|            0|  0.00%|            batch of vector-Jacobian products for each \"vector\" in the batch.\n",
      "   229|         0|            0|            0|  0.00%|            We use the vmap prototype feature as the backend to vectorize calls\n",
      "   230|         0|            0|            0|  0.00%|            to the autograd engine so that this computation can be performed in a\n",
      "   231|         0|            0|            0|  0.00%|            single call. This should lead to performance improvements when compared\n",
      "   232|         0|            0|            0|  0.00%|            to manually looping and performing backward multiple times. Note that\n",
      "   233|         0|            0|            0|  0.00%|            due to this feature being experimental, there may be performance\n",
      "   234|         0|            0|            0|  0.00%|            cliffs. Please use ``torch._C._debug_only_display_vmap_fallback_warnings(True)``\n",
      "   235|         0|            0|            0|  0.00%|            to show any performance warnings and file an issue on github if warnings exist\n",
      "   236|         0|            0|            0|  0.00%|            for your use case. Defaults to ``False``.\n",
      "   237|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   238|         0|            0|            0|  0.00%|    t_outputs = cast(Tuple[torch.Tensor, ...], (outputs,) if is_tensor_like(outputs) else tuple(outputs))\n",
      "   239|         0|            0|            0|  0.00%|    t_inputs = cast(Tuple[torch.Tensor, ...], (inputs,) if is_tensor_like(inputs) else tuple(inputs))\n",
      "   240|         0|            0|            0|  0.00%|    overridable_args = t_outputs + t_inputs\n",
      "   241|         0|            0|            0|  0.00%|    if has_torch_function(overridable_args):\n",
      "   242|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   243|         0|            0|            0|  0.00%|            grad,\n",
      "   244|         0|            0|            0|  0.00%|            overridable_args,\n",
      "   245|         0|            0|            0|  0.00%|            t_outputs,\n",
      "   246|         0|            0|            0|  0.00%|            t_inputs,\n",
      "   247|         0|            0|            0|  0.00%|            grad_outputs=grad_outputs,\n",
      "   248|         0|            0|            0|  0.00%|            retain_graph=retain_graph,\n",
      "   249|         0|            0|            0|  0.00%|            create_graph=create_graph,\n",
      "   250|         0|            0|            0|  0.00%|            only_inputs=only_inputs,\n",
      "   251|         0|            0|            0|  0.00%|            allow_unused=allow_unused,\n",
      "   252|         0|            0|            0|  0.00%|            is_grads_batched=is_grads_batched,\n",
      "   253|         0|            0|            0|  0.00%|        )\n",
      "   254|         0|            0|            0|  0.00%|\n",
      "   255|         0|            0|            0|  0.00%|    if not only_inputs:\n",
      "   256|         0|            0|            0|  0.00%|        warnings.warn(\"only_inputs argument is deprecated and is ignored now \"\n",
      "   257|         0|            0|            0|  0.00%|                      \"(defaults to True). To accumulate gradient for other \"\n",
      "   258|         0|            0|            0|  0.00%|                      \"parts of the graph, please use torch.autograd.backward.\")\n",
      "   259|         0|            0|            0|  0.00%|\n",
      "   260|         0|            0|            0|  0.00%|    grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(t_outputs))\n",
      "   261|         0|            0|            0|  0.00%|    grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)\n",
      "   262|         0|            0|            0|  0.00%|\n",
      "   263|         0|            0|            0|  0.00%|    if retain_graph is None:\n",
      "   264|         0|            0|            0|  0.00%|        retain_graph = create_graph\n",
      "   265|         0|            0|            0|  0.00%|\n",
      "   266|         0|            0|            0|  0.00%|    # The reason we repeat same the comment several times below is because\n",
      "   267|         0|            0|            0|  0.00%|    # some Python versions print out the first line of multi-line function\n",
      "   268|         0|            0|            0|  0.00%|    # calls in the traceback and some print out the last line\n",
      "   269|         0|            0|            0|  0.00%|    if is_grads_batched:\n",
      "   270|         0|            0|            0|  0.00%|        def vjp(gO):\n",
      "   271|         0|            0|            0|  0.00%|            return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "   272|         0|            0|            0|  0.00%|                t_outputs, gO, retain_graph, create_graph, t_inputs,\n",
      "   273|         0|            0|            0|  0.00%|                allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
      "   274|         0|            0|            0|  0.00%|        return _vmap_internals._vmap(vjp, 0, 0, allow_none_pass_through=True)(grad_outputs_)\n",
      "   275|         0|            0|            0|  0.00%|    else:\n",
      "   276|         0|            0|            0|  0.00%|        return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "   277|         0|            0|            0|  0.00%|            t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n",
      "   278|         0|            0|            0|  0.00%|            allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
      "   279|         0|            0|            0|  0.00%|\n",
      "   280|         0|            0|            0|  0.00%|\n",
      "   281|         0|            0|            0|  0.00%|# This function applies in case of gradient checkpointing for memory\n",
      "   282|         0|            0|            0|  0.00%|# optimization. Currently, gradient checkpointing is supported only if the\n",
      "   283|         0|            0|            0|  0.00%|# execution engine is invoked through torch.autograd.backward() and its\n",
      "   284|         0|            0|            0|  0.00%|# inputs argument is not passed. It is not supported for torch.autograd.grad().\n",
      "   285|         0|            0|            0|  0.00%|# This is because if inputs are specified, the gradient won't be calculated for\n",
      "   286|         0|            0|            0|  0.00%|# anything else e.g. model parameters like weights, bias etc.\n",
      "   287|         0|            0|            0|  0.00%|#\n",
      "   288|         0|            0|            0|  0.00%|# This function returns whether the checkpointing is valid i.e. torch.autograd.backward\n",
      "   289|         0|            0|            0|  0.00%|# or not i.e. torch.autograd.grad. The implementation works by maintaining a thread\n",
      "   290|         0|            0|            0|  0.00%|# local variable in torch/csrc/autograd/engine.cpp which looks at the NodeTask\n",
      "   291|         0|            0|            0|  0.00%|# in the stack and before a NodeTask is executed in evaluate_function, it\n",
      "   292|         0|            0|            0|  0.00%|# checks for whether reentrant backwards is imperative or not.\n",
      "   293|         0|            0|            0|  0.00%|# See https://github.com/pytorch/pytorch/pull/4594 for more discussion/context\n",
      "   294|         0|            0|            0|  0.00%|def _is_checkpoint_valid():\n",
      "   295|         0|            0|            0|  0.00%|    return Variable._execution_engine.is_checkpoint_valid()\n",
      "   296|         0|            0|            0|  0.00%|\n",
      "   297|         0|            0|            0|  0.00%|\n",
      "   298|         0|            0|            0|  0.00%|def variable(*args, **kwargs):\n",
      "   299|         0|            0|            0|  0.00%|    raise RuntimeError(\"torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead\")\n",
      "   300|         0|            0|            0|  0.00%|\n",
      "   301|         0|            0|            0|  0.00%|# Monkey patching variable.Variable to fix FX codegen. FX generates a call by roughly doing\n",
      "   302|         0|            0|            0|  0.00%|# f\"{fn.__module__}.{fn.__name__}(...). This yields torch.autograd.variable.Variable(...) in the\n",
      "   303|         0|            0|            0|  0.00%|# output of an FX graph.  Unfortunately the module name torch.autograd.variable is shadowed by the\n",
      "   304|         0|            0|            0|  0.00%|# deprecated function - variable(...).\n",
      "   305|         0|            0|            0|  0.00%|variable.Variable = Variable  # type: ignore[attr-defined]\n",
      "   306|         0|            0|            0|  0.00%|\n",
      "   307|         0|            0|            0|  0.00%|if not torch._C._autograd_init():\n",
      "   308|         0|            0|            0|  0.00%|    raise RuntimeError(\"autograd initialization failed\")\n",
      "   309|         0|            0|            0|  0.00%|\n",
      "   310|         0|            0|            0|  0.00%|# Import all native method/classes\n",
      "   311|         0|            0|            0|  0.00%|from torch._C._autograd import (DeviceType, ProfilerActivity, ProfilerState, ProfilerConfig, ProfilerEvent,\n",
      "   312|         0|            0|            0|  0.00%|                                _enable_profiler_legacy, _disable_profiler_legacy, _profiler_enabled,\n",
      "   313|         0|            0|            0|  0.00%|                                _enable_record_function, _set_empty_test_observer, kineto_available,\n",
      "   314|         0|            0|            0|  0.00%|                                _record_function_with_args_enter, _record_function_with_args_exit,\n",
      "   315|         0|            0|            0|  0.00%|                                _supported_activities, _add_metadata_json, SavedTensor,\n",
      "   316|         0|            0|            0|  0.00%|                                _push_saved_tensors_default_hooks, _pop_saved_tensors_default_hooks)\n",
      "   317|         0|            0|            0|  0.00%|\n",
      "   318|         0|            0|            0|  0.00%|from torch._C._autograd import (_ProfilerResult, _KinetoEvent, _kineto_step,\n",
      "   319|         0|            0|            0|  0.00%|                                _prepare_profiler, _enable_profiler, _disable_profiler)\n",
      "   320|         0|            0|            0|  0.00%|\n",
      "   321|         0|            0|            0|  0.00%|from . import profiler\n",
      "   322|         0|            0|            0|  0.00%|\n",
      "   323|         0|            0|            0|  0.00%|def _register_py_tensor_class_for_device(device, cls):\n",
      "   324|         0|            0|            0|  0.00%|    if not isinstance(cls, type):\n",
      "   325|         0|            0|            0|  0.00%|        raise RuntimeError(\"cls isn't a typeinfo object\")\n",
      "   326|         0|            0|            0|  0.00%|    torch._C._register_py_class_for_device(device, cls)\n",
      "File: /apps/open_spiel/open_spiel/python/examples/ubc_utils.py\n",
      "File duration: 5.14727s (0.83%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|from ..pytorch.ppo import PPOAgent\n",
      "     2|         0|            0|            0|  0.00%|import pyspiel\n",
      "     3|         0|            0|            0|  0.00%|from absl import logging\n",
      "     4|         0|            0|            0|  0.00%|import numpy as np\n",
      "     5|         0|            0|            0|  0.00%|import pandas as pd\n",
      "     6|         0|            0|            0|  0.00%|import itertools\n",
      "     7|         0|            0|            0|  0.00%|import pulp\n",
      "     8|         0|            0|            0|  0.00%|from pulp import LpStatus\n",
      "     9|         0|            0|            0|  0.00%|import random\n",
      "    10|         0|            0|            0|  0.00%|import string\n",
      "    11|         0|            0|            0|  0.00%|from open_spiel.python.rl_agent import StepOutput\n",
      "    12|         0|            0|            0|  0.00%|import torch\n",
      "    13|         0|            0|            0|  0.00%|import humanize\n",
      "    14|         0|            0|            0|  0.00%|import datetime as dt\n",
      "    15|         0|            0|            0|  0.00%|import os\n",
      "    16|         0|            0|            0|  0.00%|import json\n",
      "    17|         0|            0|            0|  0.00%|import yaml\n",
      "    18|         0|            0|            0|  0.00%|import shutil\n",
      "    19|         0|            0|            0|  0.00%|from open_spiel.python.examples.ubc_math_utils import fast_choice\n",
      "    20|         0|            0|            0|  0.00%|\n",
      "    21|         0|            0|            0|  0.00%|CONFIG_ROOT = '/apps/open_spiel/notebooks/configs'\n",
      "    22|         0|            0|            0|  0.00%|\n",
      "    23|         0|            0|            0|  0.00%|CLOCK_AUCTION = 'clock_auction'\n",
      "    24|         0|            0|            0|  0.00%|\n",
      "    25|         0|            0|            0|  0.00%|BR_DIR = 'best_responses'\n",
      "    26|         0|            0|            0|  0.00%|CHECKPOINT_FOLDER = 'solving_checkpoints' # Don't use \"checkpoints\" because jupyter bug\n",
      "    27|         0|            0|            0|  0.00%|EVAL_DIR = 'evaluations'\n",
      "    28|         0|            0|            0|  0.00%|\n",
      "    29|         0|            0|            0|  0.00%|def setup_directory_structure(output_dir, warn_on_overwrite, database=True):\n",
      "    30|         0|            0|            0|  0.00%|    if not os.path.exists(output_dir):\n",
      "    31|         0|            0|            0|  0.00%|        os.makedirs(output_dir)\n",
      "    32|         0|            0|            0|  0.00%|    else:\n",
      "    33|         0|            0|            0|  0.00%|        if warn_on_overwrite:\n",
      "    34|         0|            0|            0|  0.00%|            raise ValueError(\"You are overwriting a folder!\")\n",
      "    35|         0|            0|            0|  0.00%|        shutil.rmtree(output_dir)\n",
      "    36|         0|            0|            0|  0.00%|        os.makedirs(output_dir)\n",
      "    37|         0|            0|            0|  0.00%|\n",
      "    38|         0|            0|            0|  0.00%|    os.makedirs(os.path.join(output_dir, BR_DIR))\n",
      "    39|         0|            0|            0|  0.00%|    os.makedirs(os.path.join(output_dir, EVAL_DIR))\n",
      "    40|         0|            0|            0|  0.00%|    if not database:\n",
      "    41|         0|            0|            0|  0.00%|        os.makedirs(os.path.join(output_dir, CHECKPOINT_FOLDER))\n",
      "    42|         0|            0|            0|  0.00%|\n",
      "    43|         1|  3.33786e-06|  3.33786e-06|  0.00%|def fix_seeds(seed):\n",
      "    44|         1|  1.26362e-05|  1.26362e-05|  0.00%|    logging.info(f\"Setting numpy and torch seed to {seed}\")\n",
      "(call)|         1|  0.000130892|  0.000130892|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/absl/logging/__init__.py:413 info\n",
      "    45|         1|  1.52588e-05|  1.52588e-05|  0.00%|    np.random.seed(seed)\n",
      "    46|         1|  1.07288e-05|  1.07288e-05|  0.00%|    torch.manual_seed(seed)\n",
      "(call)|         1|     0.005476|     0.005476|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/random.py:26 manual_seed\n",
      "    47|         0|            0|            0|  0.00%|    # torch.use_deterministic_algorithms(True) # See https://github.com/pytorch/pytorch/issues/50469\n",
      "    48|         0|            0|            0|  0.00%|\n",
      "    49|         0|            0|            0|  0.00%|def single_action_result(legal_actions, num_actions, as_output=False):\n",
      "    50|         0|            0|            0|  0.00%|    probs = np.zeros(num_actions)\n",
      "    51|         0|            0|            0|  0.00%|    action = legal_actions[0]\n",
      "    52|         0|            0|            0|  0.00%|    probs[action] = 1.0\n",
      "    53|         0|            0|            0|  0.00%|    if as_output:\n",
      "    54|         0|            0|            0|  0.00%|        return StepOutput(action=action, probs=probs)\n",
      "    55|         0|            0|            0|  0.00%|    return action, probs\n",
      "    56|         0|            0|            0|  0.00%|\n",
      "    57|         0|            0|            0|  0.00%|def get_first_actionable_state(game, forced_types=None, player_id=None):\n",
      "    58|         0|            0|            0|  0.00%|    state = game.new_initial_state()\n",
      "    59|         0|            0|            0|  0.00%|    # Skip over chance nodes\n",
      "    60|         0|            0|            0|  0.00%|    i = 0\n",
      "    61|         0|            0|            0|  0.00%|    while state.current_player() < 0:\n",
      "    62|         0|            0|            0|  0.00%|        state = state.child(0 if forced_types is None else forced_types[i]) # Let chance choose first outcome. We're assuming all moves are possible at starting prices for all players, that may not really be true though\n",
      "    63|         0|            0|            0|  0.00%|        i += 1\n",
      "    64|         0|            0|            0|  0.00%|\n",
      "    65|         0|            0|            0|  0.00%|    if player_id is not None:\n",
      "    66|         0|            0|            0|  0.00%|        while state.current_player() != player_id:\n",
      "    67|         0|            0|            0|  0.00%|            state = state.child(0)\n",
      "    68|         0|            0|            0|  0.00%|\n",
      "    69|         0|            0|            0|  0.00%|    return state\n",
      "    70|         0|            0|            0|  0.00%|\n",
      "    71|         0|            0|            0|  0.00%|def get_actions(game):\n",
      "    72|         0|            0|            0|  0.00%|    state = get_first_actionable_state(game)\n",
      "    73|         0|            0|            0|  0.00%|    action_dict = dict()\n",
      "    74|         0|            0|            0|  0.00%|    for i in state.legal_actions():\n",
      "    75|         0|            0|            0|  0.00%|        action_dict[i] = state.action_to_string(i)\n",
      "    76|         0|            0|            0|  0.00%|    return action_dict\n",
      "    77|         0|            0|            0|  0.00%|\n",
      "    78|         0|            0|            0|  0.00%|def load_game_config(game_name):\n",
      "    79|         0|            0|            0|  0.00%|    game_config_path = os.path.join(os.environ['CLOCK_AUCTION_CONFIG_DIR'], game_name)\n",
      "    80|         0|            0|            0|  0.00%|    with open(game_config_path, 'r') as f:\n",
      "    81|         0|            0|            0|  0.00%|        game_config = json.load(f)\n",
      "    82|         0|            0|            0|  0.00%|    return game_config\n",
      "    83|         0|            0|            0|  0.00%|\n",
      "    84|         0|            0|            0|  0.00%|def solve(problem):\n",
      "    85|         0|            0|            0|  0.00%|    return problem.solve(solver=pulp.CPLEX_PY(msg=0, gapRel=0))\n",
      "    86|         0|            0|            0|  0.00%|    # return problem.solve(solver=pulp.CPLEX_CMD(msg=0, options=[f'set mip tolerances mipgap 0']))\n",
      "    87|         0|            0|            0|  0.00%|    # return problem.solve(solver=pulp.CPLEX_CMD(msg=0, options=[f'set mip tolerances mipgap 0'], keepFiles = 1))\n",
      "    88|         0|            0|            0|  0.00%|\n",
      "    89|         0|            0|            0|  0.00%|def objective_from_lp(problem):\n",
      "    90|         0|            0|            0|  0.00%|    return np.round(problem.objective.value())\n",
      "    91|         0|            0|            0|  0.00%|\n",
      "    92|         0|            0|            0|  0.00%|def random_string(k):\n",
      "    93|         0|            0|            0|  0.00%|    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=k))\n",
      "    94|         0|            0|            0|  0.00%|\n",
      "    95|         0|            0|            0|  0.00%|def fail_lp(problem, save_lp, status=None):\n",
      "    96|         0|            0|            0|  0.00%|    error_message = f\"Couldn't solve allocation problem optimally. Status was {status}.\"\n",
      "    97|         0|            0|            0|  0.00%|    if save_lp:\n",
      "    98|         0|            0|            0|  0.00%|        fname = ''.join(random_string(10))\n",
      "    99|         0|            0|            0|  0.00%|        # TODO: Pickle seems to no longer work TypeError: can't pickle SwigPyObject objects\n",
      "   100|         0|            0|            0|  0.00%|        # with open(f'{fname}.pkl', 'wb') as f:\n",
      "   101|         0|            0|            0|  0.00%|        #     pickle.dump(problem, f)\n",
      "   102|         0|            0|            0|  0.00%|        problem.writeLP(f'{fname}.lp')\n",
      "   103|         0|            0|            0|  0.00%|        error_message += f'Saved LP to {fname}.lp.'\n",
      "   104|         0|            0|            0|  0.00%|    raise ValueError(error_message)\n",
      "   105|         0|            0|            0|  0.00%|\n",
      "   106|         0|            0|            0|  0.00%|def pulp_solve(problem, solve_function=solve, save_if_failed=True):\n",
      "   107|         0|            0|            0|  0.00%|    try:\n",
      "   108|         0|            0|            0|  0.00%|        status = LpStatus[solve_function(problem)]\n",
      "   109|         0|            0|            0|  0.00%|        if status != \"Optimal\":\n",
      "   110|         0|            0|            0|  0.00%|            fail_lp(problem, save_if_failed, status=status)\n",
      "   111|         0|            0|            0|  0.00%|    except pulp.PulpSolverError as e:\n",
      "   112|         0|            0|            0|  0.00%|        logging.warning(e)\n",
      "   113|         0|            0|            0|  0.00%|        fail_lp(problem, save_if_failed)\n",
      "   114|         0|            0|            0|  0.00%|    return objective_from_lp(problem)\n",
      "   115|         0|            0|            0|  0.00%|\n",
      "   116|         1|   1.4782e-05|   1.4782e-05|  0.00%|def pretty_time(seconds):\n",
      "   117|         1|  1.04904e-05|  1.04904e-05|  0.00%|    delta = dt.timedelta(seconds=seconds)\n",
      "   118|         1|  2.28882e-05|  2.28882e-05|  0.00%|    return humanize.precisedelta(delta)\n",
      "(call)|         1|   0.00102282|   0.00102282|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/humanize/time.py:399 precisedelta\n",
      "   119|         0|            0|            0|  0.00%|\n",
      "   120|         0|            0|            0|  0.00%|def default_device():\n",
      "   121|         0|            0|            0|  0.00%|    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
      "   122|         0|            0|            0|  0.00%|\n",
      "   123|         0|            0|            0|  0.00%|def apply_optional_overrides(args, argv, config):\n",
      "   124|         0|            0|            0|  0.00%|    # Override any top-level yaml args with command line arguments\n",
      "   125|         0|            0|            0|  0.00%|    for arg in vars(args):\n",
      "   126|         0|            0|            0|  0.00%|        if f'--{arg}' in argv:\n",
      "   127|         0|            0|            0|  0.00%|            name = arg\n",
      "   128|         0|            0|            0|  0.00%|            value = getattr(args, arg)\n",
      "   129|         0|            0|            0|  0.00%|            if name in config:\n",
      "   130|         0|            0|            0|  0.00%|                logging.warning(f'Overriding {name} from command line')\n",
      "   131|         0|            0|            0|  0.00%|                config[name] = value\n",
      "   132|         0|            0|            0|  0.00%|\n",
      "   133|         0|            0|            0|  0.00%|    # These always get overridden from the command line\n",
      "   134|         0|            0|            0|  0.00%|    config['seed'] = args.seed\n",
      "   135|         0|            0|            0|  0.00%|    config['total_timesteps'] = args.total_timesteps\n",
      "   136|         0|            0|            0|  0.00%|    config['use_wandb'] = args.use_wandb\n",
      "   137|         0|            0|            0|  0.00%|    if hasattr(args, 'potential_function') and args.potential_function is not None:\n",
      "   138|         0|            0|            0|  0.00%|        logging.warning(f'Overriding potential function from command line to {args.potential_function}')\n",
      "   139|         0|            0|            0|  0.00%|        config['potential_function'] = args.potential_function\n",
      "   140|         0|            0|            0|  0.00%|    if hasattr(args, 'scale_coef') and args.scale_coef is not None:\n",
      "   141|         0|            0|            0|  0.00%|        logging.warning(f'Overriding scale coef from command line to {args.scale_coef}')\n",
      "   142|         0|            0|            0|  0.00%|        config['scale_coef'] = args.scale_coef\n",
      "   143|         0|            0|            0|  0.00%|    if config.get('anneal_lr', False):\n",
      "   144|         0|            0|            0|  0.00%|        config['num_annealing_updates'] = config['total_timesteps']\n",
      "   145|         0|            0|            0|  0.00%|\n",
      "   146|         0|            0|            0|  0.00%|\n",
      "   147|         0|            0|            0|  0.00%|class UBCChanceEventSampler(object):\n",
      "   148|         0|            0|            0|  0.00%|    \"\"\"Default sampler for external chance events.\"\"\"\n",
      "   149|         0|            0|            0|  0.00%|\n",
      "   150|         0|            0|            0|  0.00%|    def __init__(self, seed=None) -> None:\n",
      "   151|         0|            0|            0|  0.00%|        self.seed(seed)\n",
      "   152|         0|            0|            0|  0.00%|\n",
      "   153|         0|            0|            0|  0.00%|    def seed(self, seed=None):\n",
      "   154|         0|            0|            0|  0.00%|        self._rng = np.random.RandomState(seed)\n",
      "   155|         0|            0|            0|  0.00%|\n",
      "   156|     74880|     0.156559|   2.0908e-06|  0.03%|    def __call__(self, state):\n",
      "   157|         0|            0|            0|  0.00%|        \"\"\"Sample a chance event in the given state.\"\"\"\n",
      "   158|     74880|     0.531719|  7.10095e-06|  0.09%|        output = state.chance_outcomes()\n",
      "(call)|     74880|      2.27696|  3.04081e-05|  0.37%|# /apps/open_spiel/open_spiel/python/games/clock_auction.py:498 chance_outcomes\n",
      "   159|     74880|     0.190926|  2.54976e-06|  0.03%|        if isinstance(output, dict):\n",
      "   160|         0|            0|            0|  0.00%|            return self._rng.randint(0, high=output['upper'])\n",
      "   161|         0|            0|            0|  0.00%|        else:\n",
      "   162|     74880|      1.58115|  2.11158e-05|  0.25%|            actions, probs = zip(*output)\n",
      "   163|     74880|     0.477582|  6.37796e-06|  0.08%|        return fast_choice(actions, probs, rng=self._rng)\n",
      "(call)|     74880|      10.2605|  0.000137026|  1.65%|# /apps/open_spiel/open_spiel/python/examples/ubc_math_utils.py:4 fast_choice\n",
      "   164|         0|            0|            0|  0.00%|\n",
      "   165|         0|            0|            0|  0.00%|\n",
      "   166|         0|            0|            0|  0.00%|def series_to_quantiles(s: pd.Series):\n",
      "   167|         0|            0|            0|  0.00%|    quantiles = []\n",
      "   168|         0|            0|            0|  0.00%|    for quantile in np.arange(0, 1.01, 0.01):\n",
      "   169|         0|            0|            0|  0.00%|        quantiles.append(s.quantile(quantile)) # TODO: Think about what interpolation method you want to use\n",
      "   170|         0|            0|            0|  0.00%|    return quantiles\n",
      "   171|         0|            0|            0|  0.00%|\n",
      "   172|         0|            0|            0|  0.00%|def config_path_from_config_name(config_name):\n",
      "   173|         0|            0|            0|  0.00%|    return f'{CONFIG_ROOT}/{config_name}.yml'\n",
      "   174|         0|            0|            0|  0.00%|\n",
      "   175|         0|            0|            0|  0.00%|def safe_config_name(name):\n",
      "   176|         0|            0|            0|  0.00%|    return name.replace(\"/\", \"\").replace('.yml', '')\n",
      "   177|         0|            0|            0|  0.00%|\n",
      "   178|         0|            0|            0|  0.00%|def num_to_letter(i):\n",
      "   179|         0|            0|            0|  0.00%|    '''Maps 0 to A, 1 to B etc.'''\n",
      "   180|         0|            0|            0|  0.00%|    return chr(ord('@')+i+1)\n",
      "   181|         0|            0|            0|  0.00%|\n",
      "   182|         0|            0|            0|  0.00%|def players_not_me(my_player_id, num_players):\n",
      "   183|         0|            0|            0|  0.00%|    for i in range(num_players):\n",
      "   184|         0|            0|            0|  0.00%|        if i == my_player_id:\n",
      "   185|         0|            0|            0|  0.00%|            continue\n",
      "   186|         0|            0|            0|  0.00%|        yield i\n",
      "   187|         0|            0|            0|  0.00%|\n",
      "   188|     24962|    0.0349472|  1.40001e-06|  0.01%|def factorial(k):\n",
      "   189|     24962|    0.0591674|   2.3703e-06|  0.01%|    return np.math.factorial(k)\n",
      "   190|         0|            0|            0|  0.00%|\n",
      "   191|     24962|    0.0426631|  1.70912e-06|  0.01%|def convert_seed_to_swaps(seed):\n",
      "   192|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   193|         0|            0|            0|  0.00%|    arguments:\n",
      "   194|         0|            0|            0|  0.00%|    - seed: non-negative int\n",
      "   195|         0|            0|            0|  0.00%|\n",
      "   196|         0|            0|            0|  0.00%|    returns:\n",
      "   197|         0|            0|            0|  0.00%|    - list (f_0, f_1, f_2, ..., f_k) such that \\sum_{i=0}^k f_i * i! = seed\n",
      "   198|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   199|     24962|    0.0470135|   1.8834e-06|  0.01%|    ret = [0]\n",
      "   200|    118428|     0.168053|  1.41903e-06|  0.03%|    while seed > 0:\n",
      "   201|     93466|     0.138283|  1.47951e-06|  0.02%|        divisor = len(ret) + 1\n",
      "   202|     93466|     0.145238|  1.55391e-06|  0.02%|        ret.append(seed % divisor)\n",
      "   203|     93466|     0.131792|  1.41005e-06|  0.02%|        seed //= divisor\n",
      "   204|     24962|    0.0330317|  1.32328e-06|  0.01%|    return ret\n",
      "   205|         0|            0|            0|  0.00%|\n",
      "   206|     24964|     0.059623|  2.38836e-06|  0.01%|def permute_array(arr, seed):\n",
      "   207|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   208|         0|            0|            0|  0.00%|    Permute an array into a canonical permutation.\n",
      "   209|         0|            0|            0|  0.00%|\n",
      "   210|         0|            0|            0|  0.00%|    Arguments:\n",
      "   211|         0|            0|            0|  0.00%|    - arr: list\n",
      "   212|         0|            0|            0|  0.00%|    - seed: int in range [0, len(arr)! - 1]\n",
      "   213|         0|            0|            0|  0.00%|\n",
      "   214|         0|            0|            0|  0.00%|    Returns:\n",
      "   215|         0|            0|            0|  0.00%|    - A permutation of arr\n",
      "   216|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   217|         0|            0|            0|  0.00%|\n",
      "   218|     24964|    0.0640981|  2.56762e-06|  0.01%|    if len(arr) == 0:\n",
      "   219|         2|   3.8147e-06|  1.90735e-06|  0.00%|        return arr\n",
      "   220|         0|            0|            0|  0.00%|\n",
      "   221|     24962|     0.150396|  6.02501e-06|  0.02%|    if seed >= factorial(len(arr)):\n",
      "(call)|     24962|    0.0941145|  3.77031e-06|  0.02%|# /apps/open_spiel/open_spiel/python/examples/ubc_utils.py:188 factorial\n",
      "   222|         0|            0|            0|  0.00%|        raise ValueError(f'seed {seed} too large for array of length {len(arr)}')\n",
      "   223|         0|            0|            0|  0.00%|\n",
      "   224|         0|            0|            0|  0.00%|    # Fisher-Yates shuffle\n",
      "   225|    200341|       0.3991|   1.9921e-06|  0.06%|    permuted_arr = [a for a in arr]\n",
      "(call)|     24962|     0.248677|  9.96222e-06|  0.04%|# /apps/open_spiel/open_spiel/python/examples/ubc_utils.py:225 <listcomp>\n",
      "   226|     24962|     0.147658|  5.91531e-06|  0.02%|    swap_indices = convert_seed_to_swaps(seed)\n",
      "(call)|     24962|     0.706075|   2.8286e-05|  0.11%|# /apps/open_spiel/open_spiel/python/examples/ubc_utils.py:191 convert_seed_to_swaps\n",
      "   227|     24962|    0.0718501|  2.87838e-06|  0.01%|    swap_indices = swap_indices + [0] * (len(permuted_arr) - len(swap_indices))\n",
      "   228|         0|            0|            0|  0.00%|\n",
      "   229|    150417|     0.261768|  1.74028e-06|  0.04%|    for i, swap_idx in enumerate(swap_indices):\n",
      "   230|    125455|     0.216806|  1.72816e-06|  0.03%|        permuted_arr[i], permuted_arr[swap_idx] = permuted_arr[swap_idx], permuted_arr[i]\n",
      "   231|     24962|    0.0377495|  1.51228e-06|  0.01%|    return permuted_arr\n",
      "   232|         0|            0|            0|  0.00%|\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/distribution.py\n",
      "File duration: 4.81585s (0.78%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|import torch\n",
      "     2|         0|            0|            0|  0.00%|import warnings\n",
      "     3|         0|            0|            0|  0.00%|from torch.distributions import constraints\n",
      "     4|         0|            0|            0|  0.00%|from torch.distributions.utils import lazy_property\n",
      "     5|         0|            0|            0|  0.00%|from typing import Dict, Optional, Any\n",
      "     6|         0|            0|            0|  0.00%|\n",
      "     7|         0|            0|            0|  0.00%|\n",
      "     8|         0|            0|            0|  0.00%|class Distribution(object):\n",
      "     9|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "    10|         0|            0|            0|  0.00%|    Distribution is the abstract base class for probability distributions.\n",
      "    11|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    12|         0|            0|            0|  0.00%|\n",
      "    13|         0|            0|            0|  0.00%|    has_rsample = False\n",
      "    14|         0|            0|            0|  0.00%|    has_enumerate_support = False\n",
      "    15|         0|            0|            0|  0.00%|    _validate_args = __debug__\n",
      "    16|         0|            0|            0|  0.00%|\n",
      "    17|         0|            0|            0|  0.00%|    @staticmethod\n",
      "    18|         0|            0|            0|  0.00%|    def set_default_validate_args(value):\n",
      "    19|         0|            0|            0|  0.00%|        \"\"\"\n",
      "    20|         0|            0|            0|  0.00%|        Sets whether validation is enabled or disabled.\n",
      "    21|         0|            0|            0|  0.00%|\n",
      "    22|         0|            0|            0|  0.00%|        The default behavior mimics Python's ``assert`` statement: validation\n",
      "    23|         0|            0|            0|  0.00%|        is on by default, but is disabled if Python is run in optimized mode\n",
      "    24|         0|            0|            0|  0.00%|        (via ``python -O``). Validation may be expensive, so you may want to\n",
      "    25|         0|            0|            0|  0.00%|        disable it once a model is working.\n",
      "    26|         0|            0|            0|  0.00%|\n",
      "    27|         0|            0|            0|  0.00%|        Args:\n",
      "    28|         0|            0|            0|  0.00%|            value (bool): Whether to enable validation.\n",
      "    29|         0|            0|            0|  0.00%|        \"\"\"\n",
      "    30|         0|            0|            0|  0.00%|        if value not in [True, False]:\n",
      "    31|         0|            0|            0|  0.00%|            raise ValueError\n",
      "    32|         0|            0|            0|  0.00%|        Distribution._validate_args = value\n",
      "    33|         0|            0|            0|  0.00%|\n",
      "    34|     31590|     0.113248|  3.58492e-06|  0.02%|    def __init__(self, batch_shape=torch.Size(), event_shape=torch.Size(), validate_args=None):\n",
      "    35|     31590|      0.11239|  3.55776e-06|  0.02%|        self._batch_shape = batch_shape\n",
      "    36|     31590|    0.0919068|  2.90936e-06|  0.01%|        self._event_shape = event_shape\n",
      "    37|     31590|    0.0871103|  2.75753e-06|  0.01%|        if validate_args is not None:\n",
      "    38|         0|            0|            0|  0.00%|            self._validate_args = validate_args\n",
      "    39|     31590|     0.091469|  2.89551e-06|  0.01%|        if self._validate_args:\n",
      "    40|     31590|    0.0851192|   2.6945e-06|  0.01%|            try:\n",
      "    41|     31590|    0.0974307|  3.08423e-06|  0.02%|                arg_constraints = self.arg_constraints\n",
      "    42|         0|            0|            0|  0.00%|            except NotImplementedError:\n",
      "    43|         0|            0|            0|  0.00%|                arg_constraints = {}\n",
      "    44|         0|            0|            0|  0.00%|                warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n",
      "    45|         0|            0|            0|  0.00%|                              'Please set `arg_constraints = {}` or initialize the distribution ' +\n",
      "    46|         0|            0|            0|  0.00%|                              'with `validate_args=False` to turn off validation.')\n",
      "    47|     94770|     0.277462|  2.92774e-06|  0.04%|            for param, constraint in arg_constraints.items():\n",
      "    48|     63180|     0.450498|  7.13039e-06|  0.07%|                if constraints.is_dependent(constraint):\n",
      "(call)|     63180|     0.248411|  3.93179e-06|  0.04%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/constraints.py:142 is_dependent\n",
      "    49|         0|            0|            0|  0.00%|                    continue  # skip constraints that cannot be checked\n",
      "    50|     63180|     0.387585|  6.13461e-06|  0.06%|                if param not in self.__dict__ and isinstance(getattr(type(self), param), lazy_property):\n",
      "(call)|     31590|     0.499208|  1.58027e-05|  0.08%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/utils.py:106 __get__\n",
      "    51|     31590|    0.0831344|  2.63167e-06|  0.01%|                    continue  # skip checking lazily-constructed args\n",
      "    52|     31590|    0.0808666|  2.55988e-06|  0.01%|                value = getattr(self, param)\n",
      "    53|     31590|     0.245106|  7.75899e-06|  0.04%|                valid = constraint.check(value)\n",
      "(call)|     31590|      1.58063|  5.00356e-05|  0.25%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/constraints.py:208 check\n",
      "    54|     31590|     0.333456|  1.05558e-05|  0.05%|                if not valid.all():\n",
      "    55|         0|            0|            0|  0.00%|                    raise ValueError(\n",
      "    56|         0|            0|            0|  0.00%|                        f\"Expected parameter {param} \"\n",
      "    57|         0|            0|            0|  0.00%|                        f\"({type(value).__name__} of shape {tuple(value.shape)}) \"\n",
      "    58|         0|            0|            0|  0.00%|                        f\"of distribution {repr(self)} \"\n",
      "    59|         0|            0|            0|  0.00%|                        f\"to satisfy the constraint {repr(constraint)}, \"\n",
      "    60|         0|            0|            0|  0.00%|                        f\"but found invalid values:\\n{value}\"\n",
      "    61|         0|            0|            0|  0.00%|                    )\n",
      "    62|     31590|     0.109561|  3.46822e-06|  0.02%|        super(Distribution, self).__init__()\n",
      "    63|         0|            0|            0|  0.00%|\n",
      "    64|         0|            0|            0|  0.00%|    def expand(self, batch_shape, _instance=None):\n",
      "    65|         0|            0|            0|  0.00%|        \"\"\"\n",
      "    66|         0|            0|            0|  0.00%|        Returns a new distribution instance (or populates an existing instance\n",
      "    67|         0|            0|            0|  0.00%|        provided by a derived class) with batch dimensions expanded to\n",
      "    68|         0|            0|            0|  0.00%|        `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "    69|         0|            0|            0|  0.00%|        the distribution's parameters. As such, this does not allocate new\n",
      "    70|         0|            0|            0|  0.00%|        memory for the expanded distribution instance. Additionally,\n",
      "    71|         0|            0|            0|  0.00%|        this does not repeat any args checking or parameter broadcasting in\n",
      "    72|         0|            0|            0|  0.00%|        `__init__.py`, when an instance is first created.\n",
      "    73|         0|            0|            0|  0.00%|\n",
      "    74|         0|            0|            0|  0.00%|        Args:\n",
      "    75|         0|            0|            0|  0.00%|            batch_shape (torch.Size): the desired expanded size.\n",
      "    76|         0|            0|            0|  0.00%|            _instance: new instance provided by subclasses that\n",
      "    77|         0|            0|            0|  0.00%|                need to override `.expand`.\n",
      "    78|         0|            0|            0|  0.00%|\n",
      "    79|         0|            0|            0|  0.00%|        Returns:\n",
      "    80|         0|            0|            0|  0.00%|            New distribution instance with batch dimensions expanded to\n",
      "    81|         0|            0|            0|  0.00%|            `batch_size`.\n",
      "    82|         0|            0|            0|  0.00%|        \"\"\"\n",
      "    83|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "    84|         0|            0|            0|  0.00%|\n",
      "    85|         0|            0|            0|  0.00%|    @property\n",
      "    86|         0|            0|            0|  0.00%|    def batch_shape(self):\n",
      "    87|         0|            0|            0|  0.00%|        \"\"\"\n",
      "    88|         0|            0|            0|  0.00%|        Returns the shape over which parameters are batched.\n",
      "    89|         0|            0|            0|  0.00%|        \"\"\"\n",
      "    90|         0|            0|            0|  0.00%|        return self._batch_shape\n",
      "    91|         0|            0|            0|  0.00%|\n",
      "    92|         0|            0|            0|  0.00%|    @property\n",
      "    93|         0|            0|            0|  0.00%|    def event_shape(self):\n",
      "    94|         0|            0|            0|  0.00%|        \"\"\"\n",
      "    95|         0|            0|            0|  0.00%|        Returns the shape of a single sample (without batching).\n",
      "    96|         0|            0|            0|  0.00%|        \"\"\"\n",
      "    97|         0|            0|            0|  0.00%|        return self._event_shape\n",
      "    98|         0|            0|            0|  0.00%|\n",
      "    99|         0|            0|            0|  0.00%|    @property\n",
      "   100|         0|            0|            0|  0.00%|    def arg_constraints(self) -> Dict[str, constraints.Constraint]:\n",
      "   101|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   102|         0|            0|            0|  0.00%|        Returns a dictionary from argument names to\n",
      "   103|         0|            0|            0|  0.00%|        :class:`~torch.distributions.constraints.Constraint` objects that\n",
      "   104|         0|            0|            0|  0.00%|        should be satisfied by each argument of this distribution. Args that\n",
      "   105|         0|            0|            0|  0.00%|        are not tensors need not appear in this dict.\n",
      "   106|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   107|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "   108|         0|            0|            0|  0.00%|\n",
      "   109|         0|            0|            0|  0.00%|    @property\n",
      "   110|         0|            0|            0|  0.00%|    def support(self) -> Optional[Any]:\n",
      "   111|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   112|         0|            0|            0|  0.00%|        Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "   113|         0|            0|            0|  0.00%|        representing this distribution's support.\n",
      "   114|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   115|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "   116|         0|            0|            0|  0.00%|\n",
      "   117|         0|            0|            0|  0.00%|    @property\n",
      "   118|         0|            0|            0|  0.00%|    def mean(self):\n",
      "   119|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   120|         0|            0|            0|  0.00%|        Returns the mean of the distribution.\n",
      "   121|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   122|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "   123|         0|            0|            0|  0.00%|\n",
      "   124|         0|            0|            0|  0.00%|    @property\n",
      "   125|         0|            0|            0|  0.00%|    def mode(self):\n",
      "   126|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   127|         0|            0|            0|  0.00%|        Returns the mode of the distribution.\n",
      "   128|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   129|         0|            0|            0|  0.00%|        raise NotImplementedError(f\"{self.__class__} does not implement mode\")\n",
      "   130|         0|            0|            0|  0.00%|\n",
      "   131|         0|            0|            0|  0.00%|    @property\n",
      "   132|         0|            0|            0|  0.00%|    def variance(self):\n",
      "   133|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   134|         0|            0|            0|  0.00%|        Returns the variance of the distribution.\n",
      "   135|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   136|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "   137|         0|            0|            0|  0.00%|\n",
      "   138|         0|            0|            0|  0.00%|    @property\n",
      "   139|         0|            0|            0|  0.00%|    def stddev(self):\n",
      "   140|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   141|         0|            0|            0|  0.00%|        Returns the standard deviation of the distribution.\n",
      "   142|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   143|         0|            0|            0|  0.00%|        return self.variance.sqrt()\n",
      "   144|         0|            0|            0|  0.00%|\n",
      "   145|         0|            0|            0|  0.00%|    def sample(self, sample_shape=torch.Size()):\n",
      "   146|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   147|         0|            0|            0|  0.00%|        Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "   148|         0|            0|            0|  0.00%|        samples if the distribution parameters are batched.\n",
      "   149|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   150|         0|            0|            0|  0.00%|        with torch.no_grad():\n",
      "   151|         0|            0|            0|  0.00%|            return self.rsample(sample_shape)\n",
      "   152|         0|            0|            0|  0.00%|\n",
      "   153|         0|            0|            0|  0.00%|    def rsample(self, sample_shape=torch.Size()):\n",
      "   154|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   155|         0|            0|            0|  0.00%|        Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "   156|         0|            0|            0|  0.00%|        shaped batch of reparameterized samples if the distribution parameters\n",
      "   157|         0|            0|            0|  0.00%|        are batched.\n",
      "   158|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   159|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "   160|         0|            0|            0|  0.00%|\n",
      "   161|         0|            0|            0|  0.00%|    def sample_n(self, n):\n",
      "   162|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   163|         0|            0|            0|  0.00%|        Generates n samples or n batches of samples if the distribution\n",
      "   164|         0|            0|            0|  0.00%|        parameters are batched.\n",
      "   165|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   166|         0|            0|            0|  0.00%|        warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n",
      "   167|         0|            0|            0|  0.00%|        return self.sample(torch.Size((n,)))\n",
      "   168|         0|            0|            0|  0.00%|\n",
      "   169|         0|            0|            0|  0.00%|    def log_prob(self, value):\n",
      "   170|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   171|         0|            0|            0|  0.00%|        Returns the log of the probability density/mass function evaluated at\n",
      "   172|         0|            0|            0|  0.00%|        `value`.\n",
      "   173|         0|            0|            0|  0.00%|\n",
      "   174|         0|            0|            0|  0.00%|        Args:\n",
      "   175|         0|            0|            0|  0.00%|            value (Tensor):\n",
      "   176|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   177|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "   178|         0|            0|            0|  0.00%|\n",
      "   179|         0|            0|            0|  0.00%|    def cdf(self, value):\n",
      "   180|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   181|         0|            0|            0|  0.00%|        Returns the cumulative density/mass function evaluated at\n",
      "   182|         0|            0|            0|  0.00%|        `value`.\n",
      "   183|         0|            0|            0|  0.00%|\n",
      "   184|         0|            0|            0|  0.00%|        Args:\n",
      "   185|         0|            0|            0|  0.00%|            value (Tensor):\n",
      "   186|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   187|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "   188|         0|            0|            0|  0.00%|\n",
      "   189|         0|            0|            0|  0.00%|    def icdf(self, value):\n",
      "   190|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   191|         0|            0|            0|  0.00%|        Returns the inverse cumulative density/mass function evaluated at\n",
      "   192|         0|            0|            0|  0.00%|        `value`.\n",
      "   193|         0|            0|            0|  0.00%|\n",
      "   194|         0|            0|            0|  0.00%|        Args:\n",
      "   195|         0|            0|            0|  0.00%|            value (Tensor):\n",
      "   196|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   197|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "   198|         0|            0|            0|  0.00%|\n",
      "   199|         0|            0|            0|  0.00%|    def enumerate_support(self, expand=True):\n",
      "   200|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   201|         0|            0|            0|  0.00%|        Returns tensor containing all values supported by a discrete\n",
      "   202|         0|            0|            0|  0.00%|        distribution. The result will enumerate over dimension 0, so the shape\n",
      "   203|         0|            0|            0|  0.00%|        of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "   204|         0|            0|            0|  0.00%|        (where `event_shape = ()` for univariate distributions).\n",
      "   205|         0|            0|            0|  0.00%|\n",
      "   206|         0|            0|            0|  0.00%|        Note that this enumerates over all batched tensors in lock-step\n",
      "   207|         0|            0|            0|  0.00%|        `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "   208|         0|            0|            0|  0.00%|        along dim 0, but with the remaining batch dimensions being\n",
      "   209|         0|            0|            0|  0.00%|        singleton dimensions, `[[0], [1], ..`.\n",
      "   210|         0|            0|            0|  0.00%|\n",
      "   211|         0|            0|            0|  0.00%|        To iterate over the full Cartesian product use\n",
      "   212|         0|            0|            0|  0.00%|        `itertools.product(m.enumerate_support())`.\n",
      "   213|         0|            0|            0|  0.00%|\n",
      "   214|         0|            0|            0|  0.00%|        Args:\n",
      "   215|         0|            0|            0|  0.00%|            expand (bool): whether to expand the support over the\n",
      "   216|         0|            0|            0|  0.00%|                batch dims to match the distribution's `batch_shape`.\n",
      "   217|         0|            0|            0|  0.00%|\n",
      "   218|         0|            0|            0|  0.00%|        Returns:\n",
      "   219|         0|            0|            0|  0.00%|            Tensor iterating over dimension 0.\n",
      "   220|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   221|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "   222|         0|            0|            0|  0.00%|\n",
      "   223|         0|            0|            0|  0.00%|    def entropy(self):\n",
      "   224|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   225|         0|            0|            0|  0.00%|        Returns entropy of distribution, batched over batch_shape.\n",
      "   226|         0|            0|            0|  0.00%|\n",
      "   227|         0|            0|            0|  0.00%|        Returns:\n",
      "   228|         0|            0|            0|  0.00%|            Tensor of shape batch_shape.\n",
      "   229|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   230|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "   231|         0|            0|            0|  0.00%|\n",
      "   232|         0|            0|            0|  0.00%|    def perplexity(self):\n",
      "   233|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   234|         0|            0|            0|  0.00%|        Returns perplexity of distribution, batched over batch_shape.\n",
      "   235|         0|            0|            0|  0.00%|\n",
      "   236|         0|            0|            0|  0.00%|        Returns:\n",
      "   237|         0|            0|            0|  0.00%|            Tensor of shape batch_shape.\n",
      "   238|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   239|         0|            0|            0|  0.00%|        return torch.exp(self.entropy())\n",
      "   240|         0|            0|            0|  0.00%|\n",
      "   241|     24960|    0.0434039|  1.73894e-06|  0.01%|    def _extended_shape(self, sample_shape=torch.Size()):\n",
      "   242|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   243|         0|            0|            0|  0.00%|        Returns the size of the sample returned by the distribution, given\n",
      "   244|         0|            0|            0|  0.00%|        a `sample_shape`. Note, that the batch and event shapes of a distribution\n",
      "   245|         0|            0|            0|  0.00%|        instance are fixed at the time of construction. If this is empty, the\n",
      "   246|         0|            0|            0|  0.00%|        returned shape is upcast to (1,).\n",
      "   247|         0|            0|            0|  0.00%|\n",
      "   248|         0|            0|            0|  0.00%|        Args:\n",
      "   249|         0|            0|            0|  0.00%|            sample_shape (torch.Size): the size of the sample to be drawn.\n",
      "   250|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   251|     24960|    0.0548992|  2.19949e-06|  0.01%|        if not isinstance(sample_shape, torch.Size):\n",
      "   252|         0|            0|            0|  0.00%|            sample_shape = torch.Size(sample_shape)\n",
      "   253|     24960|    0.0766413|  3.07057e-06|  0.01%|        return sample_shape + self._batch_shape + self._event_shape\n",
      "   254|         0|            0|            0|  0.00%|\n",
      "   255|     31590|    0.0980253|  3.10305e-06|  0.02%|    def _validate_sample(self, value):\n",
      "   256|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   257|         0|            0|            0|  0.00%|        Argument validation for distribution methods such as `log_prob`,\n",
      "   258|         0|            0|            0|  0.00%|        `cdf` and `icdf`. The rightmost dimensions of a value to be\n",
      "   259|         0|            0|            0|  0.00%|        scored via these methods must agree with the distribution's batch\n",
      "   260|         0|            0|            0|  0.00%|        and event shapes.\n",
      "   261|         0|            0|            0|  0.00%|\n",
      "   262|         0|            0|            0|  0.00%|        Args:\n",
      "   263|         0|            0|            0|  0.00%|            value (Tensor): the tensor whose log probability is to be\n",
      "   264|         0|            0|            0|  0.00%|                computed by the `log_prob` method.\n",
      "   265|         0|            0|            0|  0.00%|        Raises\n",
      "   266|         0|            0|            0|  0.00%|            ValueError: when the rightmost dimensions of `value` do not match the\n",
      "   267|         0|            0|            0|  0.00%|                distribution's batch and event shapes.\n",
      "   268|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   269|     31590|     0.107333|  3.39769e-06|  0.02%|        if not isinstance(value, torch.Tensor):\n",
      "   270|         0|            0|            0|  0.00%|            raise ValueError('The value argument to log_prob must be a Tensor')\n",
      "   271|         0|            0|            0|  0.00%|\n",
      "   272|     31590|      0.14041|  4.44475e-06|  0.02%|        event_dim_start = len(value.size()) - len(self._event_shape)\n",
      "   273|     31590|     0.140327|  4.44213e-06|  0.02%|        if value.size()[event_dim_start:] != self._event_shape:\n",
      "   274|         0|            0|            0|  0.00%|            raise ValueError('The right-most size of value must match event_shape: {} vs {}.'.\n",
      "   275|         0|            0|            0|  0.00%|                             format(value.size(), self._event_shape))\n",
      "   276|         0|            0|            0|  0.00%|\n",
      "   277|     31590|     0.106005|  3.35566e-06|  0.02%|        actual_shape = value.size()\n",
      "   278|     31590|     0.107469|  3.40198e-06|  0.02%|        expected_shape = self._batch_shape + self._event_shape\n",
      "   279|     63180|     0.227195|    3.596e-06|  0.04%|        for i, j in zip(reversed(actual_shape), reversed(expected_shape)):\n",
      "   280|     31590|    0.0920792|  2.91482e-06|  0.01%|            if i != 1 and j != 1 and i != j:\n",
      "   281|         0|            0|            0|  0.00%|                raise ValueError('Value is not broadcastable with batch_shape+event_shape: {} vs {}.'.\n",
      "   282|         0|            0|            0|  0.00%|                                 format(actual_shape, expected_shape))\n",
      "   283|     31590|    0.0730267|   2.3117e-06|  0.01%|        try:\n",
      "   284|     31590|     0.227931|  7.21528e-06|  0.04%|            support = self.support\n",
      "(call)|     31590|     0.551718|   1.7465e-05|  0.09%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/categorical.py:84 support\n",
      "   285|         0|            0|            0|  0.00%|        except NotImplementedError:\n",
      "   286|         0|            0|            0|  0.00%|            warnings.warn(f'{self.__class__} does not define `support` to enable ' +\n",
      "   287|         0|            0|            0|  0.00%|                          'sample validation. Please initialize the distribution with ' +\n",
      "   288|         0|            0|            0|  0.00%|                          '`validate_args=False` to turn off validation.')\n",
      "   289|         0|            0|            0|  0.00%|            return\n",
      "   290|     31590|    0.0860629|  2.72437e-06|  0.01%|        assert support is not None\n",
      "   291|     31590|     0.259199|  8.20509e-06|  0.04%|        valid = support.check(value)\n",
      "(call)|     31590|      1.32938|  4.20823e-05|  0.21%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/constraints.py:256 check\n",
      "   292|     31590|     0.329505|  1.04307e-05|  0.05%|        if not valid.all():\n",
      "   293|         0|            0|            0|  0.00%|            raise ValueError(\n",
      "   294|         0|            0|            0|  0.00%|                \"Expected value argument \"\n",
      "   295|         0|            0|            0|  0.00%|                f\"({type(value).__name__} of shape {tuple(value.shape)}) \"\n",
      "   296|         0|            0|            0|  0.00%|                f\"to be within the support ({repr(support)}) \"\n",
      "   297|         0|            0|            0|  0.00%|                f\"of the distribution {repr(self)}, \"\n",
      "   298|         0|            0|            0|  0.00%|                f\"but found invalid values:\\n{value}\"\n",
      "   299|         0|            0|            0|  0.00%|            )\n",
      "   300|         0|            0|            0|  0.00%|\n",
      "   301|         0|            0|            0|  0.00%|    def _get_checked_instance(self, cls, _instance=None):\n",
      "   302|         0|            0|            0|  0.00%|        if _instance is None and type(self).__init__ != cls.__init__:\n",
      "   303|         0|            0|            0|  0.00%|            raise NotImplementedError(\"Subclass {} of {} that defines a custom __init__ method \"\n",
      "   304|         0|            0|            0|  0.00%|                                      \"must also define a custom .expand() method.\".\n",
      "   305|         0|            0|            0|  0.00%|                                      format(self.__class__.__name__, cls.__name__))\n",
      "   306|         0|            0|            0|  0.00%|        return self.__new__(type(self)) if _instance is None else _instance\n",
      "   307|         0|            0|            0|  0.00%|\n",
      "   308|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "   309|         0|            0|            0|  0.00%|        param_names = [k for k, _ in self.arg_constraints.items() if k in self.__dict__]\n",
      "   310|         0|            0|            0|  0.00%|        args_string = ', '.join(['{}: {}'.format(p, self.__dict__[p]\n",
      "   311|         0|            0|            0|  0.00%|                                if self.__dict__[p].numel() == 1\n",
      "   312|         0|            0|            0|  0.00%|                                else self.__dict__[p].size()) for p in param_names])\n",
      "   313|         0|            0|            0|  0.00%|        return self.__class__.__name__ + '(' + args_string + ')'\n",
      "File: /apps/open_spiel/open_spiel/python/vector_env.py\n",
      "File duration: 4.56547s (0.74%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|class SyncVectorEnv(object):\n",
      "     2|         0|            0|            0|  0.00%|    \"\"\"\n",
      "     3|         0|            0|            0|  0.00%|    A vectorized RL Environment. This environment is synchronized - games do not execute in parallel. Speedups are realized by calling models on many game states simultaneously.\n",
      "     4|         0|            0|            0|  0.00%|    \"\"\"\n",
      "     5|         0|            0|            0|  0.00%|    def __init__(self, envs):\n",
      "     6|         0|            0|            0|  0.00%|        if not isinstance(envs, list):\n",
      "     7|         0|            0|            0|  0.00%|            raise ValueError(\"Need to call this with a list of rl_environment.Environment objects\")\n",
      "     8|         0|            0|            0|  0.00%|        self.envs = envs\n",
      "     9|         0|            0|            0|  0.00%|\n",
      "    10|         1|  3.33786e-06|  3.33786e-06|  0.00%|    def __len__(self):\n",
      "    11|         1|  9.29832e-06|  9.29832e-06|  0.00%|        return len(self.envs)\n",
      "    12|         0|            0|            0|  0.00%|\n",
      "    13|         0|            0|            0|  0.00%|    def observation_spec(self):\n",
      "    14|         0|            0|            0|  0.00%|        return self.envs[0].observation_spec()\n",
      "    15|         0|            0|            0|  0.00%|\n",
      "    16|         0|            0|            0|  0.00%|    @property\n",
      "    17|         0|            0|            0|  0.00%|    def num_players(self):\n",
      "    18|         0|            0|            0|  0.00%|        return self.envs[0].num_players\n",
      "    19|         0|            0|            0|  0.00%|\n",
      "    20|     24960|    0.0983884|  3.94184e-06|  0.02%|    def step(self, step_outputs, reset_if_done=False):\n",
      "    21|         0|            0|            0|  0.00%|        '''\n",
      "    22|         0|            0|            0|  0.00%|        reset_if_done: if True, automatically reset the environment when the epsiode ends\n",
      "    23|         0|            0|            0|  0.00%|        '''\n",
      "    24|     24960|    0.0916946|  3.67366e-06|  0.01%|        if not isinstance(step_outputs, list):\n",
      "    25|         0|            0|            0|  0.00%|            step_outputs = [step_outputs]\n",
      "    26|         0|            0|            0|  0.00%|\n",
      "    27|    174720|      1.61221|  9.22741e-06|  0.26%|        time_steps = [self.envs[i].step([step_outputs[i].action]) for i in range(len(self.envs))]\n",
      "(call)|     99840|      318.935|   0.00319446| 51.35%|# /apps/open_spiel/open_spiel/python/env_decorator.py:29 step\n",
      "(call)|     24960|      320.289|    0.0128321| 51.57%|# /apps/open_spiel/open_spiel/python/vector_env.py:27 <listcomp>\n",
      "    28|    174720|     0.400411|  2.29173e-06|  0.06%|        reward = [step.rewards for step in time_steps]\n",
      "(call)|     24960|     0.229717|  9.20341e-06|  0.04%|# /apps/open_spiel/open_spiel/python/vector_env.py:28 <listcomp>\n",
      "    29|    174720|     0.673349|  3.85388e-06|  0.11%|        done = [step.last() for step in time_steps]\n",
      "(call)|     99840|     0.289053|  2.89516e-06|  0.05%|# /apps/open_spiel/open_spiel/python/rl_environment.py:94 last\n",
      "(call)|     24960|     0.810728|  3.24811e-05|  0.13%|# /apps/open_spiel/open_spiel/python/vector_env.py:29 <listcomp>\n",
      "    30|     24960|    0.0527551|  2.11359e-06|  0.01%|        unreset_time_steps = time_steps # Copy these because you may want to look at the unreset versions to extract information from them\n",
      "    31|         0|            0|            0|  0.00%|\n",
      "    32|     24960|    0.0502048|  2.01141e-06|  0.01%|        if reset_if_done:\n",
      "    33|     24960|     0.217515|  8.71452e-06|  0.04%|            time_steps = self.reset(envs_to_reset=done)\n",
      "(call)|     24960|      171.664|   0.00687755| 27.64%|# /apps/open_spiel/open_spiel/python/vector_env.py:37 reset\n",
      "    34|         0|            0|            0|  0.00%|\n",
      "    35|     24960|    0.0587547|  2.35395e-06|  0.01%|        return time_steps, reward, done, unreset_time_steps\n",
      "    36|         0|            0|            0|  0.00%|\n",
      "    37|     24961|    0.0551152|  2.20805e-06|  0.01%|    def reset(self, envs_to_reset=None):\n",
      "    38|     24961|    0.0545695|  2.18619e-06|  0.01%|        if envs_to_reset is None:\n",
      "    39|         7|  2.12193e-05|  3.03132e-06|  0.00%|            envs_to_reset = [True for _ in range(len(self.envs))]\n",
      "(call)|         1|  1.26362e-05|  1.26362e-05|  0.00%|# /apps/open_spiel/open_spiel/python/vector_env.py:39 <listcomp>\n",
      "    40|         0|            0|            0|  0.00%|\n",
      "    41|    174727|      1.14961|  6.57945e-06|  0.19%|        time_steps = [self.envs[i].reset() if envs_to_reset[i] else self.envs[i].get_time_step() for i in range(len(self.envs))]\n",
      "(call)|     24958|      74.5668|   0.00298769| 12.01%|# /apps/open_spiel/open_spiel/python/env_decorator.py:33 reset\n",
      "(call)|     24961|      171.311|   0.00686314| 27.58%|# /apps/open_spiel/open_spiel/python/vector_env.py:41 <listcomp>\n",
      "(call)|     74886|      95.7996|   0.00127927| 15.42%|# /apps/open_spiel/open_spiel/python/env_decorator.py:48 get_time_step\n",
      "    42|     24961|    0.0508535|  2.03732e-06|  0.01%|        return time_steps\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/container.py\n",
      "File duration: 4.37166s (0.70%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|import warnings\n",
      "     2|         0|            0|            0|  0.00%|from collections import OrderedDict, abc as container_abcs\n",
      "     3|         0|            0|            0|  0.00%|from itertools import chain, islice\n",
      "     4|         0|            0|            0|  0.00%|import operator\n",
      "     5|         0|            0|            0|  0.00%|\n",
      "     6|         0|            0|            0|  0.00%|import torch\n",
      "     7|         0|            0|            0|  0.00%|from .module import Module\n",
      "     8|         0|            0|            0|  0.00%|from ..parameter import Parameter\n",
      "     9|         0|            0|            0|  0.00%|from torch._jit_internal import _copy_to_script_wrapper\n",
      "    10|         0|            0|            0|  0.00%|\n",
      "    11|         0|            0|            0|  0.00%|from typing import Any, Dict, Iterable, Iterator, Mapping, Optional, overload, Tuple, TypeVar, Union\n",
      "    12|         0|            0|            0|  0.00%|\n",
      "    13|         0|            0|            0|  0.00%|T = TypeVar('T', bound=Module)\n",
      "    14|         0|            0|            0|  0.00%|\n",
      "    15|         0|            0|            0|  0.00%|\n",
      "    16|         0|            0|            0|  0.00%|class Container(Module):\n",
      "    17|         0|            0|            0|  0.00%|\n",
      "    18|         0|            0|            0|  0.00%|    def __init__(self, **kwargs: Any) -> None:\n",
      "    19|         0|            0|            0|  0.00%|        super(Container, self).__init__()\n",
      "    20|         0|            0|            0|  0.00%|        # DeprecationWarning is ignored by default <sigh>\n",
      "    21|         0|            0|            0|  0.00%|        warnings.warn(\"nn.Container is deprecated. All of it's functionality \"\n",
      "    22|         0|            0|            0|  0.00%|                      \"is now implemented in nn.Module. Subclass that instead.\")\n",
      "    23|         0|            0|            0|  0.00%|        for key, value in kwargs.items():\n",
      "    24|         0|            0|            0|  0.00%|            self.add_module(key, value)\n",
      "    25|         0|            0|            0|  0.00%|\n",
      "    26|         0|            0|            0|  0.00%|\n",
      "    27|         0|            0|            0|  0.00%|class Sequential(Module):\n",
      "    28|         0|            0|            0|  0.00%|    r\"\"\"A sequential container.\n",
      "    29|         0|            0|            0|  0.00%|    Modules will be added to it in the order they are passed in the\n",
      "    30|         0|            0|            0|  0.00%|    constructor. Alternatively, an ``OrderedDict`` of modules can be\n",
      "    31|         0|            0|            0|  0.00%|    passed in. The ``forward()`` method of ``Sequential`` accepts any\n",
      "    32|         0|            0|            0|  0.00%|    input and forwards it to the first module it contains. It then\n",
      "    33|         0|            0|            0|  0.00%|    \"chains\" outputs to inputs sequentially for each subsequent module,\n",
      "    34|         0|            0|            0|  0.00%|    finally returning the output of the last module.\n",
      "    35|         0|            0|            0|  0.00%|\n",
      "    36|         0|            0|            0|  0.00%|    The value a ``Sequential`` provides over manually calling a sequence\n",
      "    37|         0|            0|            0|  0.00%|    of modules is that it allows treating the whole container as a\n",
      "    38|         0|            0|            0|  0.00%|    single module, such that performing a transformation on the\n",
      "    39|         0|            0|            0|  0.00%|    ``Sequential`` applies to each of the modules it stores (which are\n",
      "    40|         0|            0|            0|  0.00%|    each a registered submodule of the ``Sequential``).\n",
      "    41|         0|            0|            0|  0.00%|\n",
      "    42|         0|            0|            0|  0.00%|    What's the difference between a ``Sequential`` and a\n",
      "    43|         0|            0|            0|  0.00%|    :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it\n",
      "    44|         0|            0|            0|  0.00%|    sounds like--a list for storing ``Module`` s! On the other hand,\n",
      "    45|         0|            0|            0|  0.00%|    the layers in a ``Sequential`` are connected in a cascading way.\n",
      "    46|         0|            0|            0|  0.00%|\n",
      "    47|         0|            0|            0|  0.00%|    Example::\n",
      "    48|         0|            0|            0|  0.00%|\n",
      "    49|         0|            0|            0|  0.00%|        # Using Sequential to create a small model. When `model` is run,\n",
      "    50|         0|            0|            0|  0.00%|        # input will first be passed to `Conv2d(1,20,5)`. The output of\n",
      "    51|         0|            0|            0|  0.00%|        # `Conv2d(1,20,5)` will be used as the input to the first\n",
      "    52|         0|            0|            0|  0.00%|        # `ReLU`; the output of the first `ReLU` will become the input\n",
      "    53|         0|            0|            0|  0.00%|        # for `Conv2d(20,64,5)`. Finally, the output of\n",
      "    54|         0|            0|            0|  0.00%|        # `Conv2d(20,64,5)` will be used as input to the second `ReLU`\n",
      "    55|         0|            0|            0|  0.00%|        model = nn.Sequential(\n",
      "    56|         0|            0|            0|  0.00%|                  nn.Conv2d(1,20,5),\n",
      "    57|         0|            0|            0|  0.00%|                  nn.ReLU(),\n",
      "    58|         0|            0|            0|  0.00%|                  nn.Conv2d(20,64,5),\n",
      "    59|         0|            0|            0|  0.00%|                  nn.ReLU()\n",
      "    60|         0|            0|            0|  0.00%|                )\n",
      "    61|         0|            0|            0|  0.00%|\n",
      "    62|         0|            0|            0|  0.00%|        # Using Sequential with OrderedDict. This is functionally the\n",
      "    63|         0|            0|            0|  0.00%|        # same as the above code\n",
      "    64|         0|            0|            0|  0.00%|        model = nn.Sequential(OrderedDict([\n",
      "    65|         0|            0|            0|  0.00%|                  ('conv1', nn.Conv2d(1,20,5)),\n",
      "    66|         0|            0|            0|  0.00%|                  ('relu1', nn.ReLU()),\n",
      "    67|         0|            0|            0|  0.00%|                  ('conv2', nn.Conv2d(20,64,5)),\n",
      "    68|         0|            0|            0|  0.00%|                  ('relu2', nn.ReLU())\n",
      "    69|         0|            0|            0|  0.00%|                ]))\n",
      "    70|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    71|         0|            0|            0|  0.00%|\n",
      "    72|         0|            0|            0|  0.00%|    _modules: Dict[str, Module]  # type: ignore[assignment]\n",
      "    73|         0|            0|            0|  0.00%|\n",
      "    74|         0|            0|            0|  0.00%|    @overload\n",
      "    75|         0|            0|            0|  0.00%|    def __init__(self, *args: Module) -> None:\n",
      "    76|         0|            0|            0|  0.00%|        ...\n",
      "    77|         0|            0|            0|  0.00%|\n",
      "    78|         0|            0|            0|  0.00%|    @overload\n",
      "    79|         0|            0|            0|  0.00%|    def __init__(self, arg: 'OrderedDict[str, Module]') -> None:\n",
      "    80|         0|            0|            0|  0.00%|        ...\n",
      "    81|         0|            0|            0|  0.00%|\n",
      "    82|         0|            0|            0|  0.00%|    def __init__(self, *args):\n",
      "    83|         0|            0|            0|  0.00%|        super(Sequential, self).__init__()\n",
      "    84|         0|            0|            0|  0.00%|        if len(args) == 1 and isinstance(args[0], OrderedDict):\n",
      "    85|         0|            0|            0|  0.00%|            for key, module in args[0].items():\n",
      "    86|         0|            0|            0|  0.00%|                self.add_module(key, module)\n",
      "    87|         0|            0|            0|  0.00%|        else:\n",
      "    88|         0|            0|            0|  0.00%|            for idx, module in enumerate(args):\n",
      "    89|         0|            0|            0|  0.00%|                self.add_module(str(idx), module)\n",
      "    90|         0|            0|            0|  0.00%|\n",
      "    91|         0|            0|            0|  0.00%|    def _get_item_by_idx(self, iterator, idx) -> T:\n",
      "    92|         0|            0|            0|  0.00%|        \"\"\"Get the idx-th item of the iterator\"\"\"\n",
      "    93|         0|            0|            0|  0.00%|        size = len(self)\n",
      "    94|         0|            0|            0|  0.00%|        idx = operator.index(idx)\n",
      "    95|         0|            0|            0|  0.00%|        if not -size <= idx < size:\n",
      "    96|         0|            0|            0|  0.00%|            raise IndexError('index {} is out of range'.format(idx))\n",
      "    97|         0|            0|            0|  0.00%|        idx %= size\n",
      "    98|         0|            0|            0|  0.00%|        return next(islice(iterator, idx, None))\n",
      "    99|         0|            0|            0|  0.00%|\n",
      "   100|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper\n",
      "   101|         0|            0|            0|  0.00%|    def __getitem__(self, idx) -> Union['Sequential', T]:\n",
      "   102|         0|            0|            0|  0.00%|        if isinstance(idx, slice):\n",
      "   103|         0|            0|            0|  0.00%|            return self.__class__(OrderedDict(list(self._modules.items())[idx]))\n",
      "   104|         0|            0|            0|  0.00%|        else:\n",
      "   105|         0|            0|            0|  0.00%|            return self._get_item_by_idx(self._modules.values(), idx)\n",
      "   106|         0|            0|            0|  0.00%|\n",
      "   107|         0|            0|            0|  0.00%|    def __setitem__(self, idx: int, module: Module) -> None:\n",
      "   108|         0|            0|            0|  0.00%|        key: str = self._get_item_by_idx(self._modules.keys(), idx)\n",
      "   109|         0|            0|            0|  0.00%|        return setattr(self, key, module)\n",
      "   110|         0|            0|            0|  0.00%|\n",
      "   111|         0|            0|            0|  0.00%|    def __delitem__(self, idx: Union[slice, int]) -> None:\n",
      "   112|         0|            0|            0|  0.00%|        if isinstance(idx, slice):\n",
      "   113|         0|            0|            0|  0.00%|            for key in list(self._modules.keys())[idx]:\n",
      "   114|         0|            0|            0|  0.00%|                delattr(self, key)\n",
      "   115|         0|            0|            0|  0.00%|        else:\n",
      "   116|         0|            0|            0|  0.00%|            key = self._get_item_by_idx(self._modules.keys(), idx)\n",
      "   117|         0|            0|            0|  0.00%|            delattr(self, key)\n",
      "   118|         0|            0|            0|  0.00%|\n",
      "   119|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper\n",
      "   120|         0|            0|            0|  0.00%|    def __len__(self) -> int:\n",
      "   121|         0|            0|            0|  0.00%|        return len(self._modules)\n",
      "   122|         0|            0|            0|  0.00%|\n",
      "   123|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper\n",
      "   124|         0|            0|            0|  0.00%|    def __dir__(self):\n",
      "   125|         0|            0|            0|  0.00%|        keys = super(Sequential, self).__dir__()\n",
      "   126|         0|            0|            0|  0.00%|        keys = [key for key in keys if not key.isdigit()]\n",
      "   127|         0|            0|            0|  0.00%|        return keys\n",
      "   128|         0|            0|            0|  0.00%|\n",
      "   129|     63570|    0.0791821|  1.24559e-06|  0.01%|    @_copy_to_script_wrapper\n",
      "   130|         0|            0|            0|  0.00%|    def __iter__(self) -> Iterator[Module]:\n",
      "   131|     63570|     0.175108|  2.75457e-06|  0.03%|        return iter(self._modules.values())\n",
      "   132|         0|            0|            0|  0.00%|\n",
      "   133|         0|            0|            0|  0.00%|    # NB: We can't really type check this function as the type of input\n",
      "   134|         0|            0|            0|  0.00%|    # may change dynamically (as is tested in\n",
      "   135|         0|            0|            0|  0.00%|    # TestScript.test_sequential_intermediary_types).  Cannot annotate\n",
      "   136|         0|            0|            0|  0.00%|    # with Any as TorchScript expects a more precise type\n",
      "   137|     63570|     0.112968|  1.77706e-06|  0.02%|    def forward(self, input):\n",
      "   138|    381420|      1.12381|  2.94639e-06|  0.18%|        for module in self:\n",
      "(call)|     63570|      0.25429|  4.00016e-06|  0.04%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/container.py:129 __iter__\n",
      "   139|    317850|      2.77244|  8.72247e-06|  0.45%|            input = module(input)\n",
      "(call)|    317850|      22.8449|  7.18731e-05|  3.68%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1124 _call_impl\n",
      "   140|     63570|     0.108152|  1.70131e-06|  0.02%|        return input\n",
      "   141|         0|            0|            0|  0.00%|\n",
      "   142|         0|            0|            0|  0.00%|    def append(self, module: Module) -> 'Sequential':\n",
      "   143|         0|            0|            0|  0.00%|        r\"\"\"Appends a given module to the end.\n",
      "   144|         0|            0|            0|  0.00%|\n",
      "   145|         0|            0|            0|  0.00%|        Args:\n",
      "   146|         0|            0|            0|  0.00%|            module (nn.Module): module to append\n",
      "   147|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   148|         0|            0|            0|  0.00%|        self.add_module(str(len(self)), module)\n",
      "   149|         0|            0|            0|  0.00%|        return self\n",
      "   150|         0|            0|            0|  0.00%|\n",
      "   151|         0|            0|            0|  0.00%|\n",
      "   152|         0|            0|            0|  0.00%|class ModuleList(Module):\n",
      "   153|         0|            0|            0|  0.00%|    r\"\"\"Holds submodules in a list.\n",
      "   154|         0|            0|            0|  0.00%|\n",
      "   155|         0|            0|            0|  0.00%|    :class:`~torch.nn.ModuleList` can be indexed like a regular Python list, but\n",
      "   156|         0|            0|            0|  0.00%|    modules it contains are properly registered, and will be visible by all\n",
      "   157|         0|            0|            0|  0.00%|    :class:`~torch.nn.Module` methods.\n",
      "   158|         0|            0|            0|  0.00%|\n",
      "   159|         0|            0|            0|  0.00%|    Args:\n",
      "   160|         0|            0|            0|  0.00%|        modules (iterable, optional): an iterable of modules to add\n",
      "   161|         0|            0|            0|  0.00%|\n",
      "   162|         0|            0|            0|  0.00%|    Example::\n",
      "   163|         0|            0|            0|  0.00%|\n",
      "   164|         0|            0|            0|  0.00%|        class MyModule(nn.Module):\n",
      "   165|         0|            0|            0|  0.00%|            def __init__(self):\n",
      "   166|         0|            0|            0|  0.00%|                super(MyModule, self).__init__()\n",
      "   167|         0|            0|            0|  0.00%|                self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n",
      "   168|         0|            0|            0|  0.00%|\n",
      "   169|         0|            0|            0|  0.00%|            def forward(self, x):\n",
      "   170|         0|            0|            0|  0.00%|                # ModuleList can act as an iterable, or be indexed using ints\n",
      "   171|         0|            0|            0|  0.00%|                for i, l in enumerate(self.linears):\n",
      "   172|         0|            0|            0|  0.00%|                    x = self.linears[i // 2](x) + l(x)\n",
      "   173|         0|            0|            0|  0.00%|                return x\n",
      "   174|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   175|         0|            0|            0|  0.00%|\n",
      "   176|         0|            0|            0|  0.00%|    _modules: Dict[str, Module]  # type: ignore[assignment]\n",
      "   177|         0|            0|            0|  0.00%|\n",
      "   178|         0|            0|            0|  0.00%|    def __init__(self, modules: Optional[Iterable[Module]] = None) -> None:\n",
      "   179|         0|            0|            0|  0.00%|        super(ModuleList, self).__init__()\n",
      "   180|         0|            0|            0|  0.00%|        if modules is not None:\n",
      "   181|         0|            0|            0|  0.00%|            self += modules\n",
      "   182|         0|            0|            0|  0.00%|\n",
      "   183|         0|            0|            0|  0.00%|    def _get_abs_string_index(self, idx):\n",
      "   184|         0|            0|            0|  0.00%|        \"\"\"Get the absolute index for the list of modules\"\"\"\n",
      "   185|         0|            0|            0|  0.00%|        idx = operator.index(idx)\n",
      "   186|         0|            0|            0|  0.00%|        if not (-len(self) <= idx < len(self)):\n",
      "   187|         0|            0|            0|  0.00%|            raise IndexError('index {} is out of range'.format(idx))\n",
      "   188|         0|            0|            0|  0.00%|        if idx < 0:\n",
      "   189|         0|            0|            0|  0.00%|            idx += len(self)\n",
      "   190|         0|            0|            0|  0.00%|        return str(idx)\n",
      "   191|         0|            0|            0|  0.00%|\n",
      "   192|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper\n",
      "   193|         0|            0|            0|  0.00%|    def __getitem__(self, idx: int) -> Union[Module, 'ModuleList']:\n",
      "   194|         0|            0|            0|  0.00%|        if isinstance(idx, slice):\n",
      "   195|         0|            0|            0|  0.00%|            return self.__class__(list(self._modules.values())[idx])\n",
      "   196|         0|            0|            0|  0.00%|        else:\n",
      "   197|         0|            0|            0|  0.00%|            return self._modules[self._get_abs_string_index(idx)]\n",
      "   198|         0|            0|            0|  0.00%|\n",
      "   199|         0|            0|            0|  0.00%|    def __setitem__(self, idx: int, module: Module) -> None:\n",
      "   200|         0|            0|            0|  0.00%|        idx = self._get_abs_string_index(idx)\n",
      "   201|         0|            0|            0|  0.00%|        return setattr(self, str(idx), module)\n",
      "   202|         0|            0|            0|  0.00%|\n",
      "   203|         0|            0|            0|  0.00%|    def __delitem__(self, idx: Union[int, slice]) -> None:\n",
      "   204|         0|            0|            0|  0.00%|        if isinstance(idx, slice):\n",
      "   205|         0|            0|            0|  0.00%|            for k in range(len(self._modules))[idx]:\n",
      "   206|         0|            0|            0|  0.00%|                delattr(self, str(k))\n",
      "   207|         0|            0|            0|  0.00%|        else:\n",
      "   208|         0|            0|            0|  0.00%|            delattr(self, self._get_abs_string_index(idx))\n",
      "   209|         0|            0|            0|  0.00%|        # To preserve numbering, self._modules is being reconstructed with modules after deletion\n",
      "   210|         0|            0|            0|  0.00%|        str_indices = [str(i) for i in range(len(self._modules))]\n",
      "   211|         0|            0|            0|  0.00%|        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n",
      "   212|         0|            0|            0|  0.00%|\n",
      "   213|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper\n",
      "   214|         0|            0|            0|  0.00%|    def __len__(self) -> int:\n",
      "   215|         0|            0|            0|  0.00%|        return len(self._modules)\n",
      "   216|         0|            0|            0|  0.00%|\n",
      "   217|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper\n",
      "   218|         0|            0|            0|  0.00%|    def __iter__(self) -> Iterator[Module]:\n",
      "   219|         0|            0|            0|  0.00%|        return iter(self._modules.values())\n",
      "   220|         0|            0|            0|  0.00%|\n",
      "   221|         0|            0|            0|  0.00%|    def __iadd__(self, modules: Iterable[Module]) -> 'ModuleList':\n",
      "   222|         0|            0|            0|  0.00%|        return self.extend(modules)\n",
      "   223|         0|            0|            0|  0.00%|\n",
      "   224|         0|            0|            0|  0.00%|    def __add__(self, other: Iterable[Module]) -> 'ModuleList':\n",
      "   225|         0|            0|            0|  0.00%|        combined = ModuleList()\n",
      "   226|         0|            0|            0|  0.00%|        for i, module in enumerate(chain(self, other)):\n",
      "   227|         0|            0|            0|  0.00%|            combined.add_module(str(i), module)\n",
      "   228|         0|            0|            0|  0.00%|        return combined\n",
      "   229|         0|            0|            0|  0.00%|\n",
      "   230|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper\n",
      "   231|         0|            0|            0|  0.00%|    def __dir__(self):\n",
      "   232|         0|            0|            0|  0.00%|        keys = super(ModuleList, self).__dir__()\n",
      "   233|         0|            0|            0|  0.00%|        keys = [key for key in keys if not key.isdigit()]\n",
      "   234|         0|            0|            0|  0.00%|        return keys\n",
      "   235|         0|            0|            0|  0.00%|\n",
      "   236|         0|            0|            0|  0.00%|    def insert(self, index: int, module: Module) -> None:\n",
      "   237|         0|            0|            0|  0.00%|        r\"\"\"Insert a given module before a given index in the list.\n",
      "   238|         0|            0|            0|  0.00%|\n",
      "   239|         0|            0|            0|  0.00%|        Args:\n",
      "   240|         0|            0|            0|  0.00%|            index (int): index to insert.\n",
      "   241|         0|            0|            0|  0.00%|            module (nn.Module): module to insert\n",
      "   242|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   243|         0|            0|            0|  0.00%|        for i in range(len(self._modules), index, -1):\n",
      "   244|         0|            0|            0|  0.00%|            self._modules[str(i)] = self._modules[str(i - 1)]\n",
      "   245|         0|            0|            0|  0.00%|        self._modules[str(index)] = module\n",
      "   246|         0|            0|            0|  0.00%|\n",
      "   247|         0|            0|            0|  0.00%|    def append(self, module: Module) -> 'ModuleList':\n",
      "   248|         0|            0|            0|  0.00%|        r\"\"\"Appends a given module to the end of the list.\n",
      "   249|         0|            0|            0|  0.00%|\n",
      "   250|         0|            0|            0|  0.00%|        Args:\n",
      "   251|         0|            0|            0|  0.00%|            module (nn.Module): module to append\n",
      "   252|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   253|         0|            0|            0|  0.00%|        self.add_module(str(len(self)), module)\n",
      "   254|         0|            0|            0|  0.00%|        return self\n",
      "   255|         0|            0|            0|  0.00%|\n",
      "   256|         0|            0|            0|  0.00%|    def extend(self, modules: Iterable[Module]) -> 'ModuleList':\n",
      "   257|         0|            0|            0|  0.00%|        r\"\"\"Appends modules from a Python iterable to the end of the list.\n",
      "   258|         0|            0|            0|  0.00%|\n",
      "   259|         0|            0|            0|  0.00%|        Args:\n",
      "   260|         0|            0|            0|  0.00%|            modules (iterable): iterable of modules to append\n",
      "   261|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   262|         0|            0|            0|  0.00%|        if not isinstance(modules, container_abcs.Iterable):\n",
      "   263|         0|            0|            0|  0.00%|            raise TypeError(\"ModuleList.extend should be called with an \"\n",
      "   264|         0|            0|            0|  0.00%|                            \"iterable, but got \" + type(modules).__name__)\n",
      "   265|         0|            0|            0|  0.00%|        offset = len(self)\n",
      "   266|         0|            0|            0|  0.00%|        for i, module in enumerate(modules):\n",
      "   267|         0|            0|            0|  0.00%|            self.add_module(str(offset + i), module)\n",
      "   268|         0|            0|            0|  0.00%|        return self\n",
      "   269|         0|            0|            0|  0.00%|\n",
      "   270|         0|            0|            0|  0.00%|    # remove forward alltogether to fallback on Module's _forward_unimplemented\n",
      "   271|         0|            0|            0|  0.00%|\n",
      "   272|         0|            0|            0|  0.00%|\n",
      "   273|         0|            0|            0|  0.00%|class ModuleDict(Module):\n",
      "   274|         0|            0|            0|  0.00%|    r\"\"\"Holds submodules in a dictionary.\n",
      "   275|         0|            0|            0|  0.00%|\n",
      "   276|         0|            0|            0|  0.00%|    :class:`~torch.nn.ModuleDict` can be indexed like a regular Python dictionary,\n",
      "   277|         0|            0|            0|  0.00%|    but modules it contains are properly registered, and will be visible by all\n",
      "   278|         0|            0|            0|  0.00%|    :class:`~torch.nn.Module` methods.\n",
      "   279|         0|            0|            0|  0.00%|\n",
      "   280|         0|            0|            0|  0.00%|    :class:`~torch.nn.ModuleDict` is an **ordered** dictionary that respects\n",
      "   281|         0|            0|            0|  0.00%|\n",
      "   282|         0|            0|            0|  0.00%|    * the order of insertion, and\n",
      "   283|         0|            0|            0|  0.00%|\n",
      "   284|         0|            0|            0|  0.00%|    * in :meth:`~torch.nn.ModuleDict.update`, the order of the merged\n",
      "   285|         0|            0|            0|  0.00%|      ``OrderedDict``, ``dict`` (started from Python 3.6) or another\n",
      "   286|         0|            0|            0|  0.00%|      :class:`~torch.nn.ModuleDict` (the argument to\n",
      "   287|         0|            0|            0|  0.00%|      :meth:`~torch.nn.ModuleDict.update`).\n",
      "   288|         0|            0|            0|  0.00%|\n",
      "   289|         0|            0|            0|  0.00%|    Note that :meth:`~torch.nn.ModuleDict.update` with other unordered mapping\n",
      "   290|         0|            0|            0|  0.00%|    types (e.g., Python's plain ``dict`` before Python version 3.6) does not\n",
      "   291|         0|            0|            0|  0.00%|    preserve the order of the merged mapping.\n",
      "   292|         0|            0|            0|  0.00%|\n",
      "   293|         0|            0|            0|  0.00%|    Args:\n",
      "   294|         0|            0|            0|  0.00%|        modules (iterable, optional): a mapping (dictionary) of (string: module)\n",
      "   295|         0|            0|            0|  0.00%|            or an iterable of key-value pairs of type (string, module)\n",
      "   296|         0|            0|            0|  0.00%|\n",
      "   297|         0|            0|            0|  0.00%|    Example::\n",
      "   298|         0|            0|            0|  0.00%|\n",
      "   299|         0|            0|            0|  0.00%|        class MyModule(nn.Module):\n",
      "   300|         0|            0|            0|  0.00%|            def __init__(self):\n",
      "   301|         0|            0|            0|  0.00%|                super(MyModule, self).__init__()\n",
      "   302|         0|            0|            0|  0.00%|                self.choices = nn.ModuleDict({\n",
      "   303|         0|            0|            0|  0.00%|                        'conv': nn.Conv2d(10, 10, 3),\n",
      "   304|         0|            0|            0|  0.00%|                        'pool': nn.MaxPool2d(3)\n",
      "   305|         0|            0|            0|  0.00%|                })\n",
      "   306|         0|            0|            0|  0.00%|                self.activations = nn.ModuleDict([\n",
      "   307|         0|            0|            0|  0.00%|                        ['lrelu', nn.LeakyReLU()],\n",
      "   308|         0|            0|            0|  0.00%|                        ['prelu', nn.PReLU()]\n",
      "   309|         0|            0|            0|  0.00%|                ])\n",
      "   310|         0|            0|            0|  0.00%|\n",
      "   311|         0|            0|            0|  0.00%|            def forward(self, x, choice, act):\n",
      "   312|         0|            0|            0|  0.00%|                x = self.choices[choice](x)\n",
      "   313|         0|            0|            0|  0.00%|                x = self.activations[act](x)\n",
      "   314|         0|            0|            0|  0.00%|                return x\n",
      "   315|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   316|         0|            0|            0|  0.00%|\n",
      "   317|         0|            0|            0|  0.00%|    _modules: Dict[str, Module]  # type: ignore[assignment]\n",
      "   318|         0|            0|            0|  0.00%|\n",
      "   319|         0|            0|            0|  0.00%|    def __init__(self, modules: Optional[Mapping[str, Module]] = None) -> None:\n",
      "   320|         0|            0|            0|  0.00%|        super(ModuleDict, self).__init__()\n",
      "   321|         0|            0|            0|  0.00%|        if modules is not None:\n",
      "   322|         0|            0|            0|  0.00%|            self.update(modules)\n",
      "   323|         0|            0|            0|  0.00%|\n",
      "   324|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper\n",
      "   325|         0|            0|            0|  0.00%|    def __getitem__(self, key: str) -> Module:\n",
      "   326|         0|            0|            0|  0.00%|        return self._modules[key]\n",
      "   327|         0|            0|            0|  0.00%|\n",
      "   328|         0|            0|            0|  0.00%|    def __setitem__(self, key: str, module: Module) -> None:\n",
      "   329|         0|            0|            0|  0.00%|        self.add_module(key, module)\n",
      "   330|         0|            0|            0|  0.00%|\n",
      "   331|         0|            0|            0|  0.00%|    def __delitem__(self, key: str) -> None:\n",
      "   332|         0|            0|            0|  0.00%|        del self._modules[key]\n",
      "   333|         0|            0|            0|  0.00%|\n",
      "   334|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper\n",
      "   335|         0|            0|            0|  0.00%|    def __len__(self) -> int:\n",
      "   336|         0|            0|            0|  0.00%|        return len(self._modules)\n",
      "   337|         0|            0|            0|  0.00%|\n",
      "   338|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper\n",
      "   339|         0|            0|            0|  0.00%|    def __iter__(self) -> Iterator[str]:\n",
      "   340|         0|            0|            0|  0.00%|        return iter(self._modules)\n",
      "   341|         0|            0|            0|  0.00%|\n",
      "   342|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper\n",
      "   343|         0|            0|            0|  0.00%|    def __contains__(self, key: str) -> bool:\n",
      "   344|         0|            0|            0|  0.00%|        return key in self._modules\n",
      "   345|         0|            0|            0|  0.00%|\n",
      "   346|         0|            0|            0|  0.00%|    def clear(self) -> None:\n",
      "   347|         0|            0|            0|  0.00%|        \"\"\"Remove all items from the ModuleDict.\n",
      "   348|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   349|         0|            0|            0|  0.00%|        self._modules.clear()\n",
      "   350|         0|            0|            0|  0.00%|\n",
      "   351|         0|            0|            0|  0.00%|    def pop(self, key: str) -> Module:\n",
      "   352|         0|            0|            0|  0.00%|        r\"\"\"Remove key from the ModuleDict and return its module.\n",
      "   353|         0|            0|            0|  0.00%|\n",
      "   354|         0|            0|            0|  0.00%|        Args:\n",
      "   355|         0|            0|            0|  0.00%|            key (string): key to pop from the ModuleDict\n",
      "   356|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   357|         0|            0|            0|  0.00%|        v = self[key]\n",
      "   358|         0|            0|            0|  0.00%|        del self[key]\n",
      "   359|         0|            0|            0|  0.00%|        return v\n",
      "   360|         0|            0|            0|  0.00%|\n",
      "   361|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper\n",
      "   362|         0|            0|            0|  0.00%|    def keys(self) -> Iterable[str]:\n",
      "   363|         0|            0|            0|  0.00%|        r\"\"\"Return an iterable of the ModuleDict keys.\n",
      "   364|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   365|         0|            0|            0|  0.00%|        return self._modules.keys()\n",
      "   366|         0|            0|            0|  0.00%|\n",
      "   367|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper\n",
      "   368|         0|            0|            0|  0.00%|    def items(self) -> Iterable[Tuple[str, Module]]:\n",
      "   369|         0|            0|            0|  0.00%|        r\"\"\"Return an iterable of the ModuleDict key/value pairs.\n",
      "   370|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   371|         0|            0|            0|  0.00%|        return self._modules.items()\n",
      "   372|         0|            0|            0|  0.00%|\n",
      "   373|         0|            0|            0|  0.00%|    @_copy_to_script_wrapper\n",
      "   374|         0|            0|            0|  0.00%|    def values(self) -> Iterable[Module]:\n",
      "   375|         0|            0|            0|  0.00%|        r\"\"\"Return an iterable of the ModuleDict values.\n",
      "   376|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   377|         0|            0|            0|  0.00%|        return self._modules.values()\n",
      "   378|         0|            0|            0|  0.00%|\n",
      "   379|         0|            0|            0|  0.00%|    def update(self, modules: Mapping[str, Module]) -> None:\n",
      "   380|         0|            0|            0|  0.00%|        r\"\"\"Update the :class:`~torch.nn.ModuleDict` with the key-value pairs from a\n",
      "   381|         0|            0|            0|  0.00%|        mapping or an iterable, overwriting existing keys.\n",
      "   382|         0|            0|            0|  0.00%|\n",
      "   383|         0|            0|            0|  0.00%|        .. note::\n",
      "   384|         0|            0|            0|  0.00%|            If :attr:`modules` is an ``OrderedDict``, a :class:`~torch.nn.ModuleDict`, or\n",
      "   385|         0|            0|            0|  0.00%|            an iterable of key-value pairs, the order of new elements in it is preserved.\n",
      "   386|         0|            0|            0|  0.00%|\n",
      "   387|         0|            0|            0|  0.00%|        Args:\n",
      "   388|         0|            0|            0|  0.00%|            modules (iterable): a mapping (dictionary) from string to :class:`~torch.nn.Module`,\n",
      "   389|         0|            0|            0|  0.00%|                or an iterable of key-value pairs of type (string, :class:`~torch.nn.Module`)\n",
      "   390|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   391|         0|            0|            0|  0.00%|        if not isinstance(modules, container_abcs.Iterable):\n",
      "   392|         0|            0|            0|  0.00%|            raise TypeError(\"ModuleDict.update should be called with an \"\n",
      "   393|         0|            0|            0|  0.00%|                            \"iterable of key/value pairs, but got \" +\n",
      "   394|         0|            0|            0|  0.00%|                            type(modules).__name__)\n",
      "   395|         0|            0|            0|  0.00%|\n",
      "   396|         0|            0|            0|  0.00%|        if isinstance(modules, (OrderedDict, ModuleDict, container_abcs.Mapping)):\n",
      "   397|         0|            0|            0|  0.00%|            for key, module in modules.items():\n",
      "   398|         0|            0|            0|  0.00%|                self[key] = module\n",
      "   399|         0|            0|            0|  0.00%|        else:\n",
      "   400|         0|            0|            0|  0.00%|            # modules here can be a list with two items\n",
      "   401|         0|            0|            0|  0.00%|            for j, m in enumerate(modules):\n",
      "   402|         0|            0|            0|  0.00%|                if not isinstance(m, container_abcs.Iterable):\n",
      "   403|         0|            0|            0|  0.00%|                    raise TypeError(\"ModuleDict update sequence element \"\n",
      "   404|         0|            0|            0|  0.00%|                                    \"#\" + str(j) + \" should be Iterable; is\" +\n",
      "   405|         0|            0|            0|  0.00%|                                    type(m).__name__)\n",
      "   406|         0|            0|            0|  0.00%|                if not len(m) == 2:\n",
      "   407|         0|            0|            0|  0.00%|                    raise ValueError(\"ModuleDict update sequence element \"\n",
      "   408|         0|            0|            0|  0.00%|                                     \"#\" + str(j) + \" has length \" + str(len(m)) +\n",
      "   409|         0|            0|            0|  0.00%|                                     \"; 2 is required\")\n",
      "   410|         0|            0|            0|  0.00%|                # modules can be Mapping (what it's typed at), or a list: [(name1, module1), (name2, module2)]\n",
      "   411|         0|            0|            0|  0.00%|                # that's too cumbersome to type correctly with overloads, so we add an ignore here\n",
      "   412|         0|            0|            0|  0.00%|                self[m[0]] = m[1]  # type: ignore[assignment]\n",
      "   413|         0|            0|            0|  0.00%|\n",
      "   414|         0|            0|            0|  0.00%|    # remove forward alltogether to fallback on Module's _forward_unimplemented\n",
      "   415|         0|            0|            0|  0.00%|\n",
      "   416|         0|            0|            0|  0.00%|\n",
      "   417|         0|            0|            0|  0.00%|class ParameterList(Module):\n",
      "   418|         0|            0|            0|  0.00%|    r\"\"\"Holds parameters in a list.\n",
      "   419|         0|            0|            0|  0.00%|\n",
      "   420|         0|            0|            0|  0.00%|    :class:`~torch.nn.ParameterList` can be used like a regular Python\n",
      "   421|         0|            0|            0|  0.00%|    list, but Tensors that are :class:`~torch.nn.Parameter` are properly registered,\n",
      "   422|         0|            0|            0|  0.00%|    and will be visible by all :class:`~torch.nn.Module` methods.\n",
      "   423|         0|            0|            0|  0.00%|\n",
      "   424|         0|            0|            0|  0.00%|    Note that the constructor, assigning an element of the list, the\n",
      "   425|         0|            0|            0|  0.00%|    :meth:`~torch.nn.ParameterDict.append` method and the :meth:`~torch.nn.ParameterDict.extend`\n",
      "   426|         0|            0|            0|  0.00%|    method will convert any :class:`~torch.Tensor` into :class:`~torch.nn.Parameter`.\n",
      "   427|         0|            0|            0|  0.00%|\n",
      "   428|         0|            0|            0|  0.00%|    Args:\n",
      "   429|         0|            0|            0|  0.00%|        parameters (iterable, optional): an iterable of elements to add to the list.\n",
      "   430|         0|            0|            0|  0.00%|\n",
      "   431|         0|            0|            0|  0.00%|    Example::\n",
      "   432|         0|            0|            0|  0.00%|\n",
      "   433|         0|            0|            0|  0.00%|        class MyModule(nn.Module):\n",
      "   434|         0|            0|            0|  0.00%|            def __init__(self):\n",
      "   435|         0|            0|            0|  0.00%|                super(MyModule, self).__init__()\n",
      "   436|         0|            0|            0|  0.00%|                self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n",
      "   437|         0|            0|            0|  0.00%|\n",
      "   438|         0|            0|            0|  0.00%|            def forward(self, x):\n",
      "   439|         0|            0|            0|  0.00%|                # ParameterList can act as an iterable, or be indexed using ints\n",
      "   440|         0|            0|            0|  0.00%|                for i, p in enumerate(self.params):\n",
      "   441|         0|            0|            0|  0.00%|                    x = self.params[i // 2].mm(x) + p.mm(x)\n",
      "   442|         0|            0|            0|  0.00%|                return x\n",
      "   443|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   444|         0|            0|            0|  0.00%|\n",
      "   445|         0|            0|            0|  0.00%|    def __init__(self, values: Optional[Iterable[Any]] = None) -> None:\n",
      "   446|         0|            0|            0|  0.00%|        super(ParameterList, self).__init__()\n",
      "   447|         0|            0|            0|  0.00%|        self._size = 0\n",
      "   448|         0|            0|            0|  0.00%|        if values is not None:\n",
      "   449|         0|            0|            0|  0.00%|            self += values\n",
      "   450|         0|            0|            0|  0.00%|\n",
      "   451|         0|            0|            0|  0.00%|    def _get_abs_string_index(self, idx):\n",
      "   452|         0|            0|            0|  0.00%|        \"\"\"Get the absolute index for the list of modules\"\"\"\n",
      "   453|         0|            0|            0|  0.00%|        idx = operator.index(idx)\n",
      "   454|         0|            0|            0|  0.00%|        if not (-len(self) <= idx < len(self)):\n",
      "   455|         0|            0|            0|  0.00%|            raise IndexError('index {} is out of range'.format(idx))\n",
      "   456|         0|            0|            0|  0.00%|        if idx < 0:\n",
      "   457|         0|            0|            0|  0.00%|            idx += len(self)\n",
      "   458|         0|            0|            0|  0.00%|        return str(idx)\n",
      "   459|         0|            0|            0|  0.00%|\n",
      "   460|         0|            0|            0|  0.00%|    @overload\n",
      "   461|         0|            0|            0|  0.00%|    def __getitem__(self, idx: int) -> Any:\n",
      "   462|         0|            0|            0|  0.00%|        ...\n",
      "   463|         0|            0|            0|  0.00%|\n",
      "   464|         0|            0|            0|  0.00%|    @overload\n",
      "   465|         0|            0|            0|  0.00%|    def __getitem__(self: T, idx: slice) -> T:\n",
      "   466|         0|            0|            0|  0.00%|        ...\n",
      "   467|         0|            0|            0|  0.00%|\n",
      "   468|         0|            0|            0|  0.00%|    def __getitem__(self, idx):\n",
      "   469|         0|            0|            0|  0.00%|        if isinstance(idx, slice):\n",
      "   470|         0|            0|            0|  0.00%|            start, stop, step = idx.indices(len(self))\n",
      "   471|         0|            0|            0|  0.00%|            out = self.__class__()\n",
      "   472|         0|            0|            0|  0.00%|            for i in range(start, stop, step):\n",
      "   473|         0|            0|            0|  0.00%|                out.append(self[i])\n",
      "   474|         0|            0|            0|  0.00%|            return out\n",
      "   475|         0|            0|            0|  0.00%|        else:\n",
      "   476|         0|            0|            0|  0.00%|            idx = self._get_abs_string_index(idx)\n",
      "   477|         0|            0|            0|  0.00%|            return getattr(self, str(idx))\n",
      "   478|         0|            0|            0|  0.00%|\n",
      "   479|         0|            0|            0|  0.00%|    def __setitem__(self, idx: int, param: Any) -> None:\n",
      "   480|         0|            0|            0|  0.00%|        # Note that all other function that add an entry to the list part of\n",
      "   481|         0|            0|            0|  0.00%|        # the ParameterList end up here. So this is the only place where we need\n",
      "   482|         0|            0|            0|  0.00%|        # to wrap things into Parameter if needed.\n",
      "   483|         0|            0|            0|  0.00%|        # Objects added via setattr() are not in the list part and thus won't\n",
      "   484|         0|            0|            0|  0.00%|        # call into this function.\n",
      "   485|         0|            0|            0|  0.00%|        idx = self._get_abs_string_index(idx)\n",
      "   486|         0|            0|            0|  0.00%|        if isinstance(param, torch.Tensor) and not isinstance(param, Parameter):\n",
      "   487|         0|            0|            0|  0.00%|            param = Parameter(param)\n",
      "   488|         0|            0|            0|  0.00%|        return setattr(self, str(idx), param)\n",
      "   489|         0|            0|            0|  0.00%|\n",
      "   490|         0|            0|            0|  0.00%|    def __len__(self) -> int:\n",
      "   491|         0|            0|            0|  0.00%|        return self._size\n",
      "   492|         0|            0|            0|  0.00%|\n",
      "   493|         0|            0|            0|  0.00%|    def __iter__(self) -> Iterator[Any]:\n",
      "   494|         0|            0|            0|  0.00%|        return iter(self[i] for i in range(len(self)))\n",
      "   495|         0|            0|            0|  0.00%|\n",
      "   496|         0|            0|            0|  0.00%|    def __iadd__(self, parameters: Iterable[Any]) -> 'ParameterList':\n",
      "   497|         0|            0|            0|  0.00%|        return self.extend(parameters)\n",
      "   498|         0|            0|            0|  0.00%|\n",
      "   499|         0|            0|            0|  0.00%|    def __dir__(self):\n",
      "   500|         0|            0|            0|  0.00%|        keys = super(ParameterList, self).__dir__()\n",
      "   501|         0|            0|            0|  0.00%|        keys = [key for key in keys if not key.isdigit()]\n",
      "   502|         0|            0|            0|  0.00%|        return keys\n",
      "   503|         0|            0|            0|  0.00%|\n",
      "   504|         0|            0|            0|  0.00%|    def append(self, value: Any) -> 'ParameterList':\n",
      "   505|         0|            0|            0|  0.00%|        \"\"\"Appends a given value at the end of the list.\n",
      "   506|         0|            0|            0|  0.00%|\n",
      "   507|         0|            0|            0|  0.00%|        Args:\n",
      "   508|         0|            0|            0|  0.00%|            value (Any): value to append\n",
      "   509|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   510|         0|            0|            0|  0.00%|        new_idx = len(self)\n",
      "   511|         0|            0|            0|  0.00%|        self._size += 1\n",
      "   512|         0|            0|            0|  0.00%|        self[new_idx] = value\n",
      "   513|         0|            0|            0|  0.00%|        return self\n",
      "   514|         0|            0|            0|  0.00%|\n",
      "   515|         0|            0|            0|  0.00%|    def extend(self, values: Iterable[Any]) -> 'ParameterList':\n",
      "   516|         0|            0|            0|  0.00%|        \"\"\"Appends values from a Python iterable to the end of the list.\n",
      "   517|         0|            0|            0|  0.00%|\n",
      "   518|         0|            0|            0|  0.00%|        Args:\n",
      "   519|         0|            0|            0|  0.00%|            values (iterable): iterable of values to append\n",
      "   520|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   521|         0|            0|            0|  0.00%|        # Tensor is an iterable but we never want to unpack it here\n",
      "   522|         0|            0|            0|  0.00%|        if not isinstance(values, container_abcs.Iterable) or isinstance(values, torch.Tensor):\n",
      "   523|         0|            0|            0|  0.00%|            raise TypeError(\"ParameterList.extend should be called with an \"\n",
      "   524|         0|            0|            0|  0.00%|                            \"iterable, but got \" + type(values).__name__)\n",
      "   525|         0|            0|            0|  0.00%|        for value in values:\n",
      "   526|         0|            0|            0|  0.00%|            self.append(value)\n",
      "   527|         0|            0|            0|  0.00%|        return self\n",
      "   528|         0|            0|            0|  0.00%|\n",
      "   529|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   530|         0|            0|            0|  0.00%|        child_lines = []\n",
      "   531|         0|            0|            0|  0.00%|        for k, p in enumerate(self):\n",
      "   532|         0|            0|            0|  0.00%|            if isinstance(p, torch.Tensor):\n",
      "   533|         0|            0|            0|  0.00%|                size_str = 'x'.join(str(size) for size in p.size())\n",
      "   534|         0|            0|            0|  0.00%|                device_str = '' if not p.is_cuda else ' (GPU {})'.format(p.get_device())\n",
      "   535|         0|            0|            0|  0.00%|                parastr = '{} containing: [{} of size {}{}]'.format(\n",
      "   536|         0|            0|            0|  0.00%|                    \"Parameter\" if isinstance(p, Parameter) else \"Tensor\",\n",
      "   537|         0|            0|            0|  0.00%|                    torch.typename(p), size_str, device_str)\n",
      "   538|         0|            0|            0|  0.00%|                child_lines.append('  (' + str(k) + '): ' + parastr)\n",
      "   539|         0|            0|            0|  0.00%|            else:\n",
      "   540|         0|            0|            0|  0.00%|                child_lines.append('  (' + str(k) + '): Object of type: ' + type(p).__name__)\n",
      "   541|         0|            0|            0|  0.00%|\n",
      "   542|         0|            0|            0|  0.00%|        tmpstr = '\\n'.join(child_lines)\n",
      "   543|         0|            0|            0|  0.00%|        return tmpstr\n",
      "   544|         0|            0|            0|  0.00%|\n",
      "   545|         0|            0|            0|  0.00%|    def __call__(self, *args, **kwargs):\n",
      "   546|         0|            0|            0|  0.00%|        raise RuntimeError('ParameterList should not be called.')\n",
      "   547|         0|            0|            0|  0.00%|\n",
      "   548|         0|            0|            0|  0.00%|\n",
      "   549|         0|            0|            0|  0.00%|class ParameterDict(Module):\n",
      "   550|         0|            0|            0|  0.00%|    r\"\"\"Holds parameters in a dictionary.\n",
      "   551|         0|            0|            0|  0.00%|\n",
      "   552|         0|            0|            0|  0.00%|    ParameterDict can be indexed like a regular Python dictionary, but Parameters it\n",
      "   553|         0|            0|            0|  0.00%|    contains are properly registered, and will be visible by all Module methods.\n",
      "   554|         0|            0|            0|  0.00%|    Other objects are treated as would be done by a regular Python dictionary\n",
      "   555|         0|            0|            0|  0.00%|\n",
      "   556|         0|            0|            0|  0.00%|    :class:`~torch.nn.ParameterDict` is an **ordered** dictionary.\n",
      "   557|         0|            0|            0|  0.00%|    :meth:`~torch.nn.ParameterDict.update` with other unordered mapping\n",
      "   558|         0|            0|            0|  0.00%|    types (e.g., Python's plain ``dict``) does not preserve the order of the\n",
      "   559|         0|            0|            0|  0.00%|    merged mapping. On the other hand, ``OrderedDict`` or another :class:`~torch.nn.ParameterDict`\n",
      "   560|         0|            0|            0|  0.00%|    will preserve their ordering.\n",
      "   561|         0|            0|            0|  0.00%|\n",
      "   562|         0|            0|            0|  0.00%|    Note that the constructor, assigning an element of the dictionary and the\n",
      "   563|         0|            0|            0|  0.00%|    :meth:`~torch.nn.ParameterDict.update` method will convert any :class:`~torch.Tensor` into\n",
      "   564|         0|            0|            0|  0.00%|    :class:`~torch.nn.Parameter`.\n",
      "   565|         0|            0|            0|  0.00%|\n",
      "   566|         0|            0|            0|  0.00%|    Args:\n",
      "   567|         0|            0|            0|  0.00%|        values (iterable, optional): a mapping (dictionary) of\n",
      "   568|         0|            0|            0|  0.00%|            (string : Any) or an iterable of key-value pairs\n",
      "   569|         0|            0|            0|  0.00%|            of type (string, Any)\n",
      "   570|         0|            0|            0|  0.00%|\n",
      "   571|         0|            0|            0|  0.00%|    Example::\n",
      "   572|         0|            0|            0|  0.00%|\n",
      "   573|         0|            0|            0|  0.00%|        class MyModule(nn.Module):\n",
      "   574|         0|            0|            0|  0.00%|            def __init__(self):\n",
      "   575|         0|            0|            0|  0.00%|                super(MyModule, self).__init__()\n",
      "   576|         0|            0|            0|  0.00%|                self.params = nn.ParameterDict({\n",
      "   577|         0|            0|            0|  0.00%|                        'left': nn.Parameter(torch.randn(5, 10)),\n",
      "   578|         0|            0|            0|  0.00%|                        'right': nn.Parameter(torch.randn(5, 10))\n",
      "   579|         0|            0|            0|  0.00%|                })\n",
      "   580|         0|            0|            0|  0.00%|\n",
      "   581|         0|            0|            0|  0.00%|            def forward(self, x, choice):\n",
      "   582|         0|            0|            0|  0.00%|                x = self.params[choice].mm(x)\n",
      "   583|         0|            0|            0|  0.00%|                return x\n",
      "   584|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   585|         0|            0|            0|  0.00%|\n",
      "   586|         0|            0|            0|  0.00%|    def __init__(self, parameters: Any = None) -> None:\n",
      "   587|         0|            0|            0|  0.00%|        super(ParameterDict, self).__init__()\n",
      "   588|         0|            0|            0|  0.00%|        self._keys: Dict[str, None] = {}\n",
      "   589|         0|            0|            0|  0.00%|        if parameters is not None:\n",
      "   590|         0|            0|            0|  0.00%|            self.update(parameters)\n",
      "   591|         0|            0|            0|  0.00%|\n",
      "   592|         0|            0|            0|  0.00%|    def _key_to_attr(self, key: str) -> str:\n",
      "   593|         0|            0|            0|  0.00%|        if not isinstance(key, str):\n",
      "   594|         0|            0|            0|  0.00%|            raise TypeError(\"Index given to ParameterDict cannot be used as a key as it is \"\n",
      "   595|         0|            0|            0|  0.00%|                            f\"not a string (type is '{type(key).__name__}'). Open an issue on \"\n",
      "   596|         0|            0|            0|  0.00%|                            \"github if you need non-string keys.\")\n",
      "   597|         0|            0|            0|  0.00%|        else:\n",
      "   598|         0|            0|            0|  0.00%|            # Use the key as-is so that `.named_parameters()` returns the right thing\n",
      "   599|         0|            0|            0|  0.00%|            return key\n",
      "   600|         0|            0|            0|  0.00%|\n",
      "   601|         0|            0|            0|  0.00%|    def __getitem__(self, key: str) -> Any:\n",
      "   602|         0|            0|            0|  0.00%|        attr = self._key_to_attr(key)\n",
      "   603|         0|            0|            0|  0.00%|        return getattr(self, attr)\n",
      "   604|         0|            0|            0|  0.00%|\n",
      "   605|         0|            0|            0|  0.00%|    def __setitem__(self, key: str, value: Any) -> None:\n",
      "   606|         0|            0|            0|  0.00%|        # Note that all other function that add an entry to the dictionary part of\n",
      "   607|         0|            0|            0|  0.00%|        # the ParameterDict end up here. So this is the only place where we need\n",
      "   608|         0|            0|            0|  0.00%|        # to wrap things into Parameter if needed.\n",
      "   609|         0|            0|            0|  0.00%|        # Objects added via setattr() are not in the dictionary part and thus won't\n",
      "   610|         0|            0|            0|  0.00%|        # call into this function.\n",
      "   611|         0|            0|            0|  0.00%|        self._keys[key] = None\n",
      "   612|         0|            0|            0|  0.00%|        attr = self._key_to_attr(key)\n",
      "   613|         0|            0|            0|  0.00%|        if isinstance(value, torch.Tensor) and not isinstance(value, Parameter):\n",
      "   614|         0|            0|            0|  0.00%|            value = Parameter(value)\n",
      "   615|         0|            0|            0|  0.00%|        setattr(self, attr, value)\n",
      "   616|         0|            0|            0|  0.00%|\n",
      "   617|         0|            0|            0|  0.00%|    def __delitem__(self, key: str) -> None:\n",
      "   618|         0|            0|            0|  0.00%|        del self._keys[key]\n",
      "   619|         0|            0|            0|  0.00%|        attr = self._key_to_attr(key)\n",
      "   620|         0|            0|            0|  0.00%|        delattr(self, attr)\n",
      "   621|         0|            0|            0|  0.00%|\n",
      "   622|         0|            0|            0|  0.00%|    def __len__(self) -> int:\n",
      "   623|         0|            0|            0|  0.00%|        return len(self._keys)\n",
      "   624|         0|            0|            0|  0.00%|\n",
      "   625|         0|            0|            0|  0.00%|    def __iter__(self) -> Iterator[str]:\n",
      "   626|         0|            0|            0|  0.00%|        return iter(self._keys)\n",
      "   627|         0|            0|            0|  0.00%|\n",
      "   628|         0|            0|            0|  0.00%|    def __reversed__(self) -> Iterator[str]:\n",
      "   629|         0|            0|            0|  0.00%|        return reversed(list(self._keys))\n",
      "   630|         0|            0|            0|  0.00%|\n",
      "   631|         0|            0|            0|  0.00%|    def copy(self) -> 'ParameterDict':\n",
      "   632|         0|            0|            0|  0.00%|        \"\"\"Returns a copy of this :class:`~torch.nn.ParameterDict` instance.\n",
      "   633|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   634|         0|            0|            0|  0.00%|        # We have to use an OrderedDict because the ParameterDict constructor\n",
      "   635|         0|            0|            0|  0.00%|        # behaves differently on plain dict vs OrderedDict\n",
      "   636|         0|            0|            0|  0.00%|        return ParameterDict(OrderedDict((k, self[k]) for k in self._keys))\n",
      "   637|         0|            0|            0|  0.00%|\n",
      "   638|         0|            0|            0|  0.00%|    def __contains__(self, key: str) -> bool:\n",
      "   639|         0|            0|            0|  0.00%|        return key in self._keys\n",
      "   640|         0|            0|            0|  0.00%|\n",
      "   641|         0|            0|            0|  0.00%|    def setdefault(self, key: str, default: Optional[Any] = None) -> Any:\n",
      "   642|         0|            0|            0|  0.00%|        \"\"\"If key is in the ParameterDict, return its value.\n",
      "   643|         0|            0|            0|  0.00%|        If not, insert `key` with a parameter `default` and return `default`.\n",
      "   644|         0|            0|            0|  0.00%|        `default` defaults to `None`.\n",
      "   645|         0|            0|            0|  0.00%|\n",
      "   646|         0|            0|            0|  0.00%|        Args:\n",
      "   647|         0|            0|            0|  0.00%|            key (string): key to set default for\n",
      "   648|         0|            0|            0|  0.00%|            default (Any): the parameter set to the key\n",
      "   649|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   650|         0|            0|            0|  0.00%|\n",
      "   651|         0|            0|            0|  0.00%|        if key not in self:\n",
      "   652|         0|            0|            0|  0.00%|            self[key] = default\n",
      "   653|         0|            0|            0|  0.00%|        return self[key]\n",
      "   654|         0|            0|            0|  0.00%|\n",
      "   655|         0|            0|            0|  0.00%|    def clear(self) -> None:\n",
      "   656|         0|            0|            0|  0.00%|        \"\"\"Remove all items from the ParameterDict.\n",
      "   657|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   658|         0|            0|            0|  0.00%|        for k in self._keys.copy():\n",
      "   659|         0|            0|            0|  0.00%|            del self[k]\n",
      "   660|         0|            0|            0|  0.00%|\n",
      "   661|         0|            0|            0|  0.00%|    def pop(self, key: str) -> Any:\n",
      "   662|         0|            0|            0|  0.00%|        r\"\"\"Remove key from the ParameterDict and return its parameter.\n",
      "   663|         0|            0|            0|  0.00%|\n",
      "   664|         0|            0|            0|  0.00%|        Args:\n",
      "   665|         0|            0|            0|  0.00%|            key (string): key to pop from the ParameterDict\n",
      "   666|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   667|         0|            0|            0|  0.00%|        v = self[key]\n",
      "   668|         0|            0|            0|  0.00%|        del self[key]\n",
      "   669|         0|            0|            0|  0.00%|        return v\n",
      "   670|         0|            0|            0|  0.00%|\n",
      "   671|         0|            0|            0|  0.00%|    def popitem(self) -> Tuple[str, Any]:\n",
      "   672|         0|            0|            0|  0.00%|        \"\"\"Remove and return the last inserted `(key, parameter)` pair\n",
      "   673|         0|            0|            0|  0.00%|        from the ParameterDict\n",
      "   674|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   675|         0|            0|            0|  0.00%|        k, _ = self._keys.popitem()\n",
      "   676|         0|            0|            0|  0.00%|        # We need the key in the _keys to be able to access/del\n",
      "   677|         0|            0|            0|  0.00%|        self._keys[k] = None\n",
      "   678|         0|            0|            0|  0.00%|        val = self[k]\n",
      "   679|         0|            0|            0|  0.00%|        del self[k]\n",
      "   680|         0|            0|            0|  0.00%|        return k, val\n",
      "   681|         0|            0|            0|  0.00%|\n",
      "   682|         0|            0|            0|  0.00%|    def get(self, key: str, default: Optional[Any] = None) -> Any:\n",
      "   683|         0|            0|            0|  0.00%|        r\"\"\"Return the parameter associated with key if present.\n",
      "   684|         0|            0|            0|  0.00%|        Otherwise return default if provided, None if not.\n",
      "   685|         0|            0|            0|  0.00%|\n",
      "   686|         0|            0|            0|  0.00%|        Args:\n",
      "   687|         0|            0|            0|  0.00%|            key (string): key to get from the ParameterDict\n",
      "   688|         0|            0|            0|  0.00%|            default (Parameter, optional): value to return if key not present\n",
      "   689|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   690|         0|            0|            0|  0.00%|        return self[key] if key in self else default\n",
      "   691|         0|            0|            0|  0.00%|\n",
      "   692|         0|            0|            0|  0.00%|    def fromkeys(self, keys: Iterable[str], default: Optional[Any] = None) -> 'ParameterDict':\n",
      "   693|         0|            0|            0|  0.00%|        r\"\"\"Return a new ParameterDict with the keys provided\n",
      "   694|         0|            0|            0|  0.00%|\n",
      "   695|         0|            0|            0|  0.00%|        Args:\n",
      "   696|         0|            0|            0|  0.00%|            keys (iterable, string): keys to make the new ParameterDict from\n",
      "   697|         0|            0|            0|  0.00%|            default (Parameter, optional): value to set for all keys\n",
      "   698|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   699|         0|            0|            0|  0.00%|        return ParameterDict(((k, default) for k in keys))\n",
      "   700|         0|            0|            0|  0.00%|\n",
      "   701|         0|            0|            0|  0.00%|    def keys(self) -> Iterable[str]:\n",
      "   702|         0|            0|            0|  0.00%|        r\"\"\"Return an iterable of the ParameterDict keys.\n",
      "   703|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   704|         0|            0|            0|  0.00%|        return self._keys.keys()\n",
      "   705|         0|            0|            0|  0.00%|\n",
      "   706|         0|            0|            0|  0.00%|    def items(self) -> Iterable[Tuple[str, Any]]:\n",
      "   707|         0|            0|            0|  0.00%|        r\"\"\"Return an iterable of the ParameterDict key/value pairs.\n",
      "   708|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   709|         0|            0|            0|  0.00%|        return ((k, self[k]) for k in self._keys)\n",
      "   710|         0|            0|            0|  0.00%|\n",
      "   711|         0|            0|            0|  0.00%|    def values(self) -> Iterable[Any]:\n",
      "   712|         0|            0|            0|  0.00%|        r\"\"\"Return an iterable of the ParameterDict values.\n",
      "   713|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   714|         0|            0|            0|  0.00%|        return (self[k] for k in self._keys)\n",
      "   715|         0|            0|            0|  0.00%|\n",
      "   716|         0|            0|            0|  0.00%|    def update(self, parameters: Union[Mapping[str, Any], 'ParameterDict']) -> None:\n",
      "   717|         0|            0|            0|  0.00%|        r\"\"\"Update the :class:`~torch.nn.ParameterDict` with the key-value pairs from a\n",
      "   718|         0|            0|            0|  0.00%|        mapping or an iterable, overwriting existing keys.\n",
      "   719|         0|            0|            0|  0.00%|\n",
      "   720|         0|            0|            0|  0.00%|        .. note::\n",
      "   721|         0|            0|            0|  0.00%|            If :attr:`parameters` is an ``OrderedDict``, a :class:`~torch.nn.ParameterDict`, or\n",
      "   722|         0|            0|            0|  0.00%|            an iterable of key-value pairs, the order of new elements in it is preserved.\n",
      "   723|         0|            0|            0|  0.00%|\n",
      "   724|         0|            0|            0|  0.00%|        Args:\n",
      "   725|         0|            0|            0|  0.00%|            parameters (iterable): a mapping (dictionary) from string to\n",
      "   726|         0|            0|            0|  0.00%|                :class:`~torch.nn.Parameter`, or an iterable of\n",
      "   727|         0|            0|            0|  0.00%|                key-value pairs of type (string, :class:`~torch.nn.Parameter`)\n",
      "   728|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   729|         0|            0|            0|  0.00%|        if not isinstance(parameters, container_abcs.Iterable):\n",
      "   730|         0|            0|            0|  0.00%|            raise TypeError(\"ParametersDict.update should be called with an \"\n",
      "   731|         0|            0|            0|  0.00%|                            \"iterable of key/value pairs, but got \" +\n",
      "   732|         0|            0|            0|  0.00%|                            type(parameters).__name__)\n",
      "   733|         0|            0|            0|  0.00%|\n",
      "   734|         0|            0|            0|  0.00%|        if isinstance(parameters, (OrderedDict, ParameterDict)):\n",
      "   735|         0|            0|            0|  0.00%|            for key, parameter in parameters.items():\n",
      "   736|         0|            0|            0|  0.00%|                self[key] = parameter\n",
      "   737|         0|            0|            0|  0.00%|        elif isinstance(parameters, container_abcs.Mapping):\n",
      "   738|         0|            0|            0|  0.00%|            for key, parameter in sorted(parameters.items()):\n",
      "   739|         0|            0|            0|  0.00%|                self[key] = parameter\n",
      "   740|         0|            0|            0|  0.00%|        else:\n",
      "   741|         0|            0|            0|  0.00%|            for j, p in enumerate(parameters):\n",
      "   742|         0|            0|            0|  0.00%|                if not isinstance(p, container_abcs.Iterable):\n",
      "   743|         0|            0|            0|  0.00%|                    raise TypeError(\"ParameterDict update sequence element \"\n",
      "   744|         0|            0|            0|  0.00%|                                    \"#\" + str(j) + \" should be Iterable; is\" +\n",
      "   745|         0|            0|            0|  0.00%|                                    type(p).__name__)\n",
      "   746|         0|            0|            0|  0.00%|                if not len(p) == 2:\n",
      "   747|         0|            0|            0|  0.00%|                    raise ValueError(\"ParameterDict update sequence element \"\n",
      "   748|         0|            0|            0|  0.00%|                                     \"#\" + str(j) + \" has length \" + str(len(p)) +\n",
      "   749|         0|            0|            0|  0.00%|                                     \"; 2 is required\")\n",
      "   750|         0|            0|            0|  0.00%|                # parameters as length-2 list too cumbersome to type, see ModuleDict.update comment\n",
      "   751|         0|            0|            0|  0.00%|                self[p[0]] = p[1]  # type: ignore[assignment]\n",
      "   752|         0|            0|            0|  0.00%|\n",
      "   753|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   754|         0|            0|            0|  0.00%|        child_lines = []\n",
      "   755|         0|            0|            0|  0.00%|        for k, p in self.items():\n",
      "   756|         0|            0|            0|  0.00%|            if isinstance(p, torch.Tensor):\n",
      "   757|         0|            0|            0|  0.00%|                size_str = 'x'.join(str(size) for size in p.size())\n",
      "   758|         0|            0|            0|  0.00%|                device_str = '' if not p.is_cuda else ' (GPU {})'.format(p.get_device())\n",
      "   759|         0|            0|            0|  0.00%|                parastr = '{} containing: [{} of size {}{}]'.format(\n",
      "   760|         0|            0|            0|  0.00%|                    \"Parameter\" if isinstance(p, Parameter) else \"Tensor\",\n",
      "   761|         0|            0|            0|  0.00%|                    torch.typename(p), size_str, device_str)\n",
      "   762|         0|            0|            0|  0.00%|                child_lines.append('  (' + str(k) + '): ' + parastr)\n",
      "   763|         0|            0|            0|  0.00%|            else:\n",
      "   764|         0|            0|            0|  0.00%|                child_lines.append('  (' + str(k) + '): Object of type: ' + type(p).__name__)\n",
      "   765|         0|            0|            0|  0.00%|        tmpstr = '\\n'.join(child_lines)\n",
      "   766|         0|            0|            0|  0.00%|        return tmpstr\n",
      "   767|         0|            0|            0|  0.00%|\n",
      "   768|         0|            0|            0|  0.00%|    def __call__(self, input):\n",
      "   769|         0|            0|            0|  0.00%|        raise RuntimeError('ParameterDict should not be called.')\n",
      "   770|         0|            0|            0|  0.00%|\n",
      "   771|         0|            0|            0|  0.00%|    def __or__(self, other: 'ParameterDict') -> 'ParameterDict':\n",
      "   772|         0|            0|            0|  0.00%|        copy = self.copy()\n",
      "   773|         0|            0|            0|  0.00%|        copy.update(other)\n",
      "   774|         0|            0|            0|  0.00%|        return copy\n",
      "   775|         0|            0|            0|  0.00%|\n",
      "   776|         0|            0|            0|  0.00%|    def __ror__(self, other: 'ParameterDict') -> 'ParameterDict':\n",
      "   777|         0|            0|            0|  0.00%|        copy = other.copy()\n",
      "   778|         0|            0|            0|  0.00%|        copy.update(self)\n",
      "   779|         0|            0|            0|  0.00%|        return copy\n",
      "   780|         0|            0|            0|  0.00%|\n",
      "   781|         0|            0|            0|  0.00%|    def __ior__(self, other : 'ParameterDict') -> 'ParameterDict':\n",
      "   782|         0|            0|            0|  0.00%|        self.update(other)\n",
      "   783|         0|            0|            0|  0.00%|        return self\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/functional.py\n",
      "File duration: 3.91549s (0.63%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|from typing import (\n",
      "     2|         0|            0|            0|  0.00%|    List, Tuple, Optional, Union, Any, Sequence, TYPE_CHECKING\n",
      "     3|         0|            0|            0|  0.00%|)\n",
      "     4|         0|            0|            0|  0.00%|\n",
      "     5|         0|            0|            0|  0.00%|import torch\n",
      "     6|         0|            0|            0|  0.00%|from torch._C import _add_docstr\n",
      "     7|         0|            0|            0|  0.00%|import torch.nn.functional as F\n",
      "     8|         0|            0|            0|  0.00%|from ._lowrank import svd_lowrank, pca_lowrank\n",
      "     9|         0|            0|            0|  0.00%|from .overrides import (\n",
      "    10|         0|            0|            0|  0.00%|    has_torch_function, has_torch_function_unary, has_torch_function_variadic,\n",
      "    11|         0|            0|            0|  0.00%|    handle_torch_function)\n",
      "    12|         0|            0|            0|  0.00%|from ._jit_internal import boolean_dispatch\n",
      "    13|         0|            0|            0|  0.00%|from ._jit_internal import _overload as overload\n",
      "    14|         0|            0|            0|  0.00%|\n",
      "    15|         0|            0|            0|  0.00%|Tensor = torch.Tensor\n",
      "    16|         0|            0|            0|  0.00%|from torch import _VF\n",
      "    17|         0|            0|            0|  0.00%|\n",
      "    18|         0|            0|            0|  0.00%|__all__ = [\n",
      "    19|         0|            0|            0|  0.00%|    'atleast_1d',\n",
      "    20|         0|            0|            0|  0.00%|    'atleast_2d',\n",
      "    21|         0|            0|            0|  0.00%|    'atleast_3d',\n",
      "    22|         0|            0|            0|  0.00%|    'align_tensors',\n",
      "    23|         0|            0|            0|  0.00%|    'broadcast_shapes',\n",
      "    24|         0|            0|            0|  0.00%|    'broadcast_tensors',\n",
      "    25|         0|            0|            0|  0.00%|    'cartesian_prod',\n",
      "    26|         0|            0|            0|  0.00%|    'block_diag',\n",
      "    27|         0|            0|            0|  0.00%|    'cdist',\n",
      "    28|         0|            0|            0|  0.00%|    'chain_matmul',\n",
      "    29|         0|            0|            0|  0.00%|    'einsum',\n",
      "    30|         0|            0|            0|  0.00%|    'istft',\n",
      "    31|         0|            0|            0|  0.00%|    'lu',\n",
      "    32|         0|            0|            0|  0.00%|    'norm',\n",
      "    33|         0|            0|            0|  0.00%|    'meshgrid',\n",
      "    34|         0|            0|            0|  0.00%|    'pca_lowrank',\n",
      "    35|         0|            0|            0|  0.00%|    'split',\n",
      "    36|         0|            0|            0|  0.00%|    'stft',\n",
      "    37|         0|            0|            0|  0.00%|    'svd_lowrank',\n",
      "    38|         0|            0|            0|  0.00%|    'tensordot',\n",
      "    39|         0|            0|            0|  0.00%|    'unique',\n",
      "    40|         0|            0|            0|  0.00%|    'unique_consecutive',\n",
      "    41|         0|            0|            0|  0.00%|]\n",
      "    42|         0|            0|            0|  0.00%|\n",
      "    43|         0|            0|            0|  0.00%|\n",
      "    44|     31590|    0.0602281|  1.90656e-06|  0.01%|def broadcast_tensors(*tensors):\n",
      "    45|         0|            0|            0|  0.00%|    r\"\"\"broadcast_tensors(*tensors) -> List of Tensors\n",
      "    46|         0|            0|            0|  0.00%|\n",
      "    47|         0|            0|            0|  0.00%|    Broadcasts the given tensors according to :ref:`broadcasting-semantics`.\n",
      "    48|         0|            0|            0|  0.00%|\n",
      "    49|         0|            0|            0|  0.00%|    Args:\n",
      "    50|         0|            0|            0|  0.00%|        *tensors: any number of tensors of the same type\n",
      "    51|         0|            0|            0|  0.00%|\n",
      "    52|         0|            0|            0|  0.00%|    .. warning::\n",
      "    53|         0|            0|            0|  0.00%|\n",
      "    54|         0|            0|            0|  0.00%|        More than one element of a broadcasted tensor may refer to a single\n",
      "    55|         0|            0|            0|  0.00%|        memory location. As a result, in-place operations (especially ones that\n",
      "    56|         0|            0|            0|  0.00%|        are vectorized) may result in incorrect behavior. If you need to write\n",
      "    57|         0|            0|            0|  0.00%|        to the tensors, please clone them first.\n",
      "    58|         0|            0|            0|  0.00%|\n",
      "    59|         0|            0|            0|  0.00%|    Example::\n",
      "    60|         0|            0|            0|  0.00%|\n",
      "    61|         0|            0|            0|  0.00%|        >>> x = torch.arange(3).view(1, 3)\n",
      "    62|         0|            0|            0|  0.00%|        >>> y = torch.arange(2).view(2, 1)\n",
      "    63|         0|            0|            0|  0.00%|        >>> a, b = torch.broadcast_tensors(x, y)\n",
      "    64|         0|            0|            0|  0.00%|        >>> a.size()\n",
      "    65|         0|            0|            0|  0.00%|        torch.Size([2, 3])\n",
      "    66|         0|            0|            0|  0.00%|        >>> a\n",
      "    67|         0|            0|            0|  0.00%|        tensor([[0, 1, 2],\n",
      "    68|         0|            0|            0|  0.00%|                [0, 1, 2]])\n",
      "    69|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    70|         0|            0|            0|  0.00%|    # This wrapper exists to support variadic args.\n",
      "    71|     31590|    0.0739195|  2.33997e-06|  0.01%|    if has_torch_function(tensors):\n",
      "    72|         0|            0|            0|  0.00%|        return handle_torch_function(broadcast_tensors, tensors, *tensors)\n",
      "    73|     31590|     0.625181|  1.97905e-05|  0.10%|    return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]\n",
      "(call)|     31590|       0.1509|  4.77683e-06|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_VF.py:25 __getattr__\n",
      "    74|         0|            0|            0|  0.00%|\n",
      "    75|         0|            0|            0|  0.00%|\n",
      "    76|         0|            0|            0|  0.00%|def broadcast_shapes(*shapes):\n",
      "    77|         0|            0|            0|  0.00%|    r\"\"\"broadcast_shapes(*shapes) -> Size\n",
      "    78|         0|            0|            0|  0.00%|\n",
      "    79|         0|            0|            0|  0.00%|    Similar to :func:`broadcast_tensors` but for shapes.\n",
      "    80|         0|            0|            0|  0.00%|\n",
      "    81|         0|            0|            0|  0.00%|    This is equivalent to\n",
      "    82|         0|            0|            0|  0.00%|    ``torch.broadcast_tensors(*map(torch.empty, shapes))[0].shape``\n",
      "    83|         0|            0|            0|  0.00%|    but avoids the need create to intermediate tensors. This is useful for\n",
      "    84|         0|            0|            0|  0.00%|    broadcasting tensors of common batch shape but different rightmost shape,\n",
      "    85|         0|            0|            0|  0.00%|    e.g. to broadcast mean vectors with covariance matrices.\n",
      "    86|         0|            0|            0|  0.00%|\n",
      "    87|         0|            0|            0|  0.00%|    Example::\n",
      "    88|         0|            0|            0|  0.00%|\n",
      "    89|         0|            0|            0|  0.00%|        >>> torch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\n",
      "    90|         0|            0|            0|  0.00%|        torch.Size([1, 3, 2])\n",
      "    91|         0|            0|            0|  0.00%|\n",
      "    92|         0|            0|            0|  0.00%|    Args:\n",
      "    93|         0|            0|            0|  0.00%|        \\*shapes (torch.Size): Shapes of tensors.\n",
      "    94|         0|            0|            0|  0.00%|\n",
      "    95|         0|            0|            0|  0.00%|    Returns:\n",
      "    96|         0|            0|            0|  0.00%|        shape (torch.Size): A shape compatible with all input shapes.\n",
      "    97|         0|            0|            0|  0.00%|\n",
      "    98|         0|            0|            0|  0.00%|    Raises:\n",
      "    99|         0|            0|            0|  0.00%|        RuntimeError: If shapes are incompatible.\n",
      "   100|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   101|         0|            0|            0|  0.00%|    # This wrapper exists to support variadic args.\n",
      "   102|         0|            0|            0|  0.00%|    # TODO Move this to C++ once the jit has better support for torch.Size.\n",
      "   103|         0|            0|            0|  0.00%|    if not torch.jit.is_tracing():\n",
      "   104|         0|            0|            0|  0.00%|        max_len = 0\n",
      "   105|         0|            0|            0|  0.00%|        for shape in shapes:\n",
      "   106|         0|            0|            0|  0.00%|            if isinstance(shape, int):\n",
      "   107|         0|            0|            0|  0.00%|                if max_len < 1:\n",
      "   108|         0|            0|            0|  0.00%|                    max_len = 1\n",
      "   109|         0|            0|            0|  0.00%|            elif isinstance(shape, tuple) or isinstance(shape, list):\n",
      "   110|         0|            0|            0|  0.00%|                s = len(shape)\n",
      "   111|         0|            0|            0|  0.00%|                if max_len < s:\n",
      "   112|         0|            0|            0|  0.00%|                    max_len = s\n",
      "   113|         0|            0|            0|  0.00%|        result = [1] * max_len\n",
      "   114|         0|            0|            0|  0.00%|        for shape in shapes:\n",
      "   115|         0|            0|            0|  0.00%|            if isinstance(shape, int):\n",
      "   116|         0|            0|            0|  0.00%|                shape = (shape,)\n",
      "   117|         0|            0|            0|  0.00%|            if isinstance(shape, tuple) or isinstance(shape, list):\n",
      "   118|         0|            0|            0|  0.00%|                for i in range(-1, -1 - len(shape), -1):\n",
      "   119|         0|            0|            0|  0.00%|                    if shape[i] < 0:\n",
      "   120|         0|            0|            0|  0.00%|                        raise RuntimeError(\"Trying to create tensor with negative dimension ({}): ({})\"\n",
      "   121|         0|            0|            0|  0.00%|                                           .format(shape[i], shape[i]))\n",
      "   122|         0|            0|            0|  0.00%|                    if shape[i] == 1 or shape[i] == result[i]:\n",
      "   123|         0|            0|            0|  0.00%|                        continue\n",
      "   124|         0|            0|            0|  0.00%|                    if result[i] != 1:\n",
      "   125|         0|            0|            0|  0.00%|                        raise RuntimeError(\"Shape mismatch: objects cannot be broadcast to a single shape\")\n",
      "   126|         0|            0|            0|  0.00%|                    result[i] = shape[i]\n",
      "   127|         0|            0|            0|  0.00%|            else:\n",
      "   128|         0|            0|            0|  0.00%|                raise RuntimeError(\"Input shapes should be of type ints, a tuple of ints, or a list of ints, got \", shape)\n",
      "   129|         0|            0|            0|  0.00%|        return torch.Size(result)\n",
      "   130|         0|            0|            0|  0.00%|    else:\n",
      "   131|         0|            0|            0|  0.00%|        # with implementation above, torch.jit.trace hardcodes the sizes which makes subsequent replays fail\n",
      "   132|         0|            0|            0|  0.00%|        with torch.no_grad():\n",
      "   133|         0|            0|            0|  0.00%|            scalar = torch.zeros((), device=\"cpu\")\n",
      "   134|         0|            0|            0|  0.00%|            tensors = [scalar.expand(shape) for shape in shapes]\n",
      "   135|         0|            0|            0|  0.00%|            tensors = broadcast_tensors(*tensors)\n",
      "   136|         0|            0|            0|  0.00%|            return tensors[0].shape\n",
      "   137|         0|            0|            0|  0.00%|\n",
      "   138|         0|            0|            0|  0.00%|\n",
      "   139|         0|            0|            0|  0.00%|\n",
      "   140|         0|            0|            0|  0.00%|def split(\n",
      "   141|         0|            0|            0|  0.00%|    tensor: Tensor, split_size_or_sections: Union[int, List[int]], dim: int = 0\n",
      "   142|         0|            0|            0|  0.00%|) -> List[Tensor]:\n",
      "   143|         0|            0|            0|  0.00%|    r\"\"\"Splits the tensor into chunks. Each chunk is a view of the original tensor.\n",
      "   144|         0|            0|            0|  0.00%|\n",
      "   145|         0|            0|            0|  0.00%|    If :attr:`split_size_or_sections` is an integer type, then :attr:`tensor` will\n",
      "   146|         0|            0|            0|  0.00%|    be split into equally sized chunks (if possible). Last chunk will be smaller if\n",
      "   147|         0|            0|            0|  0.00%|    the tensor size along the given dimension :attr:`dim` is not divisible by\n",
      "   148|         0|            0|            0|  0.00%|    :attr:`split_size`.\n",
      "   149|         0|            0|            0|  0.00%|\n",
      "   150|         0|            0|            0|  0.00%|    If :attr:`split_size_or_sections` is a list, then :attr:`tensor` will be split\n",
      "   151|         0|            0|            0|  0.00%|    into ``len(split_size_or_sections)`` chunks with sizes in :attr:`dim` according\n",
      "   152|         0|            0|            0|  0.00%|    to :attr:`split_size_or_sections`.\n",
      "   153|         0|            0|            0|  0.00%|\n",
      "   154|         0|            0|            0|  0.00%|    Args:\n",
      "   155|         0|            0|            0|  0.00%|        tensor (Tensor): tensor to split.\n",
      "   156|         0|            0|            0|  0.00%|        split_size_or_sections (int) or (list(int)): size of a single chunk or\n",
      "   157|         0|            0|            0|  0.00%|            list of sizes for each chunk\n",
      "   158|         0|            0|            0|  0.00%|        dim (int): dimension along which to split the tensor.\n",
      "   159|         0|            0|            0|  0.00%|\n",
      "   160|         0|            0|            0|  0.00%|    Example::\n",
      "   161|         0|            0|            0|  0.00%|\n",
      "   162|         0|            0|            0|  0.00%|        >>> a = torch.arange(10).reshape(5,2)\n",
      "   163|         0|            0|            0|  0.00%|        >>> a\n",
      "   164|         0|            0|            0|  0.00%|        tensor([[0, 1],\n",
      "   165|         0|            0|            0|  0.00%|                [2, 3],\n",
      "   166|         0|            0|            0|  0.00%|                [4, 5],\n",
      "   167|         0|            0|            0|  0.00%|                [6, 7],\n",
      "   168|         0|            0|            0|  0.00%|                [8, 9]])\n",
      "   169|         0|            0|            0|  0.00%|        >>> torch.split(a, 2)\n",
      "   170|         0|            0|            0|  0.00%|        (tensor([[0, 1],\n",
      "   171|         0|            0|            0|  0.00%|                 [2, 3]]),\n",
      "   172|         0|            0|            0|  0.00%|         tensor([[4, 5],\n",
      "   173|         0|            0|            0|  0.00%|                 [6, 7]]),\n",
      "   174|         0|            0|            0|  0.00%|         tensor([[8, 9]]))\n",
      "   175|         0|            0|            0|  0.00%|        >>> torch.split(a, [1,4])\n",
      "   176|         0|            0|            0|  0.00%|        (tensor([[0, 1]]),\n",
      "   177|         0|            0|            0|  0.00%|         tensor([[2, 3],\n",
      "   178|         0|            0|            0|  0.00%|                 [4, 5],\n",
      "   179|         0|            0|            0|  0.00%|                 [6, 7],\n",
      "   180|         0|            0|            0|  0.00%|                 [8, 9]]))\n",
      "   181|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   182|         0|            0|            0|  0.00%|    if has_torch_function_unary(tensor):\n",
      "   183|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   184|         0|            0|            0|  0.00%|            split, (tensor,), tensor, split_size_or_sections, dim=dim)\n",
      "   185|         0|            0|            0|  0.00%|    # Overwriting reason:\n",
      "   186|         0|            0|            0|  0.00%|    # This dispatches to two ATen functions depending on the type of\n",
      "   187|         0|            0|            0|  0.00%|    # split_size_or_sections. The branching code is in _tensor.py, which we\n",
      "   188|         0|            0|            0|  0.00%|    # call here.\n",
      "   189|         0|            0|            0|  0.00%|    return tensor.split(split_size_or_sections, dim)\n",
      "   190|         0|            0|            0|  0.00%|\n",
      "   191|         0|            0|            0|  0.00%|\n",
      "   192|         0|            0|            0|  0.00%|def einsum(*args: Any) -> Tensor:\n",
      "   193|         0|            0|            0|  0.00%|    r\"\"\"einsum(equation, *operands) -> Tensor\n",
      "   194|         0|            0|            0|  0.00%|\n",
      "   195|         0|            0|            0|  0.00%|    Sums the product of the elements of the input :attr:`operands` along dimensions specified using a notation\n",
      "   196|         0|            0|            0|  0.00%|    based on the Einstein summation convention.\n",
      "   197|         0|            0|            0|  0.00%|\n",
      "   198|         0|            0|            0|  0.00%|    Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\n",
      "   199|         0|            0|            0|  0.00%|    in a short-hand format based on the Einstein summation convention, given by :attr:`equation`. The details of\n",
      "   200|         0|            0|            0|  0.00%|    this format are described below, but the general idea is to label every dimension of the input :attr:`operands`\n",
      "   201|         0|            0|            0|  0.00%|    with some subscript and define which subscripts are part of the output. The output is then computed by summing\n",
      "   202|         0|            0|            0|  0.00%|    the product of the elements of the :attr:`operands` along the dimensions whose subscripts are not part of the\n",
      "   203|         0|            0|            0|  0.00%|    output. For example, matrix multiplication can be computed using einsum as `torch.einsum(\"ij,jk->ik\", A, B)`.\n",
      "   204|         0|            0|            0|  0.00%|    Here, j is the summation subscript and i and k the output subscripts (see section below for more details on why).\n",
      "   205|         0|            0|            0|  0.00%|\n",
      "   206|         0|            0|            0|  0.00%|    Equation:\n",
      "   207|         0|            0|            0|  0.00%|\n",
      "   208|         0|            0|            0|  0.00%|        The :attr:`equation` string specifies the subscripts (letters in `[a-zA-Z]`) for each dimension of\n",
      "   209|         0|            0|            0|  0.00%|        the input :attr:`operands` in the same order as the dimensions, separating subcripts for each operand by a\n",
      "   210|         0|            0|            0|  0.00%|        comma (','), e.g. `'ij,jk'` specify subscripts for two 2D operands. The dimensions labeled with the same subscript\n",
      "   211|         0|            0|            0|  0.00%|        must be broadcastable, that is, their size must either match or be `1`. The exception is if a subscript is\n",
      "   212|         0|            0|            0|  0.00%|        repeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\n",
      "   213|         0|            0|            0|  0.00%|        must match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\n",
      "   214|         0|            0|            0|  0.00%|        appear exactly once in the :attr:`equation` will be part of the output, sorted in increasing alphabetical order.\n",
      "   215|         0|            0|            0|  0.00%|        The output is computed by multiplying the input :attr:`operands` element-wise, with their dimensions aligned based\n",
      "   216|         0|            0|            0|  0.00%|        on the subscripts, and then summing out the dimensions whose subscripts are not part of the output.\n",
      "   217|         0|            0|            0|  0.00%|\n",
      "   218|         0|            0|            0|  0.00%|        Optionally, the output subscripts can be explicitly defined by adding an arrow ('->') at the end of the equation\n",
      "   219|         0|            0|            0|  0.00%|        followed by the subscripts for the output. For instance, the following equation computes the transpose of a\n",
      "   220|         0|            0|            0|  0.00%|        matrix multiplication: 'ij,jk->ki'. The output subscripts must appear at least once for some input operand and\n",
      "   221|         0|            0|            0|  0.00%|        at most once for the output.\n",
      "   222|         0|            0|            0|  0.00%|\n",
      "   223|         0|            0|            0|  0.00%|        Ellipsis ('...') can be used in place of subscripts to broadcast the dimensions covered by the ellipsis.\n",
      "   224|         0|            0|            0|  0.00%|        Each input operand may contain at most one ellipsis which will cover the dimensions not covered by subscripts,\n",
      "   225|         0|            0|            0|  0.00%|        e.g. for an input operand with 5 dimensions, the ellipsis in the equation `'ab...c'` cover the third and fourth\n",
      "   226|         0|            0|            0|  0.00%|        dimensions. The ellipsis does not need to cover the same number of dimensions across the :attr:`operands` but the\n",
      "   227|         0|            0|            0|  0.00%|        'shape' of the ellipsis (the size of the dimensions covered by them) must broadcast together. If the output is not\n",
      "   228|         0|            0|            0|  0.00%|        explicitly defined with the arrow ('->') notation, the ellipsis will come first in the output (left-most dimensions),\n",
      "   229|         0|            0|            0|  0.00%|        before the subscript labels that appear exactly once for the input operands. e.g. the following equation implements\n",
      "   230|         0|            0|            0|  0.00%|        batch matrix multiplication `'...ij,...jk'`.\n",
      "   231|         0|            0|            0|  0.00%|\n",
      "   232|         0|            0|            0|  0.00%|        A few final notes: the equation may contain whitespaces between the different elements (subscripts, ellipsis,\n",
      "   233|         0|            0|            0|  0.00%|        arrow and comma) but something like `'. . .'` is not valid. An empty string `''` is valid for scalar operands.\n",
      "   234|         0|            0|            0|  0.00%|\n",
      "   235|         0|            0|            0|  0.00%|    .. note::\n",
      "   236|         0|            0|            0|  0.00%|\n",
      "   237|         0|            0|            0|  0.00%|        ``torch.einsum`` handles ellipsis ('...') differently from NumPy in that it allows dimensions\n",
      "   238|         0|            0|            0|  0.00%|        covered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output.\n",
      "   239|         0|            0|            0|  0.00%|\n",
      "   240|         0|            0|            0|  0.00%|    .. note::\n",
      "   241|         0|            0|            0|  0.00%|\n",
      "   242|         0|            0|            0|  0.00%|        This function does not optimize the given expression, so a different formula for the same computation may\n",
      "   243|         0|            0|            0|  0.00%|        run faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/)\n",
      "   244|         0|            0|            0|  0.00%|        can optimize the formula for you.\n",
      "   245|         0|            0|            0|  0.00%|\n",
      "   246|         0|            0|            0|  0.00%|    .. note::\n",
      "   247|         0|            0|            0|  0.00%|\n",
      "   248|         0|            0|            0|  0.00%|        As of PyTorch 1.10 :func:`torch.einsum` also supports the sublist format (see examples below). In this format,\n",
      "   249|         0|            0|            0|  0.00%|        subscripts for each operand are specified by sublists, list of integers in the range [0, 52). These sublists\n",
      "   250|         0|            0|            0|  0.00%|        follow their operands, and an extra sublist can appear at the end of the input to specify the output's\n",
      "   251|         0|            0|            0|  0.00%|        subscripts., e.g. `torch.einsum(op1, sublist1, op2, sublist2, ..., [subslist_out])`. Python's `Ellipsis` object\n",
      "   252|         0|            0|            0|  0.00%|        may be provided in a sublist to enable broadcasting as described in the Equation section above.\n",
      "   253|         0|            0|            0|  0.00%|\n",
      "   254|         0|            0|            0|  0.00%|    Args:\n",
      "   255|         0|            0|            0|  0.00%|        equation (string): The subscripts for the Einstein summation.\n",
      "   256|         0|            0|            0|  0.00%|        operands (List[Tensor]): The tensors to compute the Einstein summation of.\n",
      "   257|         0|            0|            0|  0.00%|\n",
      "   258|         0|            0|            0|  0.00%|    Examples::\n",
      "   259|         0|            0|            0|  0.00%|\n",
      "   260|         0|            0|            0|  0.00%|        # trace\n",
      "   261|         0|            0|            0|  0.00%|        >>> torch.einsum('ii', torch.randn(4, 4))\n",
      "   262|         0|            0|            0|  0.00%|        tensor(-1.2104)\n",
      "   263|         0|            0|            0|  0.00%|\n",
      "   264|         0|            0|            0|  0.00%|        # diagonal\n",
      "   265|         0|            0|            0|  0.00%|        >>> torch.einsum('ii->i', torch.randn(4, 4))\n",
      "   266|         0|            0|            0|  0.00%|        tensor([-0.1034,  0.7952, -0.2433,  0.4545])\n",
      "   267|         0|            0|            0|  0.00%|\n",
      "   268|         0|            0|            0|  0.00%|        # outer product\n",
      "   269|         0|            0|            0|  0.00%|        >>> x = torch.randn(5)\n",
      "   270|         0|            0|            0|  0.00%|        >>> y = torch.randn(4)\n",
      "   271|         0|            0|            0|  0.00%|        >>> torch.einsum('i,j->ij', x, y)\n",
      "   272|         0|            0|            0|  0.00%|        tensor([[ 0.1156, -0.2897, -0.3918,  0.4963],\n",
      "   273|         0|            0|            0|  0.00%|                [-0.3744,  0.9381,  1.2685, -1.6070],\n",
      "   274|         0|            0|            0|  0.00%|                [ 0.7208, -1.8058, -2.4419,  3.0936],\n",
      "   275|         0|            0|            0|  0.00%|                [ 0.1713, -0.4291, -0.5802,  0.7350],\n",
      "   276|         0|            0|            0|  0.00%|                [ 0.5704, -1.4290, -1.9323,  2.4480]])\n",
      "   277|         0|            0|            0|  0.00%|\n",
      "   278|         0|            0|            0|  0.00%|        # batch matrix multiplication\n",
      "   279|         0|            0|            0|  0.00%|        >>> As = torch.randn(3,2,5)\n",
      "   280|         0|            0|            0|  0.00%|        >>> Bs = torch.randn(3,5,4)\n",
      "   281|         0|            0|            0|  0.00%|        >>> torch.einsum('bij,bjk->bik', As, Bs)\n",
      "   282|         0|            0|            0|  0.00%|        tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n",
      "   283|         0|            0|            0|  0.00%|                [-1.6706, -0.8097, -0.8025, -2.1183]],\n",
      "   284|         0|            0|            0|  0.00%|\n",
      "   285|         0|            0|            0|  0.00%|                [[ 4.2239,  0.3107, -0.5756, -0.2354],\n",
      "   286|         0|            0|            0|  0.00%|                [-1.4558, -0.3460,  1.5087, -0.8530]],\n",
      "   287|         0|            0|            0|  0.00%|\n",
      "   288|         0|            0|            0|  0.00%|                [[ 2.8153,  1.8787, -4.3839, -1.2112],\n",
      "   289|         0|            0|            0|  0.00%|                [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n",
      "   290|         0|            0|            0|  0.00%|\n",
      "   291|         0|            0|            0|  0.00%|        # with sublist format and ellipsis\n",
      "   292|         0|            0|            0|  0.00%|        >>> torch.einsum(As, [..., 0, 1], Bs, [..., 1, 2], [..., 0, 2])\n",
      "   293|         0|            0|            0|  0.00%|        tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n",
      "   294|         0|            0|            0|  0.00%|                [-1.6706, -0.8097, -0.8025, -2.1183]],\n",
      "   295|         0|            0|            0|  0.00%|\n",
      "   296|         0|            0|            0|  0.00%|                [[ 4.2239,  0.3107, -0.5756, -0.2354],\n",
      "   297|         0|            0|            0|  0.00%|                [-1.4558, -0.3460,  1.5087, -0.8530]],\n",
      "   298|         0|            0|            0|  0.00%|\n",
      "   299|         0|            0|            0|  0.00%|                [[ 2.8153,  1.8787, -4.3839, -1.2112],\n",
      "   300|         0|            0|            0|  0.00%|                [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n",
      "   301|         0|            0|            0|  0.00%|\n",
      "   302|         0|            0|            0|  0.00%|        # batch permute\n",
      "   303|         0|            0|            0|  0.00%|        >>> A = torch.randn(2, 3, 4, 5)\n",
      "   304|         0|            0|            0|  0.00%|        >>> torch.einsum('...ij->...ji', A).shape\n",
      "   305|         0|            0|            0|  0.00%|        torch.Size([2, 3, 5, 4])\n",
      "   306|         0|            0|            0|  0.00%|\n",
      "   307|         0|            0|            0|  0.00%|        # equivalent to torch.nn.functional.bilinear\n",
      "   308|         0|            0|            0|  0.00%|        >>> A = torch.randn(3,5,4)\n",
      "   309|         0|            0|            0|  0.00%|        >>> l = torch.randn(2,5)\n",
      "   310|         0|            0|            0|  0.00%|        >>> r = torch.randn(2,4)\n",
      "   311|         0|            0|            0|  0.00%|        >>> torch.einsum('bn,anm,bm->ba', l, A, r)\n",
      "   312|         0|            0|            0|  0.00%|        tensor([[-0.3430, -5.2405,  0.4494],\n",
      "   313|         0|            0|            0|  0.00%|                [ 0.3311,  5.5201, -3.0356]])\n",
      "   314|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   315|         0|            0|            0|  0.00%|    # This wrapper exists to support variadic args.\n",
      "   316|         0|            0|            0|  0.00%|    if len(args) < 2:\n",
      "   317|         0|            0|            0|  0.00%|        raise ValueError('einsum(): must specify the equation string and at least one operand, '\n",
      "   318|         0|            0|            0|  0.00%|                         'or at least one operand and its subscripts list')\n",
      "   319|         0|            0|            0|  0.00%|\n",
      "   320|         0|            0|            0|  0.00%|    equation = None\n",
      "   321|         0|            0|            0|  0.00%|    operands = None\n",
      "   322|         0|            0|            0|  0.00%|\n",
      "   323|         0|            0|            0|  0.00%|    if isinstance(args[0], torch.Tensor):\n",
      "   324|         0|            0|            0|  0.00%|        # Convert the subscript list format which is an interleaving of operand and its subscripts\n",
      "   325|         0|            0|            0|  0.00%|        # list with an optional output subscripts list at the end (see documentation for more details on this)\n",
      "   326|         0|            0|            0|  0.00%|        # to the equation string format by creating the equation string from the subscripts list and grouping the\n",
      "   327|         0|            0|            0|  0.00%|        # input operands into a tensorlist (List[Tensor]).\n",
      "   328|         0|            0|            0|  0.00%|        def parse_subscript(n: int) -> str:\n",
      "   329|         0|            0|            0|  0.00%|            if n == Ellipsis:\n",
      "   330|         0|            0|            0|  0.00%|                return '...'\n",
      "   331|         0|            0|            0|  0.00%|            if n >= 0 and n < 26:\n",
      "   332|         0|            0|            0|  0.00%|                return chr(ord('A') + n)\n",
      "   333|         0|            0|            0|  0.00%|            if n >= 26 and n < 52:\n",
      "   334|         0|            0|            0|  0.00%|                return chr(ord('a') + n - 26)\n",
      "   335|         0|            0|            0|  0.00%|            raise ValueError('einsum(): subscript in subscript list is not within the valid range [0, 52)')\n",
      "   336|         0|            0|            0|  0.00%|\n",
      "   337|         0|            0|            0|  0.00%|        # Parse subscripts for input operands\n",
      "   338|         0|            0|            0|  0.00%|        equation = ','.join(''.join(parse_subscript(s) for s in l) for l in args[1::2])\n",
      "   339|         0|            0|            0|  0.00%|\n",
      "   340|         0|            0|            0|  0.00%|        # Parse optional output subscripts (provided when the number of arguments is odd)\n",
      "   341|         0|            0|            0|  0.00%|        if len(args) % 2 == 1:\n",
      "   342|         0|            0|            0|  0.00%|            equation += '->' + ''.join(parse_subscript(s) for s in args[-1])\n",
      "   343|         0|            0|            0|  0.00%|            operands = args[:-1:2]\n",
      "   344|         0|            0|            0|  0.00%|        else:\n",
      "   345|         0|            0|            0|  0.00%|            operands = args[::2]\n",
      "   346|         0|            0|            0|  0.00%|    else:\n",
      "   347|         0|            0|            0|  0.00%|        equation = args[0]\n",
      "   348|         0|            0|            0|  0.00%|        operands = args[1:]\n",
      "   349|         0|            0|            0|  0.00%|\n",
      "   350|         0|            0|            0|  0.00%|    if has_torch_function(operands):\n",
      "   351|         0|            0|            0|  0.00%|        return handle_torch_function(einsum, operands, equation, *operands)\n",
      "   352|         0|            0|            0|  0.00%|\n",
      "   353|         0|            0|            0|  0.00%|    if len(operands) == 1 and isinstance(operands[0], (list, tuple)):\n",
      "   354|         0|            0|            0|  0.00%|        # the old interface of passing the operands as one list argument\n",
      "   355|         0|            0|            0|  0.00%|        _operands = operands[0]\n",
      "   356|         0|            0|            0|  0.00%|        # recurse incase operands contains value that has torch function\n",
      "   357|         0|            0|            0|  0.00%|        # in the original implementation this line is omitted\n",
      "   358|         0|            0|            0|  0.00%|        return einsum(equation, *_operands)\n",
      "   359|         0|            0|            0|  0.00%|\n",
      "   360|         0|            0|            0|  0.00%|    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]\n",
      "   361|         0|            0|            0|  0.00%|\n",
      "   362|         0|            0|            0|  0.00%|\n",
      "   363|         0|            0|            0|  0.00%|# This wrapper exists to support variadic args.\n",
      "   364|         0|            0|            0|  0.00%|if TYPE_CHECKING:\n",
      "   365|         0|            0|            0|  0.00%|    # The JIT doesn't understand Union, so only add type annotation for mypy\n",
      "   366|         0|            0|            0|  0.00%|    def meshgrid(*tensors: Union[Tensor, List[Tensor]],\n",
      "   367|         0|            0|            0|  0.00%|                 indexing: Optional[str] = None) -> Tuple[Tensor, ...]:\n",
      "   368|         0|            0|            0|  0.00%|        return _meshgrid(*tensors, indexing=indexing)\n",
      "   369|         0|            0|            0|  0.00%|else:\n",
      "   370|         0|            0|            0|  0.00%|    def meshgrid(*tensors, indexing: Optional[str] = None) -> Tuple[Tensor, ...]:\n",
      "   371|         0|            0|            0|  0.00%|        r\"\"\"Creates grids of coordinates specified by the 1D inputs in `attr`:tensors.\n",
      "   372|         0|            0|            0|  0.00%|\n",
      "   373|         0|            0|            0|  0.00%|        This is helpful when you want to visualize data over some\n",
      "   374|         0|            0|            0|  0.00%|        range of inputs. See below for a plotting example.\n",
      "   375|         0|            0|            0|  0.00%|\n",
      "   376|         0|            0|            0|  0.00%|        Given :math:`N` 1D tensors :math:`T_0 \\ldots T_{N-1}` as\n",
      "   377|         0|            0|            0|  0.00%|        inputs with corresponding sizes :math:`S_0 \\ldots S_{N-1}`,\n",
      "   378|         0|            0|            0|  0.00%|        this creates :math:`N` N-dimensional tensors :math:`G_0 \\ldots\n",
      "   379|         0|            0|            0|  0.00%|        G_{N-1}`, each with shape :math:`(S_0, ..., S_{N-1})` where\n",
      "   380|         0|            0|            0|  0.00%|        the output :math:`G_i` is constructed by expanding :math:`T_i`\n",
      "   381|         0|            0|            0|  0.00%|        to the result shape.\n",
      "   382|         0|            0|            0|  0.00%|\n",
      "   383|         0|            0|            0|  0.00%|        .. note::\n",
      "   384|         0|            0|            0|  0.00%|            0D inputs are treated equivalently to 1D inputs of a\n",
      "   385|         0|            0|            0|  0.00%|            single element.\n",
      "   386|         0|            0|            0|  0.00%|\n",
      "   387|         0|            0|            0|  0.00%|        .. warning::\n",
      "   388|         0|            0|            0|  0.00%|            `torch.meshgrid(*tensors)` currently has the same behavior\n",
      "   389|         0|            0|            0|  0.00%|            as calling `numpy.meshgrid(*arrays, indexing='ij')`.\n",
      "   390|         0|            0|            0|  0.00%|\n",
      "   391|         0|            0|            0|  0.00%|            In the future `torch.meshgrid` will transition to\n",
      "   392|         0|            0|            0|  0.00%|            `indexing='xy'` as the default.\n",
      "   393|         0|            0|            0|  0.00%|\n",
      "   394|         0|            0|            0|  0.00%|            https://github.com/pytorch/pytorch/issues/50276 tracks\n",
      "   395|         0|            0|            0|  0.00%|            this issue with the goal of migrating to NumPy's behavior.\n",
      "   396|         0|            0|            0|  0.00%|\n",
      "   397|         0|            0|            0|  0.00%|        .. seealso::\n",
      "   398|         0|            0|            0|  0.00%|\n",
      "   399|         0|            0|            0|  0.00%|            :func:`torch.cartesian_prod` has the same effect but it\n",
      "   400|         0|            0|            0|  0.00%|            collects the data in a tensor of vectors.\n",
      "   401|         0|            0|            0|  0.00%|\n",
      "   402|         0|            0|            0|  0.00%|        Args:\n",
      "   403|         0|            0|            0|  0.00%|            tensors (list of Tensor): list of scalars or 1 dimensional tensors. Scalars will be\n",
      "   404|         0|            0|            0|  0.00%|                treated as tensors of size :math:`(1,)` automatically\n",
      "   405|         0|            0|            0|  0.00%|\n",
      "   406|         0|            0|            0|  0.00%|            indexing: (str, optional): the indexing mode, either \"xy\"\n",
      "   407|         0|            0|            0|  0.00%|                or \"ij\", defaults to \"ij\". See warning for future changes.\n",
      "   408|         0|            0|            0|  0.00%|\n",
      "   409|         0|            0|            0|  0.00%|                If \"xy\" is selected, the first dimension corresponds\n",
      "   410|         0|            0|            0|  0.00%|                to the cardinality of the second input and the second\n",
      "   411|         0|            0|            0|  0.00%|                dimension corresponds to the cardinality of the first\n",
      "   412|         0|            0|            0|  0.00%|                input.\n",
      "   413|         0|            0|            0|  0.00%|\n",
      "   414|         0|            0|            0|  0.00%|                If \"ij\" is selected, the dimensions are in the same\n",
      "   415|         0|            0|            0|  0.00%|                order as the cardinality of the inputs.\n",
      "   416|         0|            0|            0|  0.00%|\n",
      "   417|         0|            0|            0|  0.00%|        Returns:\n",
      "   418|         0|            0|            0|  0.00%|            seq (sequence of Tensors): If the input has :math:`N`\n",
      "   419|         0|            0|            0|  0.00%|            tensors of size :math:`S_0 \\ldots S_{N-1}``, then the\n",
      "   420|         0|            0|            0|  0.00%|            output will also have :math:`N` tensors, where each tensor\n",
      "   421|         0|            0|            0|  0.00%|            is of shape :math:`(S_0, ..., S_{N-1})`.\n",
      "   422|         0|            0|            0|  0.00%|\n",
      "   423|         0|            0|            0|  0.00%|        Example::\n",
      "   424|         0|            0|            0|  0.00%|\n",
      "   425|         0|            0|            0|  0.00%|            >>> x = torch.tensor([1, 2, 3])\n",
      "   426|         0|            0|            0|  0.00%|            >>> y = torch.tensor([4, 5, 6])\n",
      "   427|         0|            0|            0|  0.00%|\n",
      "   428|         0|            0|            0|  0.00%|            Observe the element-wise pairings across the grid, (1, 4),\n",
      "   429|         0|            0|            0|  0.00%|            (1, 5), ..., (3, 6). This is the same thing as the\n",
      "   430|         0|            0|            0|  0.00%|            cartesian product.\n",
      "   431|         0|            0|            0|  0.00%|            >>> grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\n",
      "   432|         0|            0|            0|  0.00%|            >>> grid_x\n",
      "   433|         0|            0|            0|  0.00%|            tensor([[1, 1, 1],\n",
      "   434|         0|            0|            0|  0.00%|                    [2, 2, 2],\n",
      "   435|         0|            0|            0|  0.00%|                    [3, 3, 3]])\n",
      "   436|         0|            0|            0|  0.00%|            >>> grid_y\n",
      "   437|         0|            0|            0|  0.00%|            tensor([[4, 5, 6],\n",
      "   438|         0|            0|            0|  0.00%|                    [4, 5, 6],\n",
      "   439|         0|            0|            0|  0.00%|                    [4, 5, 6]])\n",
      "   440|         0|            0|            0|  0.00%|\n",
      "   441|         0|            0|            0|  0.00%|            This correspondence can be seen when these grids are\n",
      "   442|         0|            0|            0|  0.00%|            stacked properly.\n",
      "   443|         0|            0|            0|  0.00%|            >>> torch.equal(torch.cat(tuple(torch.dstack([grid_x, grid_y]))),\n",
      "   444|         0|            0|            0|  0.00%|            ...             torch.cartesian_prod(x, y))\n",
      "   445|         0|            0|            0|  0.00%|            True\n",
      "   446|         0|            0|            0|  0.00%|\n",
      "   447|         0|            0|            0|  0.00%|            `torch.meshgrid` is commonly used to produce a grid for\n",
      "   448|         0|            0|            0|  0.00%|            plotting.\n",
      "   449|         0|            0|            0|  0.00%|            >>> import matplotlib.pyplot as plt\n",
      "   450|         0|            0|            0|  0.00%|            >>> xs = torch.linspace(-5, 5, steps=100)\n",
      "   451|         0|            0|            0|  0.00%|            >>> ys = torch.linspace(-5, 5, steps=100)\n",
      "   452|         0|            0|            0|  0.00%|            >>> x, y = torch.meshgrid(xs, ys, indexing='xy')\n",
      "   453|         0|            0|            0|  0.00%|            >>> z = torch.sin(torch.sqrt(x * x + y * y))\n",
      "   454|         0|            0|            0|  0.00%|            >>> ax = plt.axes(projection='3d')\n",
      "   455|         0|            0|            0|  0.00%|            >>> ax.plot_surface(x.numpy(), y.numpy(), z.numpy())\n",
      "   456|         0|            0|            0|  0.00%|            <mpl_toolkits.mplot3d.art3d.Poly3DCollection object at 0x7f8f30d40100>\n",
      "   457|         0|            0|            0|  0.00%|            >>> plt.show()\n",
      "   458|         0|            0|            0|  0.00%|\n",
      "   459|         0|            0|            0|  0.00%|        .. image:: ../_static/img/meshgrid.png\n",
      "   460|         0|            0|            0|  0.00%|            :width: 512\n",
      "   461|         0|            0|            0|  0.00%|\n",
      "   462|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   463|         0|            0|            0|  0.00%|        return _meshgrid(*tensors, indexing=indexing)\n",
      "   464|         0|            0|            0|  0.00%|\n",
      "   465|         0|            0|            0|  0.00%|\n",
      "   466|         0|            0|            0|  0.00%|def _meshgrid(*tensors, indexing: Optional[str]):\n",
      "   467|         0|            0|            0|  0.00%|    if has_torch_function(tensors):\n",
      "   468|         0|            0|            0|  0.00%|        return handle_torch_function(meshgrid, tensors, *tensors, indexing=indexing)\n",
      "   469|         0|            0|            0|  0.00%|    if len(tensors) == 1 and isinstance(tensors[0], (list, tuple)):\n",
      "   470|         0|            0|            0|  0.00%|        # the old interface of passing the operands as one list argument\n",
      "   471|         0|            0|            0|  0.00%|        tensors = tensors[0]  # type: ignore[assignment]\n",
      "   472|         0|            0|            0|  0.00%|\n",
      "   473|         0|            0|            0|  0.00%|    # Continue allowing call of old method that takes no indexing\n",
      "   474|         0|            0|            0|  0.00%|    # kwarg for forward compatibility reasons.\n",
      "   475|         0|            0|            0|  0.00%|    #\n",
      "   476|         0|            0|            0|  0.00%|    # Remove this two weeks after landing.\n",
      "   477|         0|            0|            0|  0.00%|    kwargs = {} if indexing is None else {'indexing': indexing}\n",
      "   478|         0|            0|            0|  0.00%|    return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "   479|         0|            0|            0|  0.00%|\n",
      "   480|         0|            0|            0|  0.00%|\n",
      "   481|         0|            0|            0|  0.00%|def stft(input: Tensor, n_fft: int, hop_length: Optional[int] = None,\n",
      "   482|         0|            0|            0|  0.00%|         win_length: Optional[int] = None, window: Optional[Tensor] = None,\n",
      "   483|         0|            0|            0|  0.00%|         center: bool = True, pad_mode: str = 'reflect', normalized: bool = False,\n",
      "   484|         0|            0|            0|  0.00%|         onesided: Optional[bool] = None,\n",
      "   485|         0|            0|            0|  0.00%|         return_complex: Optional[bool] = None) -> Tensor:\n",
      "   486|         0|            0|            0|  0.00%|    r\"\"\"Short-time Fourier transform (STFT).\n",
      "   487|         0|            0|            0|  0.00%|\n",
      "   488|         0|            0|            0|  0.00%|    .. warning::\n",
      "   489|         0|            0|            0|  0.00%|        From version 1.8.0, :attr:`return_complex` must always be given\n",
      "   490|         0|            0|            0|  0.00%|        explicitly for real inputs and `return_complex=False` has been\n",
      "   491|         0|            0|            0|  0.00%|        deprecated. Strongly prefer `return_complex=True` as in a future\n",
      "   492|         0|            0|            0|  0.00%|        pytorch release, this function will only return complex tensors.\n",
      "   493|         0|            0|            0|  0.00%|\n",
      "   494|         0|            0|            0|  0.00%|        Note that :func:`torch.view_as_real` can be used to recover a real\n",
      "   495|         0|            0|            0|  0.00%|        tensor with an extra last dimension for real and imaginary components.\n",
      "   496|         0|            0|            0|  0.00%|\n",
      "   497|         0|            0|            0|  0.00%|    The STFT computes the Fourier transform of short overlapping windows of the\n",
      "   498|         0|            0|            0|  0.00%|    input. This giving frequency components of the signal as they change over\n",
      "   499|         0|            0|            0|  0.00%|    time. The interface of this function is modeled after (but *not* a drop-in\n",
      "   500|         0|            0|            0|  0.00%|    replacement for) librosa_ stft function.\n",
      "   501|         0|            0|            0|  0.00%|\n",
      "   502|         0|            0|            0|  0.00%|    .. _librosa: https://librosa.org/doc/latest/generated/librosa.stft.html\n",
      "   503|         0|            0|            0|  0.00%|\n",
      "   504|         0|            0|            0|  0.00%|    Ignoring the optional batch dimension, this method computes the following\n",
      "   505|         0|            0|            0|  0.00%|    expression:\n",
      "   506|         0|            0|            0|  0.00%|\n",
      "   507|         0|            0|            0|  0.00%|    .. math::\n",
      "   508|         0|            0|            0|  0.00%|        X[\\omega, m] = \\sum_{k = 0}^{\\text{win\\_length-1}}%\n",
      "   509|         0|            0|            0|  0.00%|                            \\text{window}[k]\\ \\text{input}[m \\times \\text{hop\\_length} + k]\\ %\n",
      "   510|         0|            0|            0|  0.00%|                            \\exp\\left(- j \\frac{2 \\pi \\cdot \\omega k}{\\text{win\\_length}}\\right),\n",
      "   511|         0|            0|            0|  0.00%|\n",
      "   512|         0|            0|            0|  0.00%|    where :math:`m` is the index of the sliding window, and :math:`\\omega` is\n",
      "   513|         0|            0|            0|  0.00%|    the frequency :math:`0 \\leq \\omega < \\text{n\\_fft}` for ``onesided=False``,\n",
      "   514|         0|            0|            0|  0.00%|    or :math:`0 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 1` for ``onesided=True``.\n",
      "   515|         0|            0|            0|  0.00%|\n",
      "   516|         0|            0|            0|  0.00%|    * :attr:`input` must be either a 1-D time sequence or a 2-D batch of time\n",
      "   517|         0|            0|            0|  0.00%|      sequences.\n",
      "   518|         0|            0|            0|  0.00%|\n",
      "   519|         0|            0|            0|  0.00%|    * If :attr:`hop_length` is ``None`` (default), it is treated as equal to\n",
      "   520|         0|            0|            0|  0.00%|      ``floor(n_fft / 4)``.\n",
      "   521|         0|            0|            0|  0.00%|\n",
      "   522|         0|            0|            0|  0.00%|    * If :attr:`win_length` is ``None`` (default), it is treated as equal to\n",
      "   523|         0|            0|            0|  0.00%|      :attr:`n_fft`.\n",
      "   524|         0|            0|            0|  0.00%|\n",
      "   525|         0|            0|            0|  0.00%|    * :attr:`window` can be a 1-D tensor of size :attr:`win_length`, e.g., from\n",
      "   526|         0|            0|            0|  0.00%|      :meth:`torch.hann_window`. If :attr:`window` is ``None`` (default), it is\n",
      "   527|         0|            0|            0|  0.00%|      treated as if having :math:`1` everywhere in the window. If\n",
      "   528|         0|            0|            0|  0.00%|      :math:`\\text{win\\_length} < \\text{n\\_fft}`, :attr:`window` will be padded on\n",
      "   529|         0|            0|            0|  0.00%|      both sides to length :attr:`n_fft` before being applied.\n",
      "   530|         0|            0|            0|  0.00%|\n",
      "   531|         0|            0|            0|  0.00%|    * If :attr:`center` is ``True`` (default), :attr:`input` will be padded on\n",
      "   532|         0|            0|            0|  0.00%|      both sides so that the :math:`t`-th frame is centered at time\n",
      "   533|         0|            0|            0|  0.00%|      :math:`t \\times \\text{hop\\_length}`. Otherwise, the :math:`t`-th frame\n",
      "   534|         0|            0|            0|  0.00%|      begins at time  :math:`t \\times \\text{hop\\_length}`.\n",
      "   535|         0|            0|            0|  0.00%|\n",
      "   536|         0|            0|            0|  0.00%|    * :attr:`pad_mode` determines the padding method used on :attr:`input` when\n",
      "   537|         0|            0|            0|  0.00%|      :attr:`center` is ``True``. See :meth:`torch.nn.functional.pad` for\n",
      "   538|         0|            0|            0|  0.00%|      all available options. Default is ``\"reflect\"``.\n",
      "   539|         0|            0|            0|  0.00%|\n",
      "   540|         0|            0|            0|  0.00%|    * If :attr:`onesided` is ``True`` (default for real input), only values for\n",
      "   541|         0|            0|            0|  0.00%|      :math:`\\omega` in :math:`\\left[0, 1, 2, \\dots, \\left\\lfloor\n",
      "   542|         0|            0|            0|  0.00%|      \\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right]` are returned because\n",
      "   543|         0|            0|            0|  0.00%|      the real-to-complex Fourier transform satisfies the conjugate symmetry,\n",
      "   544|         0|            0|            0|  0.00%|      i.e., :math:`X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*`.\n",
      "   545|         0|            0|            0|  0.00%|      Note if the input or window tensors are complex, then :attr:`onesided`\n",
      "   546|         0|            0|            0|  0.00%|      output is not possible.\n",
      "   547|         0|            0|            0|  0.00%|\n",
      "   548|         0|            0|            0|  0.00%|    * If :attr:`normalized` is ``True`` (default is ``False``), the function\n",
      "   549|         0|            0|            0|  0.00%|      returns the normalized STFT results, i.e., multiplied by :math:`(\\text{frame\\_length})^{-0.5}`.\n",
      "   550|         0|            0|            0|  0.00%|\n",
      "   551|         0|            0|            0|  0.00%|    * If :attr:`return_complex` is ``True`` (default if input is complex), the\n",
      "   552|         0|            0|            0|  0.00%|      return is a ``input.dim() + 1`` dimensional complex tensor. If ``False``,\n",
      "   553|         0|            0|            0|  0.00%|      the output is a ``input.dim() + 2`` dimensional real tensor where the last\n",
      "   554|         0|            0|            0|  0.00%|      dimension represents the real and imaginary components.\n",
      "   555|         0|            0|            0|  0.00%|\n",
      "   556|         0|            0|            0|  0.00%|    Returns either a complex tensor of size :math:`(* \\times N \\times T)` if\n",
      "   557|         0|            0|            0|  0.00%|    :attr:`return_complex` is true, or a real tensor of size :math:`(* \\times N\n",
      "   558|         0|            0|            0|  0.00%|    \\times T \\times 2)`. Where :math:`*` is the optional batch size of\n",
      "   559|         0|            0|            0|  0.00%|    :attr:`input`, :math:`N` is the number of frequencies where STFT is applied\n",
      "   560|         0|            0|            0|  0.00%|    and :math:`T` is the total number of frames used.\n",
      "   561|         0|            0|            0|  0.00%|\n",
      "   562|         0|            0|            0|  0.00%|    .. warning::\n",
      "   563|         0|            0|            0|  0.00%|      This function changed signature at version 0.4.1. Calling with the\n",
      "   564|         0|            0|            0|  0.00%|      previous signature may cause error or return incorrect result.\n",
      "   565|         0|            0|            0|  0.00%|\n",
      "   566|         0|            0|            0|  0.00%|    Args:\n",
      "   567|         0|            0|            0|  0.00%|        input (Tensor): the input tensor\n",
      "   568|         0|            0|            0|  0.00%|        n_fft (int): size of Fourier transform\n",
      "   569|         0|            0|            0|  0.00%|        hop_length (int, optional): the distance between neighboring sliding window\n",
      "   570|         0|            0|            0|  0.00%|            frames. Default: ``None`` (treated as equal to ``floor(n_fft / 4)``)\n",
      "   571|         0|            0|            0|  0.00%|        win_length (int, optional): the size of window frame and STFT filter.\n",
      "   572|         0|            0|            0|  0.00%|            Default: ``None``  (treated as equal to :attr:`n_fft`)\n",
      "   573|         0|            0|            0|  0.00%|        window (Tensor, optional): the optional window function.\n",
      "   574|         0|            0|            0|  0.00%|            Default: ``None`` (treated as window of all :math:`1` s)\n",
      "   575|         0|            0|            0|  0.00%|        center (bool, optional): whether to pad :attr:`input` on both sides so\n",
      "   576|         0|            0|            0|  0.00%|            that the :math:`t`-th frame is centered at time :math:`t \\times \\text{hop\\_length}`.\n",
      "   577|         0|            0|            0|  0.00%|            Default: ``True``\n",
      "   578|         0|            0|            0|  0.00%|        pad_mode (string, optional): controls the padding method used when\n",
      "   579|         0|            0|            0|  0.00%|            :attr:`center` is ``True``. Default: ``\"reflect\"``\n",
      "   580|         0|            0|            0|  0.00%|        normalized (bool, optional): controls whether to return the normalized STFT results\n",
      "   581|         0|            0|            0|  0.00%|             Default: ``False``\n",
      "   582|         0|            0|            0|  0.00%|        onesided (bool, optional): controls whether to return half of results to\n",
      "   583|         0|            0|            0|  0.00%|            avoid redundancy for real inputs.\n",
      "   584|         0|            0|            0|  0.00%|            Default: ``True`` for real :attr:`input` and :attr:`window`, ``False`` otherwise.\n",
      "   585|         0|            0|            0|  0.00%|        return_complex (bool, optional): whether to return a complex tensor, or\n",
      "   586|         0|            0|            0|  0.00%|            a real tensor with an extra last dimension for the real and\n",
      "   587|         0|            0|            0|  0.00%|            imaginary components.\n",
      "   588|         0|            0|            0|  0.00%|\n",
      "   589|         0|            0|            0|  0.00%|    Returns:\n",
      "   590|         0|            0|            0|  0.00%|        Tensor: A tensor containing the STFT result with shape described above\n",
      "   591|         0|            0|            0|  0.00%|\n",
      "   592|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   593|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   594|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   595|         0|            0|            0|  0.00%|            stft, (input,), input, n_fft, hop_length=hop_length, win_length=win_length,\n",
      "   596|         0|            0|            0|  0.00%|            window=window, center=center, pad_mode=pad_mode, normalized=normalized,\n",
      "   597|         0|            0|            0|  0.00%|            onesided=onesided, return_complex=return_complex)\n",
      "   598|         0|            0|            0|  0.00%|    # NOTE: Do not edit. This code will be removed once the forward-compatibility\n",
      "   599|         0|            0|            0|  0.00%|    #       period is over for PR #73432\n",
      "   600|         0|            0|            0|  0.00%|    if center:\n",
      "   601|         0|            0|            0|  0.00%|        signal_dim = input.dim()\n",
      "   602|         0|            0|            0|  0.00%|        extended_shape = [1] * (3 - signal_dim) + list(input.size())\n",
      "   603|         0|            0|            0|  0.00%|        pad = int(n_fft // 2)\n",
      "   604|         0|            0|            0|  0.00%|        input = F.pad(input.view(extended_shape), [pad, pad], pad_mode)\n",
      "   605|         0|            0|            0|  0.00%|        input = input.view(input.shape[-signal_dim:])\n",
      "   606|         0|            0|            0|  0.00%|    return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "   607|         0|            0|            0|  0.00%|                    normalized, onesided, return_complex)\n",
      "   608|         0|            0|            0|  0.00%|\n",
      "   609|         0|            0|            0|  0.00%|\n",
      "   610|         0|            0|            0|  0.00%|istft = _add_docstr(\n",
      "   611|         0|            0|            0|  0.00%|    torch.istft,\n",
      "   612|         0|            0|            0|  0.00%|    \"istft(input, n_fft, hop_length=None, win_length=None, window=None, center=True, \"\n",
      "   613|         0|            0|            0|  0.00%|    \"normalized=False, onesided=None, length=None, return_complex=False) -> Tensor:\\n\"\n",
      "   614|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   615|         0|            0|            0|  0.00%|Inverse short time Fourier Transform. This is expected to be the inverse of :func:`~torch.stft`.\n",
      "   616|         0|            0|            0|  0.00%|\n",
      "   617|         0|            0|            0|  0.00%|It has the same parameters (+ additional optional parameter of :attr:`length`) and it should return the\n",
      "   618|         0|            0|            0|  0.00%|least squares estimation of the original signal. The algorithm will check using the NOLA condition (\n",
      "   619|         0|            0|            0|  0.00%|nonzero overlap).\n",
      "   620|         0|            0|            0|  0.00%|\n",
      "   621|         0|            0|            0|  0.00%|Important consideration in the parameters :attr:`window` and :attr:`center` so that the envelop\n",
      "   622|         0|            0|            0|  0.00%|created by the summation of all the windows is never zero at certain point in time. Specifically,\n",
      "   623|         0|            0|            0|  0.00%|:math:`\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0`.\n",
      "   624|         0|            0|            0|  0.00%|\n",
      "   625|         0|            0|            0|  0.00%|Since :func:`~torch.stft` discards elements at the end of the signal if they do not fit in a frame,\n",
      "   626|         0|            0|            0|  0.00%|``istft`` may return a shorter signal than the original signal (can occur if :attr:`center` is False\n",
      "   627|         0|            0|            0|  0.00%|since the signal isn't padded). If `length` is given in the arguments and is longer than expected,\n",
      "   628|         0|            0|            0|  0.00%|``istft`` will pad zeros to the end of the returned signal.\n",
      "   629|         0|            0|            0|  0.00%|\n",
      "   630|         0|            0|            0|  0.00%|If :attr:`center` is ``True``, then there will be padding e.g. ``'constant'``, ``'reflect'``, etc.\n",
      "   631|         0|            0|            0|  0.00%|Left padding can be trimmed off exactly because they can be calculated but right padding cannot be\n",
      "   632|         0|            0|            0|  0.00%|calculated without additional information.\n",
      "   633|         0|            0|            0|  0.00%|\n",
      "   634|         0|            0|            0|  0.00%|Example: Suppose the last window is:\n",
      "   635|         0|            0|            0|  0.00%|``[17, 18, 0, 0, 0]`` vs ``[18, 0, 0, 0, 0]``\n",
      "   636|         0|            0|            0|  0.00%|\n",
      "   637|         0|            0|            0|  0.00%|The :attr:`n_fft`, :attr:`hop_length`, :attr:`win_length` are all the same which prevents the calculation\n",
      "   638|         0|            0|            0|  0.00%|of right padding. These additional values could be zeros or a reflection of the signal so providing\n",
      "   639|         0|            0|            0|  0.00%|:attr:`length` could be useful. If :attr:`length` is ``None`` then padding will be aggressively removed\n",
      "   640|         0|            0|            0|  0.00%|(some loss of signal).\n",
      "   641|         0|            0|            0|  0.00%|\n",
      "   642|         0|            0|            0|  0.00%|[1] D. W. Griffin and J. S. Lim, \"Signal estimation from modified short-time Fourier transform,\"\n",
      "   643|         0|            0|            0|  0.00%|IEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984.\n",
      "   644|         0|            0|            0|  0.00%|\n",
      "   645|         0|            0|            0|  0.00%|Args:\n",
      "   646|         0|            0|            0|  0.00%|    input (Tensor): The input tensor. Expected to be output of :func:`~torch.stft`,\n",
      "   647|         0|            0|            0|  0.00%|        can either be complex (``channel``, ``fft_size``, ``n_frame``), or real\n",
      "   648|         0|            0|            0|  0.00%|        (``channel``, ``fft_size``, ``n_frame``, 2) where the ``channel``\n",
      "   649|         0|            0|            0|  0.00%|        dimension is optional.\n",
      "   650|         0|            0|            0|  0.00%|\n",
      "   651|         0|            0|            0|  0.00%|        .. deprecated:: 1.8.0\n",
      "   652|         0|            0|            0|  0.00%|            Real input is deprecated, use complex inputs as returned by\n",
      "   653|         0|            0|            0|  0.00%|            ``stft(..., return_complex=True)`` instead.\n",
      "   654|         0|            0|            0|  0.00%|    n_fft (int): Size of Fourier transform\n",
      "   655|         0|            0|            0|  0.00%|    hop_length (Optional[int]): The distance between neighboring sliding window frames.\n",
      "   656|         0|            0|            0|  0.00%|        (Default: ``n_fft // 4``)\n",
      "   657|         0|            0|            0|  0.00%|    win_length (Optional[int]): The size of window frame and STFT filter. (Default: ``n_fft``)\n",
      "   658|         0|            0|            0|  0.00%|    window (Optional[torch.Tensor]): The optional window function.\n",
      "   659|         0|            0|            0|  0.00%|        (Default: ``torch.ones(win_length)``)\n",
      "   660|         0|            0|            0|  0.00%|    center (bool): Whether :attr:`input` was padded on both sides so that the :math:`t`-th frame is\n",
      "   661|         0|            0|            0|  0.00%|        centered at time :math:`t \\times \\text{hop\\_length}`.\n",
      "   662|         0|            0|            0|  0.00%|        (Default: ``True``)\n",
      "   663|         0|            0|            0|  0.00%|    normalized (bool): Whether the STFT was normalized. (Default: ``False``)\n",
      "   664|         0|            0|            0|  0.00%|    onesided (Optional[bool]): Whether the STFT was onesided.\n",
      "   665|         0|            0|            0|  0.00%|        (Default: ``True`` if ``n_fft != fft_size`` in the input size)\n",
      "   666|         0|            0|            0|  0.00%|    length (Optional[int]): The amount to trim the signal by (i.e. the\n",
      "   667|         0|            0|            0|  0.00%|        original signal length). (Default: whole signal)\n",
      "   668|         0|            0|            0|  0.00%|    return_complex (Optional[bool]):\n",
      "   669|         0|            0|            0|  0.00%|        Whether the output should be complex, or if the input should be\n",
      "   670|         0|            0|            0|  0.00%|        assumed to derive from a real signal and window.\n",
      "   671|         0|            0|            0|  0.00%|        Note that this is incompatible with ``onesided=True``.\n",
      "   672|         0|            0|            0|  0.00%|        (Default: ``False``)\n",
      "   673|         0|            0|            0|  0.00%|\n",
      "   674|         0|            0|            0|  0.00%|Returns:\n",
      "   675|         0|            0|            0|  0.00%|    Tensor: Least squares estimation of the original signal of size (..., signal_length)\n",
      "   676|         0|            0|            0|  0.00%|\"\"\")\n",
      "   677|         0|            0|            0|  0.00%|\n",
      "   678|         0|            0|            0|  0.00%|\n",
      "   679|         0|            0|            0|  0.00%|if TYPE_CHECKING:\n",
      "   680|         0|            0|            0|  0.00%|    # These _impl functions return a variable number of tensors as output with\n",
      "   681|         0|            0|            0|  0.00%|    # __torch_function__; tuple unpacking is done already rather than being\n",
      "   682|         0|            0|            0|  0.00%|    # done by the caller of the _impl function\n",
      "   683|         0|            0|            0|  0.00%|    _unique_impl_out = Any\n",
      "   684|         0|            0|            0|  0.00%|else:\n",
      "   685|         0|            0|            0|  0.00%|    _unique_impl_out = Tuple[Tensor, Tensor, Tensor]\n",
      "   686|         0|            0|            0|  0.00%|\n",
      "   687|         0|            0|            0|  0.00%|\n",
      "   688|         0|            0|            0|  0.00%|def _unique_impl(input: Tensor, sorted: bool = True,\n",
      "   689|         0|            0|            0|  0.00%|                 return_inverse: bool = False, return_counts: bool = False,\n",
      "   690|         0|            0|            0|  0.00%|                 dim: Optional[int] = None) -> _unique_impl_out:\n",
      "   691|         0|            0|            0|  0.00%|    r\"\"\"unique(input, sorted=True, return_inverse=False, return_counts=False, dim=None) -> Tuple[Tensor, Tensor, Tensor]\n",
      "   692|         0|            0|            0|  0.00%|\n",
      "   693|         0|            0|            0|  0.00%|    Returns the unique elements of the input tensor.\n",
      "   694|         0|            0|            0|  0.00%|\n",
      "   695|         0|            0|            0|  0.00%|    .. note:: This function is different from :func:`torch.unique_consecutive` in the sense that\n",
      "   696|         0|            0|            0|  0.00%|        this function also eliminates non-consecutive duplicate values.\n",
      "   697|         0|            0|            0|  0.00%|\n",
      "   698|         0|            0|            0|  0.00%|    .. note:: Currently in the CUDA implementation and the CPU implementation when dim is specified,\n",
      "   699|         0|            0|            0|  0.00%|        `torch.unique` always sort the tensor at the beginning regardless of the `sort` argument.\n",
      "   700|         0|            0|            0|  0.00%|        Sorting could be slow, so if your input tensor is already sorted, it is recommended to use\n",
      "   701|         0|            0|            0|  0.00%|        :func:`torch.unique_consecutive` which avoids the sorting.\n",
      "   702|         0|            0|            0|  0.00%|\n",
      "   703|         0|            0|            0|  0.00%|    Args:\n",
      "   704|         0|            0|            0|  0.00%|        input (Tensor): the input tensor\n",
      "   705|         0|            0|            0|  0.00%|        sorted (bool): Whether to sort the unique elements in ascending order\n",
      "   706|         0|            0|            0|  0.00%|            before returning as output.\n",
      "   707|         0|            0|            0|  0.00%|        return_inverse (bool): Whether to also return the indices for where\n",
      "   708|         0|            0|            0|  0.00%|            elements in the original input ended up in the returned unique list.\n",
      "   709|         0|            0|            0|  0.00%|        return_counts (bool): Whether to also return the counts for each unique\n",
      "   710|         0|            0|            0|  0.00%|            element.\n",
      "   711|         0|            0|            0|  0.00%|        dim (int): the dimension to apply unique. If ``None``, the unique of the\n",
      "   712|         0|            0|            0|  0.00%|            flattened input is returned. default: ``None``\n",
      "   713|         0|            0|            0|  0.00%|\n",
      "   714|         0|            0|            0|  0.00%|    Returns:\n",
      "   715|         0|            0|            0|  0.00%|        (Tensor, Tensor (optional), Tensor (optional)): A tensor or a tuple of tensors containing\n",
      "   716|         0|            0|            0|  0.00%|\n",
      "   717|         0|            0|            0|  0.00%|            - **output** (*Tensor*): the output list of unique scalar elements.\n",
      "   718|         0|            0|            0|  0.00%|            - **inverse_indices** (*Tensor*): (optional) if\n",
      "   719|         0|            0|            0|  0.00%|              :attr:`return_inverse` is True, there will be an additional\n",
      "   720|         0|            0|            0|  0.00%|              returned tensor (same shape as input) representing the indices\n",
      "   721|         0|            0|            0|  0.00%|              for where elements in the original input map to in the output;\n",
      "   722|         0|            0|            0|  0.00%|              otherwise, this function will only return a single tensor.\n",
      "   723|         0|            0|            0|  0.00%|            - **counts** (*Tensor*): (optional) if\n",
      "   724|         0|            0|            0|  0.00%|              :attr:`return_counts` is True, there will be an additional\n",
      "   725|         0|            0|            0|  0.00%|              returned tensor (same shape as output or output.size(dim),\n",
      "   726|         0|            0|            0|  0.00%|              if dim was specified) representing the number of occurrences\n",
      "   727|         0|            0|            0|  0.00%|              for each unique value or tensor.\n",
      "   728|         0|            0|            0|  0.00%|\n",
      "   729|         0|            0|            0|  0.00%|    Example::\n",
      "   730|         0|            0|            0|  0.00%|\n",
      "   731|         0|            0|            0|  0.00%|        >>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))\n",
      "   732|         0|            0|            0|  0.00%|        >>> output\n",
      "   733|         0|            0|            0|  0.00%|        tensor([ 2,  3,  1])\n",
      "   734|         0|            0|            0|  0.00%|\n",
      "   735|         0|            0|            0|  0.00%|        >>> output, inverse_indices = torch.unique(\n",
      "   736|         0|            0|            0|  0.00%|        ...     torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)\n",
      "   737|         0|            0|            0|  0.00%|        >>> output\n",
      "   738|         0|            0|            0|  0.00%|        tensor([ 1,  2,  3])\n",
      "   739|         0|            0|            0|  0.00%|        >>> inverse_indices\n",
      "   740|         0|            0|            0|  0.00%|        tensor([ 0,  2,  1,  2])\n",
      "   741|         0|            0|            0|  0.00%|\n",
      "   742|         0|            0|            0|  0.00%|        >>> output, inverse_indices = torch.unique(\n",
      "   743|         0|            0|            0|  0.00%|        ...     torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)\n",
      "   744|         0|            0|            0|  0.00%|        >>> output\n",
      "   745|         0|            0|            0|  0.00%|        tensor([ 1,  2,  3])\n",
      "   746|         0|            0|            0|  0.00%|        >>> inverse_indices\n",
      "   747|         0|            0|            0|  0.00%|        tensor([[ 0,  2],\n",
      "   748|         0|            0|            0|  0.00%|                [ 1,  2]])\n",
      "   749|         0|            0|            0|  0.00%|\n",
      "   750|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   751|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   752|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   753|         0|            0|            0|  0.00%|            unique, (input,), input, sorted=sorted, return_inverse=return_inverse,\n",
      "   754|         0|            0|            0|  0.00%|            return_counts=return_counts, dim=dim)\n",
      "   755|         0|            0|            0|  0.00%|\n",
      "   756|         0|            0|            0|  0.00%|    if dim is not None:\n",
      "   757|         0|            0|            0|  0.00%|        output, inverse_indices, counts = _VF.unique_dim(\n",
      "   758|         0|            0|            0|  0.00%|            input,\n",
      "   759|         0|            0|            0|  0.00%|            dim,\n",
      "   760|         0|            0|            0|  0.00%|            sorted=sorted,\n",
      "   761|         0|            0|            0|  0.00%|            return_inverse=return_inverse,\n",
      "   762|         0|            0|            0|  0.00%|            return_counts=return_counts,\n",
      "   763|         0|            0|            0|  0.00%|        )\n",
      "   764|         0|            0|            0|  0.00%|    else:\n",
      "   765|         0|            0|            0|  0.00%|        output, inverse_indices, counts = torch._unique2(\n",
      "   766|         0|            0|            0|  0.00%|            input,\n",
      "   767|         0|            0|            0|  0.00%|            sorted=sorted,\n",
      "   768|         0|            0|            0|  0.00%|            return_inverse=return_inverse,\n",
      "   769|         0|            0|            0|  0.00%|            return_counts=return_counts,\n",
      "   770|         0|            0|            0|  0.00%|        )\n",
      "   771|         0|            0|            0|  0.00%|    return output, inverse_indices, counts\n",
      "   772|         0|            0|            0|  0.00%|\n",
      "   773|         0|            0|            0|  0.00%|\n",
      "   774|         0|            0|            0|  0.00%|def _unique_consecutive_impl(input: Tensor, return_inverse: bool = False,\n",
      "   775|         0|            0|            0|  0.00%|                             return_counts: bool = False,\n",
      "   776|         0|            0|            0|  0.00%|                             dim: Optional[int] = None) -> _unique_impl_out:\n",
      "   777|         0|            0|            0|  0.00%|    r\"\"\"Eliminates all but the first element from every consecutive group of equivalent elements.\n",
      "   778|         0|            0|            0|  0.00%|\n",
      "   779|         0|            0|            0|  0.00%|    .. note:: This function is different from :func:`torch.unique` in the sense that this function\n",
      "   780|         0|            0|            0|  0.00%|        only eliminates consecutive duplicate values. This semantics is similar to `std::unique`\n",
      "   781|         0|            0|            0|  0.00%|        in C++.\n",
      "   782|         0|            0|            0|  0.00%|\n",
      "   783|         0|            0|            0|  0.00%|    Args:\n",
      "   784|         0|            0|            0|  0.00%|        input (Tensor): the input tensor\n",
      "   785|         0|            0|            0|  0.00%|        return_inverse (bool): Whether to also return the indices for where\n",
      "   786|         0|            0|            0|  0.00%|            elements in the original input ended up in the returned unique list.\n",
      "   787|         0|            0|            0|  0.00%|        return_counts (bool): Whether to also return the counts for each unique\n",
      "   788|         0|            0|            0|  0.00%|            element.\n",
      "   789|         0|            0|            0|  0.00%|        dim (int): the dimension to apply unique. If ``None``, the unique of the\n",
      "   790|         0|            0|            0|  0.00%|            flattened input is returned. default: ``None``\n",
      "   791|         0|            0|            0|  0.00%|\n",
      "   792|         0|            0|            0|  0.00%|    Returns:\n",
      "   793|         0|            0|            0|  0.00%|        (Tensor, Tensor (optional), Tensor (optional)): A tensor or a tuple of tensors containing\n",
      "   794|         0|            0|            0|  0.00%|\n",
      "   795|         0|            0|            0|  0.00%|            - **output** (*Tensor*): the output list of unique scalar elements.\n",
      "   796|         0|            0|            0|  0.00%|            - **inverse_indices** (*Tensor*): (optional) if\n",
      "   797|         0|            0|            0|  0.00%|              :attr:`return_inverse` is True, there will be an additional\n",
      "   798|         0|            0|            0|  0.00%|              returned tensor (same shape as input) representing the indices\n",
      "   799|         0|            0|            0|  0.00%|              for where elements in the original input map to in the output;\n",
      "   800|         0|            0|            0|  0.00%|              otherwise, this function will only return a single tensor.\n",
      "   801|         0|            0|            0|  0.00%|            - **counts** (*Tensor*): (optional) if\n",
      "   802|         0|            0|            0|  0.00%|              :attr:`return_counts` is True, there will be an additional\n",
      "   803|         0|            0|            0|  0.00%|              returned tensor (same shape as output or output.size(dim),\n",
      "   804|         0|            0|            0|  0.00%|              if dim was specified) representing the number of occurrences\n",
      "   805|         0|            0|            0|  0.00%|              for each unique value or tensor.\n",
      "   806|         0|            0|            0|  0.00%|\n",
      "   807|         0|            0|            0|  0.00%|    Example::\n",
      "   808|         0|            0|            0|  0.00%|\n",
      "   809|         0|            0|            0|  0.00%|        >>> x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])\n",
      "   810|         0|            0|            0|  0.00%|        >>> output = torch.unique_consecutive(x)\n",
      "   811|         0|            0|            0|  0.00%|        >>> output\n",
      "   812|         0|            0|            0|  0.00%|        tensor([1, 2, 3, 1, 2])\n",
      "   813|         0|            0|            0|  0.00%|\n",
      "   814|         0|            0|            0|  0.00%|        >>> output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)\n",
      "   815|         0|            0|            0|  0.00%|        >>> output\n",
      "   816|         0|            0|            0|  0.00%|        tensor([1, 2, 3, 1, 2])\n",
      "   817|         0|            0|            0|  0.00%|        >>> inverse_indices\n",
      "   818|         0|            0|            0|  0.00%|        tensor([0, 0, 1, 1, 2, 3, 3, 4])\n",
      "   819|         0|            0|            0|  0.00%|\n",
      "   820|         0|            0|            0|  0.00%|        >>> output, counts = torch.unique_consecutive(x, return_counts=True)\n",
      "   821|         0|            0|            0|  0.00%|        >>> output\n",
      "   822|         0|            0|            0|  0.00%|        tensor([1, 2, 3, 1, 2])\n",
      "   823|         0|            0|            0|  0.00%|        >>> counts\n",
      "   824|         0|            0|            0|  0.00%|        tensor([2, 2, 1, 2, 1])\n",
      "   825|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   826|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   827|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   828|         0|            0|            0|  0.00%|            unique_consecutive, (input,), input, return_inverse=return_inverse,\n",
      "   829|         0|            0|            0|  0.00%|            return_counts=return_counts, dim=dim)\n",
      "   830|         0|            0|            0|  0.00%|    output, inverse_indices, counts = _VF.unique_consecutive(  # type: ignore[attr-defined]\n",
      "   831|         0|            0|            0|  0.00%|        input, return_inverse=return_inverse, return_counts=return_counts, dim=dim)\n",
      "   832|         0|            0|            0|  0.00%|    return output, inverse_indices, counts\n",
      "   833|         0|            0|            0|  0.00%|\n",
      "   834|         0|            0|            0|  0.00%|\n",
      "   835|         0|            0|            0|  0.00%|def _return_counts(input, sorted=True, return_inverse=False, return_counts=False, dim=None):\n",
      "   836|         0|            0|            0|  0.00%|    # type: (Tensor, bool, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]\n",
      "   837|         0|            0|            0|  0.00%|\n",
      "   838|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   839|         0|            0|            0|  0.00%|        return _unique_impl(input, sorted, return_inverse, return_counts, dim)\n",
      "   840|         0|            0|            0|  0.00%|\n",
      "   841|         0|            0|            0|  0.00%|    output, _, counts = _unique_impl(input, sorted, return_inverse, return_counts, dim)\n",
      "   842|         0|            0|            0|  0.00%|    return output, counts\n",
      "   843|         0|            0|            0|  0.00%|\n",
      "   844|         0|            0|            0|  0.00%|\n",
      "   845|         0|            0|            0|  0.00%|def _return_output(input, sorted=True, return_inverse=False, return_counts=False, dim=None):\n",
      "   846|         0|            0|            0|  0.00%|    # type: (Tensor, bool, bool, bool, Optional[int]) -> Tensor\n",
      "   847|         0|            0|            0|  0.00%|\n",
      "   848|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   849|         0|            0|            0|  0.00%|        return _unique_impl(input, sorted, return_inverse, return_counts, dim)\n",
      "   850|         0|            0|            0|  0.00%|\n",
      "   851|         0|            0|            0|  0.00%|    output, _, _ = _unique_impl(input, sorted, return_inverse, return_counts, dim)\n",
      "   852|         0|            0|            0|  0.00%|    return output\n",
      "   853|         0|            0|            0|  0.00%|\n",
      "   854|         0|            0|            0|  0.00%|\n",
      "   855|         0|            0|            0|  0.00%|def _return_inverse(input, sorted=True, return_inverse=False, return_counts=False, dim=None):\n",
      "   856|         0|            0|            0|  0.00%|    # type: (Tensor, bool, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]\n",
      "   857|         0|            0|            0|  0.00%|\n",
      "   858|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   859|         0|            0|            0|  0.00%|        return _unique_impl(input, sorted, return_inverse, return_counts, dim)\n",
      "   860|         0|            0|            0|  0.00%|\n",
      "   861|         0|            0|            0|  0.00%|    output, inverse_indices, _ = _unique_impl(input, sorted, return_inverse, return_counts, dim)\n",
      "   862|         0|            0|            0|  0.00%|    return output, inverse_indices\n",
      "   863|         0|            0|            0|  0.00%|\n",
      "   864|         0|            0|            0|  0.00%|\n",
      "   865|         0|            0|            0|  0.00%|_return_inverse_false = boolean_dispatch(\n",
      "   866|         0|            0|            0|  0.00%|    arg_name='return_counts',\n",
      "   867|         0|            0|            0|  0.00%|    arg_index=3,\n",
      "   868|         0|            0|            0|  0.00%|    default=False,\n",
      "   869|         0|            0|            0|  0.00%|    if_true=_return_counts,\n",
      "   870|         0|            0|            0|  0.00%|    if_false=_return_output,\n",
      "   871|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "   872|         0|            0|            0|  0.00%|    func_name='unique')\n",
      "   873|         0|            0|            0|  0.00%|\n",
      "   874|         0|            0|            0|  0.00%|_return_inverse_true = boolean_dispatch(\n",
      "   875|         0|            0|            0|  0.00%|    arg_name='return_counts',\n",
      "   876|         0|            0|            0|  0.00%|    arg_index=3,\n",
      "   877|         0|            0|            0|  0.00%|    default=False,\n",
      "   878|         0|            0|            0|  0.00%|    if_true=_unique_impl,\n",
      "   879|         0|            0|            0|  0.00%|    if_false=_return_inverse,\n",
      "   880|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "   881|         0|            0|            0|  0.00%|    func_name='unique')\n",
      "   882|         0|            0|            0|  0.00%|\n",
      "   883|         0|            0|            0|  0.00%|# The return type of unique depends on `return_inverse`, and `return_counts` so in order to\n",
      "   884|         0|            0|            0|  0.00%|# resolve the output type in TorchScript we need to statically know the value of both parameters\n",
      "   885|         0|            0|            0|  0.00%|\n",
      "   886|         0|            0|            0|  0.00%|unique = boolean_dispatch(\n",
      "   887|         0|            0|            0|  0.00%|    arg_name='return_inverse',\n",
      "   888|         0|            0|            0|  0.00%|    arg_index=2,\n",
      "   889|         0|            0|            0|  0.00%|    default=False,\n",
      "   890|         0|            0|            0|  0.00%|    if_true=_return_inverse_true,\n",
      "   891|         0|            0|            0|  0.00%|    if_false=_return_inverse_false,\n",
      "   892|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "   893|         0|            0|            0|  0.00%|    func_name='unique')\n",
      "   894|         0|            0|            0|  0.00%|unique.__doc__ = _unique_impl.__doc__\n",
      "   895|         0|            0|            0|  0.00%|\n",
      "   896|         0|            0|            0|  0.00%|\n",
      "   897|         0|            0|            0|  0.00%|def _consecutive_return_counts(input, return_inverse=False, return_counts=False, dim=None):\n",
      "   898|         0|            0|            0|  0.00%|    # type: (Tensor, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]\n",
      "   899|         0|            0|            0|  0.00%|\n",
      "   900|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   901|         0|            0|            0|  0.00%|        return _unique_consecutive_impl(input, return_inverse, return_counts, dim)\n",
      "   902|         0|            0|            0|  0.00%|\n",
      "   903|         0|            0|            0|  0.00%|    output, _, counts = _unique_consecutive_impl(input, return_inverse, return_counts, dim)\n",
      "   904|         0|            0|            0|  0.00%|    return output, counts\n",
      "   905|         0|            0|            0|  0.00%|\n",
      "   906|         0|            0|            0|  0.00%|\n",
      "   907|         0|            0|            0|  0.00%|def _consecutive_return_output(input, return_inverse=False, return_counts=False, dim=None):\n",
      "   908|         0|            0|            0|  0.00%|    # type: (Tensor, bool, bool, Optional[int]) -> Tensor\n",
      "   909|         0|            0|            0|  0.00%|\n",
      "   910|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   911|         0|            0|            0|  0.00%|        return _unique_consecutive_impl(input, return_inverse, return_counts, dim)\n",
      "   912|         0|            0|            0|  0.00%|\n",
      "   913|         0|            0|            0|  0.00%|    output, _, _ = _unique_consecutive_impl(input, return_inverse, return_counts, dim)\n",
      "   914|         0|            0|            0|  0.00%|    return output\n",
      "   915|         0|            0|            0|  0.00%|\n",
      "   916|         0|            0|            0|  0.00%|\n",
      "   917|         0|            0|            0|  0.00%|def _consecutive_return_inverse(input, return_inverse=False, return_counts=False, dim=None):\n",
      "   918|         0|            0|            0|  0.00%|    # type: (Tensor, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]\n",
      "   919|         0|            0|            0|  0.00%|\n",
      "   920|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   921|         0|            0|            0|  0.00%|        return _unique_consecutive_impl(input, return_inverse, return_counts, dim)\n",
      "   922|         0|            0|            0|  0.00%|\n",
      "   923|         0|            0|            0|  0.00%|    output, inverse_indices, _ = _unique_consecutive_impl(input, return_inverse, return_counts, dim)\n",
      "   924|         0|            0|            0|  0.00%|    return output, inverse_indices\n",
      "   925|         0|            0|            0|  0.00%|\n",
      "   926|         0|            0|            0|  0.00%|\n",
      "   927|         0|            0|            0|  0.00%|_consecutive_return_inverse_false = boolean_dispatch(\n",
      "   928|         0|            0|            0|  0.00%|    arg_name='return_counts',\n",
      "   929|         0|            0|            0|  0.00%|    arg_index=1,\n",
      "   930|         0|            0|            0|  0.00%|    default=False,\n",
      "   931|         0|            0|            0|  0.00%|    if_true=_consecutive_return_counts,\n",
      "   932|         0|            0|            0|  0.00%|    if_false=_consecutive_return_output,\n",
      "   933|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "   934|         0|            0|            0|  0.00%|    func_name='unique_consecutive')\n",
      "   935|         0|            0|            0|  0.00%|\n",
      "   936|         0|            0|            0|  0.00%|_consecutive_return_inverse_true = boolean_dispatch(\n",
      "   937|         0|            0|            0|  0.00%|    arg_name='return_counts',\n",
      "   938|         0|            0|            0|  0.00%|    arg_index=1,\n",
      "   939|         0|            0|            0|  0.00%|    default=False,\n",
      "   940|         0|            0|            0|  0.00%|    if_true=_unique_consecutive_impl,\n",
      "   941|         0|            0|            0|  0.00%|    if_false=_consecutive_return_inverse,\n",
      "   942|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "   943|         0|            0|            0|  0.00%|    func_name='unique_consecutive')\n",
      "   944|         0|            0|            0|  0.00%|\n",
      "   945|         0|            0|            0|  0.00%|# The return type of unique depends on `return_inverse`, and `return_counts` so in order to\n",
      "   946|         0|            0|            0|  0.00%|# resolve the output type in TorchScript we need to statically know the value of both parameters\n",
      "   947|         0|            0|            0|  0.00%|\n",
      "   948|         0|            0|            0|  0.00%|unique_consecutive = boolean_dispatch(\n",
      "   949|         0|            0|            0|  0.00%|    arg_name='return_inverse',\n",
      "   950|         0|            0|            0|  0.00%|    arg_index=2,\n",
      "   951|         0|            0|            0|  0.00%|    default=False,\n",
      "   952|         0|            0|            0|  0.00%|    if_true=_consecutive_return_inverse_true,\n",
      "   953|         0|            0|            0|  0.00%|    if_false=_consecutive_return_inverse_false,\n",
      "   954|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "   955|         0|            0|            0|  0.00%|    func_name='unique_consecutive')\n",
      "   956|         0|            0|            0|  0.00%|unique_consecutive.__doc__ = _unique_consecutive_impl.__doc__\n",
      "   957|         0|            0|            0|  0.00%|\n",
      "   958|         0|            0|            0|  0.00%|if TYPE_CHECKING:\n",
      "   959|         0|            0|            0|  0.00%|    pass\n",
      "   960|         0|            0|            0|  0.00%|    # There's no good way to use this type annotation without breaking JIT\n",
      "   961|         0|            0|            0|  0.00%|    # overloads. So leave untyped for mypy for now.\n",
      "   962|         0|            0|            0|  0.00%|else:\n",
      "   963|         0|            0|            0|  0.00%|    @overload\n",
      "   964|         0|            0|            0|  0.00%|    def tensordot(a, b, dims: int = 2, out: Optional[torch.Tensor] = None):\n",
      "   965|         0|            0|            0|  0.00%|        pass\n",
      "   966|         0|            0|            0|  0.00%|\n",
      "   967|         0|            0|            0|  0.00%|    @overload  # noqa: F811\n",
      "   968|         0|            0|            0|  0.00%|    def tensordot(a, b, dims: Tuple[List[int], List[int]], out: Optional[torch.Tensor] = None):  # noqa: F811\n",
      "   969|         0|            0|            0|  0.00%|        pass\n",
      "   970|         0|            0|            0|  0.00%|\n",
      "   971|         0|            0|            0|  0.00%|    @overload  # noqa: F811\n",
      "   972|         0|            0|            0|  0.00%|    def tensordot(a, b, dims: List[List[int]], out: Optional[torch.Tensor] = None):  # noqa: F811\n",
      "   973|         0|            0|            0|  0.00%|        pass\n",
      "   974|         0|            0|            0|  0.00%|\n",
      "   975|         0|            0|            0|  0.00%|    @overload  # noqa: F811\n",
      "   976|         0|            0|            0|  0.00%|    def tensordot(a, b, dims: torch.Tensor, out: Optional[torch.Tensor] = None):  # noqa: F811\n",
      "   977|         0|            0|            0|  0.00%|        pass\n",
      "   978|         0|            0|            0|  0.00%|\n",
      "   979|         0|            0|            0|  0.00%|def tensordot(a, b, dims=2, out: Optional[torch.Tensor] = None):  # noqa: F811\n",
      "   980|         0|            0|            0|  0.00%|    r\"\"\"Returns a contraction of a and b over multiple dimensions.\n",
      "   981|         0|            0|            0|  0.00%|\n",
      "   982|         0|            0|            0|  0.00%|    :attr:`tensordot` implements a generalized matrix product.\n",
      "   983|         0|            0|            0|  0.00%|\n",
      "   984|         0|            0|            0|  0.00%|    Args:\n",
      "   985|         0|            0|            0|  0.00%|      a (Tensor): Left tensor to contract\n",
      "   986|         0|            0|            0|  0.00%|      b (Tensor): Right tensor to contract\n",
      "   987|         0|            0|            0|  0.00%|      dims (int or Tuple[List[int], List[int]] or List[List[int]] containing two lists or Tensor): number of dimensions to\n",
      "   988|         0|            0|            0|  0.00%|         contract or explicit lists of dimensions for :attr:`a` and\n",
      "   989|         0|            0|            0|  0.00%|         :attr:`b` respectively\n",
      "   990|         0|            0|            0|  0.00%|\n",
      "   991|         0|            0|            0|  0.00%|    When called with a non-negative integer argument :attr:`dims` = :math:`d`, and\n",
      "   992|         0|            0|            0|  0.00%|    the number of dimensions of :attr:`a` and :attr:`b` is :math:`m` and :math:`n`,\n",
      "   993|         0|            0|            0|  0.00%|    respectively, :func:`~torch.tensordot` computes\n",
      "   994|         0|            0|            0|  0.00%|\n",
      "   995|         0|            0|            0|  0.00%|    .. math::\n",
      "   996|         0|            0|            0|  0.00%|        r_{i_0,...,i_{m-d}, i_d,...,i_n}\n",
      "   997|         0|            0|            0|  0.00%|          = \\sum_{k_0,...,k_{d-1}} a_{i_0,...,i_{m-d},k_0,...,k_{d-1}} \\times b_{k_0,...,k_{d-1}, i_d,...,i_n}.\n",
      "   998|         0|            0|            0|  0.00%|\n",
      "   999|         0|            0|            0|  0.00%|    When called with :attr:`dims` of the list form, the given dimensions will be contracted\n",
      "  1000|         0|            0|            0|  0.00%|    in place of the last :math:`d` of :attr:`a` and the first :math:`d` of :math:`b`. The sizes\n",
      "  1001|         0|            0|            0|  0.00%|    in these dimensions must match, but :func:`~torch.tensordot` will deal with broadcasted\n",
      "  1002|         0|            0|            0|  0.00%|    dimensions.\n",
      "  1003|         0|            0|            0|  0.00%|\n",
      "  1004|         0|            0|            0|  0.00%|    Examples::\n",
      "  1005|         0|            0|            0|  0.00%|\n",
      "  1006|         0|            0|            0|  0.00%|        >>> a = torch.arange(60.).reshape(3, 4, 5)\n",
      "  1007|         0|            0|            0|  0.00%|        >>> b = torch.arange(24.).reshape(4, 3, 2)\n",
      "  1008|         0|            0|            0|  0.00%|        >>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))\n",
      "  1009|         0|            0|            0|  0.00%|        tensor([[4400., 4730.],\n",
      "  1010|         0|            0|            0|  0.00%|                [4532., 4874.],\n",
      "  1011|         0|            0|            0|  0.00%|                [4664., 5018.],\n",
      "  1012|         0|            0|            0|  0.00%|                [4796., 5162.],\n",
      "  1013|         0|            0|            0|  0.00%|                [4928., 5306.]])\n",
      "  1014|         0|            0|            0|  0.00%|\n",
      "  1015|         0|            0|            0|  0.00%|        >>> a = torch.randn(3, 4, 5, device='cuda')\n",
      "  1016|         0|            0|            0|  0.00%|        >>> b = torch.randn(4, 5, 6, device='cuda')\n",
      "  1017|         0|            0|            0|  0.00%|        >>> c = torch.tensordot(a, b, dims=2).cpu()\n",
      "  1018|         0|            0|            0|  0.00%|        tensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],\n",
      "  1019|         0|            0|            0|  0.00%|                [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],\n",
      "  1020|         0|            0|            0|  0.00%|                [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])\n",
      "  1021|         0|            0|            0|  0.00%|\n",
      "  1022|         0|            0|            0|  0.00%|        >>> a = torch.randn(3, 5, 4, 6)\n",
      "  1023|         0|            0|            0|  0.00%|        >>> b = torch.randn(6, 4, 5, 3)\n",
      "  1024|         0|            0|            0|  0.00%|        >>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0]))\n",
      "  1025|         0|            0|            0|  0.00%|        tensor([[  7.7193,  -2.4867, -10.3204],\n",
      "  1026|         0|            0|            0|  0.00%|                [  1.5513, -14.4737,  -6.5113],\n",
      "  1027|         0|            0|            0|  0.00%|                [ -0.2850,   4.2573,  -3.5997]])\n",
      "  1028|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1029|         0|            0|            0|  0.00%|    if has_torch_function_variadic(a, b):\n",
      "  1030|         0|            0|            0|  0.00%|        return handle_torch_function(tensordot, (a, b), a, b, dims=dims, out=out)\n",
      "  1031|         0|            0|            0|  0.00%|\n",
      "  1032|         0|            0|            0|  0.00%|    if not isinstance(dims, (tuple, list, torch.Tensor, int)):\n",
      "  1033|         0|            0|            0|  0.00%|        raise RuntimeError(\"tensordot expects dims to be int or \"\n",
      "  1034|         0|            0|            0|  0.00%|                           + \"Tuple[List[int], List[int]] or \"\n",
      "  1035|         0|            0|            0|  0.00%|                           + \"List[List[int]] containing two lists, but got \"\n",
      "  1036|         0|            0|            0|  0.00%|                           + f\"dims={dims}\")\n",
      "  1037|         0|            0|            0|  0.00%|\n",
      "  1038|         0|            0|            0|  0.00%|    dims_a: List[int] = []\n",
      "  1039|         0|            0|            0|  0.00%|    dims_b: List[int] = []\n",
      "  1040|         0|            0|            0|  0.00%|\n",
      "  1041|         0|            0|            0|  0.00%|    if isinstance(dims, (tuple, list)):\n",
      "  1042|         0|            0|            0|  0.00%|        dims_a, dims_b = dims\n",
      "  1043|         0|            0|            0|  0.00%|\n",
      "  1044|         0|            0|            0|  0.00%|    if isinstance(dims, torch.Tensor):\n",
      "  1045|         0|            0|            0|  0.00%|        num_elements = dims.numel()\n",
      "  1046|         0|            0|            0|  0.00%|        if num_elements > 1:\n",
      "  1047|         0|            0|            0|  0.00%|            assert dims.size()[0] == 2\n",
      "  1048|         0|            0|            0|  0.00%|            dims_a = torch.jit.annotate(List[int], dims[0].tolist())\n",
      "  1049|         0|            0|            0|  0.00%|            dims_b = torch.jit.annotate(List[int], dims[1].tolist())\n",
      "  1050|         0|            0|            0|  0.00%|        else:\n",
      "  1051|         0|            0|            0|  0.00%|            dims_val = int(dims.item())\n",
      "  1052|         0|            0|            0|  0.00%|            if dims_val < 0:\n",
      "  1053|         0|            0|            0|  0.00%|                raise RuntimeError(f\"tensordot expects dims >= 0, but got dims={dims}\")\n",
      "  1054|         0|            0|            0|  0.00%|            dims_a = list(range(-dims_val, 0))\n",
      "  1055|         0|            0|            0|  0.00%|            dims_b = list(range(dims_val))\n",
      "  1056|         0|            0|            0|  0.00%|\n",
      "  1057|         0|            0|            0|  0.00%|    if isinstance(dims, int):\n",
      "  1058|         0|            0|            0|  0.00%|        if dims < 0:\n",
      "  1059|         0|            0|            0|  0.00%|            raise RuntimeError(f\"tensordot expects dims >= 0, but got dims={dims}\")\n",
      "  1060|         0|            0|            0|  0.00%|        dims_a = list(range(-dims, 0))\n",
      "  1061|         0|            0|            0|  0.00%|        dims_b = list(range(dims))\n",
      "  1062|         0|            0|            0|  0.00%|\n",
      "  1063|         0|            0|            0|  0.00%|    if out is None:\n",
      "  1064|         0|            0|            0|  0.00%|        return _VF.tensordot(a, b, dims_a, dims_b)  # type: ignore[attr-defined]\n",
      "  1065|         0|            0|            0|  0.00%|    else:\n",
      "  1066|         0|            0|            0|  0.00%|        return _VF.tensordot(a, b, dims_a, dims_b, out=out)  # type: ignore[attr-defined]\n",
      "  1067|         0|            0|            0|  0.00%|\n",
      "  1068|         0|            0|            0|  0.00%|def cartesian_prod(*tensors):\n",
      "  1069|         0|            0|            0|  0.00%|    \"\"\"Do cartesian product of the given sequence of tensors. The behavior is similar to\n",
      "  1070|         0|            0|            0|  0.00%|    python's `itertools.product`.\n",
      "  1071|         0|            0|            0|  0.00%|\n",
      "  1072|         0|            0|            0|  0.00%|    Args:\n",
      "  1073|         0|            0|            0|  0.00%|        *tensors: any number of 1 dimensional tensors.\n",
      "  1074|         0|            0|            0|  0.00%|\n",
      "  1075|         0|            0|            0|  0.00%|    Returns:\n",
      "  1076|         0|            0|            0|  0.00%|        Tensor: A tensor equivalent to converting all the input tensors into lists,\n",
      "  1077|         0|            0|            0|  0.00%|        do `itertools.product` on these lists, and finally convert the resulting list\n",
      "  1078|         0|            0|            0|  0.00%|        into tensor.\n",
      "  1079|         0|            0|            0|  0.00%|\n",
      "  1080|         0|            0|            0|  0.00%|    Example::\n",
      "  1081|         0|            0|            0|  0.00%|\n",
      "  1082|         0|            0|            0|  0.00%|        >>> a = [1, 2, 3]\n",
      "  1083|         0|            0|            0|  0.00%|        >>> b = [4, 5]\n",
      "  1084|         0|            0|            0|  0.00%|        >>> list(itertools.product(a, b))\n",
      "  1085|         0|            0|            0|  0.00%|        [(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]\n",
      "  1086|         0|            0|            0|  0.00%|        >>> tensor_a = torch.tensor(a)\n",
      "  1087|         0|            0|            0|  0.00%|        >>> tensor_b = torch.tensor(b)\n",
      "  1088|         0|            0|            0|  0.00%|        >>> torch.cartesian_prod(tensor_a, tensor_b)\n",
      "  1089|         0|            0|            0|  0.00%|        tensor([[1, 4],\n",
      "  1090|         0|            0|            0|  0.00%|                [1, 5],\n",
      "  1091|         0|            0|            0|  0.00%|                [2, 4],\n",
      "  1092|         0|            0|            0|  0.00%|                [2, 5],\n",
      "  1093|         0|            0|            0|  0.00%|                [3, 4],\n",
      "  1094|         0|            0|            0|  0.00%|                [3, 5]])\n",
      "  1095|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1096|         0|            0|            0|  0.00%|    # This wrapper exists to support variadic args.\n",
      "  1097|         0|            0|            0|  0.00%|    if has_torch_function(tensors):\n",
      "  1098|         0|            0|            0|  0.00%|        return handle_torch_function(cartesian_prod, tensors, *tensors)\n",
      "  1099|         0|            0|            0|  0.00%|    return _VF.cartesian_prod(tensors)  # type: ignore[attr-defined]\n",
      "  1100|         0|            0|            0|  0.00%|\n",
      "  1101|         0|            0|            0|  0.00%|def block_diag(*tensors):\n",
      "  1102|         0|            0|            0|  0.00%|    \"\"\"Create a block diagonal matrix from provided tensors.\n",
      "  1103|         0|            0|            0|  0.00%|\n",
      "  1104|         0|            0|            0|  0.00%|    Args:\n",
      "  1105|         0|            0|            0|  0.00%|        *tensors: One or more tensors with 0, 1, or 2 dimensions.\n",
      "  1106|         0|            0|            0|  0.00%|\n",
      "  1107|         0|            0|            0|  0.00%|    Returns:\n",
      "  1108|         0|            0|            0|  0.00%|        Tensor: A 2 dimensional tensor with all the input tensors arranged in\n",
      "  1109|         0|            0|            0|  0.00%|        order such that their upper left and lower right corners are\n",
      "  1110|         0|            0|            0|  0.00%|        diagonally adjacent. All other elements are set to 0.\n",
      "  1111|         0|            0|            0|  0.00%|\n",
      "  1112|         0|            0|            0|  0.00%|    Example::\n",
      "  1113|         0|            0|            0|  0.00%|\n",
      "  1114|         0|            0|            0|  0.00%|        >>> import torch\n",
      "  1115|         0|            0|            0|  0.00%|        >>> A = torch.tensor([[0, 1], [1, 0]])\n",
      "  1116|         0|            0|            0|  0.00%|        >>> B = torch.tensor([[3, 4, 5], [6, 7, 8]])\n",
      "  1117|         0|            0|            0|  0.00%|        >>> C = torch.tensor(7)\n",
      "  1118|         0|            0|            0|  0.00%|        >>> D = torch.tensor([1, 2, 3])\n",
      "  1119|         0|            0|            0|  0.00%|        >>> E = torch.tensor([[4], [5], [6]])\n",
      "  1120|         0|            0|            0|  0.00%|        >>> torch.block_diag(A, B, C, D, E)\n",
      "  1121|         0|            0|            0|  0.00%|        tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "  1122|         0|            0|            0|  0.00%|                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "  1123|         0|            0|            0|  0.00%|                [0, 0, 3, 4, 5, 0, 0, 0, 0, 0],\n",
      "  1124|         0|            0|            0|  0.00%|                [0, 0, 6, 7, 8, 0, 0, 0, 0, 0],\n",
      "  1125|         0|            0|            0|  0.00%|                [0, 0, 0, 0, 0, 7, 0, 0, 0, 0],\n",
      "  1126|         0|            0|            0|  0.00%|                [0, 0, 0, 0, 0, 0, 1, 2, 3, 0],\n",
      "  1127|         0|            0|            0|  0.00%|                [0, 0, 0, 0, 0, 0, 0, 0, 0, 4],\n",
      "  1128|         0|            0|            0|  0.00%|                [0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n",
      "  1129|         0|            0|            0|  0.00%|                [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])\n",
      "  1130|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1131|         0|            0|            0|  0.00%|    # This wrapper exists to support variadic args.\n",
      "  1132|         0|            0|            0|  0.00%|    if has_torch_function(tensors):\n",
      "  1133|         0|            0|            0|  0.00%|        return handle_torch_function(block_diag, tensors, *tensors)\n",
      "  1134|         0|            0|            0|  0.00%|    return torch._C._VariableFunctions.block_diag(tensors)  # type: ignore[attr-defined]\n",
      "  1135|         0|            0|            0|  0.00%|\n",
      "  1136|         0|            0|            0|  0.00%|\n",
      "  1137|         0|            0|            0|  0.00%|def cdist(x1, x2, p=2., compute_mode='use_mm_for_euclid_dist_if_necessary'):\n",
      "  1138|         0|            0|            0|  0.00%|    # type: (Tensor, Tensor, float, str) -> (Tensor)\n",
      "  1139|         0|            0|            0|  0.00%|    r\"\"\"Computes batched the p-norm distance between each pair of the two collections of row vectors.\n",
      "  1140|         0|            0|            0|  0.00%|\n",
      "  1141|         0|            0|            0|  0.00%|    Args:\n",
      "  1142|         0|            0|            0|  0.00%|        x1 (Tensor): input tensor of shape :math:`B \\times P \\times M`.\n",
      "  1143|         0|            0|            0|  0.00%|        x2 (Tensor): input tensor of shape :math:`B \\times R \\times M`.\n",
      "  1144|         0|            0|            0|  0.00%|        p: p value for the p-norm distance to calculate between each vector pair\n",
      "  1145|         0|            0|            0|  0.00%|            :math:`\\in [0, \\infty]`.\n",
      "  1146|         0|            0|            0|  0.00%|        compute_mode:\n",
      "  1147|         0|            0|            0|  0.00%|            'use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate\n",
      "  1148|         0|            0|            0|  0.00%|            euclidean distance (p = 2) if P > 25 or R > 25\n",
      "  1149|         0|            0|            0|  0.00%|            'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate\n",
      "  1150|         0|            0|            0|  0.00%|            euclidean distance (p = 2)\n",
      "  1151|         0|            0|            0|  0.00%|            'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate\n",
      "  1152|         0|            0|            0|  0.00%|            euclidean distance (p = 2)\n",
      "  1153|         0|            0|            0|  0.00%|            Default: use_mm_for_euclid_dist_if_necessary.\n",
      "  1154|         0|            0|            0|  0.00%|\n",
      "  1155|         0|            0|            0|  0.00%|    If x1 has shape :math:`B \\times P \\times M` and x2 has shape :math:`B \\times R \\times M` then the\n",
      "  1156|         0|            0|            0|  0.00%|    output will have shape :math:`B \\times P \\times R`.\n",
      "  1157|         0|            0|            0|  0.00%|\n",
      "  1158|         0|            0|            0|  0.00%|    This function is equivalent to `scipy.spatial.distance.cdist(input,'minkowski', p=p)`\n",
      "  1159|         0|            0|            0|  0.00%|    if :math:`p \\in (0, \\infty)`. When :math:`p = 0` it is equivalent to\n",
      "  1160|         0|            0|            0|  0.00%|    `scipy.spatial.distance.cdist(input, 'hamming') * M`. When :math:`p = \\infty`, the closest\n",
      "  1161|         0|            0|            0|  0.00%|    scipy function is `scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max())`.\n",
      "  1162|         0|            0|            0|  0.00%|\n",
      "  1163|         0|            0|            0|  0.00%|    Example:\n",
      "  1164|         0|            0|            0|  0.00%|\n",
      "  1165|         0|            0|            0|  0.00%|        >>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])\n",
      "  1166|         0|            0|            0|  0.00%|        >>> a\n",
      "  1167|         0|            0|            0|  0.00%|        tensor([[ 0.9041,  0.0196],\n",
      "  1168|         0|            0|            0|  0.00%|                [-0.3108, -2.4423],\n",
      "  1169|         0|            0|            0|  0.00%|                [-0.4821,  1.0590]])\n",
      "  1170|         0|            0|            0|  0.00%|        >>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])\n",
      "  1171|         0|            0|            0|  0.00%|        >>> b\n",
      "  1172|         0|            0|            0|  0.00%|        tensor([[-2.1763, -0.4713],\n",
      "  1173|         0|            0|            0|  0.00%|                [-0.6986,  1.3702]])\n",
      "  1174|         0|            0|            0|  0.00%|        >>> torch.cdist(a, b, p=2)\n",
      "  1175|         0|            0|            0|  0.00%|        tensor([[3.1193, 2.0959],\n",
      "  1176|         0|            0|            0|  0.00%|                [2.7138, 3.8322],\n",
      "  1177|         0|            0|            0|  0.00%|                [2.2830, 0.3791]])\n",
      "  1178|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1179|         0|            0|            0|  0.00%|    if has_torch_function_variadic(x1, x2):\n",
      "  1180|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  1181|         0|            0|            0|  0.00%|            cdist, (x1, x2), x1, x2, p=p, compute_mode=compute_mode)\n",
      "  1182|         0|            0|            0|  0.00%|    if compute_mode == 'use_mm_for_euclid_dist_if_necessary':\n",
      "  1183|         0|            0|            0|  0.00%|        return _VF.cdist(x1, x2, p, None)  # type: ignore[attr-defined]\n",
      "  1184|         0|            0|            0|  0.00%|    elif compute_mode == 'use_mm_for_euclid_dist':\n",
      "  1185|         0|            0|            0|  0.00%|        return _VF.cdist(x1, x2, p, 1)  # type: ignore[attr-defined]\n",
      "  1186|         0|            0|            0|  0.00%|    elif compute_mode == 'donot_use_mm_for_euclid_dist':\n",
      "  1187|         0|            0|            0|  0.00%|        return _VF.cdist(x1, x2, p, 2)  # type: ignore[attr-defined]\n",
      "  1188|         0|            0|            0|  0.00%|    else:\n",
      "  1189|         0|            0|            0|  0.00%|        raise ValueError(f\"{compute_mode} is not a valid value for compute_mode\")\n",
      "  1190|         0|            0|            0|  0.00%|\n",
      "  1191|         0|            0|            0|  0.00%|def atleast_1d(*tensors):\n",
      "  1192|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1193|         0|            0|            0|  0.00%|    Returns a 1-dimensional view of each input tensor with zero dimensions.\n",
      "  1194|         0|            0|            0|  0.00%|    Input tensors with one or more dimensions are returned as-is.\n",
      "  1195|         0|            0|            0|  0.00%|\n",
      "  1196|         0|            0|            0|  0.00%|    Args:\n",
      "  1197|         0|            0|            0|  0.00%|        input (Tensor or list of Tensors)\n",
      "  1198|         0|            0|            0|  0.00%|\n",
      "  1199|         0|            0|            0|  0.00%|    Returns:\n",
      "  1200|         0|            0|            0|  0.00%|        output (Tensor or tuple of Tensors)\n",
      "  1201|         0|            0|            0|  0.00%|\n",
      "  1202|         0|            0|            0|  0.00%|    Example::\n",
      "  1203|         0|            0|            0|  0.00%|\n",
      "  1204|         0|            0|            0|  0.00%|        >>> x = torch.randn(2)\n",
      "  1205|         0|            0|            0|  0.00%|        >>> x\n",
      "  1206|         0|            0|            0|  0.00%|        tensor([1.4584, 0.7583])\n",
      "  1207|         0|            0|            0|  0.00%|        >>> torch.atleast_1d(x)\n",
      "  1208|         0|            0|            0|  0.00%|        tensor([1.4584, 0.7583])\n",
      "  1209|         0|            0|            0|  0.00%|        >>> x = torch.tensor(1.)\n",
      "  1210|         0|            0|            0|  0.00%|        >>> x\n",
      "  1211|         0|            0|            0|  0.00%|        tensor(1.)\n",
      "  1212|         0|            0|            0|  0.00%|        >>> torch.atleast_1d(x)\n",
      "  1213|         0|            0|            0|  0.00%|        tensor([1.])\n",
      "  1214|         0|            0|            0|  0.00%|        >>> x = torch.tensor(0.5)\n",
      "  1215|         0|            0|            0|  0.00%|        >>> y = torch.tensor(1.)\n",
      "  1216|         0|            0|            0|  0.00%|        >>> torch.atleast_1d((x,y))\n",
      "  1217|         0|            0|            0|  0.00%|        (tensor([0.5000]), tensor([1.]))\n",
      "  1218|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1219|         0|            0|            0|  0.00%|    # This wrapper exists to support variadic args.\n",
      "  1220|         0|            0|            0|  0.00%|    if has_torch_function(tensors):\n",
      "  1221|         0|            0|            0|  0.00%|        return handle_torch_function(atleast_1d, tensors, *tensors)\n",
      "  1222|         0|            0|            0|  0.00%|    if len(tensors) == 1:\n",
      "  1223|         0|            0|            0|  0.00%|        tensors = tensors[0]\n",
      "  1224|         0|            0|            0|  0.00%|    return _VF.atleast_1d(tensors)  # type: ignore[attr-defined]\n",
      "  1225|         0|            0|            0|  0.00%|\n",
      "  1226|         0|            0|            0|  0.00%|def atleast_2d(*tensors):\n",
      "  1227|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1228|         0|            0|            0|  0.00%|    Returns a 2-dimensional view of each input tensor with zero dimensions.\n",
      "  1229|         0|            0|            0|  0.00%|    Input tensors with two or more dimensions are returned as-is.\n",
      "  1230|         0|            0|            0|  0.00%|\n",
      "  1231|         0|            0|            0|  0.00%|    Args:\n",
      "  1232|         0|            0|            0|  0.00%|        input (Tensor or list of Tensors)\n",
      "  1233|         0|            0|            0|  0.00%|\n",
      "  1234|         0|            0|            0|  0.00%|    Returns:\n",
      "  1235|         0|            0|            0|  0.00%|        output (Tensor or tuple of Tensors)\n",
      "  1236|         0|            0|            0|  0.00%|\n",
      "  1237|         0|            0|            0|  0.00%|    Example::\n",
      "  1238|         0|            0|            0|  0.00%|\n",
      "  1239|         0|            0|            0|  0.00%|        >>> x = torch.tensor(1.)\n",
      "  1240|         0|            0|            0|  0.00%|        >>> x\n",
      "  1241|         0|            0|            0|  0.00%|        tensor(1.)\n",
      "  1242|         0|            0|            0|  0.00%|        >>> torch.atleast_2d(x)\n",
      "  1243|         0|            0|            0|  0.00%|        tensor([[1.]])\n",
      "  1244|         0|            0|            0|  0.00%|        >>> x = torch.randn(2,2)\n",
      "  1245|         0|            0|            0|  0.00%|        >>> x\n",
      "  1246|         0|            0|            0|  0.00%|        tensor([[2.2086, 2.5165],\n",
      "  1247|         0|            0|            0|  0.00%|                [0.1757, 0.5194]])\n",
      "  1248|         0|            0|            0|  0.00%|        >>> torch.atleast_2d(x)\n",
      "  1249|         0|            0|            0|  0.00%|        tensor([[2.2086, 2.5165],\n",
      "  1250|         0|            0|            0|  0.00%|                [0.1757, 0.5194]])\n",
      "  1251|         0|            0|            0|  0.00%|        >>> x = torch.tensor(0.5)\n",
      "  1252|         0|            0|            0|  0.00%|        >>> y = torch.tensor(1.)\n",
      "  1253|         0|            0|            0|  0.00%|        >>> torch.atleast_2d((x,y))\n",
      "  1254|         0|            0|            0|  0.00%|        (tensor([[0.5000]]), tensor([[1.]]))\n",
      "  1255|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1256|         0|            0|            0|  0.00%|    # This wrapper exists to support variadic args.\n",
      "  1257|         0|            0|            0|  0.00%|    if has_torch_function(tensors):\n",
      "  1258|         0|            0|            0|  0.00%|        return handle_torch_function(atleast_2d, tensors, *tensors)\n",
      "  1259|         0|            0|            0|  0.00%|    if len(tensors) == 1:\n",
      "  1260|         0|            0|            0|  0.00%|        tensors = tensors[0]\n",
      "  1261|         0|            0|            0|  0.00%|    return _VF.atleast_2d(tensors)  # type: ignore[attr-defined]\n",
      "  1262|         0|            0|            0|  0.00%|\n",
      "  1263|         0|            0|            0|  0.00%|def atleast_3d(*tensors):\n",
      "  1264|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1265|         0|            0|            0|  0.00%|    Returns a 3-dimensional view of each input tensor with zero dimensions.\n",
      "  1266|         0|            0|            0|  0.00%|    Input tensors with three or more dimensions are returned as-is.\n",
      "  1267|         0|            0|            0|  0.00%|\n",
      "  1268|         0|            0|            0|  0.00%|    Args:\n",
      "  1269|         0|            0|            0|  0.00%|        input (Tensor or list of Tensors)\n",
      "  1270|         0|            0|            0|  0.00%|\n",
      "  1271|         0|            0|            0|  0.00%|    Returns:\n",
      "  1272|         0|            0|            0|  0.00%|        output (Tensor or tuple of Tensors)\n",
      "  1273|         0|            0|            0|  0.00%|\n",
      "  1274|         0|            0|            0|  0.00%|    Example:\n",
      "  1275|         0|            0|            0|  0.00%|\n",
      "  1276|         0|            0|            0|  0.00%|        >>> x = torch.tensor(0.5)\n",
      "  1277|         0|            0|            0|  0.00%|        >>> x\n",
      "  1278|         0|            0|            0|  0.00%|        tensor(0.5000)\n",
      "  1279|         0|            0|            0|  0.00%|        >>> torch.atleast_3d(x)\n",
      "  1280|         0|            0|            0|  0.00%|        tensor([[[0.5000]]])\n",
      "  1281|         0|            0|            0|  0.00%|        >>> y = torch.randn(2,2)\n",
      "  1282|         0|            0|            0|  0.00%|        >>> y\n",
      "  1283|         0|            0|            0|  0.00%|        tensor([[-0.8079,  0.7460],\n",
      "  1284|         0|            0|            0|  0.00%|                [-1.1647,  1.4734]])\n",
      "  1285|         0|            0|            0|  0.00%|        >>> torch.atleast_3d(y)\n",
      "  1286|         0|            0|            0|  0.00%|        tensor([[[-0.8079],\n",
      "  1287|         0|            0|            0|  0.00%|                [ 0.7460]],\n",
      "  1288|         0|            0|            0|  0.00%|                <BLANKLINE>\n",
      "  1289|         0|            0|            0|  0.00%|                [[-1.1647],\n",
      "  1290|         0|            0|            0|  0.00%|                [ 1.4734]]])\n",
      "  1291|         0|            0|            0|  0.00%|        >>> x = torch.randn(1,1,1)\n",
      "  1292|         0|            0|            0|  0.00%|        >>> x\n",
      "  1293|         0|            0|            0|  0.00%|        tensor([[[-1.5689]]])\n",
      "  1294|         0|            0|            0|  0.00%|        >>> torch.atleast_3d(x)\n",
      "  1295|         0|            0|            0|  0.00%|        tensor([[[-1.5689]]])\n",
      "  1296|         0|            0|            0|  0.00%|        >>> x = torch.tensor(0.5)\n",
      "  1297|         0|            0|            0|  0.00%|        >>> y = torch.tensor(1.)\n",
      "  1298|         0|            0|            0|  0.00%|        >>> torch.atleast_3d((x,y))\n",
      "  1299|         0|            0|            0|  0.00%|        (tensor([[[0.5000]]]), tensor([[[1.]]]))\n",
      "  1300|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1301|         0|            0|            0|  0.00%|    # This wrapper exists to support variadic args.\n",
      "  1302|         0|            0|            0|  0.00%|    if has_torch_function(tensors):\n",
      "  1303|         0|            0|            0|  0.00%|        return handle_torch_function(atleast_3d, tensors, *tensors)\n",
      "  1304|         0|            0|            0|  0.00%|    if len(tensors) == 1:\n",
      "  1305|         0|            0|            0|  0.00%|        tensors = tensors[0]\n",
      "  1306|         0|            0|            0|  0.00%|    return _VF.atleast_3d(tensors)  # type: ignore[attr-defined]\n",
      "  1307|         0|            0|            0|  0.00%|\n",
      "  1308|         0|            0|            0|  0.00%|\n",
      "  1309|         0|            0|            0|  0.00%|if TYPE_CHECKING:\n",
      "  1310|         0|            0|            0|  0.00%|    pass\n",
      "  1311|         0|            0|            0|  0.00%|    # There's no good way to use this type annotation; cannot rename norm() to\n",
      "  1312|         0|            0|            0|  0.00%|    # _norm_impl() in a way that doesn't break JIT overloads. So leave untyped\n",
      "  1313|         0|            0|            0|  0.00%|    # for mypy for now.\n",
      "  1314|         0|            0|            0|  0.00%|    #    def norm(input: Tensor,\n",
      "  1315|         0|            0|            0|  0.00%|    #             p: Optional[Union[str, Number]] = \"fro\",\n",
      "  1316|         0|            0|            0|  0.00%|    #             dim: Optional[Union[int, List[int]]] = None,\n",
      "  1317|         0|            0|            0|  0.00%|    #             keepdim: bool = False,\n",
      "  1318|         0|            0|            0|  0.00%|    #             out: Optional[Tensor] = None,\n",
      "  1319|         0|            0|            0|  0.00%|    #             dtype: _dtype = None) -> Tensor:\n",
      "  1320|         0|            0|            0|  0.00%|    #        return _norm_impl(input, p, dim, keepdim, out, dtype)\n",
      "  1321|         0|            0|            0|  0.00%|else:\n",
      "  1322|         0|            0|            0|  0.00%|    # TODO: type dim as BroadcastingList when\n",
      "  1323|         0|            0|            0|  0.00%|    # https://github.com/pytorch/pytorch/issues/33782 is fixed\n",
      "  1324|         0|            0|            0|  0.00%|    @overload\n",
      "  1325|         0|            0|            0|  0.00%|    def norm(input, p=\"fro\", dim=None, keepdim=False, out=None, dtype=None):\n",
      "  1326|         0|            0|            0|  0.00%|        # type: (Tensor, str, Optional[List[int]], bool, Optional[Tensor], Optional[int]) -> Tensor\n",
      "  1327|         0|            0|            0|  0.00%|        pass\n",
      "  1328|         0|            0|            0|  0.00%|\n",
      "  1329|         0|            0|            0|  0.00%|    @overload  # noqa: F811\n",
      "  1330|         0|            0|            0|  0.00%|    def norm(input, p=\"fro\", dim=None, keepdim=False, out=None, dtype=None):  # noqa: F811\n",
      "  1331|         0|            0|            0|  0.00%|        # type: (Tensor, Optional[number], Optional[List[int]], bool, Optional[Tensor], Optional[int]) -> Tensor\n",
      "  1332|         0|            0|            0|  0.00%|        pass\n",
      "  1333|         0|            0|            0|  0.00%|\n",
      "  1334|         0|            0|            0|  0.00%|    @overload  # noqa: F811\n",
      "  1335|         0|            0|            0|  0.00%|    def norm(input, p=\"fro\", dim=None, keepdim=False, out=None, dtype=None):  # noqa: F811\n",
      "  1336|         0|            0|            0|  0.00%|        # type: (Tensor, Optional[number], Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor\n",
      "  1337|         0|            0|            0|  0.00%|        pass\n",
      "  1338|         0|            0|            0|  0.00%|\n",
      "  1339|         0|            0|            0|  0.00%|    @overload  # noqa: F811\n",
      "  1340|         0|            0|            0|  0.00%|    def norm(input, p=\"fro\", dim=None, keepdim=False, out=None, dtype=None):  # noqa: F811\n",
      "  1341|         0|            0|            0|  0.00%|        # type: (Tensor, str, Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor\n",
      "  1342|         0|            0|            0|  0.00%|        pass\n",
      "  1343|         0|            0|            0|  0.00%|\n",
      "  1344|         0|            0|            0|  0.00%|\n",
      "  1345|     81120|     0.175582|  2.16448e-06|  0.03%|def norm(input, p=\"fro\", dim=None, keepdim=False, out=None, dtype=None):  # noqa: F811\n",
      "  1346|         0|            0|            0|  0.00%|    r\"\"\"Returns the matrix norm or vector norm of a given tensor.\n",
      "  1347|         0|            0|            0|  0.00%|\n",
      "  1348|         0|            0|            0|  0.00%|    .. warning::\n",
      "  1349|         0|            0|            0|  0.00%|\n",
      "  1350|         0|            0|            0|  0.00%|        torch.norm is deprecated and may be removed in a future PyTorch release.\n",
      "  1351|         0|            0|            0|  0.00%|        Its documentation and behavior may be incorrect, and it is no longer\n",
      "  1352|         0|            0|            0|  0.00%|        actively maintained.\n",
      "  1353|         0|            0|            0|  0.00%|\n",
      "  1354|         0|            0|            0|  0.00%|        Use :func:`torch.linalg.norm`, instead, or :func:`torch.linalg.vector_norm`\n",
      "  1355|         0|            0|            0|  0.00%|        when computing vector norms and :func:`torch.linalg.matrix_norm` when\n",
      "  1356|         0|            0|            0|  0.00%|        computing matrix norms. Note, however, the signature for these functions\n",
      "  1357|         0|            0|            0|  0.00%|        is slightly different than the signature for torch.norm.\n",
      "  1358|         0|            0|            0|  0.00%|\n",
      "  1359|         0|            0|            0|  0.00%|    Args:\n",
      "  1360|         0|            0|            0|  0.00%|        input (Tensor): The input tensor. Its data type must be either a floating\n",
      "  1361|         0|            0|            0|  0.00%|            point or complex type. For complex inputs, the norm is calculated using the\n",
      "  1362|         0|            0|            0|  0.00%|            absolute value of each element. If the input is complex and neither\n",
      "  1363|         0|            0|            0|  0.00%|            :attr:`dtype` nor :attr:`out` is specified, the result's data type will\n",
      "  1364|         0|            0|            0|  0.00%|            be the corresponding floating point type (e.g. float if :attr:`input` is\n",
      "  1365|         0|            0|            0|  0.00%|            complexfloat).\n",
      "  1366|         0|            0|            0|  0.00%|\n",
      "  1367|         0|            0|            0|  0.00%|        p (int, float, inf, -inf, 'fro', 'nuc', optional): the order of norm. Default: ``'fro'``\n",
      "  1368|         0|            0|            0|  0.00%|            The following norms can be calculated:\n",
      "  1369|         0|            0|            0|  0.00%|\n",
      "  1370|         0|            0|            0|  0.00%|            ======  ==============  ==========================\n",
      "  1371|         0|            0|            0|  0.00%|            ord     matrix norm     vector norm\n",
      "  1372|         0|            0|            0|  0.00%|            ======  ==============  ==========================\n",
      "  1373|         0|            0|            0|  0.00%|            'fro'   Frobenius norm  --\n",
      "  1374|         0|            0|            0|  0.00%|            'nuc'   nuclear norm    --\n",
      "  1375|         0|            0|            0|  0.00%|            Number  --              sum(abs(x)**ord)**(1./ord)\n",
      "  1376|         0|            0|            0|  0.00%|            ======  ==============  ==========================\n",
      "  1377|         0|            0|            0|  0.00%|\n",
      "  1378|         0|            0|            0|  0.00%|            The vector norm can be calculated across any number of dimensions.\n",
      "  1379|         0|            0|            0|  0.00%|            The corresponding dimensions of :attr:`input` are flattened into\n",
      "  1380|         0|            0|            0|  0.00%|            one dimension, and the norm is calculated on the flattened\n",
      "  1381|         0|            0|            0|  0.00%|            dimension.\n",
      "  1382|         0|            0|            0|  0.00%|\n",
      "  1383|         0|            0|            0|  0.00%|            Frobenius norm produces the same result as ``p=2`` in all cases\n",
      "  1384|         0|            0|            0|  0.00%|            except when :attr:`dim` is a list of three or more dims, in which\n",
      "  1385|         0|            0|            0|  0.00%|            case Frobenius norm throws an error.\n",
      "  1386|         0|            0|            0|  0.00%|\n",
      "  1387|         0|            0|            0|  0.00%|            Nuclear norm can only be calculated across exactly two dimensions.\n",
      "  1388|         0|            0|            0|  0.00%|\n",
      "  1389|         0|            0|            0|  0.00%|        dim (int, tuple of ints, list of ints, optional):\n",
      "  1390|         0|            0|            0|  0.00%|            Specifies which dimension or dimensions of :attr:`input` to\n",
      "  1391|         0|            0|            0|  0.00%|            calculate the norm across. If :attr:`dim` is ``None``, the norm will\n",
      "  1392|         0|            0|            0|  0.00%|            be calculated across all dimensions of :attr:`input`. If the norm\n",
      "  1393|         0|            0|            0|  0.00%|            type indicated by :attr:`p` does not support the specified number of\n",
      "  1394|         0|            0|            0|  0.00%|            dimensions, an error will occur.\n",
      "  1395|         0|            0|            0|  0.00%|        keepdim (bool, optional): whether the output tensors have :attr:`dim`\n",
      "  1396|         0|            0|            0|  0.00%|            retained or not. Ignored if :attr:`dim` = ``None`` and\n",
      "  1397|         0|            0|            0|  0.00%|            :attr:`out` = ``None``. Default: ``False``\n",
      "  1398|         0|            0|            0|  0.00%|        out (Tensor, optional): the output tensor. Ignored if\n",
      "  1399|         0|            0|            0|  0.00%|            :attr:`dim` = ``None`` and :attr:`out` = ``None``.\n",
      "  1400|         0|            0|            0|  0.00%|        dtype (:class:`torch.dtype`, optional): the desired data type of\n",
      "  1401|         0|            0|            0|  0.00%|            returned tensor. If specified, the input tensor is casted to\n",
      "  1402|         0|            0|            0|  0.00%|            :attr:`dtype` while performing the operation. Default: None.\n",
      "  1403|         0|            0|            0|  0.00%|\n",
      "  1404|         0|            0|            0|  0.00%|    .. note::\n",
      "  1405|         0|            0|            0|  0.00%|        Even though ``p='fro'`` supports any number of dimensions, the true\n",
      "  1406|         0|            0|            0|  0.00%|        mathematical definition of Frobenius norm only applies to tensors with\n",
      "  1407|         0|            0|            0|  0.00%|        exactly two dimensions. :func:`torch.linalg.norm` with ``ord='fro'`` aligns\n",
      "  1408|         0|            0|            0|  0.00%|        with the mathematical definition, since it can only be applied across\n",
      "  1409|         0|            0|            0|  0.00%|        exactly two dimensions.\n",
      "  1410|         0|            0|            0|  0.00%|\n",
      "  1411|         0|            0|            0|  0.00%|    Example::\n",
      "  1412|         0|            0|            0|  0.00%|\n",
      "  1413|         0|            0|            0|  0.00%|        >>> import torch\n",
      "  1414|         0|            0|            0|  0.00%|        >>> a = torch.arange(9, dtype= torch.float) - 4\n",
      "  1415|         0|            0|            0|  0.00%|        >>> b = a.reshape((3, 3))\n",
      "  1416|         0|            0|            0|  0.00%|        >>> torch.norm(a)\n",
      "  1417|         0|            0|            0|  0.00%|        tensor(7.7460)\n",
      "  1418|         0|            0|            0|  0.00%|        >>> torch.norm(b)\n",
      "  1419|         0|            0|            0|  0.00%|        tensor(7.7460)\n",
      "  1420|         0|            0|            0|  0.00%|        >>> torch.norm(a, float('inf'))\n",
      "  1421|         0|            0|            0|  0.00%|        tensor(4.)\n",
      "  1422|         0|            0|            0|  0.00%|        >>> torch.norm(b, float('inf'))\n",
      "  1423|         0|            0|            0|  0.00%|        tensor(4.)\n",
      "  1424|         0|            0|            0|  0.00%|        >>> c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)\n",
      "  1425|         0|            0|            0|  0.00%|        >>> torch.norm(c, dim=0)\n",
      "  1426|         0|            0|            0|  0.00%|        tensor([1.4142, 2.2361, 5.0000])\n",
      "  1427|         0|            0|            0|  0.00%|        >>> torch.norm(c, dim=1)\n",
      "  1428|         0|            0|            0|  0.00%|        tensor([3.7417, 4.2426])\n",
      "  1429|         0|            0|            0|  0.00%|        >>> torch.norm(c, p=1, dim=1)\n",
      "  1430|         0|            0|            0|  0.00%|        tensor([6., 6.])\n",
      "  1431|         0|            0|            0|  0.00%|        >>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2)\n",
      "  1432|         0|            0|            0|  0.00%|        >>> torch.norm(d, dim=(1,2))\n",
      "  1433|         0|            0|            0|  0.00%|        tensor([ 3.7417, 11.2250])\n",
      "  1434|         0|            0|            0|  0.00%|        >>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\n",
      "  1435|         0|            0|            0|  0.00%|        (tensor(3.7417), tensor(11.2250))\n",
      "  1436|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1437|         0|            0|            0|  0.00%|\n",
      "  1438|     81120|     0.182735|  2.25265e-06|  0.03%|    if has_torch_function_unary(input):\n",
      "  1439|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  1440|         0|            0|            0|  0.00%|            norm, (input,), input, p=p, dim=dim, keepdim=keepdim, out=out, dtype=dtype)\n",
      "  1441|         0|            0|            0|  0.00%|\n",
      "  1442|     81120|     0.178546|  2.20101e-06|  0.03%|    ndim = input.dim()\n",
      "  1443|         0|            0|            0|  0.00%|\n",
      "  1444|         0|            0|            0|  0.00%|    # catch default case\n",
      "  1445|     81120|     0.169589|  2.09059e-06|  0.03%|    if dim is None and out is None and dtype is None and p is not None:\n",
      "  1446|     81120|     0.169357|  2.08773e-06|  0.03%|        if isinstance(p, str):\n",
      "  1447|         0|            0|            0|  0.00%|            if p == \"fro\":\n",
      "  1448|         0|            0|            0|  0.00%|                return _VF.frobenius_norm(input, dim=(), keepdim=keepdim)\n",
      "  1449|     81120|     0.160661|  1.98054e-06|  0.03%|        if not isinstance(p, str):\n",
      "  1450|    361920|      0.91801|   2.5365e-06|  0.15%|            _dim = [i for i in range(ndim)]  # noqa: C416 TODO: rewrite as list(range(m))\n",
      "(call)|     81120|     0.420373|  5.18211e-06|  0.07%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/functional.py:1450 <listcomp>\n",
      "  1451|     81120|      1.20168|  1.48136e-05|  0.19%|            return _VF.norm(input, p, dim=_dim, keepdim=keepdim)  # type: ignore[attr-defined]\n",
      "(call)|     81120|     0.266698|   3.2877e-06|  0.04%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_VF.py:25 __getattr__\n",
      "  1452|         0|            0|            0|  0.00%|\n",
      "  1453|         0|            0|            0|  0.00%|    # TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\n",
      "  1454|         0|            0|            0|  0.00%|    # remove the overloads where dim is an int and replace with BraodcastingList1\n",
      "  1455|         0|            0|            0|  0.00%|    # and remove next four lines, replace _dim with dim\n",
      "  1456|         0|            0|            0|  0.00%|    if dim is not None:\n",
      "  1457|         0|            0|            0|  0.00%|        if isinstance(dim, int):\n",
      "  1458|         0|            0|            0|  0.00%|            _dim = [dim]\n",
      "  1459|         0|            0|            0|  0.00%|        else:\n",
      "  1460|         0|            0|            0|  0.00%|            _dim = dim\n",
      "  1461|         0|            0|            0|  0.00%|    else:\n",
      "  1462|         0|            0|            0|  0.00%|        _dim = None  # type: ignore[assignment]\n",
      "  1463|         0|            0|            0|  0.00%|\n",
      "  1464|         0|            0|            0|  0.00%|    if isinstance(p, str):\n",
      "  1465|         0|            0|            0|  0.00%|        if p == \"fro\":\n",
      "  1466|         0|            0|            0|  0.00%|            if dtype is not None:\n",
      "  1467|         0|            0|            0|  0.00%|                raise ValueError(\"dtype argument is not supported in frobenius norm\")\n",
      "  1468|         0|            0|            0|  0.00%|\n",
      "  1469|         0|            0|            0|  0.00%|            if _dim is None:\n",
      "  1470|         0|            0|            0|  0.00%|                _dim = list(range(ndim))\n",
      "  1471|         0|            0|            0|  0.00%|            if out is None:\n",
      "  1472|         0|            0|            0|  0.00%|                return _VF.frobenius_norm(input, _dim, keepdim=keepdim)\n",
      "  1473|         0|            0|            0|  0.00%|            else:\n",
      "  1474|         0|            0|            0|  0.00%|                return _VF.frobenius_norm(input, _dim, keepdim=keepdim, out=out)\n",
      "  1475|         0|            0|            0|  0.00%|        elif p == \"nuc\":\n",
      "  1476|         0|            0|            0|  0.00%|            if dtype is not None:\n",
      "  1477|         0|            0|            0|  0.00%|                raise ValueError(\"dtype argument is not supported in nuclear norm\")\n",
      "  1478|         0|            0|            0|  0.00%|            if _dim is None:\n",
      "  1479|         0|            0|            0|  0.00%|                if out is None:\n",
      "  1480|         0|            0|            0|  0.00%|                    return _VF.nuclear_norm(input, keepdim=keepdim)\n",
      "  1481|         0|            0|            0|  0.00%|                else:\n",
      "  1482|         0|            0|            0|  0.00%|                    return _VF.nuclear_norm(input, keepdim=keepdim, out=out)\n",
      "  1483|         0|            0|            0|  0.00%|            else:\n",
      "  1484|         0|            0|            0|  0.00%|                if out is None:\n",
      "  1485|         0|            0|            0|  0.00%|                    return _VF.nuclear_norm(input, _dim, keepdim=keepdim)\n",
      "  1486|         0|            0|            0|  0.00%|                else:\n",
      "  1487|         0|            0|            0|  0.00%|                    return _VF.nuclear_norm(input, _dim, keepdim=keepdim, out=out)\n",
      "  1488|         0|            0|            0|  0.00%|        raise RuntimeError(f\"only valid string values are 'fro' and 'nuc', found {p}\")\n",
      "  1489|         0|            0|            0|  0.00%|    else:\n",
      "  1490|         0|            0|            0|  0.00%|        if _dim is None:\n",
      "  1491|         0|            0|            0|  0.00%|            _dim = list(range(ndim))\n",
      "  1492|         0|            0|            0|  0.00%|\n",
      "  1493|         0|            0|            0|  0.00%|        if out is None:\n",
      "  1494|         0|            0|            0|  0.00%|            if dtype is None:\n",
      "  1495|         0|            0|            0|  0.00%|                return _VF.norm(input, p, _dim, keepdim=keepdim)  # type: ignore[attr-defined]\n",
      "  1496|         0|            0|            0|  0.00%|            else:\n",
      "  1497|         0|            0|            0|  0.00%|                return _VF.norm(input, p, _dim, keepdim=keepdim, dtype=dtype)  # type: ignore[attr-defined]\n",
      "  1498|         0|            0|            0|  0.00%|        else:\n",
      "  1499|         0|            0|            0|  0.00%|            if dtype is None:\n",
      "  1500|         0|            0|            0|  0.00%|                return _VF.norm(input, p, _dim, keepdim=keepdim, out=out)  # type: ignore[attr-defined]\n",
      "  1501|         0|            0|            0|  0.00%|            else:\n",
      "  1502|         0|            0|            0|  0.00%|                return _VF.norm(input, p, _dim, keepdim=keepdim, dtype=dtype, out=out)  # type: ignore[attr-defined]\n",
      "  1503|         0|            0|            0|  0.00%|\n",
      "  1504|         0|            0|            0|  0.00%|def chain_matmul(*matrices, out=None):\n",
      "  1505|         0|            0|            0|  0.00%|    r\"\"\"Returns the matrix product of the :math:`N` 2-D tensors. This product is efficiently computed\n",
      "  1506|         0|            0|            0|  0.00%|    using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms\n",
      "  1507|         0|            0|            0|  0.00%|    of arithmetic operations (`[CLRS]`_). Note that since this is a function to compute the product, :math:`N`\n",
      "  1508|         0|            0|            0|  0.00%|    needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.\n",
      "  1509|         0|            0|            0|  0.00%|    If :math:`N` is 1, then this is a no-op - the original matrix is returned as is.\n",
      "  1510|         0|            0|            0|  0.00%|\n",
      "  1511|         0|            0|            0|  0.00%|    .. warning::\n",
      "  1512|         0|            0|            0|  0.00%|\n",
      "  1513|         0|            0|            0|  0.00%|        :func:`torch.chain_matmul` is deprecated and will be removed in a future PyTorch release.\n",
      "  1514|         0|            0|            0|  0.00%|        Use :func:`torch.linalg.multi_dot` instead, which accepts a list of two or more tensors\n",
      "  1515|         0|            0|            0|  0.00%|        rather than multiple arguments.\n",
      "  1516|         0|            0|            0|  0.00%|\n",
      "  1517|         0|            0|            0|  0.00%|    Args:\n",
      "  1518|         0|            0|            0|  0.00%|        matrices (Tensors...): a sequence of 2 or more 2-D tensors whose product is to be determined.\n",
      "  1519|         0|            0|            0|  0.00%|        out (Tensor, optional): the output tensor. Ignored if :attr:`out` = ``None``.\n",
      "  1520|         0|            0|            0|  0.00%|\n",
      "  1521|         0|            0|            0|  0.00%|    Returns:\n",
      "  1522|         0|            0|            0|  0.00%|        Tensor: if the :math:`i^{th}` tensor was of dimensions :math:`p_{i} \\times p_{i + 1}`, then the product\n",
      "  1523|         0|            0|            0|  0.00%|        would be of dimensions :math:`p_{1} \\times p_{N + 1}`.\n",
      "  1524|         0|            0|            0|  0.00%|\n",
      "  1525|         0|            0|            0|  0.00%|    Example::\n",
      "  1526|         0|            0|            0|  0.00%|\n",
      "  1527|         0|            0|            0|  0.00%|        >>> a = torch.randn(3, 4)\n",
      "  1528|         0|            0|            0|  0.00%|        >>> b = torch.randn(4, 5)\n",
      "  1529|         0|            0|            0|  0.00%|        >>> c = torch.randn(5, 6)\n",
      "  1530|         0|            0|            0|  0.00%|        >>> d = torch.randn(6, 7)\n",
      "  1531|         0|            0|            0|  0.00%|        >>> torch.chain_matmul(a, b, c, d)\n",
      "  1532|         0|            0|            0|  0.00%|        tensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],\n",
      "  1533|         0|            0|            0|  0.00%|                [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],\n",
      "  1534|         0|            0|            0|  0.00%|                [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])\n",
      "  1535|         0|            0|            0|  0.00%|\n",
      "  1536|         0|            0|            0|  0.00%|    .. _`[CLRS]`: https://mitpress.mit.edu/books/introduction-algorithms-third-edition\n",
      "  1537|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1538|         0|            0|            0|  0.00%|    # This wrapper exists to support variadic args.\n",
      "  1539|         0|            0|            0|  0.00%|    if has_torch_function(matrices):\n",
      "  1540|         0|            0|            0|  0.00%|        return handle_torch_function(chain_matmul, matrices, *matrices)\n",
      "  1541|         0|            0|            0|  0.00%|\n",
      "  1542|         0|            0|            0|  0.00%|    if out is None:\n",
      "  1543|         0|            0|            0|  0.00%|        return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]\n",
      "  1544|         0|            0|            0|  0.00%|    else:\n",
      "  1545|         0|            0|            0|  0.00%|        return _VF.chain_matmul(matrices, out=out)  # type: ignore[attr-defined]\n",
      "  1546|         0|            0|            0|  0.00%|\n",
      "  1547|         0|            0|            0|  0.00%|\n",
      "  1548|         0|            0|            0|  0.00%|def _lu_impl(A, pivot=True, get_infos=False, out=None):\n",
      "  1549|         0|            0|            0|  0.00%|    # type: (Tensor, bool, bool, Any) -> Tuple[Tensor, Tensor, Tensor]\n",
      "  1550|         0|            0|            0|  0.00%|    r\"\"\"Computes the LU factorization of a matrix or batches of matrices\n",
      "  1551|         0|            0|            0|  0.00%|    :attr:`A`. Returns a tuple containing the LU factorization and\n",
      "  1552|         0|            0|            0|  0.00%|    pivots of :attr:`A`.  Pivoting is done if :attr:`pivot` is set to\n",
      "  1553|         0|            0|            0|  0.00%|    ``True``.\n",
      "  1554|         0|            0|            0|  0.00%|\n",
      "  1555|         0|            0|            0|  0.00%|    .. note::\n",
      "  1556|         0|            0|            0|  0.00%|        * The returned permutation matrix for every matrix in the batch is\n",
      "  1557|         0|            0|            0|  0.00%|          represented by a 1-indexed vector of size ``min(A.shape[-2], A.shape[-1])``.\n",
      "  1558|         0|            0|            0|  0.00%|          ``pivots[i] == j`` represents that in the ``i``-th step of the algorithm,\n",
      "  1559|         0|            0|            0|  0.00%|          the ``i``-th row was permuted with the ``j-1``-th row.\n",
      "  1560|         0|            0|            0|  0.00%|        * LU factorization with :attr:`pivot` = ``False`` is not available\n",
      "  1561|         0|            0|            0|  0.00%|          for CPU, and attempting to do so will throw an error. However,\n",
      "  1562|         0|            0|            0|  0.00%|          LU factorization with :attr:`pivot` = ``False`` is available for\n",
      "  1563|         0|            0|            0|  0.00%|          CUDA.\n",
      "  1564|         0|            0|            0|  0.00%|        * This function does not check if the factorization was successful\n",
      "  1565|         0|            0|            0|  0.00%|          or not if :attr:`get_infos` is ``True`` since the status of the\n",
      "  1566|         0|            0|            0|  0.00%|          factorization is present in the third element of the return tuple.\n",
      "  1567|         0|            0|            0|  0.00%|        * In the case of batches of square matrices with size less or equal\n",
      "  1568|         0|            0|            0|  0.00%|          to 32 on a CUDA device, the LU factorization is repeated for\n",
      "  1569|         0|            0|            0|  0.00%|          singular matrices due to the bug in the MAGMA library\n",
      "  1570|         0|            0|            0|  0.00%|          (see magma issue 13).\n",
      "  1571|         0|            0|            0|  0.00%|        * ``L``, ``U``, and ``P`` can be derived using :func:`torch.lu_unpack`.\n",
      "  1572|         0|            0|            0|  0.00%|\n",
      "  1573|         0|            0|            0|  0.00%|    .. warning::\n",
      "  1574|         0|            0|            0|  0.00%|        The gradients of this function will only be finite when :attr:`A` is full rank.\n",
      "  1575|         0|            0|            0|  0.00%|        This is because the LU decomposition is just differentiable at full rank matrices.\n",
      "  1576|         0|            0|            0|  0.00%|        Furthermore, if :attr:`A` is close to not being full rank,\n",
      "  1577|         0|            0|            0|  0.00%|        the gradient will be numerically unstable as it depends on the computation of :math:`L^{-1}` and :math:`U^{-1}`.\n",
      "  1578|         0|            0|            0|  0.00%|\n",
      "  1579|         0|            0|            0|  0.00%|    Args:\n",
      "  1580|         0|            0|            0|  0.00%|        A (Tensor): the tensor to factor of size :math:`(*, m, n)`\n",
      "  1581|         0|            0|            0|  0.00%|        pivot (bool, optional): controls whether pivoting is done. Default: ``True``\n",
      "  1582|         0|            0|            0|  0.00%|        get_infos (bool, optional): if set to ``True``, returns an info IntTensor.\n",
      "  1583|         0|            0|            0|  0.00%|                                    Default: ``False``\n",
      "  1584|         0|            0|            0|  0.00%|        out (tuple, optional): optional output tuple. If :attr:`get_infos` is ``True``,\n",
      "  1585|         0|            0|            0|  0.00%|                               then the elements in the tuple are Tensor, IntTensor,\n",
      "  1586|         0|            0|            0|  0.00%|                               and IntTensor. If :attr:`get_infos` is ``False``, then the\n",
      "  1587|         0|            0|            0|  0.00%|                               elements in the tuple are Tensor, IntTensor. Default: ``None``\n",
      "  1588|         0|            0|            0|  0.00%|\n",
      "  1589|         0|            0|            0|  0.00%|    Returns:\n",
      "  1590|         0|            0|            0|  0.00%|        (Tensor, IntTensor, IntTensor (optional)): A tuple of tensors containing\n",
      "  1591|         0|            0|            0|  0.00%|\n",
      "  1592|         0|            0|            0|  0.00%|            - **factorization** (*Tensor*): the factorization of size :math:`(*, m, n)`\n",
      "  1593|         0|            0|            0|  0.00%|\n",
      "  1594|         0|            0|            0|  0.00%|            - **pivots** (*IntTensor*): the pivots of size :math:`(*, \\text{min}(m, n))`.\n",
      "  1595|         0|            0|            0|  0.00%|              ``pivots`` stores all the intermediate transpositions of rows.\n",
      "  1596|         0|            0|            0|  0.00%|              The final permutation ``perm`` could be reconstructed by\n",
      "  1597|         0|            0|            0|  0.00%|              applying ``swap(perm[i], perm[pivots[i] - 1])`` for ``i = 0, ..., pivots.size(-1) - 1``,\n",
      "  1598|         0|            0|            0|  0.00%|              where ``perm`` is initially the identity permutation of :math:`m` elements\n",
      "  1599|         0|            0|            0|  0.00%|              (essentially this is what :func:`torch.lu_unpack` is doing).\n",
      "  1600|         0|            0|            0|  0.00%|\n",
      "  1601|         0|            0|            0|  0.00%|            - **infos** (*IntTensor*, *optional*): if :attr:`get_infos` is ``True``, this is a tensor of\n",
      "  1602|         0|            0|            0|  0.00%|              size :math:`(*)` where non-zero values indicate whether factorization for the matrix or\n",
      "  1603|         0|            0|            0|  0.00%|              each minibatch has succeeded or failed\n",
      "  1604|         0|            0|            0|  0.00%|\n",
      "  1605|         0|            0|            0|  0.00%|    Example::\n",
      "  1606|         0|            0|            0|  0.00%|\n",
      "  1607|         0|            0|            0|  0.00%|        >>> A = torch.randn(2, 3, 3)\n",
      "  1608|         0|            0|            0|  0.00%|        >>> A_LU, pivots = torch.lu(A)\n",
      "  1609|         0|            0|            0|  0.00%|        >>> A_LU\n",
      "  1610|         0|            0|            0|  0.00%|        tensor([[[ 1.3506,  2.5558, -0.0816],\n",
      "  1611|         0|            0|            0|  0.00%|                 [ 0.1684,  1.1551,  0.1940],\n",
      "  1612|         0|            0|            0|  0.00%|                 [ 0.1193,  0.6189, -0.5497]],\n",
      "  1613|         0|            0|            0|  0.00%|\n",
      "  1614|         0|            0|            0|  0.00%|                [[ 0.4526,  1.2526, -0.3285],\n",
      "  1615|         0|            0|            0|  0.00%|                 [-0.7988,  0.7175, -0.9701],\n",
      "  1616|         0|            0|            0|  0.00%|                 [ 0.2634, -0.9255, -0.3459]]])\n",
      "  1617|         0|            0|            0|  0.00%|        >>> pivots\n",
      "  1618|         0|            0|            0|  0.00%|        tensor([[ 3,  3,  3],\n",
      "  1619|         0|            0|            0|  0.00%|                [ 3,  3,  3]], dtype=torch.int32)\n",
      "  1620|         0|            0|            0|  0.00%|        >>> A_LU, pivots, info = torch.lu(A, get_infos=True)\n",
      "  1621|         0|            0|            0|  0.00%|        >>> if info.nonzero().size(0) == 0:\n",
      "  1622|         0|            0|            0|  0.00%|        ...   print('LU factorization succeeded for all samples!')\n",
      "  1623|         0|            0|            0|  0.00%|        LU factorization succeeded for all samples!\n",
      "  1624|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1625|         0|            0|            0|  0.00%|    # If get_infos is True, then we don't need to check for errors and vice versa\n",
      "  1626|         0|            0|            0|  0.00%|    return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))\n",
      "  1627|         0|            0|            0|  0.00%|\n",
      "  1628|         0|            0|            0|  0.00%|if TYPE_CHECKING:\n",
      "  1629|         0|            0|            0|  0.00%|    _ListOrSeq = Sequence[Tensor]\n",
      "  1630|         0|            0|            0|  0.00%|else:\n",
      "  1631|         0|            0|            0|  0.00%|    _ListOrSeq = List[Tensor]\n",
      "  1632|         0|            0|            0|  0.00%|\n",
      "  1633|         0|            0|            0|  0.00%|def _check_list_size(out_len: int, get_infos: bool, out: _ListOrSeq) -> None:\n",
      "  1634|         0|            0|            0|  0.00%|    get_infos_int = 1 if get_infos else 0\n",
      "  1635|         0|            0|            0|  0.00%|    if out_len - get_infos_int != 2:\n",
      "  1636|         0|            0|            0|  0.00%|        raise TypeError(f\"expected tuple of {2 + int(get_infos)} elements but got {out_len}\")\n",
      "  1637|         0|            0|            0|  0.00%|    if not isinstance(out, (tuple, list)):\n",
      "  1638|         0|            0|            0|  0.00%|        raise TypeError(f\"argument 'out' must be tuple of Tensors, not {type(out).__name__}\")\n",
      "  1639|         0|            0|            0|  0.00%|\n",
      "  1640|         0|            0|            0|  0.00%|def _lu_with_infos(A, pivot=True, get_infos=False, out=None):\n",
      "  1641|         0|            0|            0|  0.00%|    # type: (Tensor, bool, bool, Optional[Tuple[Tensor, Tensor, Tensor]]) -> Tuple[Tensor, Tensor, Tensor]\n",
      "  1642|         0|            0|            0|  0.00%|    if has_torch_function_unary(A):\n",
      "  1643|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  1644|         0|            0|            0|  0.00%|            lu, (A,), A, pivot=pivot, get_infos=get_infos, out=out)\n",
      "  1645|         0|            0|            0|  0.00%|    result = _lu_impl(A, pivot, get_infos, out)\n",
      "  1646|         0|            0|            0|  0.00%|    if out is not None:\n",
      "  1647|         0|            0|            0|  0.00%|        _check_list_size(len(out), get_infos, out)\n",
      "  1648|         0|            0|            0|  0.00%|        for i in range(len(out)):\n",
      "  1649|         0|            0|            0|  0.00%|            out[i].resize_as_(result[i]).copy_(result[i])\n",
      "  1650|         0|            0|            0|  0.00%|        return out\n",
      "  1651|         0|            0|            0|  0.00%|    else:\n",
      "  1652|         0|            0|            0|  0.00%|        return result  # A_LU, pivots, infos\n",
      "  1653|         0|            0|            0|  0.00%|\n",
      "  1654|         0|            0|            0|  0.00%|def _lu_no_infos(A, pivot=True, get_infos=False, out=None):\n",
      "  1655|         0|            0|            0|  0.00%|    # type: (Tensor, bool, bool, Optional[Tuple[Tensor, Tensor]]) -> Tuple[Tensor, Tensor]\n",
      "  1656|         0|            0|            0|  0.00%|    # need to check for torch_function here so that we exit if\n",
      "  1657|         0|            0|            0|  0.00%|    if has_torch_function_unary(A):\n",
      "  1658|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  1659|         0|            0|            0|  0.00%|            lu, (A,), A, pivot=pivot, get_infos=get_infos, out=out)\n",
      "  1660|         0|            0|            0|  0.00%|    result = _lu_impl(A, pivot, get_infos, out)\n",
      "  1661|         0|            0|            0|  0.00%|    if out is not None:\n",
      "  1662|         0|            0|            0|  0.00%|        _check_list_size(len(out), get_infos, out)\n",
      "  1663|         0|            0|            0|  0.00%|        for i in range(len(out)):\n",
      "  1664|         0|            0|            0|  0.00%|            out[i].resize_as_(result[i]).copy_(result[i])\n",
      "  1665|         0|            0|            0|  0.00%|        return out\n",
      "  1666|         0|            0|            0|  0.00%|    else:\n",
      "  1667|         0|            0|            0|  0.00%|        return result[0], result[1]  # A_LU, pivots\n",
      "  1668|         0|            0|            0|  0.00%|\n",
      "  1669|         0|            0|            0|  0.00%|# The return type of lu depends on `get_infos`, so in order to resolve the output type\n",
      "  1670|         0|            0|            0|  0.00%|# of lu in TorchScript we need to statically know the value of `get_infos`\n",
      "  1671|         0|            0|            0|  0.00%|lu = boolean_dispatch(\n",
      "  1672|         0|            0|            0|  0.00%|    arg_name='get_infos',\n",
      "  1673|         0|            0|            0|  0.00%|    arg_index=2,\n",
      "  1674|         0|            0|            0|  0.00%|    default=False,\n",
      "  1675|         0|            0|            0|  0.00%|    if_true=_lu_with_infos,\n",
      "  1676|         0|            0|            0|  0.00%|    if_false=_lu_no_infos,\n",
      "  1677|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "  1678|         0|            0|            0|  0.00%|    func_name='lu')\n",
      "  1679|         0|            0|            0|  0.00%|lu.__doc__ = _lu_impl.__doc__\n",
      "  1680|         0|            0|            0|  0.00%|\n",
      "  1681|         0|            0|            0|  0.00%|def align_tensors(*tensors):\n",
      "  1682|         0|            0|            0|  0.00%|    raise RuntimeError('`align_tensors` not yet implemented.')\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py\n",
      "File duration: 3.86917s (0.62%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|import warnings\n",
      "     2|         0|            0|            0|  0.00%|import torch\n",
      "     3|         0|            0|            0|  0.00%|from torch._six import inf\n",
      "     4|         0|            0|            0|  0.00%|from typing import Union, Iterable\n",
      "     5|         0|            0|            0|  0.00%|\n",
      "     6|         0|            0|            0|  0.00%|_tensor_or_tensors = Union[torch.Tensor, Iterable[torch.Tensor]]\n",
      "     7|         0|            0|            0|  0.00%|\n",
      "     8|         0|            0|            0|  0.00%|\n",
      "     9|      6240|     0.041419|  6.63766e-06|  0.01%|def clip_grad_norm_(\n",
      "    10|         0|            0|            0|  0.00%|        parameters: _tensor_or_tensors, max_norm: float, norm_type: float = 2.0,\n",
      "    11|         0|            0|            0|  0.00%|        error_if_nonfinite: bool = False) -> torch.Tensor:\n",
      "    12|         0|            0|            0|  0.00%|    r\"\"\"Clips gradient norm of an iterable of parameters.\n",
      "    13|         0|            0|            0|  0.00%|\n",
      "    14|         0|            0|            0|  0.00%|    The norm is computed over all gradients together, as if they were\n",
      "    15|         0|            0|            0|  0.00%|    concatenated into a single vector. Gradients are modified in-place.\n",
      "    16|         0|            0|            0|  0.00%|\n",
      "    17|         0|            0|            0|  0.00%|    Args:\n",
      "    18|         0|            0|            0|  0.00%|        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n",
      "    19|         0|            0|            0|  0.00%|            single Tensor that will have gradients normalized\n",
      "    20|         0|            0|            0|  0.00%|        max_norm (float or int): max norm of the gradients\n",
      "    21|         0|            0|            0|  0.00%|        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n",
      "    22|         0|            0|            0|  0.00%|            infinity norm.\n",
      "    23|         0|            0|            0|  0.00%|        error_if_nonfinite (bool): if True, an error is thrown if the total\n",
      "    24|         0|            0|            0|  0.00%|            norm of the gradients from :attr:`parameters` is ``nan``,\n",
      "    25|         0|            0|            0|  0.00%|            ``inf``, or ``-inf``. Default: False (will switch to True in the future)\n",
      "    26|         0|            0|            0|  0.00%|\n",
      "    27|         0|            0|            0|  0.00%|    Returns:\n",
      "    28|         0|            0|            0|  0.00%|        Total norm of the parameter gradients (viewed as a single vector).\n",
      "    29|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    30|      6240|     0.037921|  6.07708e-06|  0.01%|    if isinstance(parameters, torch.Tensor):\n",
      "    31|         0|            0|            0|  0.00%|        parameters = [parameters]\n",
      "    32|     93600|     0.721808|  7.71162e-06|  0.12%|    parameters = [p for p in parameters if p.grad is not None]\n",
      "(call)|     81120|      9.52696|  0.000117443|  1.53%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1621 parameters\n",
      "(call)|     74880|      0.35251|  4.70767e-06|  0.06%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:1071 grad\n",
      "(call)|      6240|      10.5426|   0.00168952|  1.70%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py:32 <listcomp>\n",
      "    33|      6240|    0.0215621|  3.45546e-06|  0.00%|    max_norm = float(max_norm)\n",
      "    34|      6240|    0.0192699|  3.08813e-06|  0.00%|    norm_type = float(norm_type)\n",
      "    35|      6240|    0.0193243|  3.09684e-06|  0.00%|    if len(parameters) == 0:\n",
      "    36|         0|            0|            0|  0.00%|        return torch.tensor(0.)\n",
      "    37|      6240|    0.0474124|  7.59814e-06|  0.01%|    device = parameters[0].grad.device\n",
      "(call)|      6240|    0.0315516|  5.05635e-06|  0.01%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:1071 grad\n",
      "    38|      6240|    0.0200131|  3.20723e-06|  0.00%|    if norm_type == inf:\n",
      "    39|         0|            0|            0|  0.00%|        norms = [p.grad.detach().abs().max().to(device) for p in parameters]\n",
      "    40|         0|            0|            0|  0.00%|        total_norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))\n",
      "    41|         0|            0|            0|  0.00%|    else:\n",
      "    42|     93600|      1.28633|  1.37429e-05|  0.21%|        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
      "(call)|     74880|     0.334818|   4.4714e-06|  0.05%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:1071 grad\n",
      "(call)|     74880|       3.1669|  4.22931e-05|  0.51%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/functional.py:1345 norm\n",
      "(call)|      6240|      4.57401|  0.000733014|  0.74%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py:42 <listcomp>\n",
      "(call)|      6240|     0.255957|  4.10187e-05|  0.04%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/functional.py:1345 norm\n",
      "    43|      6240|    0.0194099|  3.11056e-06|  0.00%|    if error_if_nonfinite and torch.logical_or(total_norm.isnan(), total_norm.isinf()):\n",
      "    44|         0|            0|            0|  0.00%|        raise RuntimeError(\n",
      "    45|         0|            0|            0|  0.00%|            f'The total norm of order {norm_type} for gradients from '\n",
      "    46|         0|            0|            0|  0.00%|            '`parameters` is non-finite, so it cannot be clipped. To disable '\n",
      "    47|         0|            0|            0|  0.00%|            'this error and scale the gradients by the non-finite norm anyway, '\n",
      "    48|         0|            0|            0|  0.00%|            'set `error_if_nonfinite=False`')\n",
      "    49|      6240|     0.146724|  2.35135e-05|  0.02%|    clip_coef = max_norm / (total_norm + 1e-6)\n",
      "(call)|      6240|     0.178977|  2.86823e-05|  0.03%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:26 wrapped\n",
      "    50|         0|            0|            0|  0.00%|    # Note: multiplying by the clamped coef is redundant when the coef is clamped to 1, but doing so\n",
      "    51|         0|            0|            0|  0.00%|    # avoids a `if clip_coef < 1:` conditional which can require a CPU <=> device synchronization\n",
      "    52|         0|            0|            0|  0.00%|    # when the gradients do not reside in CPU memory.\n",
      "    53|      6240|    0.0600283|  9.61992e-06|  0.01%|    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
      "    54|     81120|     0.208178|   2.5663e-06|  0.03%|    for p in parameters:\n",
      "    55|     74880|      1.20567|  1.61014e-05|  0.19%|        p.grad.detach().mul_(clip_coef_clamped.to(p.grad.device))\n",
      "(call)|    149760|     0.777277|  5.19015e-06|  0.13%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:1071 grad\n",
      "    56|      6240|    0.0140994|  2.25951e-06|  0.00%|    return total_norm\n",
      "    57|         0|            0|            0|  0.00%|\n",
      "    58|         0|            0|            0|  0.00%|\n",
      "    59|         0|            0|            0|  0.00%|def clip_grad_norm(\n",
      "    60|         0|            0|            0|  0.00%|        parameters: _tensor_or_tensors, max_norm: float, norm_type: float = 2.,\n",
      "    61|         0|            0|            0|  0.00%|        error_if_nonfinite: bool = False) -> torch.Tensor:\n",
      "    62|         0|            0|            0|  0.00%|    r\"\"\"Clips gradient norm of an iterable of parameters.\n",
      "    63|         0|            0|            0|  0.00%|\n",
      "    64|         0|            0|            0|  0.00%|    .. warning::\n",
      "    65|         0|            0|            0|  0.00%|        This method is now deprecated in favor of\n",
      "    66|         0|            0|            0|  0.00%|        :func:`torch.nn.utils.clip_grad_norm_`.\n",
      "    67|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    68|         0|            0|            0|  0.00%|    warnings.warn(\"torch.nn.utils.clip_grad_norm is now deprecated in favor \"\n",
      "    69|         0|            0|            0|  0.00%|                  \"of torch.nn.utils.clip_grad_norm_.\", stacklevel=2)\n",
      "    70|         0|            0|            0|  0.00%|    return clip_grad_norm_(parameters, max_norm, norm_type, error_if_nonfinite)\n",
      "    71|         0|            0|            0|  0.00%|\n",
      "    72|         0|            0|            0|  0.00%|\n",
      "    73|         0|            0|            0|  0.00%|def clip_grad_value_(parameters: _tensor_or_tensors, clip_value: float) -> None:\n",
      "    74|         0|            0|            0|  0.00%|    r\"\"\"Clips gradient of an iterable of parameters at specified value.\n",
      "    75|         0|            0|            0|  0.00%|\n",
      "    76|         0|            0|            0|  0.00%|    Gradients are modified in-place.\n",
      "    77|         0|            0|            0|  0.00%|\n",
      "    78|         0|            0|            0|  0.00%|    Args:\n",
      "    79|         0|            0|            0|  0.00%|        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n",
      "    80|         0|            0|            0|  0.00%|            single Tensor that will have gradients normalized\n",
      "    81|         0|            0|            0|  0.00%|        clip_value (float or int): maximum allowed value of the gradients.\n",
      "    82|         0|            0|            0|  0.00%|            The gradients are clipped in the range\n",
      "    83|         0|            0|            0|  0.00%|            :math:`\\left[\\text{-clip\\_value}, \\text{clip\\_value}\\right]`\n",
      "    84|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    85|         0|            0|            0|  0.00%|    if isinstance(parameters, torch.Tensor):\n",
      "    86|         0|            0|            0|  0.00%|        parameters = [parameters]\n",
      "    87|         0|            0|            0|  0.00%|    clip_value = float(clip_value)\n",
      "    88|         0|            0|            0|  0.00%|    for p in filter(lambda p: p.grad is not None, parameters):\n",
      "    89|         0|            0|            0|  0.00%|        p.grad.data.clamp_(min=-clip_value, max=clip_value)\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/constraints.py\n",
      "File duration: 3.43455s (0.55%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|r\"\"\"\n",
      "     2|         0|            0|            0|  0.00%|The following constraints are implemented:\n",
      "     3|         0|            0|            0|  0.00%|\n",
      "     4|         0|            0|            0|  0.00%|- ``constraints.boolean``\n",
      "     5|         0|            0|            0|  0.00%|- ``constraints.cat``\n",
      "     6|         0|            0|            0|  0.00%|- ``constraints.corr_cholesky``\n",
      "     7|         0|            0|            0|  0.00%|- ``constraints.dependent``\n",
      "     8|         0|            0|            0|  0.00%|- ``constraints.greater_than(lower_bound)``\n",
      "     9|         0|            0|            0|  0.00%|- ``constraints.greater_than_eq(lower_bound)``\n",
      "    10|         0|            0|            0|  0.00%|- ``constraints.independent(constraint, reinterpreted_batch_ndims)``\n",
      "    11|         0|            0|            0|  0.00%|- ``constraints.integer_interval(lower_bound, upper_bound)``\n",
      "    12|         0|            0|            0|  0.00%|- ``constraints.interval(lower_bound, upper_bound)``\n",
      "    13|         0|            0|            0|  0.00%|- ``constraints.less_than(upper_bound)``\n",
      "    14|         0|            0|            0|  0.00%|- ``constraints.lower_cholesky``\n",
      "    15|         0|            0|            0|  0.00%|- ``constraints.lower_triangular``\n",
      "    16|         0|            0|            0|  0.00%|- ``constraints.multinomial``\n",
      "    17|         0|            0|            0|  0.00%|- ``constraints.nonnegative_integer``\n",
      "    18|         0|            0|            0|  0.00%|- ``constraints.one_hot``\n",
      "    19|         0|            0|            0|  0.00%|- ``constraints.positive_integer``\n",
      "    20|         0|            0|            0|  0.00%|- ``constraints.positive``\n",
      "    21|         0|            0|            0|  0.00%|- ``constraints.positive_semidefinite``\n",
      "    22|         0|            0|            0|  0.00%|- ``constraints.positive_definite``\n",
      "    23|         0|            0|            0|  0.00%|- ``constraints.real_vector``\n",
      "    24|         0|            0|            0|  0.00%|- ``constraints.real``\n",
      "    25|         0|            0|            0|  0.00%|- ``constraints.simplex``\n",
      "    26|         0|            0|            0|  0.00%|- ``constraints.symmetric``\n",
      "    27|         0|            0|            0|  0.00%|- ``constraints.stack``\n",
      "    28|         0|            0|            0|  0.00%|- ``constraints.square``\n",
      "    29|         0|            0|            0|  0.00%|- ``constraints.symmetric``\n",
      "    30|         0|            0|            0|  0.00%|- ``constraints.unit_interval``\n",
      "    31|         0|            0|            0|  0.00%|\"\"\"\n",
      "    32|         0|            0|            0|  0.00%|\n",
      "    33|         0|            0|            0|  0.00%|import torch\n",
      "    34|         0|            0|            0|  0.00%|\n",
      "    35|         0|            0|            0|  0.00%|__all__ = [\n",
      "    36|         0|            0|            0|  0.00%|    'Constraint',\n",
      "    37|         0|            0|            0|  0.00%|    'boolean',\n",
      "    38|         0|            0|            0|  0.00%|    'cat',\n",
      "    39|         0|            0|            0|  0.00%|    'corr_cholesky',\n",
      "    40|         0|            0|            0|  0.00%|    'dependent',\n",
      "    41|         0|            0|            0|  0.00%|    'dependent_property',\n",
      "    42|         0|            0|            0|  0.00%|    'greater_than',\n",
      "    43|         0|            0|            0|  0.00%|    'greater_than_eq',\n",
      "    44|         0|            0|            0|  0.00%|    'independent',\n",
      "    45|         0|            0|            0|  0.00%|    'integer_interval',\n",
      "    46|         0|            0|            0|  0.00%|    'interval',\n",
      "    47|         0|            0|            0|  0.00%|    'half_open_interval',\n",
      "    48|         0|            0|            0|  0.00%|    'is_dependent',\n",
      "    49|         0|            0|            0|  0.00%|    'less_than',\n",
      "    50|         0|            0|            0|  0.00%|    'lower_cholesky',\n",
      "    51|         0|            0|            0|  0.00%|    'lower_triangular',\n",
      "    52|         0|            0|            0|  0.00%|    'multinomial',\n",
      "    53|         0|            0|            0|  0.00%|    'nonnegative_integer',\n",
      "    54|         0|            0|            0|  0.00%|    'positive',\n",
      "    55|         0|            0|            0|  0.00%|    'positive_semidefinite',\n",
      "    56|         0|            0|            0|  0.00%|    'positive_definite',\n",
      "    57|         0|            0|            0|  0.00%|    'positive_integer',\n",
      "    58|         0|            0|            0|  0.00%|    'real',\n",
      "    59|         0|            0|            0|  0.00%|    'real_vector',\n",
      "    60|         0|            0|            0|  0.00%|    'simplex',\n",
      "    61|         0|            0|            0|  0.00%|    'square',\n",
      "    62|         0|            0|            0|  0.00%|    'stack',\n",
      "    63|         0|            0|            0|  0.00%|    'symmetric',\n",
      "    64|         0|            0|            0|  0.00%|    'unit_interval',\n",
      "    65|         0|            0|            0|  0.00%|]\n",
      "    66|         0|            0|            0|  0.00%|\n",
      "    67|         0|            0|            0|  0.00%|\n",
      "    68|         0|            0|            0|  0.00%|class Constraint(object):\n",
      "    69|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    70|         0|            0|            0|  0.00%|    Abstract base class for constraints.\n",
      "    71|         0|            0|            0|  0.00%|\n",
      "    72|         0|            0|            0|  0.00%|    A constraint object represents a region over which a variable is valid,\n",
      "    73|         0|            0|            0|  0.00%|    e.g. within which a variable can be optimized.\n",
      "    74|         0|            0|            0|  0.00%|\n",
      "    75|         0|            0|            0|  0.00%|    Attributes:\n",
      "    76|         0|            0|            0|  0.00%|        is_discrete (bool): Whether constrained space is discrete.\n",
      "    77|         0|            0|            0|  0.00%|            Defaults to False.\n",
      "    78|         0|            0|            0|  0.00%|        event_dim (int): Number of rightmost dimensions that together define\n",
      "    79|         0|            0|            0|  0.00%|            an event. The :meth:`check` method will remove this many dimensions\n",
      "    80|         0|            0|            0|  0.00%|            when computing validity.\n",
      "    81|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    82|         0|            0|            0|  0.00%|    is_discrete = False  # Default to continuous.\n",
      "    83|         0|            0|            0|  0.00%|    event_dim = 0  # Default to univariate.\n",
      "    84|         0|            0|            0|  0.00%|\n",
      "    85|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "    86|         0|            0|            0|  0.00%|        \"\"\"\n",
      "    87|         0|            0|            0|  0.00%|        Returns a byte tensor of ``sample_shape + batch_shape`` indicating\n",
      "    88|         0|            0|            0|  0.00%|        whether each event in value satisfies this constraint.\n",
      "    89|         0|            0|            0|  0.00%|        \"\"\"\n",
      "    90|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "    91|         0|            0|            0|  0.00%|\n",
      "    92|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "    93|         0|            0|            0|  0.00%|        return self.__class__.__name__[1:] + '()'\n",
      "    94|         0|            0|            0|  0.00%|\n",
      "    95|         0|            0|            0|  0.00%|\n",
      "    96|         0|            0|            0|  0.00%|class _Dependent(Constraint):\n",
      "    97|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    98|         0|            0|            0|  0.00%|    Placeholder for variables whose support depends on other variables.\n",
      "    99|         0|            0|            0|  0.00%|    These variables obey no simple coordinate-wise constraints.\n",
      "   100|         0|            0|            0|  0.00%|\n",
      "   101|         0|            0|            0|  0.00%|    Args:\n",
      "   102|         0|            0|            0|  0.00%|        is_discrete (bool): Optional value of ``.is_discrete`` in case this\n",
      "   103|         0|            0|            0|  0.00%|            can be computed statically. If not provided, access to the\n",
      "   104|         0|            0|            0|  0.00%|            ``.is_discrete`` attribute will raise a NotImplementedError.\n",
      "   105|         0|            0|            0|  0.00%|        event_dim (int): Optional value of ``.event_dim`` in case this\n",
      "   106|         0|            0|            0|  0.00%|            can be computed statically. If not provided, access to the\n",
      "   107|         0|            0|            0|  0.00%|            ``.event_dim`` attribute will raise a NotImplementedError.\n",
      "   108|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   109|         0|            0|            0|  0.00%|    def __init__(self, *, is_discrete=NotImplemented, event_dim=NotImplemented):\n",
      "   110|         0|            0|            0|  0.00%|        self._is_discrete = is_discrete\n",
      "   111|         0|            0|            0|  0.00%|        self._event_dim = event_dim\n",
      "   112|         0|            0|            0|  0.00%|        super().__init__()\n",
      "   113|         0|            0|            0|  0.00%|\n",
      "   114|         0|            0|            0|  0.00%|    @property\n",
      "   115|         0|            0|            0|  0.00%|    def is_discrete(self):\n",
      "   116|         0|            0|            0|  0.00%|        if self._is_discrete is NotImplemented:\n",
      "   117|         0|            0|            0|  0.00%|            raise NotImplementedError(\".is_discrete cannot be determined statically\")\n",
      "   118|         0|            0|            0|  0.00%|        return self._is_discrete\n",
      "   119|         0|            0|            0|  0.00%|\n",
      "   120|         0|            0|            0|  0.00%|    @property\n",
      "   121|         0|            0|            0|  0.00%|    def event_dim(self):\n",
      "   122|         0|            0|            0|  0.00%|        if self._event_dim is NotImplemented:\n",
      "   123|         0|            0|            0|  0.00%|            raise NotImplementedError(\".event_dim cannot be determined statically\")\n",
      "   124|         0|            0|            0|  0.00%|        return self._event_dim\n",
      "   125|         0|            0|            0|  0.00%|\n",
      "   126|         0|            0|            0|  0.00%|    def __call__(self, *, is_discrete=NotImplemented, event_dim=NotImplemented):\n",
      "   127|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   128|         0|            0|            0|  0.00%|        Support for syntax to customize static attributes::\n",
      "   129|         0|            0|            0|  0.00%|\n",
      "   130|         0|            0|            0|  0.00%|            constraints.dependent(is_discrete=True, event_dim=1)\n",
      "   131|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   132|         0|            0|            0|  0.00%|        if is_discrete is NotImplemented:\n",
      "   133|         0|            0|            0|  0.00%|            is_discrete = self._is_discrete\n",
      "   134|         0|            0|            0|  0.00%|        if event_dim is NotImplemented:\n",
      "   135|         0|            0|            0|  0.00%|            event_dim = self._event_dim\n",
      "   136|         0|            0|            0|  0.00%|        return _Dependent(is_discrete=is_discrete, event_dim=event_dim)\n",
      "   137|         0|            0|            0|  0.00%|\n",
      "   138|         0|            0|            0|  0.00%|    def check(self, x):\n",
      "   139|         0|            0|            0|  0.00%|        raise ValueError('Cannot determine validity of dependent constraint')\n",
      "   140|         0|            0|            0|  0.00%|\n",
      "   141|         0|            0|            0|  0.00%|\n",
      "   142|     63180|     0.101274|  1.60294e-06|  0.02%|def is_dependent(constraint):\n",
      "   143|     63180|     0.147137|  2.32885e-06|  0.02%|    return isinstance(constraint, _Dependent)\n",
      "   144|         0|            0|            0|  0.00%|\n",
      "   145|         0|            0|            0|  0.00%|\n",
      "   146|         0|            0|            0|  0.00%|class _DependentProperty(property, _Dependent):\n",
      "   147|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   148|         0|            0|            0|  0.00%|    Decorator that extends @property to act like a `Dependent` constraint when\n",
      "   149|         0|            0|            0|  0.00%|    called on a class and act like a property when called on an object.\n",
      "   150|         0|            0|            0|  0.00%|\n",
      "   151|         0|            0|            0|  0.00%|    Example::\n",
      "   152|         0|            0|            0|  0.00%|\n",
      "   153|         0|            0|            0|  0.00%|        class Uniform(Distribution):\n",
      "   154|         0|            0|            0|  0.00%|            def __init__(self, low, high):\n",
      "   155|         0|            0|            0|  0.00%|                self.low = low\n",
      "   156|         0|            0|            0|  0.00%|                self.high = high\n",
      "   157|         0|            0|            0|  0.00%|            @constraints.dependent_property(is_discrete=False, event_dim=0)\n",
      "   158|         0|            0|            0|  0.00%|            def support(self):\n",
      "   159|         0|            0|            0|  0.00%|                return constraints.interval(self.low, self.high)\n",
      "   160|         0|            0|            0|  0.00%|\n",
      "   161|         0|            0|            0|  0.00%|    Args:\n",
      "   162|         0|            0|            0|  0.00%|        fn (callable): The function to be decorated.\n",
      "   163|         0|            0|            0|  0.00%|        is_discrete (bool): Optional value of ``.is_discrete`` in case this\n",
      "   164|         0|            0|            0|  0.00%|            can be computed statically. If not provided, access to the\n",
      "   165|         0|            0|            0|  0.00%|            ``.is_discrete`` attribute will raise a NotImplementedError.\n",
      "   166|         0|            0|            0|  0.00%|        event_dim (int): Optional value of ``.event_dim`` in case this\n",
      "   167|         0|            0|            0|  0.00%|            can be computed statically. If not provided, access to the\n",
      "   168|         0|            0|            0|  0.00%|            ``.event_dim`` attribute will raise a NotImplementedError.\n",
      "   169|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   170|         0|            0|            0|  0.00%|    def __init__(self, fn=None, *, is_discrete=NotImplemented, event_dim=NotImplemented):\n",
      "   171|         0|            0|            0|  0.00%|        super().__init__(fn)\n",
      "   172|         0|            0|            0|  0.00%|        self._is_discrete = is_discrete\n",
      "   173|         0|            0|            0|  0.00%|        self._event_dim = event_dim\n",
      "   174|         0|            0|            0|  0.00%|\n",
      "   175|         0|            0|            0|  0.00%|    def __call__(self, fn):\n",
      "   176|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   177|         0|            0|            0|  0.00%|        Support for syntax to customize static attributes::\n",
      "   178|         0|            0|            0|  0.00%|\n",
      "   179|         0|            0|            0|  0.00%|            @constraints.dependent_property(is_discrete=True, event_dim=1)\n",
      "   180|         0|            0|            0|  0.00%|            def support(self):\n",
      "   181|         0|            0|            0|  0.00%|                ...\n",
      "   182|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   183|         0|            0|            0|  0.00%|        return _DependentProperty(fn, is_discrete=self._is_discrete, event_dim=self._event_dim)\n",
      "   184|         0|            0|            0|  0.00%|\n",
      "   185|         0|            0|            0|  0.00%|\n",
      "   186|         0|            0|            0|  0.00%|class _IndependentConstraint(Constraint):\n",
      "   187|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   188|         0|            0|            0|  0.00%|    Wraps a constraint by aggregating over ``reinterpreted_batch_ndims``-many\n",
      "   189|         0|            0|            0|  0.00%|    dims in :meth:`check`, so that an event is valid only if all its\n",
      "   190|         0|            0|            0|  0.00%|    independent entries are valid.\n",
      "   191|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   192|         0|            0|            0|  0.00%|    def __init__(self, base_constraint, reinterpreted_batch_ndims):\n",
      "   193|         0|            0|            0|  0.00%|        assert isinstance(base_constraint, Constraint)\n",
      "   194|         0|            0|            0|  0.00%|        assert isinstance(reinterpreted_batch_ndims, int)\n",
      "   195|         0|            0|            0|  0.00%|        assert reinterpreted_batch_ndims >= 0\n",
      "   196|         0|            0|            0|  0.00%|        self.base_constraint = base_constraint\n",
      "   197|         0|            0|            0|  0.00%|        self.reinterpreted_batch_ndims = reinterpreted_batch_ndims\n",
      "   198|         0|            0|            0|  0.00%|        super().__init__()\n",
      "   199|         0|            0|            0|  0.00%|\n",
      "   200|         0|            0|            0|  0.00%|    @property\n",
      "   201|         0|            0|            0|  0.00%|    def is_discrete(self):\n",
      "   202|         0|            0|            0|  0.00%|        return self.base_constraint.is_discrete\n",
      "   203|         0|            0|            0|  0.00%|\n",
      "   204|         0|            0|            0|  0.00%|    @property\n",
      "   205|         0|            0|            0|  0.00%|    def event_dim(self):\n",
      "   206|         0|            0|            0|  0.00%|        return self.base_constraint.event_dim + self.reinterpreted_batch_ndims\n",
      "   207|         0|            0|            0|  0.00%|\n",
      "   208|     31590|    0.0679126|  2.14981e-06|  0.01%|    def check(self, value):\n",
      "   209|     31590|      0.21973|  6.95569e-06|  0.04%|        result = self.base_constraint.check(value)\n",
      "(call)|     31590|     0.362005|  1.14595e-05|  0.06%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/constraints.py:307 check\n",
      "   210|     31590|    0.0836418|  2.64773e-06|  0.01%|        if result.dim() < self.reinterpreted_batch_ndims:\n",
      "   211|         0|            0|            0|  0.00%|            expected = self.base_constraint.event_dim + self.reinterpreted_batch_ndims\n",
      "   212|         0|            0|            0|  0.00%|            raise ValueError(f\"Expected value.dim() >= {expected} but got {value.dim()}\")\n",
      "   213|     31590|     0.352956|   1.1173e-05|  0.06%|        result = result.reshape(result.shape[:result.dim() - self.reinterpreted_batch_ndims] + (-1,))\n",
      "   214|     31590|     0.419701|  1.32859e-05|  0.07%|        result = result.all(-1)\n",
      "   215|     31590|    0.0746791|  2.36401e-06|  0.01%|        return result\n",
      "   216|         0|            0|            0|  0.00%|\n",
      "   217|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "   218|         0|            0|            0|  0.00%|        return \"{}({}, {})\".format(self.__class__.__name__[1:], repr(self.base_constraint),\n",
      "   219|         0|            0|            0|  0.00%|                                   self.reinterpreted_batch_ndims)\n",
      "   220|         0|            0|            0|  0.00%|\n",
      "   221|         0|            0|            0|  0.00%|\n",
      "   222|         0|            0|            0|  0.00%|class _Boolean(Constraint):\n",
      "   223|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   224|         0|            0|            0|  0.00%|    Constrain to the two values `{0, 1}`.\n",
      "   225|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   226|         0|            0|            0|  0.00%|    is_discrete = True\n",
      "   227|         0|            0|            0|  0.00%|\n",
      "   228|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   229|         0|            0|            0|  0.00%|        return (value == 0) | (value == 1)\n",
      "   230|         0|            0|            0|  0.00%|\n",
      "   231|         0|            0|            0|  0.00%|\n",
      "   232|         0|            0|            0|  0.00%|class _OneHot(Constraint):\n",
      "   233|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   234|         0|            0|            0|  0.00%|    Constrain to one-hot vectors.\n",
      "   235|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   236|         0|            0|            0|  0.00%|    is_discrete = True\n",
      "   237|         0|            0|            0|  0.00%|    event_dim = 1\n",
      "   238|         0|            0|            0|  0.00%|\n",
      "   239|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   240|         0|            0|            0|  0.00%|        is_boolean = (value == 0) | (value == 1)\n",
      "   241|         0|            0|            0|  0.00%|        is_normalized = value.sum(-1).eq(1)\n",
      "   242|         0|            0|            0|  0.00%|        return is_boolean.all(-1) & is_normalized\n",
      "   243|         0|            0|            0|  0.00%|\n",
      "   244|         0|            0|            0|  0.00%|\n",
      "   245|         0|            0|            0|  0.00%|class _IntegerInterval(Constraint):\n",
      "   246|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   247|         0|            0|            0|  0.00%|    Constrain to an integer interval `[lower_bound, upper_bound]`.\n",
      "   248|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   249|         0|            0|            0|  0.00%|    is_discrete = True\n",
      "   250|         0|            0|            0|  0.00%|\n",
      "   251|     31590|     0.053426|  1.69123e-06|  0.01%|    def __init__(self, lower_bound, upper_bound):\n",
      "   252|     31590|    0.0706625|  2.23686e-06|  0.01%|        self.lower_bound = lower_bound\n",
      "   253|     31590|    0.0595469|  1.88499e-06|  0.01%|        self.upper_bound = upper_bound\n",
      "   254|     31590|    0.0925043|  2.92828e-06|  0.01%|        super().__init__()\n",
      "   255|         0|            0|            0|  0.00%|\n",
      "   256|     31590|    0.0557396|  1.76447e-06|  0.01%|    def check(self, value):\n",
      "   257|     31590|      1.27364|  4.03178e-05|  0.21%|        return (value % 1 == 0) & (self.lower_bound <= value) & (value <= self.upper_bound)\n",
      "   258|         0|            0|            0|  0.00%|\n",
      "   259|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "   260|         0|            0|            0|  0.00%|        fmt_string = self.__class__.__name__[1:]\n",
      "   261|         0|            0|            0|  0.00%|        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n",
      "   262|         0|            0|            0|  0.00%|        return fmt_string\n",
      "   263|         0|            0|            0|  0.00%|\n",
      "   264|         0|            0|            0|  0.00%|\n",
      "   265|         0|            0|            0|  0.00%|class _IntegerLessThan(Constraint):\n",
      "   266|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   267|         0|            0|            0|  0.00%|    Constrain to an integer interval `(-inf, upper_bound]`.\n",
      "   268|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   269|         0|            0|            0|  0.00%|    is_discrete = True\n",
      "   270|         0|            0|            0|  0.00%|\n",
      "   271|         0|            0|            0|  0.00%|    def __init__(self, upper_bound):\n",
      "   272|         0|            0|            0|  0.00%|        self.upper_bound = upper_bound\n",
      "   273|         0|            0|            0|  0.00%|        super().__init__()\n",
      "   274|         0|            0|            0|  0.00%|\n",
      "   275|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   276|         0|            0|            0|  0.00%|        return (value % 1 == 0) & (value <= self.upper_bound)\n",
      "   277|         0|            0|            0|  0.00%|\n",
      "   278|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "   279|         0|            0|            0|  0.00%|        fmt_string = self.__class__.__name__[1:]\n",
      "   280|         0|            0|            0|  0.00%|        fmt_string += '(upper_bound={})'.format(self.upper_bound)\n",
      "   281|         0|            0|            0|  0.00%|        return fmt_string\n",
      "   282|         0|            0|            0|  0.00%|\n",
      "   283|         0|            0|            0|  0.00%|\n",
      "   284|         0|            0|            0|  0.00%|class _IntegerGreaterThan(Constraint):\n",
      "   285|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   286|         0|            0|            0|  0.00%|    Constrain to an integer interval `[lower_bound, inf)`.\n",
      "   287|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   288|         0|            0|            0|  0.00%|    is_discrete = True\n",
      "   289|         0|            0|            0|  0.00%|\n",
      "   290|         0|            0|            0|  0.00%|    def __init__(self, lower_bound):\n",
      "   291|         0|            0|            0|  0.00%|        self.lower_bound = lower_bound\n",
      "   292|         0|            0|            0|  0.00%|        super().__init__()\n",
      "   293|         0|            0|            0|  0.00%|\n",
      "   294|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   295|         0|            0|            0|  0.00%|        return (value % 1 == 0) & (value >= self.lower_bound)\n",
      "   296|         0|            0|            0|  0.00%|\n",
      "   297|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "   298|         0|            0|            0|  0.00%|        fmt_string = self.__class__.__name__[1:]\n",
      "   299|         0|            0|            0|  0.00%|        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n",
      "   300|         0|            0|            0|  0.00%|        return fmt_string\n",
      "   301|         0|            0|            0|  0.00%|\n",
      "   302|         0|            0|            0|  0.00%|\n",
      "   303|         0|            0|            0|  0.00%|class _Real(Constraint):\n",
      "   304|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   305|         0|            0|            0|  0.00%|    Trivially constrain to the extended real line `[-inf, inf]`.\n",
      "   306|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   307|     31590|    0.0479517|  1.51794e-06|  0.01%|    def check(self, value):\n",
      "   308|     31590|     0.314054|  9.94156e-06|  0.05%|        return value == value  # False for NANs.\n",
      "   309|         0|            0|            0|  0.00%|\n",
      "   310|         0|            0|            0|  0.00%|\n",
      "   311|         0|            0|            0|  0.00%|class _GreaterThan(Constraint):\n",
      "   312|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   313|         0|            0|            0|  0.00%|    Constrain to a real half line `(lower_bound, inf]`.\n",
      "   314|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   315|         0|            0|            0|  0.00%|    def __init__(self, lower_bound):\n",
      "   316|         0|            0|            0|  0.00%|        self.lower_bound = lower_bound\n",
      "   317|         0|            0|            0|  0.00%|        super().__init__()\n",
      "   318|         0|            0|            0|  0.00%|\n",
      "   319|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   320|         0|            0|            0|  0.00%|        return self.lower_bound < value\n",
      "   321|         0|            0|            0|  0.00%|\n",
      "   322|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "   323|         0|            0|            0|  0.00%|        fmt_string = self.__class__.__name__[1:]\n",
      "   324|         0|            0|            0|  0.00%|        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n",
      "   325|         0|            0|            0|  0.00%|        return fmt_string\n",
      "   326|         0|            0|            0|  0.00%|\n",
      "   327|         0|            0|            0|  0.00%|\n",
      "   328|         0|            0|            0|  0.00%|class _GreaterThanEq(Constraint):\n",
      "   329|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   330|         0|            0|            0|  0.00%|    Constrain to a real half line `[lower_bound, inf)`.\n",
      "   331|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   332|         0|            0|            0|  0.00%|    def __init__(self, lower_bound):\n",
      "   333|         0|            0|            0|  0.00%|        self.lower_bound = lower_bound\n",
      "   334|         0|            0|            0|  0.00%|        super().__init__()\n",
      "   335|         0|            0|            0|  0.00%|\n",
      "   336|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   337|         0|            0|            0|  0.00%|        return self.lower_bound <= value\n",
      "   338|         0|            0|            0|  0.00%|\n",
      "   339|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "   340|         0|            0|            0|  0.00%|        fmt_string = self.__class__.__name__[1:]\n",
      "   341|         0|            0|            0|  0.00%|        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n",
      "   342|         0|            0|            0|  0.00%|        return fmt_string\n",
      "   343|         0|            0|            0|  0.00%|\n",
      "   344|         0|            0|            0|  0.00%|\n",
      "   345|         0|            0|            0|  0.00%|class _LessThan(Constraint):\n",
      "   346|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   347|         0|            0|            0|  0.00%|    Constrain to a real half line `[-inf, upper_bound)`.\n",
      "   348|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   349|         0|            0|            0|  0.00%|    def __init__(self, upper_bound):\n",
      "   350|         0|            0|            0|  0.00%|        self.upper_bound = upper_bound\n",
      "   351|         0|            0|            0|  0.00%|        super().__init__()\n",
      "   352|         0|            0|            0|  0.00%|\n",
      "   353|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   354|         0|            0|            0|  0.00%|        return value < self.upper_bound\n",
      "   355|         0|            0|            0|  0.00%|\n",
      "   356|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "   357|         0|            0|            0|  0.00%|        fmt_string = self.__class__.__name__[1:]\n",
      "   358|         0|            0|            0|  0.00%|        fmt_string += '(upper_bound={})'.format(self.upper_bound)\n",
      "   359|         0|            0|            0|  0.00%|        return fmt_string\n",
      "   360|         0|            0|            0|  0.00%|\n",
      "   361|         0|            0|            0|  0.00%|\n",
      "   362|         0|            0|            0|  0.00%|class _Interval(Constraint):\n",
      "   363|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   364|         0|            0|            0|  0.00%|    Constrain to a real interval `[lower_bound, upper_bound]`.\n",
      "   365|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   366|         0|            0|            0|  0.00%|    def __init__(self, lower_bound, upper_bound):\n",
      "   367|         0|            0|            0|  0.00%|        self.lower_bound = lower_bound\n",
      "   368|         0|            0|            0|  0.00%|        self.upper_bound = upper_bound\n",
      "   369|         0|            0|            0|  0.00%|        super().__init__()\n",
      "   370|         0|            0|            0|  0.00%|\n",
      "   371|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   372|         0|            0|            0|  0.00%|        return (self.lower_bound <= value) & (value <= self.upper_bound)\n",
      "   373|         0|            0|            0|  0.00%|\n",
      "   374|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "   375|         0|            0|            0|  0.00%|        fmt_string = self.__class__.__name__[1:]\n",
      "   376|         0|            0|            0|  0.00%|        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n",
      "   377|         0|            0|            0|  0.00%|        return fmt_string\n",
      "   378|         0|            0|            0|  0.00%|\n",
      "   379|         0|            0|            0|  0.00%|\n",
      "   380|         0|            0|            0|  0.00%|class _HalfOpenInterval(Constraint):\n",
      "   381|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   382|         0|            0|            0|  0.00%|    Constrain to a real interval `[lower_bound, upper_bound)`.\n",
      "   383|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   384|         0|            0|            0|  0.00%|    def __init__(self, lower_bound, upper_bound):\n",
      "   385|         0|            0|            0|  0.00%|        self.lower_bound = lower_bound\n",
      "   386|         0|            0|            0|  0.00%|        self.upper_bound = upper_bound\n",
      "   387|         0|            0|            0|  0.00%|        super().__init__()\n",
      "   388|         0|            0|            0|  0.00%|\n",
      "   389|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   390|         0|            0|            0|  0.00%|        return (self.lower_bound <= value) & (value < self.upper_bound)\n",
      "   391|         0|            0|            0|  0.00%|\n",
      "   392|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "   393|         0|            0|            0|  0.00%|        fmt_string = self.__class__.__name__[1:]\n",
      "   394|         0|            0|            0|  0.00%|        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n",
      "   395|         0|            0|            0|  0.00%|        return fmt_string\n",
      "   396|         0|            0|            0|  0.00%|\n",
      "   397|         0|            0|            0|  0.00%|\n",
      "   398|         0|            0|            0|  0.00%|class _Simplex(Constraint):\n",
      "   399|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   400|         0|            0|            0|  0.00%|    Constrain to the unit simplex in the innermost (rightmost) dimension.\n",
      "   401|         0|            0|            0|  0.00%|    Specifically: `x >= 0` and `x.sum(-1) == 1`.\n",
      "   402|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   403|         0|            0|            0|  0.00%|    event_dim = 1\n",
      "   404|         0|            0|            0|  0.00%|\n",
      "   405|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   406|         0|            0|            0|  0.00%|        return torch.all(value >= 0, dim=-1) & ((value.sum(-1) - 1).abs() < 1e-6)\n",
      "   407|         0|            0|            0|  0.00%|\n",
      "   408|         0|            0|            0|  0.00%|\n",
      "   409|         0|            0|            0|  0.00%|class _Multinomial(Constraint):\n",
      "   410|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   411|         0|            0|            0|  0.00%|    Constrain to nonnegative integer values summing to at most an upper bound.\n",
      "   412|         0|            0|            0|  0.00%|\n",
      "   413|         0|            0|            0|  0.00%|    Note due to limitations of the Multinomial distribution, this currently\n",
      "   414|         0|            0|            0|  0.00%|    checks the weaker condition ``value.sum(-1) <= upper_bound``. In the future\n",
      "   415|         0|            0|            0|  0.00%|    this may be strengthened to ``value.sum(-1) == upper_bound``.\n",
      "   416|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   417|         0|            0|            0|  0.00%|    is_discrete = True\n",
      "   418|         0|            0|            0|  0.00%|    event_dim = 1\n",
      "   419|         0|            0|            0|  0.00%|\n",
      "   420|         0|            0|            0|  0.00%|    def __init__(self, upper_bound):\n",
      "   421|         0|            0|            0|  0.00%|        self.upper_bound = upper_bound\n",
      "   422|         0|            0|            0|  0.00%|\n",
      "   423|         0|            0|            0|  0.00%|    def check(self, x):\n",
      "   424|         0|            0|            0|  0.00%|        return (x >= 0).all(dim=-1) & (x.sum(dim=-1) <= self.upper_bound)\n",
      "   425|         0|            0|            0|  0.00%|\n",
      "   426|         0|            0|            0|  0.00%|\n",
      "   427|         0|            0|            0|  0.00%|class _LowerTriangular(Constraint):\n",
      "   428|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   429|         0|            0|            0|  0.00%|    Constrain to lower-triangular square matrices.\n",
      "   430|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   431|         0|            0|            0|  0.00%|    event_dim = 2\n",
      "   432|         0|            0|            0|  0.00%|\n",
      "   433|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   434|         0|            0|            0|  0.00%|        value_tril = value.tril()\n",
      "   435|         0|            0|            0|  0.00%|        return (value_tril == value).view(value.shape[:-2] + (-1,)).min(-1)[0]\n",
      "   436|         0|            0|            0|  0.00%|\n",
      "   437|         0|            0|            0|  0.00%|\n",
      "   438|         0|            0|            0|  0.00%|class _LowerCholesky(Constraint):\n",
      "   439|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   440|         0|            0|            0|  0.00%|    Constrain to lower-triangular square matrices with positive diagonals.\n",
      "   441|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   442|         0|            0|            0|  0.00%|    event_dim = 2\n",
      "   443|         0|            0|            0|  0.00%|\n",
      "   444|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   445|         0|            0|            0|  0.00%|        value_tril = value.tril()\n",
      "   446|         0|            0|            0|  0.00%|        lower_triangular = (value_tril == value).view(value.shape[:-2] + (-1,)).min(-1)[0]\n",
      "   447|         0|            0|            0|  0.00%|\n",
      "   448|         0|            0|            0|  0.00%|        positive_diagonal = (value.diagonal(dim1=-2, dim2=-1) > 0).min(-1)[0]\n",
      "   449|         0|            0|            0|  0.00%|        return lower_triangular & positive_diagonal\n",
      "   450|         0|            0|            0|  0.00%|\n",
      "   451|         0|            0|            0|  0.00%|\n",
      "   452|         0|            0|            0|  0.00%|class _CorrCholesky(Constraint):\n",
      "   453|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   454|         0|            0|            0|  0.00%|    Constrain to lower-triangular square matrices with positive diagonals and each\n",
      "   455|         0|            0|            0|  0.00%|    row vector being of unit length.\n",
      "   456|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   457|         0|            0|            0|  0.00%|    event_dim = 2\n",
      "   458|         0|            0|            0|  0.00%|\n",
      "   459|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   460|         0|            0|            0|  0.00%|        tol = torch.finfo(value.dtype).eps * value.size(-1) * 10  # 10 is an adjustable fudge factor\n",
      "   461|         0|            0|            0|  0.00%|        row_norm = torch.linalg.norm(value.detach(), dim=-1)\n",
      "   462|         0|            0|            0|  0.00%|        unit_row_norm = (row_norm - 1.).abs().le(tol).all(dim=-1)\n",
      "   463|         0|            0|            0|  0.00%|        return _LowerCholesky().check(value) & unit_row_norm\n",
      "   464|         0|            0|            0|  0.00%|\n",
      "   465|         0|            0|            0|  0.00%|\n",
      "   466|         0|            0|            0|  0.00%|class _Square(Constraint):\n",
      "   467|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   468|         0|            0|            0|  0.00%|    Constrain to square matrices.\n",
      "   469|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   470|         0|            0|            0|  0.00%|    event_dim = 2\n",
      "   471|         0|            0|            0|  0.00%|\n",
      "   472|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   473|         0|            0|            0|  0.00%|        return torch.full(\n",
      "   474|         0|            0|            0|  0.00%|            size=value.shape[:-2],\n",
      "   475|         0|            0|            0|  0.00%|            fill_value=(value.shape[-2] == value.shape[-1]),\n",
      "   476|         0|            0|            0|  0.00%|            dtype=torch.bool,\n",
      "   477|         0|            0|            0|  0.00%|            device=value.device\n",
      "   478|         0|            0|            0|  0.00%|        )\n",
      "   479|         0|            0|            0|  0.00%|\n",
      "   480|         0|            0|            0|  0.00%|\n",
      "   481|         0|            0|            0|  0.00%|class _Symmetric(_Square):\n",
      "   482|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   483|         0|            0|            0|  0.00%|    Constrain to Symmetric square matrices.\n",
      "   484|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   485|         0|            0|            0|  0.00%|\n",
      "   486|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   487|         0|            0|            0|  0.00%|        square_check = super().check(value)\n",
      "   488|         0|            0|            0|  0.00%|        if not square_check.all():\n",
      "   489|         0|            0|            0|  0.00%|            return square_check\n",
      "   490|         0|            0|            0|  0.00%|        return torch.isclose(value, value.mT, atol=1e-6).all(-2).all(-1)\n",
      "   491|         0|            0|            0|  0.00%|\n",
      "   492|         0|            0|            0|  0.00%|\n",
      "   493|         0|            0|            0|  0.00%|class _PositiveSemidefinite(_Symmetric):\n",
      "   494|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   495|         0|            0|            0|  0.00%|    Constrain to positive-semidefinite matrices.\n",
      "   496|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   497|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   498|         0|            0|            0|  0.00%|        sym_check = super().check(value)\n",
      "   499|         0|            0|            0|  0.00%|        if not sym_check.all():\n",
      "   500|         0|            0|            0|  0.00%|            return sym_check\n",
      "   501|         0|            0|            0|  0.00%|        return torch.linalg.eigvalsh(value).ge(0).all(-1)\n",
      "   502|         0|            0|            0|  0.00%|\n",
      "   503|         0|            0|            0|  0.00%|\n",
      "   504|         0|            0|            0|  0.00%|class _PositiveDefinite(_Symmetric):\n",
      "   505|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   506|         0|            0|            0|  0.00%|    Constrain to positive-definite matrices.\n",
      "   507|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   508|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   509|         0|            0|            0|  0.00%|        sym_check = super().check(value)\n",
      "   510|         0|            0|            0|  0.00%|        if not sym_check.all():\n",
      "   511|         0|            0|            0|  0.00%|            return sym_check\n",
      "   512|         0|            0|            0|  0.00%|        return torch.linalg.cholesky_ex(value).info.eq(0)\n",
      "   513|         0|            0|            0|  0.00%|\n",
      "   514|         0|            0|            0|  0.00%|\n",
      "   515|         0|            0|            0|  0.00%|class _Cat(Constraint):\n",
      "   516|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   517|         0|            0|            0|  0.00%|    Constraint functor that applies a sequence of constraints\n",
      "   518|         0|            0|            0|  0.00%|    `cseq` at the submatrices at dimension `dim`,\n",
      "   519|         0|            0|            0|  0.00%|    each of size `lengths[dim]`, in a way compatible with :func:`torch.cat`.\n",
      "   520|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   521|         0|            0|            0|  0.00%|    def __init__(self, cseq, dim=0, lengths=None):\n",
      "   522|         0|            0|            0|  0.00%|        assert all(isinstance(c, Constraint) for c in cseq)\n",
      "   523|         0|            0|            0|  0.00%|        self.cseq = list(cseq)\n",
      "   524|         0|            0|            0|  0.00%|        if lengths is None:\n",
      "   525|         0|            0|            0|  0.00%|            lengths = [1] * len(self.cseq)\n",
      "   526|         0|            0|            0|  0.00%|        self.lengths = list(lengths)\n",
      "   527|         0|            0|            0|  0.00%|        assert len(self.lengths) == len(self.cseq)\n",
      "   528|         0|            0|            0|  0.00%|        self.dim = dim\n",
      "   529|         0|            0|            0|  0.00%|        super().__init__()\n",
      "   530|         0|            0|            0|  0.00%|\n",
      "   531|         0|            0|            0|  0.00%|    @property\n",
      "   532|         0|            0|            0|  0.00%|    def is_discrete(self):\n",
      "   533|         0|            0|            0|  0.00%|        return any(c.is_discrete for c in self.cseq)\n",
      "   534|         0|            0|            0|  0.00%|\n",
      "   535|         0|            0|            0|  0.00%|    @property\n",
      "   536|         0|            0|            0|  0.00%|    def event_dim(self):\n",
      "   537|         0|            0|            0|  0.00%|        return max(c.event_dim for c in self.cseq)\n",
      "   538|         0|            0|            0|  0.00%|\n",
      "   539|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   540|         0|            0|            0|  0.00%|        assert -value.dim() <= self.dim < value.dim()\n",
      "   541|         0|            0|            0|  0.00%|        checks = []\n",
      "   542|         0|            0|            0|  0.00%|        start = 0\n",
      "   543|         0|            0|            0|  0.00%|        for constr, length in zip(self.cseq, self.lengths):\n",
      "   544|         0|            0|            0|  0.00%|            v = value.narrow(self.dim, start, length)\n",
      "   545|         0|            0|            0|  0.00%|            checks.append(constr.check(v))\n",
      "   546|         0|            0|            0|  0.00%|            start = start + length  # avoid += for jit compat\n",
      "   547|         0|            0|            0|  0.00%|        return torch.cat(checks, self.dim)\n",
      "   548|         0|            0|            0|  0.00%|\n",
      "   549|         0|            0|            0|  0.00%|\n",
      "   550|         0|            0|            0|  0.00%|class _Stack(Constraint):\n",
      "   551|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   552|         0|            0|            0|  0.00%|    Constraint functor that applies a sequence of constraints\n",
      "   553|         0|            0|            0|  0.00%|    `cseq` at the submatrices at dimension `dim`,\n",
      "   554|         0|            0|            0|  0.00%|    in a way compatible with :func:`torch.stack`.\n",
      "   555|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   556|         0|            0|            0|  0.00%|    def __init__(self, cseq, dim=0):\n",
      "   557|         0|            0|            0|  0.00%|        assert all(isinstance(c, Constraint) for c in cseq)\n",
      "   558|         0|            0|            0|  0.00%|        self.cseq = list(cseq)\n",
      "   559|         0|            0|            0|  0.00%|        self.dim = dim\n",
      "   560|         0|            0|            0|  0.00%|        super().__init__()\n",
      "   561|         0|            0|            0|  0.00%|\n",
      "   562|         0|            0|            0|  0.00%|    @property\n",
      "   563|         0|            0|            0|  0.00%|    def is_discrete(self):\n",
      "   564|         0|            0|            0|  0.00%|        return any(c.is_discrete for c in self.cseq)\n",
      "   565|         0|            0|            0|  0.00%|\n",
      "   566|         0|            0|            0|  0.00%|    @property\n",
      "   567|         0|            0|            0|  0.00%|    def event_dim(self):\n",
      "   568|         0|            0|            0|  0.00%|        dim = max(c.event_dim for c in self.cseq)\n",
      "   569|         0|            0|            0|  0.00%|        if self.dim + dim < 0:\n",
      "   570|         0|            0|            0|  0.00%|            dim += 1\n",
      "   571|         0|            0|            0|  0.00%|        return dim\n",
      "   572|         0|            0|            0|  0.00%|\n",
      "   573|         0|            0|            0|  0.00%|    def check(self, value):\n",
      "   574|         0|            0|            0|  0.00%|        assert -value.dim() <= self.dim < value.dim()\n",
      "   575|         0|            0|            0|  0.00%|        vs = [value.select(self.dim, i) for i in range(value.size(self.dim))]\n",
      "   576|         0|            0|            0|  0.00%|        return torch.stack([constr.check(v)\n",
      "   577|         0|            0|            0|  0.00%|                            for v, constr in zip(vs, self.cseq)], self.dim)\n",
      "   578|         0|            0|            0|  0.00%|\n",
      "   579|         0|            0|            0|  0.00%|\n",
      "   580|         0|            0|            0|  0.00%|# Public interface.\n",
      "   581|         0|            0|            0|  0.00%|dependent = _Dependent()\n",
      "   582|         0|            0|            0|  0.00%|dependent_property = _DependentProperty\n",
      "   583|         0|            0|            0|  0.00%|independent = _IndependentConstraint\n",
      "   584|         0|            0|            0|  0.00%|boolean = _Boolean()\n",
      "   585|         0|            0|            0|  0.00%|one_hot = _OneHot()\n",
      "   586|         0|            0|            0|  0.00%|nonnegative_integer = _IntegerGreaterThan(0)\n",
      "   587|         0|            0|            0|  0.00%|positive_integer = _IntegerGreaterThan(1)\n",
      "   588|         0|            0|            0|  0.00%|integer_interval = _IntegerInterval\n",
      "   589|         0|            0|            0|  0.00%|real = _Real()\n",
      "   590|         0|            0|            0|  0.00%|real_vector = independent(real, 1)\n",
      "   591|         0|            0|            0|  0.00%|positive = _GreaterThan(0.)\n",
      "   592|         0|            0|            0|  0.00%|nonnegative = _GreaterThanEq(0.)\n",
      "   593|         0|            0|            0|  0.00%|greater_than = _GreaterThan\n",
      "   594|         0|            0|            0|  0.00%|greater_than_eq = _GreaterThanEq\n",
      "   595|         0|            0|            0|  0.00%|less_than = _LessThan\n",
      "   596|         0|            0|            0|  0.00%|multinomial = _Multinomial\n",
      "   597|         0|            0|            0|  0.00%|unit_interval = _Interval(0., 1.)\n",
      "   598|         0|            0|            0|  0.00%|interval = _Interval\n",
      "   599|         0|            0|            0|  0.00%|half_open_interval = _HalfOpenInterval\n",
      "   600|         0|            0|            0|  0.00%|simplex = _Simplex()\n",
      "   601|         0|            0|            0|  0.00%|lower_triangular = _LowerTriangular()\n",
      "   602|         0|            0|            0|  0.00%|lower_cholesky = _LowerCholesky()\n",
      "   603|         0|            0|            0|  0.00%|corr_cholesky = _CorrCholesky()\n",
      "   604|         0|            0|            0|  0.00%|square = _Square()\n",
      "   605|         0|            0|            0|  0.00%|symmetric = _Symmetric()\n",
      "   606|         0|            0|            0|  0.00%|positive_semidefinite = _PositiveSemidefinite()\n",
      "   607|         0|            0|            0|  0.00%|positive_definite = _PositiveDefinite()\n",
      "   608|         0|            0|            0|  0.00%|cat = _Cat\n",
      "   609|         0|            0|            0|  0.00%|stack = _Stack\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/optimizer.py\n",
      "File duration: 2.84434s (0.46%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|from collections import defaultdict, abc as container_abcs\n",
      "     2|         0|            0|            0|  0.00%|\n",
      "     3|         0|            0|            0|  0.00%|import torch\n",
      "     4|         0|            0|            0|  0.00%|from copy import deepcopy\n",
      "     5|         0|            0|            0|  0.00%|from itertools import chain\n",
      "     6|         0|            0|            0|  0.00%|import warnings\n",
      "     7|         0|            0|            0|  0.00%|import functools\n",
      "     8|         0|            0|            0|  0.00%|\n",
      "     9|         0|            0|            0|  0.00%|\n",
      "    10|         0|            0|            0|  0.00%|class _RequiredParameter(object):\n",
      "    11|         0|            0|            0|  0.00%|    \"\"\"Singleton class representing a required parameter for an Optimizer.\"\"\"\n",
      "    12|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "    13|         0|            0|            0|  0.00%|        return \"<required parameter>\"\n",
      "    14|         0|            0|            0|  0.00%|\n",
      "    15|         0|            0|            0|  0.00%|required = _RequiredParameter()\n",
      "    16|         0|            0|            0|  0.00%|\n",
      "    17|         0|            0|            0|  0.00%|\n",
      "    18|         0|            0|            0|  0.00%|class Optimizer(object):\n",
      "    19|         0|            0|            0|  0.00%|    r\"\"\"Base class for all optimizers.\n",
      "    20|         0|            0|            0|  0.00%|\n",
      "    21|         0|            0|            0|  0.00%|    .. warning::\n",
      "    22|         0|            0|            0|  0.00%|        Parameters need to be specified as collections that have a deterministic\n",
      "    23|         0|            0|            0|  0.00%|        ordering that is consistent between runs. Examples of objects that don't\n",
      "    24|         0|            0|            0|  0.00%|        satisfy those properties are sets and iterators over values of dictionaries.\n",
      "    25|         0|            0|            0|  0.00%|\n",
      "    26|         0|            0|            0|  0.00%|    Args:\n",
      "    27|         0|            0|            0|  0.00%|        params (iterable): an iterable of :class:`torch.Tensor` s or\n",
      "    28|         0|            0|            0|  0.00%|            :class:`dict` s. Specifies what Tensors should be optimized.\n",
      "    29|         0|            0|            0|  0.00%|        defaults: (dict): a dict containing default values of optimization\n",
      "    30|         0|            0|            0|  0.00%|            options (used when a parameter group doesn't specify them).\n",
      "    31|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    32|         0|            0|            0|  0.00%|\n",
      "    33|         0|            0|            0|  0.00%|    def __init__(self, params, defaults):\n",
      "    34|         0|            0|            0|  0.00%|        torch._C._log_api_usage_once(\"python.optimizer\")\n",
      "    35|         0|            0|            0|  0.00%|        self.defaults = defaults\n",
      "    36|         0|            0|            0|  0.00%|\n",
      "    37|         0|            0|            0|  0.00%|        self._hook_for_profile()\n",
      "    38|         0|            0|            0|  0.00%|\n",
      "    39|         0|            0|            0|  0.00%|        if isinstance(params, torch.Tensor):\n",
      "    40|         0|            0|            0|  0.00%|            raise TypeError(\"params argument given to the optimizer should be \"\n",
      "    41|         0|            0|            0|  0.00%|                            \"an iterable of Tensors or dicts, but got \" +\n",
      "    42|         0|            0|            0|  0.00%|                            torch.typename(params))\n",
      "    43|         0|            0|            0|  0.00%|\n",
      "    44|         0|            0|            0|  0.00%|        self.state = defaultdict(dict)\n",
      "    45|         0|            0|            0|  0.00%|        self.param_groups = []\n",
      "    46|         0|            0|            0|  0.00%|\n",
      "    47|         0|            0|            0|  0.00%|        param_groups = list(params)\n",
      "    48|         0|            0|            0|  0.00%|        if len(param_groups) == 0:\n",
      "    49|         0|            0|            0|  0.00%|            raise ValueError(\"optimizer got an empty parameter list\")\n",
      "    50|         0|            0|            0|  0.00%|        if not isinstance(param_groups[0], dict):\n",
      "    51|         0|            0|            0|  0.00%|            param_groups = [{'params': param_groups}]\n",
      "    52|         0|            0|            0|  0.00%|\n",
      "    53|         0|            0|            0|  0.00%|        for param_group in param_groups:\n",
      "    54|         0|            0|            0|  0.00%|            self.add_param_group(param_group)\n",
      "    55|         0|            0|            0|  0.00%|\n",
      "    56|         0|            0|            0|  0.00%|        # Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\n",
      "    57|         0|            0|            0|  0.00%|        # which I don't think exists\n",
      "    58|         0|            0|            0|  0.00%|        # https://github.com/pytorch/pytorch/issues/72948\n",
      "    59|         0|            0|            0|  0.00%|        self._warned_capturable_if_run_uncaptured = True\n",
      "    60|         0|            0|            0|  0.00%|\n",
      "    61|         0|            0|            0|  0.00%|    def __getstate__(self):\n",
      "    62|         0|            0|            0|  0.00%|        return {\n",
      "    63|         0|            0|            0|  0.00%|            'defaults': self.defaults,\n",
      "    64|         0|            0|            0|  0.00%|            'state': self.state,\n",
      "    65|         0|            0|            0|  0.00%|            'param_groups': self.param_groups,\n",
      "    66|         0|            0|            0|  0.00%|        }\n",
      "    67|         0|            0|            0|  0.00%|\n",
      "    68|         0|            0|            0|  0.00%|    def __setstate__(self, state):\n",
      "    69|         0|            0|            0|  0.00%|        self.__dict__.update(state)\n",
      "    70|         0|            0|            0|  0.00%|        self._hook_for_profile()  # To support multiprocessing pickle/unpickle.\n",
      "    71|         0|            0|            0|  0.00%|\n",
      "    72|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "    73|         0|            0|            0|  0.00%|        format_string = self.__class__.__name__ + ' ('\n",
      "    74|         0|            0|            0|  0.00%|        for i, group in enumerate(self.param_groups):\n",
      "    75|         0|            0|            0|  0.00%|            format_string += '\\n'\n",
      "    76|         0|            0|            0|  0.00%|            format_string += 'Parameter Group {0}\\n'.format(i)\n",
      "    77|         0|            0|            0|  0.00%|            for key in sorted(group.keys()):\n",
      "    78|         0|            0|            0|  0.00%|                if key != 'params':\n",
      "    79|         0|            0|            0|  0.00%|                    format_string += '    {0}: {1}\\n'.format(key, group[key])\n",
      "    80|         0|            0|            0|  0.00%|        format_string += ')'\n",
      "    81|         0|            0|            0|  0.00%|        return format_string\n",
      "    82|         0|            0|            0|  0.00%|\n",
      "    83|         0|            0|            0|  0.00%|    # Currently needed by Adam and AdamW\n",
      "    84|      6240|    0.0128512|  2.05949e-06|  0.00%|    def _cuda_graph_capture_health_check(self):\n",
      "    85|      6240|    0.0400376|  6.41629e-06|  0.01%|        if torch.has_cuda and torch.cuda.is_available():\n",
      "(call)|      6240|    0.0394235|  6.31786e-06|  0.01%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:77 is_available\n",
      "    86|         0|            0|            0|  0.00%|            capturing = torch.cuda.is_current_stream_capturing()\n",
      "    87|         0|            0|            0|  0.00%|\n",
      "    88|         0|            0|            0|  0.00%|            if capturing and not self.defaults['capturable']:\n",
      "    89|         0|            0|            0|  0.00%|                raise RuntimeError(\"Attempting CUDA graph capture of step() for an instance of \" +\n",
      "    90|         0|            0|            0|  0.00%|                                   self.__class__.__name__ +\n",
      "    91|         0|            0|            0|  0.00%|                                   \" but this instance was constructed with capturable=False.\")\n",
      "    92|         0|            0|            0|  0.00%|\n",
      "    93|         0|            0|            0|  0.00%|            if (\n",
      "    94|         0|            0|            0|  0.00%|                (not getattr(self, \"_warned_capturable_if_run_uncaptured\", False))\n",
      "    95|         0|            0|            0|  0.00%|                and self.defaults[\"capturable\"]\n",
      "    96|         0|            0|            0|  0.00%|                and (not capturing)\n",
      "    97|         0|            0|            0|  0.00%|            ):\n",
      "    98|         0|            0|            0|  0.00%|                print(\"Warning: This instance was constructed with capturable=True, but step() \" +\n",
      "    99|         0|            0|            0|  0.00%|                      \"is running without CUDA graph capture. If you never intend to graph-capture this \" +\n",
      "   100|         0|            0|            0|  0.00%|                      \"instance, capturable=True can impair performance, and you should set capturable=False.\")\n",
      "   101|         0|            0|            0|  0.00%|                self._warned_capturable_if_run_uncaptured = True\n",
      "   102|         0|            0|            0|  0.00%|\n",
      "   103|         0|            0|            0|  0.00%|    def _hook_for_profile(self):\n",
      "   104|         0|            0|            0|  0.00%|        self._zero_grad_profile_name = \"Optimizer.zero_grad#{}.zero_grad\".format(self.__class__.__name__)\n",
      "   105|         0|            0|            0|  0.00%|\n",
      "   106|         0|            0|            0|  0.00%|        def profile_hook_step(func):\n",
      "   107|         0|            0|            0|  0.00%|\n",
      "   108|      6240|    0.0197945|  3.17219e-06|  0.00%|            @functools.wraps(func)\n",
      "   109|         0|            0|            0|  0.00%|            def wrapper(*args, **kwargs):\n",
      "   110|      6240|    0.0208061|  3.33431e-06|  0.00%|                obj, *_ = args\n",
      "   111|      6240|    0.0257661|  4.12919e-06|  0.00%|                profile_name = \"Optimizer.step#{}.step\".format(obj.__class__.__name__)\n",
      "   112|      6240|    0.0748463|  1.19946e-05|  0.01%|                with torch.autograd.profiler.record_function(profile_name):\n",
      "(call)|      6240|     0.105392|  1.68897e-05|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/profiler.py:436 __init__\n",
      "(call)|      6240|     0.161591|   2.5896e-05|  0.03%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/profiler.py:445 __enter__\n",
      "   113|      6240|    0.0774248|  1.24078e-05|  0.01%|                    return func(*args, **kwargs)\n",
      "(call)|      6240|      15.3241|   0.00245579|  2.47%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:24 decorate_context\n",
      "(call)|      6240|     0.139785|  2.24015e-05|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/profiler.py:449 __exit__\n",
      "   114|         0|            0|            0|  0.00%|            return wrapper\n",
      "   115|         0|            0|            0|  0.00%|\n",
      "   116|         0|            0|            0|  0.00%|        hooked = getattr(self.__class__.step, \"hooked\", None)\n",
      "   117|         0|            0|            0|  0.00%|        if not hooked:\n",
      "   118|         0|            0|            0|  0.00%|            self.__class__.step = profile_hook_step(self.__class__.step)\n",
      "   119|         0|            0|            0|  0.00%|            self.__class__.step.hooked = True\n",
      "   120|         0|            0|            0|  0.00%|\n",
      "   121|         0|            0|            0|  0.00%|    def state_dict(self):\n",
      "   122|         0|            0|            0|  0.00%|        r\"\"\"Returns the state of the optimizer as a :class:`dict`.\n",
      "   123|         0|            0|            0|  0.00%|\n",
      "   124|         0|            0|            0|  0.00%|        It contains two entries:\n",
      "   125|         0|            0|            0|  0.00%|\n",
      "   126|         0|            0|            0|  0.00%|        * state - a dict holding current optimization state. Its content\n",
      "   127|         0|            0|            0|  0.00%|            differs between optimizer classes.\n",
      "   128|         0|            0|            0|  0.00%|        * param_groups - a list containing all parameter groups where each\n",
      "   129|         0|            0|            0|  0.00%|            parameter group is a dict\n",
      "   130|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   131|         0|            0|            0|  0.00%|        # Save order indices instead of Tensors\n",
      "   132|         0|            0|            0|  0.00%|        param_mappings = {}\n",
      "   133|         0|            0|            0|  0.00%|        start_index = 0\n",
      "   134|         0|            0|            0|  0.00%|\n",
      "   135|         0|            0|            0|  0.00%|        def pack_group(group):\n",
      "   136|         0|            0|            0|  0.00%|            nonlocal start_index\n",
      "   137|         0|            0|            0|  0.00%|            packed = {k: v for k, v in group.items() if k != 'params'}\n",
      "   138|         0|            0|            0|  0.00%|            param_mappings.update({id(p): i for i, p in enumerate(group['params'], start_index)\n",
      "   139|         0|            0|            0|  0.00%|                                   if id(p) not in param_mappings})\n",
      "   140|         0|            0|            0|  0.00%|            packed['params'] = [param_mappings[id(p)] for p in group['params']]\n",
      "   141|         0|            0|            0|  0.00%|            start_index += len(packed['params'])\n",
      "   142|         0|            0|            0|  0.00%|            return packed\n",
      "   143|         0|            0|            0|  0.00%|        param_groups = [pack_group(g) for g in self.param_groups]\n",
      "   144|         0|            0|            0|  0.00%|        # Remap state to use order indices as keys\n",
      "   145|         0|            0|            0|  0.00%|        packed_state = {(param_mappings[id(k)] if isinstance(k, torch.Tensor) else k): v\n",
      "   146|         0|            0|            0|  0.00%|                        for k, v in self.state.items()}\n",
      "   147|         0|            0|            0|  0.00%|        return {\n",
      "   148|         0|            0|            0|  0.00%|            'state': packed_state,\n",
      "   149|         0|            0|            0|  0.00%|            'param_groups': param_groups,\n",
      "   150|         0|            0|            0|  0.00%|        }\n",
      "   151|         0|            0|            0|  0.00%|\n",
      "   152|         0|            0|            0|  0.00%|    def load_state_dict(self, state_dict):\n",
      "   153|         0|            0|            0|  0.00%|        r\"\"\"Loads the optimizer state.\n",
      "   154|         0|            0|            0|  0.00%|\n",
      "   155|         0|            0|            0|  0.00%|        Args:\n",
      "   156|         0|            0|            0|  0.00%|            state_dict (dict): optimizer state. Should be an object returned\n",
      "   157|         0|            0|            0|  0.00%|                from a call to :meth:`state_dict`.\n",
      "   158|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   159|         0|            0|            0|  0.00%|        # deepcopy, to be consistent with module API\n",
      "   160|         0|            0|            0|  0.00%|        state_dict = deepcopy(state_dict)\n",
      "   161|         0|            0|            0|  0.00%|        # Validate the state_dict\n",
      "   162|         0|            0|            0|  0.00%|        groups = self.param_groups\n",
      "   163|         0|            0|            0|  0.00%|        saved_groups = state_dict['param_groups']\n",
      "   164|         0|            0|            0|  0.00%|\n",
      "   165|         0|            0|            0|  0.00%|        if len(groups) != len(saved_groups):\n",
      "   166|         0|            0|            0|  0.00%|            raise ValueError(\"loaded state dict has a different number of \"\n",
      "   167|         0|            0|            0|  0.00%|                             \"parameter groups\")\n",
      "   168|         0|            0|            0|  0.00%|        param_lens = (len(g['params']) for g in groups)\n",
      "   169|         0|            0|            0|  0.00%|        saved_lens = (len(g['params']) for g in saved_groups)\n",
      "   170|         0|            0|            0|  0.00%|        if any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):\n",
      "   171|         0|            0|            0|  0.00%|            raise ValueError(\"loaded state dict contains a parameter group \"\n",
      "   172|         0|            0|            0|  0.00%|                             \"that doesn't match the size of optimizer's group\")\n",
      "   173|         0|            0|            0|  0.00%|\n",
      "   174|         0|            0|            0|  0.00%|        # Update the state\n",
      "   175|         0|            0|            0|  0.00%|        id_map = {old_id: p for old_id, p in\n",
      "   176|         0|            0|            0|  0.00%|                  zip(chain.from_iterable((g['params'] for g in saved_groups)),\n",
      "   177|         0|            0|            0|  0.00%|                      chain.from_iterable((g['params'] for g in groups)))}\n",
      "   178|         0|            0|            0|  0.00%|\n",
      "   179|         0|            0|            0|  0.00%|        def cast(param, value, key=None):\n",
      "   180|         0|            0|            0|  0.00%|            r\"\"\"Make a deep copy of value, casting all tensors to device of param.\"\"\"\n",
      "   181|         0|            0|            0|  0.00%|            if isinstance(value, torch.Tensor):\n",
      "   182|         0|            0|            0|  0.00%|                # Floating-point types are a bit special here. They are the only ones\n",
      "   183|         0|            0|            0|  0.00%|                # that are assumed to always match the type of params.\n",
      "   184|         0|            0|            0|  0.00%|                # Make sure state['step'] is not casted https://github.com/pytorch/pytorch/issues/74424\n",
      "   185|         0|            0|            0|  0.00%|                if (key != \"step\"):\n",
      "   186|         0|            0|            0|  0.00%|                    if param.is_floating_point():\n",
      "   187|         0|            0|            0|  0.00%|                        value = value.to(param.dtype)\n",
      "   188|         0|            0|            0|  0.00%|                    value = value.to(param.device)\n",
      "   189|         0|            0|            0|  0.00%|                return value\n",
      "   190|         0|            0|            0|  0.00%|            elif isinstance(value, dict):\n",
      "   191|         0|            0|            0|  0.00%|                return {k: cast(param, v, key=k) for k, v in value.items()}\n",
      "   192|         0|            0|            0|  0.00%|            elif isinstance(value, container_abcs.Iterable):\n",
      "   193|         0|            0|            0|  0.00%|                return type(value)(cast(param, v) for v in value)\n",
      "   194|         0|            0|            0|  0.00%|            else:\n",
      "   195|         0|            0|            0|  0.00%|                return value\n",
      "   196|         0|            0|            0|  0.00%|\n",
      "   197|         0|            0|            0|  0.00%|        # Copy state assigned to params (and cast tensors to appropriate types).\n",
      "   198|         0|            0|            0|  0.00%|        # State that is not assigned to params is copied as is (needed for\n",
      "   199|         0|            0|            0|  0.00%|        # backward compatibility).\n",
      "   200|         0|            0|            0|  0.00%|        state = defaultdict(dict)\n",
      "   201|         0|            0|            0|  0.00%|        for k, v in state_dict['state'].items():\n",
      "   202|         0|            0|            0|  0.00%|            if k in id_map:\n",
      "   203|         0|            0|            0|  0.00%|                param = id_map[k]\n",
      "   204|         0|            0|            0|  0.00%|                state[param] = cast(param, v)\n",
      "   205|         0|            0|            0|  0.00%|            else:\n",
      "   206|         0|            0|            0|  0.00%|                state[k] = v\n",
      "   207|         0|            0|            0|  0.00%|\n",
      "   208|         0|            0|            0|  0.00%|        # Update parameter groups, setting their 'params' value\n",
      "   209|         0|            0|            0|  0.00%|        def update_group(group, new_group):\n",
      "   210|         0|            0|            0|  0.00%|            new_group['params'] = group['params']\n",
      "   211|         0|            0|            0|  0.00%|            return new_group\n",
      "   212|         0|            0|            0|  0.00%|        param_groups = [\n",
      "   213|         0|            0|            0|  0.00%|            update_group(g, ng) for g, ng in zip(groups, saved_groups)]\n",
      "   214|         0|            0|            0|  0.00%|        self.__setstate__({'state': state, 'param_groups': param_groups})\n",
      "   215|         0|            0|            0|  0.00%|\n",
      "   216|      6240|    0.0282631|  4.52934e-06|  0.00%|    def zero_grad(self, set_to_none: bool = False):\n",
      "   217|         0|            0|            0|  0.00%|        r\"\"\"Sets the gradients of all optimized :class:`torch.Tensor` s to zero.\n",
      "   218|         0|            0|            0|  0.00%|\n",
      "   219|         0|            0|            0|  0.00%|        Args:\n",
      "   220|         0|            0|            0|  0.00%|            set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "   221|         0|            0|            0|  0.00%|                This will in general have lower memory footprint, and can modestly improve performance.\n",
      "   222|         0|            0|            0|  0.00%|                However, it changes certain behaviors. For example:\n",
      "   223|         0|            0|            0|  0.00%|                1. When the user tries to access a gradient and perform manual ops on it,\n",
      "   224|         0|            0|            0|  0.00%|                a None attribute or a Tensor full of 0s will behave differently.\n",
      "   225|         0|            0|            0|  0.00%|                2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s\n",
      "   226|         0|            0|            0|  0.00%|                are guaranteed to be None for params that did not receive a gradient.\n",
      "   227|         0|            0|            0|  0.00%|                3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n",
      "   228|         0|            0|            0|  0.00%|                (in one case it does the step with a gradient of 0 and in the other it skips\n",
      "   229|         0|            0|            0|  0.00%|                the step altogether).\n",
      "   230|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   231|      6240|    0.0231895|  3.71627e-06|  0.00%|        foreach = self.defaults.get('foreach', False)\n",
      "   232|         0|            0|            0|  0.00%|\n",
      "   233|      6240|    0.0198333|  3.17842e-06|  0.00%|        if not hasattr(self, \"_zero_grad_profile_name\"):\n",
      "   234|         0|            0|            0|  0.00%|            self._hook_for_profile()\n",
      "   235|      6240|    0.0163326|  2.61741e-06|  0.00%|        if foreach:\n",
      "   236|         0|            0|            0|  0.00%|            per_device_and_dtype_grads = defaultdict(lambda: defaultdict(list))\n",
      "   237|      6240|    0.0880702|  1.41138e-05|  0.01%|        with torch.autograd.profiler.record_function(self._zero_grad_profile_name):\n",
      "(call)|      6240|     0.102558|  1.64355e-05|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/profiler.py:436 __init__\n",
      "(call)|      6240|     0.164093|   2.6297e-05|  0.03%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/profiler.py:445 __enter__\n",
      "   238|     12480|    0.0326812|  2.61869e-06|  0.01%|            for group in self.param_groups:\n",
      "   239|     81120|     0.186897|  2.30396e-06|  0.03%|                for p in group['params']:\n",
      "   240|     74880|     0.420343|  5.61355e-06|  0.07%|                    if p.grad is not None:\n",
      "(call)|     74880|     0.334171|  4.46275e-06|  0.05%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:1071 grad\n",
      "   241|     74880|     0.162041|  2.16401e-06|  0.03%|                        if set_to_none:\n",
      "   242|         0|            0|            0|  0.00%|                            p.grad = None\n",
      "   243|         0|            0|            0|  0.00%|                        else:\n",
      "   244|     74880|     0.424505|  5.66914e-06|  0.07%|                            if p.grad.grad_fn is not None:\n",
      "(call)|     74880|     0.316719|  4.22969e-06|  0.05%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:1071 grad\n",
      "   245|         0|            0|            0|  0.00%|                                p.grad.detach_()\n",
      "   246|         0|            0|            0|  0.00%|                            else:\n",
      "   247|     74880|     0.441489|  5.89595e-06|  0.07%|                                p.grad.requires_grad_(False)\n",
      "(call)|     74880|     0.315294|  4.21065e-06|  0.05%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:1071 grad\n",
      "   248|     74880|     0.163619|  2.18508e-06|  0.03%|                            if (not foreach or p.grad.is_sparse):\n",
      "   249|     74880|       0.5165|   6.8977e-06|  0.08%|                                p.grad.zero_()\n",
      "(call)|     74880|     0.316522|  4.22705e-06|  0.05%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_tensor.py:1071 grad\n",
      "   250|         0|            0|            0|  0.00%|                            else:\n",
      "   251|         0|            0|            0|  0.00%|                                per_device_and_dtype_grads[p.grad.device][p.grad.dtype].append(p.grad)\n",
      "   252|      6240|    0.0490513|  7.86078e-06|  0.01%|            if foreach:\n",
      "(call)|      6240|     0.103932|  1.66558e-05|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/profiler.py:449 __exit__\n",
      "   253|         0|            0|            0|  0.00%|                for _, per_dtype_grads in per_device_and_dtype_grads.items():\n",
      "   254|         0|            0|            0|  0.00%|                    for grads in per_dtype_grads.values():\n",
      "   255|         0|            0|            0|  0.00%|                        torch._foreach_zero_(grads)\n",
      "   256|         0|            0|            0|  0.00%|\n",
      "   257|         0|            0|            0|  0.00%|    def step(self, closure):\n",
      "   258|         0|            0|            0|  0.00%|        r\"\"\"Performs a single optimization step (parameter update).\n",
      "   259|         0|            0|            0|  0.00%|\n",
      "   260|         0|            0|            0|  0.00%|        Args:\n",
      "   261|         0|            0|            0|  0.00%|            closure (callable): A closure that reevaluates the model and\n",
      "   262|         0|            0|            0|  0.00%|                returns the loss. Optional for most optimizers.\n",
      "   263|         0|            0|            0|  0.00%|\n",
      "   264|         0|            0|            0|  0.00%|        .. note::\n",
      "   265|         0|            0|            0|  0.00%|            Unless otherwise specified, this function should not modify the\n",
      "   266|         0|            0|            0|  0.00%|            ``.grad`` field of the parameters.\n",
      "   267|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   268|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "   269|         0|            0|            0|  0.00%|\n",
      "   270|         0|            0|            0|  0.00%|    def add_param_group(self, param_group):\n",
      "   271|         0|            0|            0|  0.00%|        r\"\"\"Add a param group to the :class:`Optimizer` s `param_groups`.\n",
      "   272|         0|            0|            0|  0.00%|\n",
      "   273|         0|            0|            0|  0.00%|        This can be useful when fine tuning a pre-trained network as frozen layers can be made\n",
      "   274|         0|            0|            0|  0.00%|        trainable and added to the :class:`Optimizer` as training progresses.\n",
      "   275|         0|            0|            0|  0.00%|\n",
      "   276|         0|            0|            0|  0.00%|        Args:\n",
      "   277|         0|            0|            0|  0.00%|            param_group (dict): Specifies what Tensors should be optimized along with group\n",
      "   278|         0|            0|            0|  0.00%|                specific optimization options.\n",
      "   279|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   280|         0|            0|            0|  0.00%|        assert isinstance(param_group, dict), \"param group must be a dict\"\n",
      "   281|         0|            0|            0|  0.00%|\n",
      "   282|         0|            0|            0|  0.00%|        params = param_group['params']\n",
      "   283|         0|            0|            0|  0.00%|        if isinstance(params, torch.Tensor):\n",
      "   284|         0|            0|            0|  0.00%|            param_group['params'] = [params]\n",
      "   285|         0|            0|            0|  0.00%|        elif isinstance(params, set):\n",
      "   286|         0|            0|            0|  0.00%|            raise TypeError('optimizer parameters need to be organized in ordered collections, but '\n",
      "   287|         0|            0|            0|  0.00%|                            'the ordering of tensors in sets will change between runs. Please use a list instead.')\n",
      "   288|         0|            0|            0|  0.00%|        else:\n",
      "   289|         0|            0|            0|  0.00%|            param_group['params'] = list(params)\n",
      "   290|         0|            0|            0|  0.00%|\n",
      "   291|         0|            0|            0|  0.00%|        for param in param_group['params']:\n",
      "   292|         0|            0|            0|  0.00%|            if not isinstance(param, torch.Tensor):\n",
      "   293|         0|            0|            0|  0.00%|                raise TypeError(\"optimizer can only optimize Tensors, \"\n",
      "   294|         0|            0|            0|  0.00%|                                \"but one of the params is \" + torch.typename(param))\n",
      "   295|         0|            0|            0|  0.00%|            if not param.is_leaf:\n",
      "   296|         0|            0|            0|  0.00%|                raise ValueError(\"can't optimize a non-leaf Tensor\")\n",
      "   297|         0|            0|            0|  0.00%|\n",
      "   298|         0|            0|            0|  0.00%|        for name, default in self.defaults.items():\n",
      "   299|         0|            0|            0|  0.00%|            if default is required and name not in param_group:\n",
      "   300|         0|            0|            0|  0.00%|                raise ValueError(\"parameter group didn't specify a value of required optimization parameter \" +\n",
      "   301|         0|            0|            0|  0.00%|                                 name)\n",
      "   302|         0|            0|            0|  0.00%|            else:\n",
      "   303|         0|            0|            0|  0.00%|                param_group.setdefault(name, default)\n",
      "   304|         0|            0|            0|  0.00%|\n",
      "   305|         0|            0|            0|  0.00%|        params = param_group['params']\n",
      "   306|         0|            0|            0|  0.00%|        if len(params) != len(set(params)):\n",
      "   307|         0|            0|            0|  0.00%|            warnings.warn(\"optimizer contains a parameter group with duplicate parameters; \"\n",
      "   308|         0|            0|            0|  0.00%|                          \"in future, this will cause an error; \"\n",
      "   309|         0|            0|            0|  0.00%|                          \"see github.com/pytorch/pytorch/issues/40967 for more information\", stacklevel=3)\n",
      "   310|         0|            0|            0|  0.00%|\n",
      "   311|         0|            0|            0|  0.00%|        param_set = set()\n",
      "   312|         0|            0|            0|  0.00%|        for group in self.param_groups:\n",
      "   313|         0|            0|            0|  0.00%|            param_set.update(set(group['params']))\n",
      "   314|         0|            0|            0|  0.00%|\n",
      "   315|         0|            0|            0|  0.00%|        if not param_set.isdisjoint(set(param_group['params'])):\n",
      "   316|         0|            0|            0|  0.00%|            raise ValueError(\"some parameters appear in more than one parameter group\")\n",
      "   317|         0|            0|            0|  0.00%|\n",
      "   318|         0|            0|            0|  0.00%|        self.param_groups.append(param_group)\n",
      "File: /apps/open_spiel/open_spiel/python/examples/ppo_utils.py\n",
      "File duration: 2.82368s (0.45%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|from dataclasses import dataclass, field\n",
      "     2|         0|            0|            0|  0.00%|from open_spiel.python import rl_environment\n",
      "     3|         0|            0|            0|  0.00%|from open_spiel.python.examples.env_and_policy import EnvAndPolicy\n",
      "     4|         0|            0|            0|  0.00%|from open_spiel.python.examples.ubc_utils import *\n",
      "     5|         0|            0|            0|  0.00%|import numpy as np\n",
      "     6|         0|            0|            0|  0.00%|from open_spiel.python.pytorch.ppo import PPO\n",
      "     7|         0|            0|            0|  0.00%|import time\n",
      "     8|         0|            0|            0|  0.00%|import logging\n",
      "     9|         0|            0|            0|  0.00%|from open_spiel.python.algorithms.exploitability import nash_conv\n",
      "    10|         0|            0|            0|  0.00%|from open_spiel.python.vector_env import SyncVectorEnv\n",
      "    11|         0|            0|            0|  0.00%|from open_spiel.python.env_decorator import NormalizingEnvDecorator, AuctionStatTrackingDecorator, StateSavingEnvDecorator, PotentialShapingEnvDecorator, TrapEnvDecorator\n",
      "    12|         0|            0|            0|  0.00%|from typing import Callable, List\n",
      "    13|         0|            0|            0|  0.00%|from dataclasses import asdict\n",
      "    14|         0|            0|            0|  0.00%|from open_spiel.python.env_decorator import AuctionStatTrackingDecorator\n",
      "    15|         0|            0|            0|  0.00%|\n",
      "    16|         0|            0|            0|  0.00%|logger = logging.getLogger(__name__)\n",
      "    17|         0|            0|            0|  0.00%|\n",
      "    18|         0|            0|            0|  0.00%|PPO_DEFAULTS = {\n",
      "    19|         0|            0|            0|  0.00%|  'num_envs': 8,\n",
      "    20|         0|            0|            0|  0.00%|  'steps_per_batch': 128,\n",
      "    21|         0|            0|            0|  0.00%|  'num_minibatches': 4,\n",
      "    22|         0|            0|            0|  0.00%|  'update_epochs': 4,\n",
      "    23|         0|            0|            0|  0.00%|  'learning_rate': 2.5e-4,\n",
      "    24|         0|            0|            0|  0.00%|  'num_annealing_updates': None,\n",
      "    25|         0|            0|            0|  0.00%|  'gae': True,\n",
      "    26|         0|            0|            0|  0.00%|  'gamma': 0.99,\n",
      "    27|         0|            0|            0|  0.00%|  'gae_lambda': 0.95,\n",
      "    28|         0|            0|            0|  0.00%|  'normalize_advantages': True,\n",
      "    29|         0|            0|            0|  0.00%|  'clip_coef': 0.2,\n",
      "    30|         0|            0|            0|  0.00%|  'clip_vloss': True,\n",
      "    31|         0|            0|            0|  0.00%|  'agent_fn': 'PPOAgent',\n",
      "    32|         0|            0|            0|  0.00%|  'agent_fn_kwargs': {},\n",
      "    33|         0|            0|            0|  0.00%|  'entropy_coef': 0.01,\n",
      "    34|         0|            0|            0|  0.00%|  'value_coef': 0.5,\n",
      "    35|         0|            0|            0|  0.00%|  'max_grad_norm': 0.5,\n",
      "    36|         0|            0|            0|  0.00%|  'target_kl': None,\n",
      "    37|         0|            0|            0|  0.00%|  'device': default_device(),\n",
      "    38|         0|            0|            0|  0.00%|  'use_wandb': False,\n",
      "    39|         0|            0|            0|  0.00%|}\n",
      "    40|         0|            0|            0|  0.00%|\n",
      "    41|         0|            0|            0|  0.00%|def read_ppo_config(config_name):\n",
      "    42|         0|            0|            0|  0.00%|    config_file = config_path_from_config_name(config_name)\n",
      "    43|         0|            0|            0|  0.00%|    logging.info(f\"Reading config from {config_file}\")\n",
      "    44|         0|            0|            0|  0.00%|    with open(config_file, 'rb') as fh:\n",
      "    45|         0|            0|            0|  0.00%|        config = yaml.load(fh, Loader=yaml.FullLoader)\n",
      "    46|         0|            0|            0|  0.00%|\n",
      "    47|         0|            0|            0|  0.00%|    config = {**PPO_DEFAULTS, **config}  # priority from right to left\n",
      "    48|         0|            0|            0|  0.00%|\n",
      "    49|         0|            0|            0|  0.00%|    print(config)\n",
      "    50|         0|            0|            0|  0.00%|\n",
      "    51|         0|            0|            0|  0.00%|    return config\n",
      "    52|         0|            0|            0|  0.00%|\n",
      "    53|         0|            0|            0|  0.00%|# def make_schedule_function(func_name, max_t, initial_frac = 0.5):\n",
      "    54|         0|            0|            0|  0.00%|#   if func_name == 'linear':\n",
      "    55|         0|            0|            0|  0.00%|#     return lambda t: initial_frac * (1- (t/max_t))\n",
      "    56|         0|            0|            0|  0.00%|#   elif func_name == 'constant':\n",
      "    57|         0|            0|            0|  0.00%|#     return lambda t: initial_frac\n",
      "    58|         0|            0|            0|  0.00%|#   else:\n",
      "    59|         0|            0|            0|  0.00%|#     raise NotImplementedError()\n",
      "    60|         0|            0|            0|  0.00%|\n",
      "    61|         0|            0|            0|  0.00%|# def make_reward_function(func_name):\n",
      "    62|         0|            0|            0|  0.00%|#   if func_name.startswith('neg_'):\n",
      "    63|         0|            0|            0|  0.00%|#     reward_function = make_reward_function(func_name[4:])\n",
      "    64|         0|            0|            0|  0.00%|#     return lambda state: -reward_function(state)\n",
      "    65|         0|            0|            0|  0.00%|#   else:\n",
      "    66|         0|            0|            0|  0.00%|#     def generic_reward(state):\n",
      "    67|         0|            0|            0|  0.00%|#       attr = getattr(state, func_name)\n",
      "    68|         0|            0|            0|  0.00%|#       if isinstance(attr, Callable):\n",
      "    69|         0|            0|            0|  0.00%|#         return attr()\n",
      "    70|         0|            0|            0|  0.00%|#       else:\n",
      "    71|         0|            0|            0|  0.00%|#         return attr\n",
      "    72|         0|            0|            0|  0.00%|#     return generic_reward\n",
      "    73|         0|            0|            0|  0.00%|\n",
      "    74|         0|            0|            0|  0.00%|\n",
      "    75|         0|            0|            0|  0.00%|def make_potential_function(func_name):\n",
      "    76|         0|            0|            0|  0.00%|  if '_potential_normalized' not in func_name:\n",
      "    77|         0|            0|            0|  0.00%|    func_name += '_potential_normalized'\n",
      "    78|         0|            0|            0|  0.00%|  if func_name.startswith('neg_'):\n",
      "    79|         0|            0|            0|  0.00%|    reward_function = make_potential_function(func_name[4:])\n",
      "    80|         0|            0|            0|  0.00%|    return lambda state: -reward_function(state)\n",
      "    81|         0|            0|            0|  0.00%|  else:\n",
      "    82|         0|            0|            0|  0.00%|    def generic_reward(state):\n",
      "    83|         0|            0|            0|  0.00%|      return getattr(state, func_name)\n",
      "    84|         0|            0|            0|  0.00%|      # attr = getattr(state, func_name)\n",
      "    85|         0|            0|            0|  0.00%|      # if isinstance(attr, Callable):\n",
      "    86|         0|            0|            0|  0.00%|      #   return attr()\n",
      "    87|         0|            0|            0|  0.00%|      # else:\n",
      "    88|         0|            0|            0|  0.00%|      #   return attr\n",
      "    89|         0|            0|            0|  0.00%|    return generic_reward\n",
      "    90|         0|            0|            0|  0.00%|\n",
      "    91|         0|            0|            0|  0.00%|@dataclass\n",
      "    92|         0|            0|            0|  0.00%|class EnvParams:\n",
      "    93|         0|            0|            0|  0.00%|\n",
      "    94|         0|            0|            0|  0.00%|  num_envs: int = 8\n",
      "    95|         0|            0|            0|  0.00%|  normalize_rewards: bool = True\n",
      "    96|         0|            0|            0|  0.00%|  seed: int = 1234\n",
      "    97|         0|            0|            0|  0.00%|  track_stats: bool = False\n",
      "    98|         0|            0|            0|  0.00%|  sync: bool = True\n",
      "    99|         0|            0|            0|  0.00%|  history_prefix: List = field(default_factory=lambda: [])\n",
      "   100|         0|            0|            0|  0.00%|  num_states_to_save: int = 0\n",
      "   101|         0|            0|            0|  0.00%|\n",
      "   102|         0|            0|            0|  0.00%|  # Stuff related to reward shaping\n",
      "   103|         0|            0|            0|  0.00%|  # reward_function: str = None\n",
      "   104|         0|            0|            0|  0.00%|  # schedule_function: str = None\n",
      "   105|         0|            0|            0|  0.00%|  # initial_frac: float = 0.5\n",
      "   106|         0|            0|            0|  0.00%|  # total_timesteps: int = None\n",
      "   107|         0|            0|            0|  0.00%|\n",
      "   108|         0|            0|            0|  0.00%|  potential_function: str =  None\n",
      "   109|         0|            0|            0|  0.00%|\n",
      "   110|         0|            0|            0|  0.00%|  use_wandb: bool = False\n",
      "   111|         0|            0|            0|  0.00%|  clear_on_report: bool = False\n",
      "   112|         0|            0|            0|  0.00%|  observer_params: dict = None\n",
      "   113|         0|            0|            0|  0.00%|\n",
      "   114|         0|            0|            0|  0.00%|  scale_coef: float = 1.\n",
      "   115|         0|            0|            0|  0.00%|\n",
      "   116|         0|            0|            0|  0.00%|  trap_value: float = None\n",
      "   117|         0|            0|            0|  0.00%|  trap_delay: int = 0\n",
      "   118|         0|            0|            0|  0.00%|\n",
      "   119|         0|            0|            0|  0.00%|  def make_env(self, game):\n",
      "   120|         0|            0|            0|  0.00%|    if not self.sync and self.num_envs > 1:\n",
      "   121|         0|            0|            0|  0.00%|      raise ValueError(\"Sync must be True if num_envs > 1\")\n",
      "   122|         0|            0|            0|  0.00%|\n",
      "   123|         0|            0|            0|  0.00%|    def gen_env(seed, env_id=0):\n",
      "   124|         0|            0|            0|  0.00%|        # Only track env_id == 0 so we don't have multi-valued metrics\n",
      "   125|         0|            0|            0|  0.00%|\n",
      "   126|         0|            0|            0|  0.00%|        env = rl_environment.Environment(game, chance_event_sampler=UBCChanceEventSampler(seed=seed), use_observer_api=True, history_prefix=self.history_prefix, observer_params=self.observer_params)\n",
      "   127|         0|            0|            0|  0.00%|        if self.num_states_to_save:\n",
      "   128|         0|            0|            0|  0.00%|          logger.info(\"State saving decorator\")\n",
      "   129|         0|            0|            0|  0.00%|          env = StateSavingEnvDecorator(env, self.num_states_to_save)\n",
      "   130|         0|            0|            0|  0.00%|        if self.track_stats:\n",
      "   131|         0|            0|            0|  0.00%|          logger.info(\"Tracking stats decorator\")\n",
      "   132|         0|            0|            0|  0.00%|          env = AuctionStatTrackingDecorator(env, self.clear_on_report)\n",
      "   133|         0|            0|            0|  0.00%|        if self.normalize_rewards:\n",
      "   134|         0|            0|            0|  0.00%|          logger.info(\"Reward normalizing decorator\")\n",
      "   135|         0|            0|            0|  0.00%|          env = NormalizingEnvDecorator(env, reward_normalizer=torch.tensor(np.maximum(game.upper_bounds, game.lower_bounds)))\n",
      "   136|         0|            0|            0|  0.00%|        if self.trap_value is not None: # Needs to happen after normalizing\n",
      "   137|         0|            0|            0|  0.00%|          logger.info(\"Traps with penalty {} and delay {}\".format(self.trap_value, self.trap_delay))\n",
      "   138|         0|            0|            0|  0.00%|          env = TrapEnvDecorator(env, self.trap_value, self.trap_delay)\n",
      "   139|         0|            0|            0|  0.00%|        if self.potential_function:\n",
      "   140|         0|            0|            0|  0.00%|          potential_function = make_potential_function(self.potential_function)\n",
      "   141|         0|            0|            0|  0.00%|          logger.info(\"Shaping potential with function: {} and scale strength {}\".format(self.potential_function, self.scale_coef))\n",
      "   142|         0|            0|            0|  0.00%|          env = PotentialShapingEnvDecorator(env, potential_function, game.num_players(), scale_coef=self.scale_coef)\n",
      "   143|         0|            0|            0|  0.00%|        return env\n",
      "   144|         0|            0|            0|  0.00%|\n",
      "   145|         0|            0|            0|  0.00%|    if self.sync:\n",
      "   146|         0|            0|            0|  0.00%|      env = SyncVectorEnv(\n",
      "   147|         0|            0|            0|  0.00%|          [gen_env(self.seed + i, env_id=i) for i in range(self.num_envs)]\n",
      "   148|         0|            0|            0|  0.00%|      )\n",
      "   149|         0|            0|            0|  0.00%|    else:\n",
      "   150|         0|            0|            0|  0.00%|      env = gen_env(self.seed)\n",
      "   151|         0|            0|            0|  0.00%|    return env\n",
      "   152|         0|            0|            0|  0.00%|\n",
      "   153|         0|            0|            0|  0.00%|  @staticmethod\n",
      "   154|         0|            0|            0|  0.00%|  def from_config(config):\n",
      "   155|         0|            0|            0|  0.00%|    ## Config is a dict of params you want to override\n",
      "   156|         0|            0|            0|  0.00%|    defaults = asdict(EnvParams())\n",
      "   157|         0|            0|            0|  0.00%|    env_config = {k:v for k,v in config.items() if k in defaults}\n",
      "   158|         0|            0|            0|  0.00%|    return EnvParams(**{**defaults, **env_config})\n",
      "   159|         0|            0|            0|  0.00%|\n",
      "   160|         0|            0|            0|  0.00%|class EpisodeTimer:\n",
      "   161|         0|            0|            0|  0.00%|\n",
      "   162|         0|            0|            0|  0.00%|  def __init__(self, frequency, early_frequency=None, fixed_episodes=None, eval_zero=False):\n",
      "   163|         0|            0|            0|  0.00%|    if fixed_episodes is None:\n",
      "   164|         0|            0|            0|  0.00%|      fixed_episodes = []\n",
      "   165|         0|            0|            0|  0.00%|    self.fixed_episodes = fixed_episodes\n",
      "   166|         0|            0|            0|  0.00%|\n",
      "   167|         0|            0|            0|  0.00%|    self.frequency = frequency\n",
      "   168|         0|            0|            0|  0.00%|    self.early_frequency = early_frequency\n",
      "   169|         0|            0|            0|  0.00%|    self.cur_frequency = self.frequency if self.early_frequency is None else self.early_frequency\n",
      "   170|         0|            0|            0|  0.00%|    self.eval_zero = eval_zero\n",
      "   171|         0|            0|            0|  0.00%|    self.last_known_ep = -1\n",
      "   172|         0|            0|            0|  0.00%|\n",
      "   173|         0|            0|            0|  0.00%|  def should_trigger(self, ep):\n",
      "   174|         0|            0|            0|  0.00%|    if ep > self.frequency: # Move on from early frequency if needed\n",
      "   175|         0|            0|            0|  0.00%|      self.cur_frequency = self.frequency\n",
      "   176|         0|            0|            0|  0.00%|\n",
      "   177|         0|            0|            0|  0.00%|    while ep > self.last_known_ep:\n",
      "   178|         0|            0|            0|  0.00%|      self.last_known_ep += 1\n",
      "   179|         0|            0|            0|  0.00%|      if self._should_trigger(self.last_known_ep):\n",
      "   180|         0|            0|            0|  0.00%|        # Note in the reports you might see unround numbers because of how we do it (e.g., logs for episode 10_007)\n",
      "   181|         0|            0|            0|  0.00%|        self.last_known_ep = ep\n",
      "   182|         0|            0|            0|  0.00%|        return True\n",
      "   183|         0|            0|            0|  0.00%|\n",
      "   184|         0|            0|            0|  0.00%|    return False\n",
      "   185|         0|            0|            0|  0.00%|\n",
      "   186|         0|            0|            0|  0.00%|  def _should_trigger(self, ep):\n",
      "   187|         0|            0|            0|  0.00%|    return (ep > 0 and ep % self.cur_frequency == 0) or \\\n",
      "   188|         0|            0|            0|  0.00%|      ep in self.fixed_episodes or\\\n",
      "   189|         0|            0|            0|  0.00%|      ep == 0 and self.eval_zero\n",
      "   190|         0|            0|            0|  0.00%|\n",
      "   191|         0|            0|            0|  0.00%|\n",
      "   192|         0|            0|            0|  0.00%|def make_ppo_kwargs_from_config(config):\n",
      "   193|         0|            0|            0|  0.00%|  ppo_kwargs = {**PPO_DEFAULTS, **config}  # priority from right to left\n",
      "   194|         0|            0|            0|  0.00%|  ppo_kwargs = {k:v for k,v in ppo_kwargs.items() if k in PPO_DEFAULTS.keys()}\n",
      "   195|         0|            0|            0|  0.00%|\n",
      "   196|         0|            0|            0|  0.00%|  return ppo_kwargs\n",
      "   197|         0|            0|            0|  0.00%|\n",
      "   198|         0|            0|            0|  0.00%|def make_ppo_agent(player_id, config, game):\n",
      "   199|         0|            0|            0|  0.00%|    num_players, num_actions, num_products = game.num_players(), game.num_distinct_actions(), game.auction_params.num_products\n",
      "   200|         0|            0|            0|  0.00%|\n",
      "   201|         0|            0|            0|  0.00%|    # Double actions when using traps in the network\n",
      "   202|         0|            0|            0|  0.00%|    if config.get('trap_value', None):\n",
      "   203|         0|            0|            0|  0.00%|      num_actions *= 2\n",
      "   204|         0|            0|            0|  0.00%|\n",
      "   205|         0|            0|            0|  0.00%|    state_size = rl_environment.Environment(game).observation_spec()[\"info_state\"]\n",
      "   206|         0|            0|            0|  0.00%|\n",
      "   207|         0|            0|            0|  0.00%|    # TODO: Do you want to parameterize NN size/architecture?\n",
      "   208|         0|            0|            0|  0.00%|    ppo_kwargs = make_ppo_kwargs_from_config(config)\n",
      "   209|         0|            0|            0|  0.00%|\n",
      "   210|         0|            0|            0|  0.00%|    return PPO(\n",
      "   211|         0|            0|            0|  0.00%|        input_shape=state_size,\n",
      "   212|         0|            0|            0|  0.00%|        num_actions=num_actions,\n",
      "   213|         0|            0|            0|  0.00%|        num_players=num_players,\n",
      "   214|         0|            0|            0|  0.00%|        player_id=player_id,\n",
      "   215|         0|            0|            0|  0.00%|        **ppo_kwargs\n",
      "   216|         0|            0|            0|  0.00%|    )\n",
      "   217|         0|            0|            0|  0.00%|\n",
      "   218|         0|            0|            0|  0.00%|def make_env_and_policy(game, config, env_params=None):\n",
      "   219|         0|            0|            0|  0.00%|  if env_params is None:\n",
      "   220|         0|            0|            0|  0.00%|    env_params = EnvParams(num_envs=config['num_envs'], seed=config['seed'])\n",
      "   221|         0|            0|            0|  0.00%|  env = env_params.make_env(game)\n",
      "   222|         0|            0|            0|  0.00%|  agents = [make_ppo_agent(player_id, config, game) for player_id in range(game.num_players())]\n",
      "   223|         0|            0|            0|  0.00%|  return EnvAndPolicy(env=env, agents=agents, game=game)\n",
      "   224|         0|            0|            0|  0.00%|\n",
      "   225|         0|            0|            0|  0.00%|class PPOTrainingLoop:\n",
      "   226|         0|            0|            0|  0.00%|\n",
      "   227|         1|  4.05312e-06|  4.05312e-06|  0.00%|  def __init__(self, game, env, agents, total_timesteps, players_to_train=None, report_timer=None, eval_timer=None, policy_diff_threshold=1e-3, max_policy_diff_count=9999, use_wandb=False):\n",
      "   228|         1|  3.33786e-06|  3.33786e-06|  0.00%|    self.game = game\n",
      "   229|         1|  2.86102e-06|  2.86102e-06|  0.00%|    self.env = env\n",
      "   230|         1|   2.6226e-06|   2.6226e-06|  0.00%|    self.agents = agents\n",
      "   231|         1|   2.6226e-06|   2.6226e-06|  0.00%|    self.total_timesteps = total_timesteps\n",
      "   232|         1|  2.69413e-05|  2.69413e-05|  0.00%|    self.players_to_train = players_to_train if players_to_train is not None else list(range(game.num_players()))\n",
      "   233|         1|  8.34465e-06|  8.34465e-06|  0.00%|    self.fixed_agents = set(range(game.num_players())) - set(self.players_to_train)\n",
      "   234|         1|  3.33786e-06|  3.33786e-06|  0.00%|    self.report_timer = report_timer\n",
      "   235|         1|  2.86102e-06|  2.86102e-06|  0.00%|    self.report_hooks = []\n",
      "   236|         1|   2.6226e-06|   2.6226e-06|  0.00%|    self.eval_timer = eval_timer\n",
      "   237|         1|  2.86102e-06|  2.86102e-06|  0.00%|    self.eval_hooks = []\n",
      "   238|         1|  2.38419e-06|  2.38419e-06|  0.00%|    self.policy_diff_threshold = policy_diff_threshold\n",
      "   239|         1|  2.86102e-06|  2.86102e-06|  0.00%|    self.max_policy_diff_count = max_policy_diff_count\n",
      "   240|         1|  3.09944e-06|  3.09944e-06|  0.00%|    self.policy_diff_count = 0\n",
      "   241|         1|  2.86102e-06|  2.86102e-06|  0.00%|    self.use_wandb = use_wandb\n",
      "   242|         0|            0|            0|  0.00%|\n",
      "   243|         1|  3.09944e-06|  3.09944e-06|  0.00%|  def add_report_hook(self, hook):\n",
      "   244|         1|  2.86102e-06|  2.86102e-06|  0.00%|    self.report_hooks.append(hook)\n",
      "   245|         0|            0|            0|  0.00%|\n",
      "   246|         1|  1.90735e-06|  1.90735e-06|  0.00%|  def add_eval_hook(self, hook):\n",
      "   247|         1|  3.09944e-06|  3.09944e-06|  0.00%|    self.eval_hooks.append(hook)\n",
      "   248|         0|            0|            0|  0.00%|\n",
      "   249|         1|  6.67572e-06|  6.67572e-06|  0.00%|  def training_loop(self):\n",
      "   250|         1|  7.86781e-06|  7.86781e-06|  0.00%|    num_steps = self.agents[self.players_to_train[0]].steps_per_batch # Assuming it's all the same across agents...\n",
      "   251|         1|   1.3113e-05|   1.3113e-05|  0.00%|    batch_size = int(len(self.env) * num_steps)\n",
      "(call)|         1|  1.26362e-05|  1.26362e-05|  0.00%|# /apps/open_spiel/open_spiel/python/vector_env.py:10 __len__\n",
      "   252|         1|  5.24521e-06|  5.24521e-06|  0.00%|    num_updates = self.total_timesteps // batch_size\n",
      "   253|         0|            0|            0|  0.00%|\n",
      "   254|         1|  1.40667e-05|  1.40667e-05|  0.00%|    logging.info(f\"Training for {num_updates} updates\")\n",
      "(call)|         1|  3.93391e-05|  3.93391e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/logging/__init__.py:2062 info\n",
      "   255|         1|   1.4782e-05|   1.4782e-05|  0.00%|    logging.info(f\"Fixed agents are {self.fixed_agents}. Learning agents are {self.players_to_train}\")\n",
      "(call)|         1|  2.21729e-05|  2.21729e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/logging/__init__.py:2062 info\n",
      "   256|         1|  2.02656e-05|  2.02656e-05|  0.00%|    time_step = self.env.reset()\n",
      "(call)|         1|    0.0128651|    0.0128651|  0.00%|# /apps/open_spiel/open_spiel/python/vector_env.py:37 reset\n",
      "   257|         0|            0|            0|  0.00%|\n",
      "   258|       196|  0.000688076|  3.51059e-06|  0.00%|    for update in range(1, num_updates + 1):\n",
      "   259|       195|  0.000818014|  4.19494e-06|  0.00%|      if self.report_timer is not None and self.report_timer.should_trigger(update * batch_size):\n",
      "   260|         0|            0|            0|  0.00%|        for hook in self.report_hooks:\n",
      "   261|         0|            0|            0|  0.00%|          hook(update, update * batch_size)\n",
      "   262|       195|  0.000751019|  3.85138e-06|  0.00%|      if self.eval_timer is not None and self.eval_timer.should_trigger(update * batch_size):\n",
      "   263|         0|            0|            0|  0.00%|        for hook in self.eval_hooks:\n",
      "   264|         0|            0|            0|  0.00%|          hook(update, update * batch_size)\n",
      "   265|         0|            0|            0|  0.00%|\n",
      "   266|     12675|    0.0388327|  3.06372e-06|  0.01%|      for _ in range(num_steps):\n",
      "   267|     37440|     0.156607|  4.18288e-06|  0.03%|          for player_id, agent in enumerate(self.agents):\n",
      "   268|     24960|     0.574774|  2.30278e-05|  0.09%|              agent_output = agent.step(time_step, is_evaluation=player_id in self.fixed_agents)\n",
      "(call)|     24960|       55.197|   0.00221142|  8.89%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:240 step\n",
      "   269|     24960|      1.19562|  4.79013e-05|  0.19%|              time_step, reward, done, unreset_time_steps = self.env.step(agent_output, reset_if_done=True)\n",
      "(call)|     24960|      494.143|    0.0197974| 79.56%|# /apps/open_spiel/open_spiel/python/vector_env.py:20 step\n",
      "   270|         0|            0|            0|  0.00%|\n",
      "   271|     37440|     0.140876|  3.76272e-06|  0.02%|          for player_id, agent in enumerate(self.agents):\n",
      "   272|     24960|    0.0830367|  3.32679e-06|  0.01%|            if player_id in self.players_to_train:\n",
      "   273|    174720|     0.587525|  3.36267e-06|  0.09%|              agent.post_step([r[player_id] for r in reward], done)\n",
      "(call)|     24960|      0.25261|  1.01206e-05|  0.04%|# /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:273 <listcomp>\n",
      "(call)|     24960|      3.40042|  0.000136235|  0.55%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:279 post_step\n",
      "   274|         0|            0|            0|  0.00%|\n",
      "   275|       195|   0.00058198|  2.98451e-06|  0.00%|      policy_changed = False\n",
      "   276|       585|   0.00243545|  4.16316e-06|  0.00%|      for player_id, agent in enumerate(self.agents):\n",
      "   277|       390|   0.00160193|  4.10752e-06|  0.00%|        if player_id in self.players_to_train:\n",
      "   278|       390|    0.0309515|  7.93628e-05|  0.00%|          agent.learn(time_step)\n",
      "(call)|       390|      65.4894|     0.167922| 10.54%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:287 learn\n",
      "   279|       390|   0.00499606|  1.28104e-05|  0.00%|        if agent.get_max_policy_diff() >= self.policy_diff_threshold:\n",
      "(call)|       390|   0.00194907|  4.99762e-06|  0.00%|# /apps/open_spiel/open_spiel/python/pytorch/ppo.py:439 get_max_policy_diff\n",
      "   280|       198|  0.000762463|  3.85082e-06|  0.00%|          policy_changed = True\n",
      "   281|         0|            0|            0|  0.00%|\n",
      "   282|         0|            0|            0|  0.00%|      # Commit wandb\n",
      "   283|       195|  0.000897884|  4.60454e-06|  0.00%|      if self.use_wandb:\n",
      "   284|         0|            0|            0|  0.00%|        # TODO: Make this way less specific to our game/abstract it more\n",
      "   285|         0|            0|            0|  0.00%|        import wandb\n",
      "   286|         0|            0|            0|  0.00%|        stats_dict = AuctionStatTrackingDecorator.merge_stats(self.env)\n",
      "   287|         0|            0|            0|  0.00%|        log_stats_dict = dict()\n",
      "   288|         0|            0|            0|  0.00%|        prefix = 'metrics'\n",
      "   289|         0|            0|            0|  0.00%|        if 'revenues' in stats_dict:\n",
      "   290|         0|            0|            0|  0.00%|            log_stats_dict[f'{prefix}/mean_revenue'] = np.mean(stats_dict['revenues'])\n",
      "   291|         0|            0|            0|  0.00%|        if 'auction_lengths' in stats_dict:\n",
      "   292|         0|            0|            0|  0.00%|            log_stats_dict[f'{prefix}/mean_auction_length'] = np.mean(stats_dict['auction_lengths'])\n",
      "   293|         0|            0|            0|  0.00%|        if 'welfares' in stats_dict:\n",
      "   294|         0|            0|            0|  0.00%|            log_stats_dict[f'{prefix}/mean_welfare'] = np.mean(stats_dict['welfares'])\n",
      "   295|         0|            0|            0|  0.00%|\n",
      "   296|         0|            0|            0|  0.00%|        for player_id in range(len(self.agents)):\n",
      "   297|         0|            0|            0|  0.00%|          if 'raw_rewards' in stats_dict:\n",
      "   298|         0|            0|            0|  0.00%|            log_stats_dict[f'{prefix}/player_{player_id}_mean_reward'] = np.mean(stats_dict['raw_rewards'][player_id])\n",
      "   299|         0|            0|            0|  0.00%|          if 'payments' in stats_dict:\n",
      "   300|         0|            0|            0|  0.00%|            log_stats_dict[f'{prefix}/player_{player_id}_payment'] = np.mean(stats_dict['payments'][player_id])\n",
      "   301|         0|            0|            0|  0.00%|            # f'player_{player_id}_allocation': self.allocations[player_id][-1], # TODO? Probably needs to be per product\n",
      "   302|         0|            0|            0|  0.00%|\n",
      "   303|         0|            0|            0|  0.00%|          if 'traps' in stats_dict:\n",
      "   304|         0|            0|            0|  0.00%|            log_stats_dict[f'{prefix}/player_{player_id}_traps'] = np.mean(stats_dict['traps'][player_id])\n",
      "   305|         0|            0|            0|  0.00%|\n",
      "   306|         0|            0|            0|  0.00%|        # TODO:\n",
      "   307|         0|            0|            0|  0.00%|        # for player_id in range(self.n_players):\n",
      "   308|         0|            0|            0|  0.00%|        #     metrics[f'metrics/player_{player_id}_unshaped_reward'] = time_step.rewards[player_id]\n",
      "   309|         0|            0|            0|  0.00%|        #     metrics[f'metrics/player_{player_id}_shaped_reward'] = new_rewards[player_id] - time_step.rewards[player_id]\n",
      "   310|         0|            0|            0|  0.00%|\n",
      "   311|         0|            0|            0|  0.00%|        # This should be the ONLY commit=True. Step sizes will now be in terms of updates\n",
      "   312|         0|            0|            0|  0.00%|        wandb.log(log_stats_dict, commit=True)\n",
      "   313|         0|            0|            0|  0.00%|\n",
      "   314|       195|  0.000659466|  3.38188e-06|  0.00%|      if not policy_changed:\n",
      "   315|         2|  8.10623e-06|  4.05312e-06|  0.00%|        self.policy_diff_count += 1\n",
      "   316|         2|  1.23978e-05|  6.19888e-06|  0.00%|        if self.policy_diff_count >= self.max_policy_diff_count:\n",
      "   317|         0|            0|            0|  0.00%|          logging.info(\"Policy has not changed for {} updates. Stopping training\".format(self.max_policy_diff_count))\n",
      "   318|         0|            0|            0|  0.00%|          break\n",
      "   319|         0|            0|            0|  0.00%|      else:\n",
      "   320|       193|   0.00070858|   3.6714e-06|  0.00%|        self.policy_diff_count = 0\n",
      "   321|         0|            0|            0|  0.00%|\n",
      "   322|         0|            0|            0|  0.00%|\n",
      "   323|         1|  1.74046e-05|  1.74046e-05|  0.00%|    logging.info(f\"Terminating PPO training after {update} updates and {update * batch_size} steps\")\n",
      "(call)|         1|  3.91006e-05|  3.91006e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/logging/__init__.py:2062 info\n",
      "   324|         1|  7.39098e-06|  7.39098e-06|  0.00%|    update += 1 # Prevent stupid DB mismatch errors\n",
      "   325|         2|  9.77516e-06|  4.88758e-06|  0.00%|    for hook in self.report_hooks:\n",
      "   326|         1|  1.14441e-05|  1.14441e-05|  0.00%|      hook(update, update * batch_size)\n",
      "(call)|         1|  4.64916e-05|  4.64916e-05|  0.00%|# /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:363 report_hook\n",
      "   327|         2|  8.34465e-06|  4.17233e-06|  0.00%|    for hook in self.eval_hooks:\n",
      "   328|         1|  2.24113e-05|  2.24113e-05|  0.00%|      hook(update, update * batch_size)\n",
      "(call)|         1|   0.00182247|   0.00182247|  0.00%|# /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:356 eval_hook\n",
      "   329|         0|            0|            0|  0.00%|\n",
      "   330|         0|            0|            0|  0.00%|\n",
      "   331|         1|  3.33786e-06|  3.33786e-06|  0.00%|def ppo_checkpoint(env_and_model, step, alg_start_time, compute_nash_conv=False, update=None):\n",
      "   332|         1|  2.86102e-06|  2.86102e-06|  0.00%|  if compute_nash_conv:\n",
      "   333|         0|            0|            0|  0.00%|    raise ValueError(\"Nash conv not supported for PPO\")\n",
      "   334|         0|            0|            0|  0.00%|\n",
      "   335|         1|  3.09944e-06|  3.09944e-06|  0.00%|  msg = f\"EVALUATION AFTER {step} steps\"\n",
      "   336|         1|  2.86102e-06|  2.86102e-06|  0.00%|  if update is not None:\n",
      "   337|         1|  3.33786e-06|  3.33786e-06|  0.00%|    msg += f\" (update {update})\"\n",
      "   338|         1|  1.12057e-05|  1.12057e-05|  0.00%|  logger.info(msg)\n",
      "(call)|         1|  2.19345e-05|  2.19345e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/logging/__init__.py:1424 info\n",
      "   339|         0|            0|            0|  0.00%|\n",
      "   340|         1|  1.16825e-05|  1.16825e-05|  0.00%|  policy = env_and_model.make_policy()\n",
      "(call)|         1|  0.000119448|  0.000119448|  0.00%|# /apps/open_spiel/open_spiel/python/examples/env_and_policy.py:13 make_policy\n",
      "   341|         1|   2.6226e-06|   2.6226e-06|  0.00%|  checkpoint = {\n",
      "   342|         1|  4.05312e-06|  4.05312e-06|  0.00%|      'walltime': time.time() - alg_start_time,\n",
      "   343|         1|  1.04904e-05|  1.04904e-05|  0.00%|      'policy': policy.save(),\n",
      "(call)|         1|   0.00160074|   0.00160074|  0.00%|# /apps/open_spiel/open_spiel/python/rl_agent_policy.py:82 save\n",
      "   344|         1|  3.09944e-06|  3.09944e-06|  0.00%|      'episode': step,\n",
      "   345|         0|            0|            0|  0.00%|  }\n",
      "   346|         1|   2.6226e-06|   2.6226e-06|  0.00%|  return checkpoint\n",
      "   347|         0|            0|            0|  0.00%|\n",
      "   348|         1|  1.62125e-05|  1.62125e-05|  0.00%|def run_ppo(env_and_policy, total_steps, result_saver=None, seed=1234, compute_nash_conv=False, dispatcher=None, report_timer=None, eval_timer=None, use_wandb=False):\n",
      "   349|         0|            0|            0|  0.00%|  # This may have already been done, but do it again. Required to do it outside to ensure that networks get initilized the same way, which usually happens elsewhere\n",
      "   350|         1|  1.78814e-05|  1.78814e-05|  0.00%|  fix_seeds(seed)\n",
      "(call)|         1|   0.00564885|   0.00564885|  0.00%|# /apps/open_spiel/open_spiel/python/examples/ubc_utils.py:43 fix_seeds\n",
      "   351|         1|  5.96046e-06|  5.96046e-06|  0.00%|  game, env, agents = env_and_policy.game, env_and_policy.env, env_and_policy.agents\n",
      "   352|         0|            0|            0|  0.00%|\n",
      "   353|         1|  5.00679e-06|  5.00679e-06|  0.00%|  alg_start_time = time.time()\n",
      "   354|         1|  1.45435e-05|  1.45435e-05|  0.00%|  trainer = PPOTrainingLoop(game, env, agents, total_steps, report_timer=report_timer, eval_timer=eval_timer, use_wandb=use_wandb)\n",
      "(call)|         1|  7.36713e-05|  7.36713e-05|  0.00%|# /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:227 __init__\n",
      "   355|         0|            0|            0|  0.00%|\n",
      "   356|         2|  1.00136e-05|  5.00679e-06|  0.00%|  def eval_hook(update, total_steps):\n",
      "   357|         1|  1.23978e-05|  1.23978e-05|  0.00%|    checkpoint = ppo_checkpoint(env_and_policy, total_steps, alg_start_time, compute_nash_conv=compute_nash_conv, update=update)\n",
      "(call)|         1|    0.0018034|    0.0018034|  0.00%|# /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:331 ppo_checkpoint\n",
      "   358|         1|  3.09944e-06|  3.09944e-06|  0.00%|    if result_saver is not None:\n",
      "   359|         0|            0|            0|  0.00%|      checkpoint_name = result_saver.save(checkpoint)\n",
      "   360|         0|            0|            0|  0.00%|      if dispatcher is not None:\n",
      "   361|         0|            0|            0|  0.00%|        dispatcher.dispatch(checkpoint_name)\n",
      "   362|         0|            0|            0|  0.00%|\n",
      "   363|         2|  7.15256e-06|  3.57628e-06|  0.00%|  def report_hook(update, total_steps):\n",
      "   364|         1|  8.10623e-06|  8.10623e-06|  0.00%|    logging.info(f\"Update {update}\")\n",
      "(call)|         1|  3.48091e-05|  3.48091e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/logging/__init__.py:2062 info\n",
      "   365|         0|            0|            0|  0.00%|\n",
      "   366|         1|  1.04904e-05|  1.04904e-05|  0.00%|  trainer.add_report_hook(report_hook)\n",
      "(call)|         1|  5.96046e-06|  5.96046e-06|  0.00%|# /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:243 add_report_hook\n",
      "   367|         1|  9.29832e-06|  9.29832e-06|  0.00%|  trainer.add_eval_hook(eval_hook)\n",
      "(call)|         1|  5.00679e-06|  5.00679e-06|  0.00%|# /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:246 add_eval_hook\n",
      "   368|         1|  7.89165e-05|  7.89165e-05|  0.00%|  trainer.training_loop()\n",
      "(call)|         1|       621.07|       621.07|100.00%|# /apps/open_spiel/open_spiel/python/examples/ppo_utils.py:249 training_loop\n",
      "   369|         0|            0|            0|  0.00%|\n",
      "   370|         1|  2.40803e-05|  2.40803e-05|  0.00%|  logging.info(f\"Walltime: {pretty_time(time.time() - alg_start_time)}\")\n",
      "(call)|         1|   0.00107098|   0.00107098|  0.00%|# /apps/open_spiel/open_spiel/python/examples/ubc_utils.py:116 pretty_time\n",
      "(call)|         1|  3.31402e-05|  3.31402e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/logging/__init__.py:2062 info\n",
      "   371|         1|  9.05991e-06|  9.05991e-06|  0.00%|  logging.info('All done. Goodbye!')\n",
      "(call)|         1|  2.07424e-05|  2.07424e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/logging/__init__.py:2062 info\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py\n",
      "File duration: 2.45851s (0.40%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|import sys\n",
      "     2|         0|            0|            0|  0.00%|import torch\n",
      "     3|         0|            0|            0|  0.00%|import functools\n",
      "     4|         0|            0|            0|  0.00%|import inspect\n",
      "     5|         0|            0|            0|  0.00%|from typing import Any, Callable, TypeVar, cast\n",
      "     6|         0|            0|            0|  0.00%|\n",
      "     7|         0|            0|            0|  0.00%|__all__ = ['no_grad', 'enable_grad', 'set_grad_enabled',\n",
      "     8|         0|            0|            0|  0.00%|           'inference_mode']\n",
      "     9|         0|            0|            0|  0.00%|\n",
      "    10|         0|            0|            0|  0.00%|\n",
      "    11|         0|            0|            0|  0.00%|# Used for annotating the decorator usage of 'no_grad' and 'enable_grad'.\n",
      "    12|         0|            0|            0|  0.00%|# See https://mypy.readthedocs.io/en/latest/generics.html#declaring-decorators\n",
      "    13|         0|            0|            0|  0.00%|FuncType = Callable[..., Any]\n",
      "    14|         0|            0|            0|  0.00%|F = TypeVar('F', bound=FuncType)\n",
      "    15|         0|            0|            0|  0.00%|\n",
      "    16|         0|            0|            0|  0.00%|\n",
      "    17|         0|            0|            0|  0.00%|class _DecoratorContextManager:\n",
      "    18|         0|            0|            0|  0.00%|    \"\"\"Allow a context manager to be used as a decorator\"\"\"\n",
      "    19|         0|            0|            0|  0.00%|\n",
      "    20|         0|            0|            0|  0.00%|    def __call__(self, func: F) -> F:\n",
      "    21|         0|            0|            0|  0.00%|        if inspect.isgeneratorfunction(func):\n",
      "    22|         0|            0|            0|  0.00%|            return self._wrap_generator(func)\n",
      "    23|         0|            0|            0|  0.00%|\n",
      "    24|      6240|   0.00987434|  1.58243e-06|  0.00%|        @functools.wraps(func)\n",
      "    25|         0|            0|            0|  0.00%|        def decorate_context(*args, **kwargs):\n",
      "    26|      6240|    0.0553639|  8.87242e-06|  0.01%|            with self.clone():\n",
      "(call)|      6240|     0.140399|  2.24999e-05|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:82 clone\n",
      "(call)|      6240|     0.101358|  1.62433e-05|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:131 __enter__\n",
      "    27|      6240|    0.0904686|  1.44982e-05|  0.01%|                return func(*args, **kwargs)\n",
      "(call)|      6240|      14.8134|   0.00237395|  2.39%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/optim/adam.py:105 step\n",
      "(call)|      6240|     0.113259|  1.81505e-05|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:135 __exit__\n",
      "    28|         0|            0|            0|  0.00%|        return cast(F, decorate_context)\n",
      "    29|         0|            0|            0|  0.00%|\n",
      "    30|         0|            0|            0|  0.00%|    def _wrap_generator(self, func):\n",
      "    31|         0|            0|            0|  0.00%|        \"\"\"Wrap each generator invocation with the context manager\"\"\"\n",
      "    32|         0|            0|            0|  0.00%|        @functools.wraps(func)\n",
      "    33|         0|            0|            0|  0.00%|        def generator_context(*args, **kwargs):\n",
      "    34|         0|            0|            0|  0.00%|            gen = func(*args, **kwargs)\n",
      "    35|         0|            0|            0|  0.00%|\n",
      "    36|         0|            0|            0|  0.00%|            # Generators are suspended and unsuspended at `yield`, hence we\n",
      "    37|         0|            0|            0|  0.00%|            # make sure the grad mode is properly set every time the execution\n",
      "    38|         0|            0|            0|  0.00%|            # flow returns into the wrapped generator and restored when it\n",
      "    39|         0|            0|            0|  0.00%|            # returns through our `yield` to our caller (see PR #49017).\n",
      "    40|         0|            0|            0|  0.00%|            try:\n",
      "    41|         0|            0|            0|  0.00%|                # Issuing `None` to a generator fires it up\n",
      "    42|         0|            0|            0|  0.00%|                with self.clone():\n",
      "    43|         0|            0|            0|  0.00%|                    response = gen.send(None)\n",
      "    44|         0|            0|            0|  0.00%|\n",
      "    45|         0|            0|            0|  0.00%|                while True:\n",
      "    46|         0|            0|            0|  0.00%|                    try:\n",
      "    47|         0|            0|            0|  0.00%|                        # Forward the response to our caller and get its next request\n",
      "    48|         0|            0|            0|  0.00%|                        request = yield response\n",
      "    49|         0|            0|            0|  0.00%|\n",
      "    50|         0|            0|            0|  0.00%|                    except GeneratorExit:\n",
      "    51|         0|            0|            0|  0.00%|                        # Inform the still active generator about its imminent closure\n",
      "    52|         0|            0|            0|  0.00%|                        with self.clone():\n",
      "    53|         0|            0|            0|  0.00%|                            gen.close()\n",
      "    54|         0|            0|            0|  0.00%|                        raise\n",
      "    55|         0|            0|            0|  0.00%|\n",
      "    56|         0|            0|            0|  0.00%|                    except BaseException:\n",
      "    57|         0|            0|            0|  0.00%|                        # Propagate the exception thrown at us by the caller\n",
      "    58|         0|            0|            0|  0.00%|                        with self.clone():\n",
      "    59|         0|            0|            0|  0.00%|                            response = gen.throw(*sys.exc_info())\n",
      "    60|         0|            0|            0|  0.00%|\n",
      "    61|         0|            0|            0|  0.00%|                    else:\n",
      "    62|         0|            0|            0|  0.00%|                        # Pass the last request to the generator and get its response\n",
      "    63|         0|            0|            0|  0.00%|                        with self.clone():\n",
      "    64|         0|            0|            0|  0.00%|                            response = gen.send(request)\n",
      "    65|         0|            0|            0|  0.00%|\n",
      "    66|         0|            0|            0|  0.00%|            # We let the exceptions raised above by the generator's `.throw` or\n",
      "    67|         0|            0|            0|  0.00%|            # `.send` methods bubble up to our caller, except for StopIteration\n",
      "    68|         0|            0|            0|  0.00%|            except StopIteration as e:\n",
      "    69|         0|            0|            0|  0.00%|                # The generator informed us that it is done: take whatever its\n",
      "    70|         0|            0|            0|  0.00%|                # returned value (if any) was and indicate that we're done too\n",
      "    71|         0|            0|            0|  0.00%|                # by returning it (see docs for python's return-statement).\n",
      "    72|         0|            0|            0|  0.00%|                return e.value\n",
      "    73|         0|            0|            0|  0.00%|\n",
      "    74|         0|            0|            0|  0.00%|        return generator_context\n",
      "    75|         0|            0|            0|  0.00%|\n",
      "    76|         0|            0|            0|  0.00%|    def __enter__(self) -> None:\n",
      "    77|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "    78|         0|            0|            0|  0.00%|\n",
      "    79|         0|            0|            0|  0.00%|    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n",
      "    80|         0|            0|            0|  0.00%|        raise NotImplementedError\n",
      "    81|         0|            0|            0|  0.00%|\n",
      "    82|      6240|   0.00774503|  1.24119e-06|  0.00%|    def clone(self):\n",
      "    83|         0|            0|            0|  0.00%|        # override this method if your children class takes __init__ parameters\n",
      "    84|      6240|    0.0377355|  6.04735e-06|  0.01%|        return self.__class__()\n",
      "(call)|      6240|     0.094919|  1.52114e-05|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:126 __init__\n",
      "    85|         0|            0|            0|  0.00%|\n",
      "    86|         0|            0|            0|  0.00%|\n",
      "    87|         0|            0|            0|  0.00%|class no_grad(_DecoratorContextManager):\n",
      "    88|         0|            0|            0|  0.00%|    r\"\"\"Context-manager that disabled gradient calculation.\n",
      "    89|         0|            0|            0|  0.00%|\n",
      "    90|         0|            0|            0|  0.00%|    Disabling gradient calculation is useful for inference, when you are sure\n",
      "    91|         0|            0|            0|  0.00%|    that you will not call :meth:`Tensor.backward()`. It will reduce memory\n",
      "    92|         0|            0|            0|  0.00%|    consumption for computations that would otherwise have `requires_grad=True`.\n",
      "    93|         0|            0|            0|  0.00%|\n",
      "    94|         0|            0|            0|  0.00%|    In this mode, the result of every computation will have\n",
      "    95|         0|            0|            0|  0.00%|    `requires_grad=False`, even when the inputs have `requires_grad=True`.\n",
      "    96|         0|            0|            0|  0.00%|\n",
      "    97|         0|            0|            0|  0.00%|    This context manager is thread local; it will not affect computation\n",
      "    98|         0|            0|            0|  0.00%|    in other threads.\n",
      "    99|         0|            0|            0|  0.00%|\n",
      "   100|         0|            0|            0|  0.00%|    Also functions as a decorator. (Make sure to instantiate with parenthesis.)\n",
      "   101|         0|            0|            0|  0.00%|\n",
      "   102|         0|            0|            0|  0.00%|    .. note::\n",
      "   103|         0|            0|            0|  0.00%|        No-grad is one of several mechanisms that can enable or\n",
      "   104|         0|            0|            0|  0.00%|        disable gradients locally see :ref:`locally-disable-grad-doc` for\n",
      "   105|         0|            0|            0|  0.00%|        more information on how they compare.\n",
      "   106|         0|            0|            0|  0.00%|\n",
      "   107|         0|            0|            0|  0.00%|    .. note::\n",
      "   108|         0|            0|            0|  0.00%|        This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.\n",
      "   109|         0|            0|            0|  0.00%|        If you want to disable forward AD for a computation, you can unpack\n",
      "   110|         0|            0|            0|  0.00%|        your dual tensors.\n",
      "   111|         0|            0|            0|  0.00%|\n",
      "   112|         0|            0|            0|  0.00%|    Example::\n",
      "   113|         0|            0|            0|  0.00%|\n",
      "   114|         0|            0|            0|  0.00%|        >>> x = torch.tensor([1.], requires_grad=True)\n",
      "   115|         0|            0|            0|  0.00%|        >>> with torch.no_grad():\n",
      "   116|         0|            0|            0|  0.00%|        ...   y = x * 2\n",
      "   117|         0|            0|            0|  0.00%|        >>> y.requires_grad\n",
      "   118|         0|            0|            0|  0.00%|        False\n",
      "   119|         0|            0|            0|  0.00%|        >>> @torch.no_grad()\n",
      "   120|         0|            0|            0|  0.00%|        ... def doubler(x):\n",
      "   121|         0|            0|            0|  0.00%|        ...     return x * 2\n",
      "   122|         0|            0|            0|  0.00%|        >>> z = doubler(x)\n",
      "   123|         0|            0|            0|  0.00%|        >>> z.requires_grad\n",
      "   124|         0|            0|            0|  0.00%|        False\n",
      "   125|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   126|     37830|    0.0872295|  2.30583e-06|  0.01%|    def __init__(self) -> None:\n",
      "   127|     37830|     0.242731|  6.41635e-06|  0.04%|        if not torch._jit_internal.is_scripting():\n",
      "(call)|     37830|     0.119938|  3.17046e-06|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_jit_internal.py:958 is_scripting\n",
      "   128|     37830|     0.113539|  3.00129e-06|  0.02%|            super().__init__()\n",
      "   129|     37830|    0.0810893|  2.14352e-06|  0.01%|        self.prev = False\n",
      "   130|         0|            0|            0|  0.00%|\n",
      "   131|     37830|     0.077229|  2.04148e-06|  0.01%|    def __enter__(self) -> None:\n",
      "   132|     37830|    0.0896993|  2.37111e-06|  0.01%|        self.prev = torch.is_grad_enabled()\n",
      "   133|     37830|     0.228573|  6.04212e-06|  0.04%|        torch.set_grad_enabled(False)\n",
      "(call)|     37830|     0.281121|  7.43115e-06|  0.05%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:226 __init__\n",
      "   134|         0|            0|            0|  0.00%|\n",
      "   135|     37830|     0.100936|  2.66815e-06|  0.02%|    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n",
      "   136|     37830|     0.286634|  7.57691e-06|  0.05%|        torch.set_grad_enabled(self.prev)\n",
      "(call)|     37830|     0.321593|  8.50101e-06|  0.05%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:226 __init__\n",
      "   137|         0|            0|            0|  0.00%|\n",
      "   138|         0|            0|            0|  0.00%|\n",
      "   139|         0|            0|            0|  0.00%|class enable_grad(_DecoratorContextManager):\n",
      "   140|         0|            0|            0|  0.00%|    r\"\"\"Context-manager that enables gradient calculation.\n",
      "   141|         0|            0|            0|  0.00%|\n",
      "   142|         0|            0|            0|  0.00%|    Enables gradient calculation, if it has been disabled via :class:`~no_grad`\n",
      "   143|         0|            0|            0|  0.00%|    or :class:`~set_grad_enabled`.\n",
      "   144|         0|            0|            0|  0.00%|\n",
      "   145|         0|            0|            0|  0.00%|    This context manager is thread local; it will not affect computation\n",
      "   146|         0|            0|            0|  0.00%|    in other threads.\n",
      "   147|         0|            0|            0|  0.00%|\n",
      "   148|         0|            0|            0|  0.00%|    Also functions as a decorator. (Make sure to instantiate with parenthesis.)\n",
      "   149|         0|            0|            0|  0.00%|\n",
      "   150|         0|            0|            0|  0.00%|    .. note::\n",
      "   151|         0|            0|            0|  0.00%|        enable_grad is one of several mechanisms that can enable or\n",
      "   152|         0|            0|            0|  0.00%|        disable gradients locally see :ref:`locally-disable-grad-doc` for\n",
      "   153|         0|            0|            0|  0.00%|        more information on how they compare.\n",
      "   154|         0|            0|            0|  0.00%|\n",
      "   155|         0|            0|            0|  0.00%|    .. note::\n",
      "   156|         0|            0|            0|  0.00%|        This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.\n",
      "   157|         0|            0|            0|  0.00%|\n",
      "   158|         0|            0|            0|  0.00%|    Example::\n",
      "   159|         0|            0|            0|  0.00%|\n",
      "   160|         0|            0|            0|  0.00%|        >>> x = torch.tensor([1.], requires_grad=True)\n",
      "   161|         0|            0|            0|  0.00%|        >>> with torch.no_grad():\n",
      "   162|         0|            0|            0|  0.00%|        ...   with torch.enable_grad():\n",
      "   163|         0|            0|            0|  0.00%|        ...     y = x * 2\n",
      "   164|         0|            0|            0|  0.00%|        >>> y.requires_grad\n",
      "   165|         0|            0|            0|  0.00%|        True\n",
      "   166|         0|            0|            0|  0.00%|        >>> y.backward()\n",
      "   167|         0|            0|            0|  0.00%|        >>> x.grad\n",
      "   168|         0|            0|            0|  0.00%|        >>> @torch.enable_grad()\n",
      "   169|         0|            0|            0|  0.00%|        ... def doubler(x):\n",
      "   170|         0|            0|            0|  0.00%|        ...     return x * 2\n",
      "   171|         0|            0|            0|  0.00%|        >>> with torch.no_grad():\n",
      "   172|         0|            0|            0|  0.00%|        ...     z = doubler(x)\n",
      "   173|         0|            0|            0|  0.00%|        >>> z.requires_grad\n",
      "   174|         0|            0|            0|  0.00%|        True\n",
      "   175|         0|            0|            0|  0.00%|\n",
      "   176|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   177|     31590|    0.0499811|  1.58218e-06|  0.01%|    def __enter__(self) -> None:\n",
      "   178|     31590|    0.0891614|  2.82246e-06|  0.01%|        self.prev = torch.is_grad_enabled()\n",
      "   179|     31590|    0.0753579|   2.3855e-06|  0.01%|        torch._C._set_grad_enabled(True)\n",
      "   180|         0|            0|            0|  0.00%|\n",
      "   181|     31590|    0.0531943|   1.6839e-06|  0.01%|    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n",
      "   182|     31590|    0.0792549|  2.50886e-06|  0.01%|        torch._C._set_grad_enabled(self.prev)\n",
      "   183|         0|            0|            0|  0.00%|\n",
      "   184|         0|            0|            0|  0.00%|\n",
      "   185|         0|            0|            0|  0.00%|class set_grad_enabled(_DecoratorContextManager):\n",
      "   186|         0|            0|            0|  0.00%|    r\"\"\"Context-manager that sets gradient calculation to on or off.\n",
      "   187|         0|            0|            0|  0.00%|\n",
      "   188|         0|            0|            0|  0.00%|    ``set_grad_enabled`` will enable or disable grads based on its argument :attr:`mode`.\n",
      "   189|         0|            0|            0|  0.00%|    It can be used as a context-manager or as a function.\n",
      "   190|         0|            0|            0|  0.00%|\n",
      "   191|         0|            0|            0|  0.00%|    This context manager is thread local; it will not affect computation\n",
      "   192|         0|            0|            0|  0.00%|    in other threads.\n",
      "   193|         0|            0|            0|  0.00%|\n",
      "   194|         0|            0|            0|  0.00%|    Args:\n",
      "   195|         0|            0|            0|  0.00%|        mode (bool): Flag whether to enable grad (``True``), or disable\n",
      "   196|         0|            0|            0|  0.00%|                     (``False``). This can be used to conditionally enable\n",
      "   197|         0|            0|            0|  0.00%|                     gradients.\n",
      "   198|         0|            0|            0|  0.00%|\n",
      "   199|         0|            0|            0|  0.00%|    .. note::\n",
      "   200|         0|            0|            0|  0.00%|        set_grad_enabled is one of several mechanisms that can enable or\n",
      "   201|         0|            0|            0|  0.00%|        disable gradients locally see :ref:`locally-disable-grad-doc` for\n",
      "   202|         0|            0|            0|  0.00%|        more information on how they compare.\n",
      "   203|         0|            0|            0|  0.00%|\n",
      "   204|         0|            0|            0|  0.00%|    .. note::\n",
      "   205|         0|            0|            0|  0.00%|        This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.\n",
      "   206|         0|            0|            0|  0.00%|\n",
      "   207|         0|            0|            0|  0.00%|    Example::\n",
      "   208|         0|            0|            0|  0.00%|\n",
      "   209|         0|            0|            0|  0.00%|        >>> x = torch.tensor([1.], requires_grad=True)\n",
      "   210|         0|            0|            0|  0.00%|        >>> is_train = False\n",
      "   211|         0|            0|            0|  0.00%|        >>> with torch.set_grad_enabled(is_train):\n",
      "   212|         0|            0|            0|  0.00%|        ...   y = x * 2\n",
      "   213|         0|            0|            0|  0.00%|        >>> y.requires_grad\n",
      "   214|         0|            0|            0|  0.00%|        False\n",
      "   215|         0|            0|            0|  0.00%|        >>> torch.set_grad_enabled(True)\n",
      "   216|         0|            0|            0|  0.00%|        >>> y = x * 2\n",
      "   217|         0|            0|            0|  0.00%|        >>> y.requires_grad\n",
      "   218|         0|            0|            0|  0.00%|        True\n",
      "   219|         0|            0|            0|  0.00%|        >>> torch.set_grad_enabled(False)\n",
      "   220|         0|            0|            0|  0.00%|        >>> y = x * 2\n",
      "   221|         0|            0|            0|  0.00%|        >>> y.requires_grad\n",
      "   222|         0|            0|            0|  0.00%|        False\n",
      "   223|         0|            0|            0|  0.00%|\n",
      "   224|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   225|         0|            0|            0|  0.00%|\n",
      "   226|     75660|     0.118086|  1.56075e-06|  0.02%|    def __init__(self, mode: bool) -> None:\n",
      "   227|     75660|     0.185143|  2.44704e-06|  0.03%|        self.prev = torch.is_grad_enabled()\n",
      "   228|     75660|     0.166315|  2.19819e-06|  0.03%|        torch._C._set_grad_enabled(mode)\n",
      "   229|     75660|     0.133169|   1.7601e-06|  0.02%|        self.mode = mode\n",
      "   230|         0|            0|            0|  0.00%|\n",
      "   231|         0|            0|            0|  0.00%|    def __enter__(self) -> None:\n",
      "   232|         0|            0|            0|  0.00%|        pass\n",
      "   233|         0|            0|            0|  0.00%|\n",
      "   234|         0|            0|            0|  0.00%|    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n",
      "   235|         0|            0|            0|  0.00%|        torch._C._set_grad_enabled(self.prev)\n",
      "   236|         0|            0|            0|  0.00%|\n",
      "   237|         0|            0|            0|  0.00%|    def clone(self):\n",
      "   238|         0|            0|            0|  0.00%|        return self.__class__(self.mode)\n",
      "   239|         0|            0|            0|  0.00%|\n",
      "   240|         0|            0|            0|  0.00%|\n",
      "   241|         0|            0|            0|  0.00%|class inference_mode(_DecoratorContextManager):\n",
      "   242|         0|            0|            0|  0.00%|    r\"\"\"Context-manager that enables or disables inference mode\n",
      "   243|         0|            0|            0|  0.00%|\n",
      "   244|         0|            0|            0|  0.00%|    InferenceMode is a new context manager analogous to :class:`~no_grad`\n",
      "   245|         0|            0|            0|  0.00%|    to be used when you are certain your operations will have no interactions\n",
      "   246|         0|            0|            0|  0.00%|    with autograd (e.g., model training). Code run under this mode gets better\n",
      "   247|         0|            0|            0|  0.00%|    performance by disabling view tracking and version counter bumps. Note that\n",
      "   248|         0|            0|            0|  0.00%|    unlike some other mechanisms that locally enable or disable grad,\n",
      "   249|         0|            0|            0|  0.00%|    entering inference_mode also disables to :ref:`forward-mode AD <forward-mode-ad>`.\n",
      "   250|         0|            0|            0|  0.00%|\n",
      "   251|         0|            0|            0|  0.00%|    This context manager is thread local; it will not affect computation\n",
      "   252|         0|            0|            0|  0.00%|    in other threads.\n",
      "   253|         0|            0|            0|  0.00%|\n",
      "   254|         0|            0|            0|  0.00%|    Also functions as a decorator. (Make sure to instantiate with parenthesis.)\n",
      "   255|         0|            0|            0|  0.00%|\n",
      "   256|         0|            0|            0|  0.00%|    .. note::\n",
      "   257|         0|            0|            0|  0.00%|        Inference mode is one of several mechanisms that can enable or\n",
      "   258|         0|            0|            0|  0.00%|        disable gradients locally see :ref:`locally-disable-grad-doc` for\n",
      "   259|         0|            0|            0|  0.00%|        more information on how they compare.\n",
      "   260|         0|            0|            0|  0.00%|\n",
      "   261|         0|            0|            0|  0.00%|    Args:\n",
      "   262|         0|            0|            0|  0.00%|        mode (bool): Flag whether to enable or disable inference mode\n",
      "   263|         0|            0|            0|  0.00%|\n",
      "   264|         0|            0|            0|  0.00%|    Example::\n",
      "   265|         0|            0|            0|  0.00%|        >>> import torch\n",
      "   266|         0|            0|            0|  0.00%|        >>> x = torch.ones(1, 2, 3, requires_grad=True)\n",
      "   267|         0|            0|            0|  0.00%|        >>> with torch.inference_mode():\n",
      "   268|         0|            0|            0|  0.00%|        ...   y = x * x\n",
      "   269|         0|            0|            0|  0.00%|        >>> y.requires_grad\n",
      "   270|         0|            0|            0|  0.00%|        False\n",
      "   271|         0|            0|            0|  0.00%|        >>> y._version\n",
      "   272|         0|            0|            0|  0.00%|        Traceback (most recent call last):\n",
      "   273|         0|            0|            0|  0.00%|        File \"<stdin>\", line 1, in <module>\n",
      "   274|         0|            0|            0|  0.00%|        RuntimeError: Inference tensors do not track version counter.\n",
      "   275|         0|            0|            0|  0.00%|        >>> @torch.inference_mode()\n",
      "   276|         0|            0|            0|  0.00%|        ... def func(x):\n",
      "   277|         0|            0|            0|  0.00%|        ...   return x * x\n",
      "   278|         0|            0|            0|  0.00%|        >>> out = func(x)\n",
      "   279|         0|            0|            0|  0.00%|        >>> out.requires_grad\n",
      "   280|         0|            0|            0|  0.00%|        False\n",
      "   281|         0|            0|            0|  0.00%|\n",
      "   282|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   283|         0|            0|            0|  0.00%|    def __init__(self, mode=True):\n",
      "   284|         0|            0|            0|  0.00%|        if not torch._jit_internal.is_scripting():\n",
      "   285|         0|            0|            0|  0.00%|            super().__init__()\n",
      "   286|         0|            0|            0|  0.00%|        # Holds a python binding to a RAII guard that can enable or disable\n",
      "   287|         0|            0|            0|  0.00%|        # inference mode\n",
      "   288|         0|            0|            0|  0.00%|        self._inference_mode_raii_guard = None\n",
      "   289|         0|            0|            0|  0.00%|        self.mode = mode\n",
      "   290|         0|            0|            0|  0.00%|\n",
      "   291|         0|            0|            0|  0.00%|    def __enter__(self):\n",
      "   292|         0|            0|            0|  0.00%|        self._inference_mode_raii_guard = torch._C._InferenceMode(self.mode)\n",
      "   293|         0|            0|            0|  0.00%|\n",
      "   294|         0|            0|            0|  0.00%|    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n",
      "   295|         0|            0|            0|  0.00%|        del self._inference_mode_raii_guard\n",
      "   296|         0|            0|            0|  0.00%|\n",
      "   297|         0|            0|            0|  0.00%|    def clone(self):\n",
      "   298|         0|            0|            0|  0.00%|        return self.__class__(self.mode)\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/numeric.py\n",
      "File duration: 2.22157s (0.36%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|import functools\n",
      "     2|         0|            0|            0|  0.00%|import itertools\n",
      "     3|         0|            0|            0|  0.00%|import operator\n",
      "     4|         0|            0|            0|  0.00%|import sys\n",
      "     5|         0|            0|            0|  0.00%|import warnings\n",
      "     6|         0|            0|            0|  0.00%|import numbers\n",
      "     7|         0|            0|            0|  0.00%|\n",
      "     8|         0|            0|            0|  0.00%|import numpy as np\n",
      "     9|         0|            0|            0|  0.00%|from . import multiarray\n",
      "    10|         0|            0|            0|  0.00%|from .multiarray import (\n",
      "    11|         0|            0|            0|  0.00%|    _fastCopyAndTranspose as fastCopyAndTranspose, ALLOW_THREADS,\n",
      "    12|         0|            0|            0|  0.00%|    BUFSIZE, CLIP, MAXDIMS, MAY_SHARE_BOUNDS, MAY_SHARE_EXACT, RAISE,\n",
      "    13|         0|            0|            0|  0.00%|    WRAP, arange, array, asarray, asanyarray, ascontiguousarray,\n",
      "    14|         0|            0|            0|  0.00%|    asfortranarray, broadcast, can_cast, compare_chararrays,\n",
      "    15|         0|            0|            0|  0.00%|    concatenate, copyto, dot, dtype, empty,\n",
      "    16|         0|            0|            0|  0.00%|    empty_like, flatiter, frombuffer, _from_dlpack, fromfile, fromiter,\n",
      "    17|         0|            0|            0|  0.00%|    fromstring, inner, lexsort, matmul, may_share_memory,\n",
      "    18|         0|            0|            0|  0.00%|    min_scalar_type, ndarray, nditer, nested_iters, promote_types,\n",
      "    19|         0|            0|            0|  0.00%|    putmask, result_type, set_numeric_ops, shares_memory, vdot, where,\n",
      "    20|         0|            0|            0|  0.00%|    zeros, normalize_axis_index)\n",
      "    21|         0|            0|            0|  0.00%|\n",
      "    22|         0|            0|            0|  0.00%|from . import overrides\n",
      "    23|         0|            0|            0|  0.00%|from . import umath\n",
      "    24|         0|            0|            0|  0.00%|from . import shape_base\n",
      "    25|         0|            0|            0|  0.00%|from .overrides import set_array_function_like_doc, set_module\n",
      "    26|         0|            0|            0|  0.00%|from .umath import (multiply, invert, sin, PINF, NAN)\n",
      "    27|         0|            0|            0|  0.00%|from . import numerictypes\n",
      "    28|         0|            0|            0|  0.00%|from .numerictypes import longlong, intc, int_, float_, complex_, bool_\n",
      "    29|         0|            0|            0|  0.00%|from ._exceptions import TooHardError, AxisError\n",
      "    30|         0|            0|            0|  0.00%|from ._ufunc_config import errstate\n",
      "    31|         0|            0|            0|  0.00%|\n",
      "    32|         0|            0|            0|  0.00%|bitwise_not = invert\n",
      "    33|         0|            0|            0|  0.00%|ufunc = type(sin)\n",
      "    34|         0|            0|            0|  0.00%|newaxis = None\n",
      "    35|         0|            0|            0|  0.00%|\n",
      "    36|         0|            0|            0|  0.00%|array_function_dispatch = functools.partial(\n",
      "    37|         0|            0|            0|  0.00%|    overrides.array_function_dispatch, module='numpy')\n",
      "    38|         0|            0|            0|  0.00%|\n",
      "    39|         0|            0|            0|  0.00%|\n",
      "    40|         0|            0|            0|  0.00%|__all__ = [\n",
      "    41|         0|            0|            0|  0.00%|    'newaxis', 'ndarray', 'flatiter', 'nditer', 'nested_iters', 'ufunc',\n",
      "    42|         0|            0|            0|  0.00%|    'arange', 'array', 'asarray', 'asanyarray', 'ascontiguousarray',\n",
      "    43|         0|            0|            0|  0.00%|    'asfortranarray', 'zeros', 'count_nonzero', 'empty', 'broadcast', 'dtype',\n",
      "    44|         0|            0|            0|  0.00%|    'fromstring', 'fromfile', 'frombuffer', '_from_dlpack', 'where',\n",
      "    45|         0|            0|            0|  0.00%|    'argwhere', 'copyto', 'concatenate', 'fastCopyAndTranspose', 'lexsort',\n",
      "    46|         0|            0|            0|  0.00%|    'set_numeric_ops', 'can_cast', 'promote_types', 'min_scalar_type',\n",
      "    47|         0|            0|            0|  0.00%|    'result_type', 'isfortran', 'empty_like', 'zeros_like', 'ones_like',\n",
      "    48|         0|            0|            0|  0.00%|    'correlate', 'convolve', 'inner', 'dot', 'outer', 'vdot', 'roll',\n",
      "    49|         0|            0|            0|  0.00%|    'rollaxis', 'moveaxis', 'cross', 'tensordot', 'little_endian',\n",
      "    50|         0|            0|            0|  0.00%|    'fromiter', 'array_equal', 'array_equiv', 'indices', 'fromfunction',\n",
      "    51|         0|            0|            0|  0.00%|    'isclose', 'isscalar', 'binary_repr', 'base_repr', 'ones',\n",
      "    52|         0|            0|            0|  0.00%|    'identity', 'allclose', 'compare_chararrays', 'putmask',\n",
      "    53|         0|            0|            0|  0.00%|    'flatnonzero', 'Inf', 'inf', 'infty', 'Infinity', 'nan', 'NaN',\n",
      "    54|         0|            0|            0|  0.00%|    'False_', 'True_', 'bitwise_not', 'CLIP', 'RAISE', 'WRAP', 'MAXDIMS',\n",
      "    55|         0|            0|            0|  0.00%|    'BUFSIZE', 'ALLOW_THREADS', 'ComplexWarning', 'full', 'full_like',\n",
      "    56|         0|            0|            0|  0.00%|    'matmul', 'shares_memory', 'may_share_memory', 'MAY_SHARE_BOUNDS',\n",
      "    57|         0|            0|            0|  0.00%|    'MAY_SHARE_EXACT', 'TooHardError', 'AxisError']\n",
      "    58|         0|            0|            0|  0.00%|\n",
      "    59|         0|            0|            0|  0.00%|\n",
      "    60|         0|            0|            0|  0.00%|@set_module('numpy')\n",
      "    61|         0|            0|            0|  0.00%|class ComplexWarning(RuntimeWarning):\n",
      "    62|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    63|         0|            0|            0|  0.00%|    The warning raised when casting a complex dtype to a real dtype.\n",
      "    64|         0|            0|            0|  0.00%|\n",
      "    65|         0|            0|            0|  0.00%|    As implemented, casting a complex number to a real discards its imaginary\n",
      "    66|         0|            0|            0|  0.00%|    part, but this behavior may not be what the user actually wants.\n",
      "    67|         0|            0|            0|  0.00%|\n",
      "    68|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    69|         0|            0|            0|  0.00%|    pass\n",
      "    70|         0|            0|            0|  0.00%|\n",
      "    71|         0|            0|            0|  0.00%|\n",
      "    72|     74874|     0.111626|  1.49085e-06|  0.02%|def _zeros_like_dispatcher(a, dtype=None, order=None, subok=None, shape=None):\n",
      "    73|     74874|     0.139961|  1.86928e-06|  0.02%|    return (a,)\n",
      "    74|         0|            0|            0|  0.00%|\n",
      "    75|         0|            0|            0|  0.00%|\n",
      "    76|     74874|     0.164009|  2.19046e-06|  0.03%|@array_function_dispatch(_zeros_like_dispatcher)\n",
      "    77|         0|            0|            0|  0.00%|def zeros_like(a, dtype=None, order='K', subok=True, shape=None):\n",
      "    78|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    79|         0|            0|            0|  0.00%|    Return an array of zeros with the same shape and type as a given array.\n",
      "    80|         0|            0|            0|  0.00%|\n",
      "    81|         0|            0|            0|  0.00%|    Parameters\n",
      "    82|         0|            0|            0|  0.00%|    ----------\n",
      "    83|         0|            0|            0|  0.00%|    a : array_like\n",
      "    84|         0|            0|            0|  0.00%|        The shape and data-type of `a` define these same attributes of\n",
      "    85|         0|            0|            0|  0.00%|        the returned array.\n",
      "    86|         0|            0|            0|  0.00%|    dtype : data-type, optional\n",
      "    87|         0|            0|            0|  0.00%|        Overrides the data type of the result.\n",
      "    88|         0|            0|            0|  0.00%|\n",
      "    89|         0|            0|            0|  0.00%|        .. versionadded:: 1.6.0\n",
      "    90|         0|            0|            0|  0.00%|    order : {'C', 'F', 'A', or 'K'}, optional\n",
      "    91|         0|            0|            0|  0.00%|        Overrides the memory layout of the result. 'C' means C-order,\n",
      "    92|         0|            0|            0|  0.00%|        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n",
      "    93|         0|            0|            0|  0.00%|        'C' otherwise. 'K' means match the layout of `a` as closely\n",
      "    94|         0|            0|            0|  0.00%|        as possible.\n",
      "    95|         0|            0|            0|  0.00%|\n",
      "    96|         0|            0|            0|  0.00%|        .. versionadded:: 1.6.0\n",
      "    97|         0|            0|            0|  0.00%|    subok : bool, optional.\n",
      "    98|         0|            0|            0|  0.00%|        If True, then the newly created array will use the sub-class\n",
      "    99|         0|            0|            0|  0.00%|        type of `a`, otherwise it will be a base-class array. Defaults\n",
      "   100|         0|            0|            0|  0.00%|        to True.\n",
      "   101|         0|            0|            0|  0.00%|    shape : int or sequence of ints, optional.\n",
      "   102|         0|            0|            0|  0.00%|        Overrides the shape of the result. If order='K' and the number of\n",
      "   103|         0|            0|            0|  0.00%|        dimensions is unchanged, will try to keep order, otherwise,\n",
      "   104|         0|            0|            0|  0.00%|        order='C' is implied.\n",
      "   105|         0|            0|            0|  0.00%|\n",
      "   106|         0|            0|            0|  0.00%|        .. versionadded:: 1.17.0\n",
      "   107|         0|            0|            0|  0.00%|\n",
      "   108|         0|            0|            0|  0.00%|    Returns\n",
      "   109|         0|            0|            0|  0.00%|    -------\n",
      "   110|         0|            0|            0|  0.00%|    out : ndarray\n",
      "   111|         0|            0|            0|  0.00%|        Array of zeros with the same shape and type as `a`.\n",
      "   112|         0|            0|            0|  0.00%|\n",
      "   113|         0|            0|            0|  0.00%|    See Also\n",
      "   114|         0|            0|            0|  0.00%|    --------\n",
      "   115|         0|            0|            0|  0.00%|    empty_like : Return an empty array with shape and type of input.\n",
      "   116|         0|            0|            0|  0.00%|    ones_like : Return an array of ones with shape and type of input.\n",
      "   117|         0|            0|            0|  0.00%|    full_like : Return a new array with shape of input filled with value.\n",
      "   118|         0|            0|            0|  0.00%|    zeros : Return a new array setting values to zero.\n",
      "   119|         0|            0|            0|  0.00%|\n",
      "   120|         0|            0|            0|  0.00%|    Examples\n",
      "   121|         0|            0|            0|  0.00%|    --------\n",
      "   122|         0|            0|            0|  0.00%|    >>> x = np.arange(6)\n",
      "   123|         0|            0|            0|  0.00%|    >>> x = x.reshape((2, 3))\n",
      "   124|         0|            0|            0|  0.00%|    >>> x\n",
      "   125|         0|            0|            0|  0.00%|    array([[0, 1, 2],\n",
      "   126|         0|            0|            0|  0.00%|           [3, 4, 5]])\n",
      "   127|         0|            0|            0|  0.00%|    >>> np.zeros_like(x)\n",
      "   128|         0|            0|            0|  0.00%|    array([[0, 0, 0],\n",
      "   129|         0|            0|            0|  0.00%|           [0, 0, 0]])\n",
      "   130|         0|            0|            0|  0.00%|\n",
      "   131|         0|            0|            0|  0.00%|    >>> y = np.arange(3, dtype=float)\n",
      "   132|         0|            0|            0|  0.00%|    >>> y\n",
      "   133|         0|            0|            0|  0.00%|    array([0., 1., 2.])\n",
      "   134|         0|            0|            0|  0.00%|    >>> np.zeros_like(y)\n",
      "   135|         0|            0|            0|  0.00%|    array([0.,  0.,  0.])\n",
      "   136|         0|            0|            0|  0.00%|\n",
      "   137|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   138|     74874|     0.517441|  6.91082e-06|  0.08%|    res = empty_like(a, dtype=dtype, order=order, subok=subok, shape=shape)\n",
      "(call)|     74874|      1.35689|  1.81223e-05|  0.22%|# <__array_function__ internals>:177 empty_like\n",
      "   139|         0|            0|            0|  0.00%|    # needed instead of a 0 to get same result as zeros for for string dtypes\n",
      "   140|     74874|     0.268437|  3.58519e-06|  0.04%|    z = zeros(1, dtype=res.dtype)\n",
      "   141|     74874|     0.468382|   6.2556e-06|  0.08%|    multiarray.copyto(res, z, casting='unsafe')\n",
      "(call)|     74874|      1.29297|  1.72686e-05|  0.21%|# <__array_function__ internals>:177 copyto\n",
      "   142|     74874|     0.136591|  1.82428e-06|  0.02%|    return res\n",
      "   143|         0|            0|            0|  0.00%|\n",
      "   144|         0|            0|            0|  0.00%|\n",
      "   145|         0|            0|            0|  0.00%|def _ones_dispatcher(shape, dtype=None, order=None, *, like=None):\n",
      "   146|         0|            0|            0|  0.00%|    return(like,)\n",
      "   147|         0|            0|            0|  0.00%|\n",
      "   148|         0|            0|            0|  0.00%|\n",
      "   149|     24958|    0.0546989|  2.19164e-06|  0.01%|@set_array_function_like_doc\n",
      "   150|         0|            0|            0|  0.00%|@set_module('numpy')\n",
      "   151|         0|            0|            0|  0.00%|def ones(shape, dtype=None, order='C', *, like=None):\n",
      "   152|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   153|         0|            0|            0|  0.00%|    Return a new array of given shape and type, filled with ones.\n",
      "   154|         0|            0|            0|  0.00%|\n",
      "   155|         0|            0|            0|  0.00%|    Parameters\n",
      "   156|         0|            0|            0|  0.00%|    ----------\n",
      "   157|         0|            0|            0|  0.00%|    shape : int or sequence of ints\n",
      "   158|         0|            0|            0|  0.00%|        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n",
      "   159|         0|            0|            0|  0.00%|    dtype : data-type, optional\n",
      "   160|         0|            0|            0|  0.00%|        The desired data-type for the array, e.g., `numpy.int8`.  Default is\n",
      "   161|         0|            0|            0|  0.00%|        `numpy.float64`.\n",
      "   162|         0|            0|            0|  0.00%|    order : {'C', 'F'}, optional, default: C\n",
      "   163|         0|            0|            0|  0.00%|        Whether to store multi-dimensional data in row-major\n",
      "   164|         0|            0|            0|  0.00%|        (C-style) or column-major (Fortran-style) order in\n",
      "   165|         0|            0|            0|  0.00%|        memory.\n",
      "   166|         0|            0|            0|  0.00%|    ${ARRAY_FUNCTION_LIKE}\n",
      "   167|         0|            0|            0|  0.00%|\n",
      "   168|         0|            0|            0|  0.00%|        .. versionadded:: 1.20.0\n",
      "   169|         0|            0|            0|  0.00%|\n",
      "   170|         0|            0|            0|  0.00%|    Returns\n",
      "   171|         0|            0|            0|  0.00%|    -------\n",
      "   172|         0|            0|            0|  0.00%|    out : ndarray\n",
      "   173|         0|            0|            0|  0.00%|        Array of ones with the given shape, dtype, and order.\n",
      "   174|         0|            0|            0|  0.00%|\n",
      "   175|         0|            0|            0|  0.00%|    See Also\n",
      "   176|         0|            0|            0|  0.00%|    --------\n",
      "   177|         0|            0|            0|  0.00%|    ones_like : Return an array of ones with shape and type of input.\n",
      "   178|         0|            0|            0|  0.00%|    empty : Return a new uninitialized array.\n",
      "   179|         0|            0|            0|  0.00%|    zeros : Return a new array setting values to zero.\n",
      "   180|         0|            0|            0|  0.00%|    full : Return a new array of given shape filled with value.\n",
      "   181|         0|            0|            0|  0.00%|\n",
      "   182|         0|            0|            0|  0.00%|\n",
      "   183|         0|            0|            0|  0.00%|    Examples\n",
      "   184|         0|            0|            0|  0.00%|    --------\n",
      "   185|         0|            0|            0|  0.00%|    >>> np.ones(5)\n",
      "   186|         0|            0|            0|  0.00%|    array([1., 1., 1., 1., 1.])\n",
      "   187|         0|            0|            0|  0.00%|\n",
      "   188|         0|            0|            0|  0.00%|    >>> np.ones((5,), dtype=int)\n",
      "   189|         0|            0|            0|  0.00%|    array([1, 1, 1, 1, 1])\n",
      "   190|         0|            0|            0|  0.00%|\n",
      "   191|         0|            0|            0|  0.00%|    >>> np.ones((2, 1))\n",
      "   192|         0|            0|            0|  0.00%|    array([[1.],\n",
      "   193|         0|            0|            0|  0.00%|           [1.]])\n",
      "   194|         0|            0|            0|  0.00%|\n",
      "   195|         0|            0|            0|  0.00%|    >>> s = (2,2)\n",
      "   196|         0|            0|            0|  0.00%|    >>> np.ones(s)\n",
      "   197|         0|            0|            0|  0.00%|    array([[1.,  1.],\n",
      "   198|         0|            0|            0|  0.00%|           [1.,  1.]])\n",
      "   199|         0|            0|            0|  0.00%|\n",
      "   200|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   201|     24958|    0.0567582|  2.27415e-06|  0.01%|    if like is not None:\n",
      "   202|         0|            0|            0|  0.00%|        return _ones_with_like(shape, dtype=dtype, order=order, like=like)\n",
      "   203|         0|            0|            0|  0.00%|\n",
      "   204|     24958|     0.084507|  3.38597e-06|  0.01%|    a = empty(shape, dtype, order)\n",
      "   205|     24958|     0.173324|  6.94461e-06|  0.03%|    multiarray.copyto(a, 1, casting='unsafe')\n",
      "(call)|     24958|     0.539578|  2.16194e-05|  0.09%|# <__array_function__ internals>:177 copyto\n",
      "   206|     24958|    0.0458367|  1.83655e-06|  0.01%|    return a\n",
      "   207|         0|            0|            0|  0.00%|\n",
      "   208|         0|            0|            0|  0.00%|\n",
      "   209|         0|            0|            0|  0.00%|_ones_with_like = array_function_dispatch(\n",
      "   210|         0|            0|            0|  0.00%|    _ones_dispatcher\n",
      "   211|         0|            0|            0|  0.00%|)(ones)\n",
      "   212|         0|            0|            0|  0.00%|\n",
      "   213|         0|            0|            0|  0.00%|\n",
      "   214|         0|            0|            0|  0.00%|def _ones_like_dispatcher(a, dtype=None, order=None, subok=None, shape=None):\n",
      "   215|         0|            0|            0|  0.00%|    return (a,)\n",
      "   216|         0|            0|            0|  0.00%|\n",
      "   217|         0|            0|            0|  0.00%|\n",
      "   218|         0|            0|            0|  0.00%|@array_function_dispatch(_ones_like_dispatcher)\n",
      "   219|         0|            0|            0|  0.00%|def ones_like(a, dtype=None, order='K', subok=True, shape=None):\n",
      "   220|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   221|         0|            0|            0|  0.00%|    Return an array of ones with the same shape and type as a given array.\n",
      "   222|         0|            0|            0|  0.00%|\n",
      "   223|         0|            0|            0|  0.00%|    Parameters\n",
      "   224|         0|            0|            0|  0.00%|    ----------\n",
      "   225|         0|            0|            0|  0.00%|    a : array_like\n",
      "   226|         0|            0|            0|  0.00%|        The shape and data-type of `a` define these same attributes of\n",
      "   227|         0|            0|            0|  0.00%|        the returned array.\n",
      "   228|         0|            0|            0|  0.00%|    dtype : data-type, optional\n",
      "   229|         0|            0|            0|  0.00%|        Overrides the data type of the result.\n",
      "   230|         0|            0|            0|  0.00%|\n",
      "   231|         0|            0|            0|  0.00%|        .. versionadded:: 1.6.0\n",
      "   232|         0|            0|            0|  0.00%|    order : {'C', 'F', 'A', or 'K'}, optional\n",
      "   233|         0|            0|            0|  0.00%|        Overrides the memory layout of the result. 'C' means C-order,\n",
      "   234|         0|            0|            0|  0.00%|        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n",
      "   235|         0|            0|            0|  0.00%|        'C' otherwise. 'K' means match the layout of `a` as closely\n",
      "   236|         0|            0|            0|  0.00%|        as possible.\n",
      "   237|         0|            0|            0|  0.00%|\n",
      "   238|         0|            0|            0|  0.00%|        .. versionadded:: 1.6.0\n",
      "   239|         0|            0|            0|  0.00%|    subok : bool, optional.\n",
      "   240|         0|            0|            0|  0.00%|        If True, then the newly created array will use the sub-class\n",
      "   241|         0|            0|            0|  0.00%|        type of `a`, otherwise it will be a base-class array. Defaults\n",
      "   242|         0|            0|            0|  0.00%|        to True.\n",
      "   243|         0|            0|            0|  0.00%|    shape : int or sequence of ints, optional.\n",
      "   244|         0|            0|            0|  0.00%|        Overrides the shape of the result. If order='K' and the number of\n",
      "   245|         0|            0|            0|  0.00%|        dimensions is unchanged, will try to keep order, otherwise,\n",
      "   246|         0|            0|            0|  0.00%|        order='C' is implied.\n",
      "   247|         0|            0|            0|  0.00%|\n",
      "   248|         0|            0|            0|  0.00%|        .. versionadded:: 1.17.0\n",
      "   249|         0|            0|            0|  0.00%|\n",
      "   250|         0|            0|            0|  0.00%|    Returns\n",
      "   251|         0|            0|            0|  0.00%|    -------\n",
      "   252|         0|            0|            0|  0.00%|    out : ndarray\n",
      "   253|         0|            0|            0|  0.00%|        Array of ones with the same shape and type as `a`.\n",
      "   254|         0|            0|            0|  0.00%|\n",
      "   255|         0|            0|            0|  0.00%|    See Also\n",
      "   256|         0|            0|            0|  0.00%|    --------\n",
      "   257|         0|            0|            0|  0.00%|    empty_like : Return an empty array with shape and type of input.\n",
      "   258|         0|            0|            0|  0.00%|    zeros_like : Return an array of zeros with shape and type of input.\n",
      "   259|         0|            0|            0|  0.00%|    full_like : Return a new array with shape of input filled with value.\n",
      "   260|         0|            0|            0|  0.00%|    ones : Return a new array setting values to one.\n",
      "   261|         0|            0|            0|  0.00%|\n",
      "   262|         0|            0|            0|  0.00%|    Examples\n",
      "   263|         0|            0|            0|  0.00%|    --------\n",
      "   264|         0|            0|            0|  0.00%|    >>> x = np.arange(6)\n",
      "   265|         0|            0|            0|  0.00%|    >>> x = x.reshape((2, 3))\n",
      "   266|         0|            0|            0|  0.00%|    >>> x\n",
      "   267|         0|            0|            0|  0.00%|    array([[0, 1, 2],\n",
      "   268|         0|            0|            0|  0.00%|           [3, 4, 5]])\n",
      "   269|         0|            0|            0|  0.00%|    >>> np.ones_like(x)\n",
      "   270|         0|            0|            0|  0.00%|    array([[1, 1, 1],\n",
      "   271|         0|            0|            0|  0.00%|           [1, 1, 1]])\n",
      "   272|         0|            0|            0|  0.00%|\n",
      "   273|         0|            0|            0|  0.00%|    >>> y = np.arange(3, dtype=float)\n",
      "   274|         0|            0|            0|  0.00%|    >>> y\n",
      "   275|         0|            0|            0|  0.00%|    array([0., 1., 2.])\n",
      "   276|         0|            0|            0|  0.00%|    >>> np.ones_like(y)\n",
      "   277|         0|            0|            0|  0.00%|    array([1.,  1.,  1.])\n",
      "   278|         0|            0|            0|  0.00%|\n",
      "   279|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   280|         0|            0|            0|  0.00%|    res = empty_like(a, dtype=dtype, order=order, subok=subok, shape=shape)\n",
      "   281|         0|            0|            0|  0.00%|    multiarray.copyto(res, 1, casting='unsafe')\n",
      "   282|         0|            0|            0|  0.00%|    return res\n",
      "   283|         0|            0|            0|  0.00%|\n",
      "   284|         0|            0|            0|  0.00%|\n",
      "   285|         0|            0|            0|  0.00%|def _full_dispatcher(shape, fill_value, dtype=None, order=None, *, like=None):\n",
      "   286|         0|            0|            0|  0.00%|    return(like,)\n",
      "   287|         0|            0|            0|  0.00%|\n",
      "   288|         0|            0|            0|  0.00%|\n",
      "   289|         0|            0|            0|  0.00%|@set_array_function_like_doc\n",
      "   290|         0|            0|            0|  0.00%|@set_module('numpy')\n",
      "   291|         0|            0|            0|  0.00%|def full(shape, fill_value, dtype=None, order='C', *, like=None):\n",
      "   292|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   293|         0|            0|            0|  0.00%|    Return a new array of given shape and type, filled with `fill_value`.\n",
      "   294|         0|            0|            0|  0.00%|\n",
      "   295|         0|            0|            0|  0.00%|    Parameters\n",
      "   296|         0|            0|            0|  0.00%|    ----------\n",
      "   297|         0|            0|            0|  0.00%|    shape : int or sequence of ints\n",
      "   298|         0|            0|            0|  0.00%|        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n",
      "   299|         0|            0|            0|  0.00%|    fill_value : scalar or array_like\n",
      "   300|         0|            0|            0|  0.00%|        Fill value.\n",
      "   301|         0|            0|            0|  0.00%|    dtype : data-type, optional\n",
      "   302|         0|            0|            0|  0.00%|        The desired data-type for the array  The default, None, means\n",
      "   303|         0|            0|            0|  0.00%|         ``np.array(fill_value).dtype``.\n",
      "   304|         0|            0|            0|  0.00%|    order : {'C', 'F'}, optional\n",
      "   305|         0|            0|            0|  0.00%|        Whether to store multidimensional data in C- or Fortran-contiguous\n",
      "   306|         0|            0|            0|  0.00%|        (row- or column-wise) order in memory.\n",
      "   307|         0|            0|            0|  0.00%|    ${ARRAY_FUNCTION_LIKE}\n",
      "   308|         0|            0|            0|  0.00%|\n",
      "   309|         0|            0|            0|  0.00%|        .. versionadded:: 1.20.0\n",
      "   310|         0|            0|            0|  0.00%|\n",
      "   311|         0|            0|            0|  0.00%|    Returns\n",
      "   312|         0|            0|            0|  0.00%|    -------\n",
      "   313|         0|            0|            0|  0.00%|    out : ndarray\n",
      "   314|         0|            0|            0|  0.00%|        Array of `fill_value` with the given shape, dtype, and order.\n",
      "   315|         0|            0|            0|  0.00%|\n",
      "   316|         0|            0|            0|  0.00%|    See Also\n",
      "   317|         0|            0|            0|  0.00%|    --------\n",
      "   318|         0|            0|            0|  0.00%|    full_like : Return a new array with shape of input filled with value.\n",
      "   319|         0|            0|            0|  0.00%|    empty : Return a new uninitialized array.\n",
      "   320|         0|            0|            0|  0.00%|    ones : Return a new array setting values to one.\n",
      "   321|         0|            0|            0|  0.00%|    zeros : Return a new array setting values to zero.\n",
      "   322|         0|            0|            0|  0.00%|\n",
      "   323|         0|            0|            0|  0.00%|    Examples\n",
      "   324|         0|            0|            0|  0.00%|    --------\n",
      "   325|         0|            0|            0|  0.00%|    >>> np.full((2, 2), np.inf)\n",
      "   326|         0|            0|            0|  0.00%|    array([[inf, inf],\n",
      "   327|         0|            0|            0|  0.00%|           [inf, inf]])\n",
      "   328|         0|            0|            0|  0.00%|    >>> np.full((2, 2), 10)\n",
      "   329|         0|            0|            0|  0.00%|    array([[10, 10],\n",
      "   330|         0|            0|            0|  0.00%|           [10, 10]])\n",
      "   331|         0|            0|            0|  0.00%|\n",
      "   332|         0|            0|            0|  0.00%|    >>> np.full((2, 2), [1, 2])\n",
      "   333|         0|            0|            0|  0.00%|    array([[1, 2],\n",
      "   334|         0|            0|            0|  0.00%|           [1, 2]])\n",
      "   335|         0|            0|            0|  0.00%|\n",
      "   336|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   337|         0|            0|            0|  0.00%|    if like is not None:\n",
      "   338|         0|            0|            0|  0.00%|        return _full_with_like(shape, fill_value, dtype=dtype, order=order, like=like)\n",
      "   339|         0|            0|            0|  0.00%|\n",
      "   340|         0|            0|            0|  0.00%|    if dtype is None:\n",
      "   341|         0|            0|            0|  0.00%|        fill_value = asarray(fill_value)\n",
      "   342|         0|            0|            0|  0.00%|        dtype = fill_value.dtype\n",
      "   343|         0|            0|            0|  0.00%|    a = empty(shape, dtype, order)\n",
      "   344|         0|            0|            0|  0.00%|    multiarray.copyto(a, fill_value, casting='unsafe')\n",
      "   345|         0|            0|            0|  0.00%|    return a\n",
      "   346|         0|            0|            0|  0.00%|\n",
      "   347|         0|            0|            0|  0.00%|\n",
      "   348|         0|            0|            0|  0.00%|_full_with_like = array_function_dispatch(\n",
      "   349|         0|            0|            0|  0.00%|    _full_dispatcher\n",
      "   350|         0|            0|            0|  0.00%|)(full)\n",
      "   351|         0|            0|            0|  0.00%|\n",
      "   352|         0|            0|            0|  0.00%|\n",
      "   353|         0|            0|            0|  0.00%|def _full_like_dispatcher(a, fill_value, dtype=None, order=None, subok=None, shape=None):\n",
      "   354|         0|            0|            0|  0.00%|    return (a,)\n",
      "   355|         0|            0|            0|  0.00%|\n",
      "   356|         0|            0|            0|  0.00%|\n",
      "   357|         0|            0|            0|  0.00%|@array_function_dispatch(_full_like_dispatcher)\n",
      "   358|         0|            0|            0|  0.00%|def full_like(a, fill_value, dtype=None, order='K', subok=True, shape=None):\n",
      "   359|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   360|         0|            0|            0|  0.00%|    Return a full array with the same shape and type as a given array.\n",
      "   361|         0|            0|            0|  0.00%|\n",
      "   362|         0|            0|            0|  0.00%|    Parameters\n",
      "   363|         0|            0|            0|  0.00%|    ----------\n",
      "   364|         0|            0|            0|  0.00%|    a : array_like\n",
      "   365|         0|            0|            0|  0.00%|        The shape and data-type of `a` define these same attributes of\n",
      "   366|         0|            0|            0|  0.00%|        the returned array.\n",
      "   367|         0|            0|            0|  0.00%|    fill_value : scalar\n",
      "   368|         0|            0|            0|  0.00%|        Fill value.\n",
      "   369|         0|            0|            0|  0.00%|    dtype : data-type, optional\n",
      "   370|         0|            0|            0|  0.00%|        Overrides the data type of the result.\n",
      "   371|         0|            0|            0|  0.00%|    order : {'C', 'F', 'A', or 'K'}, optional\n",
      "   372|         0|            0|            0|  0.00%|        Overrides the memory layout of the result. 'C' means C-order,\n",
      "   373|         0|            0|            0|  0.00%|        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n",
      "   374|         0|            0|            0|  0.00%|        'C' otherwise. 'K' means match the layout of `a` as closely\n",
      "   375|         0|            0|            0|  0.00%|        as possible.\n",
      "   376|         0|            0|            0|  0.00%|    subok : bool, optional.\n",
      "   377|         0|            0|            0|  0.00%|        If True, then the newly created array will use the sub-class\n",
      "   378|         0|            0|            0|  0.00%|        type of `a`, otherwise it will be a base-class array. Defaults\n",
      "   379|         0|            0|            0|  0.00%|        to True.\n",
      "   380|         0|            0|            0|  0.00%|    shape : int or sequence of ints, optional.\n",
      "   381|         0|            0|            0|  0.00%|        Overrides the shape of the result. If order='K' and the number of\n",
      "   382|         0|            0|            0|  0.00%|        dimensions is unchanged, will try to keep order, otherwise,\n",
      "   383|         0|            0|            0|  0.00%|        order='C' is implied.\n",
      "   384|         0|            0|            0|  0.00%|\n",
      "   385|         0|            0|            0|  0.00%|        .. versionadded:: 1.17.0\n",
      "   386|         0|            0|            0|  0.00%|\n",
      "   387|         0|            0|            0|  0.00%|    Returns\n",
      "   388|         0|            0|            0|  0.00%|    -------\n",
      "   389|         0|            0|            0|  0.00%|    out : ndarray\n",
      "   390|         0|            0|            0|  0.00%|        Array of `fill_value` with the same shape and type as `a`.\n",
      "   391|         0|            0|            0|  0.00%|\n",
      "   392|         0|            0|            0|  0.00%|    See Also\n",
      "   393|         0|            0|            0|  0.00%|    --------\n",
      "   394|         0|            0|            0|  0.00%|    empty_like : Return an empty array with shape and type of input.\n",
      "   395|         0|            0|            0|  0.00%|    ones_like : Return an array of ones with shape and type of input.\n",
      "   396|         0|            0|            0|  0.00%|    zeros_like : Return an array of zeros with shape and type of input.\n",
      "   397|         0|            0|            0|  0.00%|    full : Return a new array of given shape filled with value.\n",
      "   398|         0|            0|            0|  0.00%|\n",
      "   399|         0|            0|            0|  0.00%|    Examples\n",
      "   400|         0|            0|            0|  0.00%|    --------\n",
      "   401|         0|            0|            0|  0.00%|    >>> x = np.arange(6, dtype=int)\n",
      "   402|         0|            0|            0|  0.00%|    >>> np.full_like(x, 1)\n",
      "   403|         0|            0|            0|  0.00%|    array([1, 1, 1, 1, 1, 1])\n",
      "   404|         0|            0|            0|  0.00%|    >>> np.full_like(x, 0.1)\n",
      "   405|         0|            0|            0|  0.00%|    array([0, 0, 0, 0, 0, 0])\n",
      "   406|         0|            0|            0|  0.00%|    >>> np.full_like(x, 0.1, dtype=np.double)\n",
      "   407|         0|            0|            0|  0.00%|    array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
      "   408|         0|            0|            0|  0.00%|    >>> np.full_like(x, np.nan, dtype=np.double)\n",
      "   409|         0|            0|            0|  0.00%|    array([nan, nan, nan, nan, nan, nan])\n",
      "   410|         0|            0|            0|  0.00%|\n",
      "   411|         0|            0|            0|  0.00%|    >>> y = np.arange(6, dtype=np.double)\n",
      "   412|         0|            0|            0|  0.00%|    >>> np.full_like(y, 0.1)\n",
      "   413|         0|            0|            0|  0.00%|    array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
      "   414|         0|            0|            0|  0.00%|\n",
      "   415|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   416|         0|            0|            0|  0.00%|    res = empty_like(a, dtype=dtype, order=order, subok=subok, shape=shape)\n",
      "   417|         0|            0|            0|  0.00%|    multiarray.copyto(res, fill_value, casting='unsafe')\n",
      "   418|         0|            0|            0|  0.00%|    return res\n",
      "   419|         0|            0|            0|  0.00%|\n",
      "   420|         0|            0|            0|  0.00%|\n",
      "   421|         0|            0|            0|  0.00%|def _count_nonzero_dispatcher(a, axis=None, *, keepdims=None):\n",
      "   422|         0|            0|            0|  0.00%|    return (a,)\n",
      "   423|         0|            0|            0|  0.00%|\n",
      "   424|         0|            0|            0|  0.00%|\n",
      "   425|         0|            0|            0|  0.00%|@array_function_dispatch(_count_nonzero_dispatcher)\n",
      "   426|         0|            0|            0|  0.00%|def count_nonzero(a, axis=None, *, keepdims=False):\n",
      "   427|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   428|         0|            0|            0|  0.00%|    Counts the number of non-zero values in the array ``a``.\n",
      "   429|         0|            0|            0|  0.00%|\n",
      "   430|         0|            0|            0|  0.00%|    The word \"non-zero\" is in reference to the Python 2.x\n",
      "   431|         0|            0|            0|  0.00%|    built-in method ``__nonzero__()`` (renamed ``__bool__()``\n",
      "   432|         0|            0|            0|  0.00%|    in Python 3.x) of Python objects that tests an object's\n",
      "   433|         0|            0|            0|  0.00%|    \"truthfulness\". For example, any number is considered\n",
      "   434|         0|            0|            0|  0.00%|    truthful if it is nonzero, whereas any string is considered\n",
      "   435|         0|            0|            0|  0.00%|    truthful if it is not the empty string. Thus, this function\n",
      "   436|         0|            0|            0|  0.00%|    (recursively) counts how many elements in ``a`` (and in\n",
      "   437|         0|            0|            0|  0.00%|    sub-arrays thereof) have their ``__nonzero__()`` or ``__bool__()``\n",
      "   438|         0|            0|            0|  0.00%|    method evaluated to ``True``.\n",
      "   439|         0|            0|            0|  0.00%|\n",
      "   440|         0|            0|            0|  0.00%|    Parameters\n",
      "   441|         0|            0|            0|  0.00%|    ----------\n",
      "   442|         0|            0|            0|  0.00%|    a : array_like\n",
      "   443|         0|            0|            0|  0.00%|        The array for which to count non-zeros.\n",
      "   444|         0|            0|            0|  0.00%|    axis : int or tuple, optional\n",
      "   445|         0|            0|            0|  0.00%|        Axis or tuple of axes along which to count non-zeros.\n",
      "   446|         0|            0|            0|  0.00%|        Default is None, meaning that non-zeros will be counted\n",
      "   447|         0|            0|            0|  0.00%|        along a flattened version of ``a``.\n",
      "   448|         0|            0|            0|  0.00%|\n",
      "   449|         0|            0|            0|  0.00%|        .. versionadded:: 1.12.0\n",
      "   450|         0|            0|            0|  0.00%|\n",
      "   451|         0|            0|            0|  0.00%|    keepdims : bool, optional\n",
      "   452|         0|            0|            0|  0.00%|        If this is set to True, the axes that are counted are left\n",
      "   453|         0|            0|            0|  0.00%|        in the result as dimensions with size one. With this option,\n",
      "   454|         0|            0|            0|  0.00%|        the result will broadcast correctly against the input array.\n",
      "   455|         0|            0|            0|  0.00%|\n",
      "   456|         0|            0|            0|  0.00%|        .. versionadded:: 1.19.0\n",
      "   457|         0|            0|            0|  0.00%|\n",
      "   458|         0|            0|            0|  0.00%|    Returns\n",
      "   459|         0|            0|            0|  0.00%|    -------\n",
      "   460|         0|            0|            0|  0.00%|    count : int or array of int\n",
      "   461|         0|            0|            0|  0.00%|        Number of non-zero values in the array along a given axis.\n",
      "   462|         0|            0|            0|  0.00%|        Otherwise, the total number of non-zero values in the array\n",
      "   463|         0|            0|            0|  0.00%|        is returned.\n",
      "   464|         0|            0|            0|  0.00%|\n",
      "   465|         0|            0|            0|  0.00%|    See Also\n",
      "   466|         0|            0|            0|  0.00%|    --------\n",
      "   467|         0|            0|            0|  0.00%|    nonzero : Return the coordinates of all the non-zero values.\n",
      "   468|         0|            0|            0|  0.00%|\n",
      "   469|         0|            0|            0|  0.00%|    Examples\n",
      "   470|         0|            0|            0|  0.00%|    --------\n",
      "   471|         0|            0|            0|  0.00%|    >>> np.count_nonzero(np.eye(4))\n",
      "   472|         0|            0|            0|  0.00%|    4\n",
      "   473|         0|            0|            0|  0.00%|    >>> a = np.array([[0, 1, 7, 0],\n",
      "   474|         0|            0|            0|  0.00%|    ...               [3, 0, 2, 19]])\n",
      "   475|         0|            0|            0|  0.00%|    >>> np.count_nonzero(a)\n",
      "   476|         0|            0|            0|  0.00%|    5\n",
      "   477|         0|            0|            0|  0.00%|    >>> np.count_nonzero(a, axis=0)\n",
      "   478|         0|            0|            0|  0.00%|    array([1, 1, 2, 1])\n",
      "   479|         0|            0|            0|  0.00%|    >>> np.count_nonzero(a, axis=1)\n",
      "   480|         0|            0|            0|  0.00%|    array([2, 3])\n",
      "   481|         0|            0|            0|  0.00%|    >>> np.count_nonzero(a, axis=1, keepdims=True)\n",
      "   482|         0|            0|            0|  0.00%|    array([[2],\n",
      "   483|         0|            0|            0|  0.00%|           [3]])\n",
      "   484|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   485|         0|            0|            0|  0.00%|    if axis is None and not keepdims:\n",
      "   486|         0|            0|            0|  0.00%|        return multiarray.count_nonzero(a)\n",
      "   487|         0|            0|            0|  0.00%|\n",
      "   488|         0|            0|            0|  0.00%|    a = asanyarray(a)\n",
      "   489|         0|            0|            0|  0.00%|\n",
      "   490|         0|            0|            0|  0.00%|    # TODO: this works around .astype(bool) not working properly (gh-9847)\n",
      "   491|         0|            0|            0|  0.00%|    if np.issubdtype(a.dtype, np.character):\n",
      "   492|         0|            0|            0|  0.00%|        a_bool = a != a.dtype.type()\n",
      "   493|         0|            0|            0|  0.00%|    else:\n",
      "   494|         0|            0|            0|  0.00%|        a_bool = a.astype(np.bool_, copy=False)\n",
      "   495|         0|            0|            0|  0.00%|\n",
      "   496|         0|            0|            0|  0.00%|    return a_bool.sum(axis=axis, dtype=np.intp, keepdims=keepdims)\n",
      "   497|         0|            0|            0|  0.00%|\n",
      "   498|         0|            0|            0|  0.00%|\n",
      "   499|         0|            0|            0|  0.00%|@set_module('numpy')\n",
      "   500|         0|            0|            0|  0.00%|def isfortran(a):\n",
      "   501|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   502|         0|            0|            0|  0.00%|    Check if the array is Fortran contiguous but *not* C contiguous.\n",
      "   503|         0|            0|            0|  0.00%|\n",
      "   504|         0|            0|            0|  0.00%|    This function is obsolete and, because of changes due to relaxed stride\n",
      "   505|         0|            0|            0|  0.00%|    checking, its return value for the same array may differ for versions\n",
      "   506|         0|            0|            0|  0.00%|    of NumPy >= 1.10.0 and previous versions. If you only want to check if an\n",
      "   507|         0|            0|            0|  0.00%|    array is Fortran contiguous use ``a.flags.f_contiguous`` instead.\n",
      "   508|         0|            0|            0|  0.00%|\n",
      "   509|         0|            0|            0|  0.00%|    Parameters\n",
      "   510|         0|            0|            0|  0.00%|    ----------\n",
      "   511|         0|            0|            0|  0.00%|    a : ndarray\n",
      "   512|         0|            0|            0|  0.00%|        Input array.\n",
      "   513|         0|            0|            0|  0.00%|\n",
      "   514|         0|            0|            0|  0.00%|    Returns\n",
      "   515|         0|            0|            0|  0.00%|    -------\n",
      "   516|         0|            0|            0|  0.00%|    isfortran : bool\n",
      "   517|         0|            0|            0|  0.00%|        Returns True if the array is Fortran contiguous but *not* C contiguous.\n",
      "   518|         0|            0|            0|  0.00%|\n",
      "   519|         0|            0|            0|  0.00%|\n",
      "   520|         0|            0|            0|  0.00%|    Examples\n",
      "   521|         0|            0|            0|  0.00%|    --------\n",
      "   522|         0|            0|            0|  0.00%|\n",
      "   523|         0|            0|            0|  0.00%|    np.array allows to specify whether the array is written in C-contiguous\n",
      "   524|         0|            0|            0|  0.00%|    order (last index varies the fastest), or FORTRAN-contiguous order in\n",
      "   525|         0|            0|            0|  0.00%|    memory (first index varies the fastest).\n",
      "   526|         0|            0|            0|  0.00%|\n",
      "   527|         0|            0|            0|  0.00%|    >>> a = np.array([[1, 2, 3], [4, 5, 6]], order='C')\n",
      "   528|         0|            0|            0|  0.00%|    >>> a\n",
      "   529|         0|            0|            0|  0.00%|    array([[1, 2, 3],\n",
      "   530|         0|            0|            0|  0.00%|           [4, 5, 6]])\n",
      "   531|         0|            0|            0|  0.00%|    >>> np.isfortran(a)\n",
      "   532|         0|            0|            0|  0.00%|    False\n",
      "   533|         0|            0|            0|  0.00%|\n",
      "   534|         0|            0|            0|  0.00%|    >>> b = np.array([[1, 2, 3], [4, 5, 6]], order='F')\n",
      "   535|         0|            0|            0|  0.00%|    >>> b\n",
      "   536|         0|            0|            0|  0.00%|    array([[1, 2, 3],\n",
      "   537|         0|            0|            0|  0.00%|           [4, 5, 6]])\n",
      "   538|         0|            0|            0|  0.00%|    >>> np.isfortran(b)\n",
      "   539|         0|            0|            0|  0.00%|    True\n",
      "   540|         0|            0|            0|  0.00%|\n",
      "   541|         0|            0|            0|  0.00%|\n",
      "   542|         0|            0|            0|  0.00%|    The transpose of a C-ordered array is a FORTRAN-ordered array.\n",
      "   543|         0|            0|            0|  0.00%|\n",
      "   544|         0|            0|            0|  0.00%|    >>> a = np.array([[1, 2, 3], [4, 5, 6]], order='C')\n",
      "   545|         0|            0|            0|  0.00%|    >>> a\n",
      "   546|         0|            0|            0|  0.00%|    array([[1, 2, 3],\n",
      "   547|         0|            0|            0|  0.00%|           [4, 5, 6]])\n",
      "   548|         0|            0|            0|  0.00%|    >>> np.isfortran(a)\n",
      "   549|         0|            0|            0|  0.00%|    False\n",
      "   550|         0|            0|            0|  0.00%|    >>> b = a.T\n",
      "   551|         0|            0|            0|  0.00%|    >>> b\n",
      "   552|         0|            0|            0|  0.00%|    array([[1, 4],\n",
      "   553|         0|            0|            0|  0.00%|           [2, 5],\n",
      "   554|         0|            0|            0|  0.00%|           [3, 6]])\n",
      "   555|         0|            0|            0|  0.00%|    >>> np.isfortran(b)\n",
      "   556|         0|            0|            0|  0.00%|    True\n",
      "   557|         0|            0|            0|  0.00%|\n",
      "   558|         0|            0|            0|  0.00%|    C-ordered arrays evaluate as False even if they are also FORTRAN-ordered.\n",
      "   559|         0|            0|            0|  0.00%|\n",
      "   560|         0|            0|            0|  0.00%|    >>> np.isfortran(np.array([1, 2], order='F'))\n",
      "   561|         0|            0|            0|  0.00%|    False\n",
      "   562|         0|            0|            0|  0.00%|\n",
      "   563|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   564|         0|            0|            0|  0.00%|    return a.flags.fnc\n",
      "   565|         0|            0|            0|  0.00%|\n",
      "   566|         0|            0|            0|  0.00%|\n",
      "   567|         0|            0|            0|  0.00%|def _argwhere_dispatcher(a):\n",
      "   568|         0|            0|            0|  0.00%|    return (a,)\n",
      "   569|         0|            0|            0|  0.00%|\n",
      "   570|         0|            0|            0|  0.00%|\n",
      "   571|         0|            0|            0|  0.00%|@array_function_dispatch(_argwhere_dispatcher)\n",
      "   572|         0|            0|            0|  0.00%|def argwhere(a):\n",
      "   573|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   574|         0|            0|            0|  0.00%|    Find the indices of array elements that are non-zero, grouped by element.\n",
      "   575|         0|            0|            0|  0.00%|\n",
      "   576|         0|            0|            0|  0.00%|    Parameters\n",
      "   577|         0|            0|            0|  0.00%|    ----------\n",
      "   578|         0|            0|            0|  0.00%|    a : array_like\n",
      "   579|         0|            0|            0|  0.00%|        Input data.\n",
      "   580|         0|            0|            0|  0.00%|\n",
      "   581|         0|            0|            0|  0.00%|    Returns\n",
      "   582|         0|            0|            0|  0.00%|    -------\n",
      "   583|         0|            0|            0|  0.00%|    index_array : (N, a.ndim) ndarray\n",
      "   584|         0|            0|            0|  0.00%|        Indices of elements that are non-zero. Indices are grouped by element.\n",
      "   585|         0|            0|            0|  0.00%|        This array will have shape ``(N, a.ndim)`` where ``N`` is the number of\n",
      "   586|         0|            0|            0|  0.00%|        non-zero items.\n",
      "   587|         0|            0|            0|  0.00%|\n",
      "   588|         0|            0|            0|  0.00%|    See Also\n",
      "   589|         0|            0|            0|  0.00%|    --------\n",
      "   590|         0|            0|            0|  0.00%|    where, nonzero\n",
      "   591|         0|            0|            0|  0.00%|\n",
      "   592|         0|            0|            0|  0.00%|    Notes\n",
      "   593|         0|            0|            0|  0.00%|    -----\n",
      "   594|         0|            0|            0|  0.00%|    ``np.argwhere(a)`` is almost the same as ``np.transpose(np.nonzero(a))``,\n",
      "   595|         0|            0|            0|  0.00%|    but produces a result of the correct shape for a 0D array.\n",
      "   596|         0|            0|            0|  0.00%|\n",
      "   597|         0|            0|            0|  0.00%|    The output of ``argwhere`` is not suitable for indexing arrays.\n",
      "   598|         0|            0|            0|  0.00%|    For this purpose use ``nonzero(a)`` instead.\n",
      "   599|         0|            0|            0|  0.00%|\n",
      "   600|         0|            0|            0|  0.00%|    Examples\n",
      "   601|         0|            0|            0|  0.00%|    --------\n",
      "   602|         0|            0|            0|  0.00%|    >>> x = np.arange(6).reshape(2,3)\n",
      "   603|         0|            0|            0|  0.00%|    >>> x\n",
      "   604|         0|            0|            0|  0.00%|    array([[0, 1, 2],\n",
      "   605|         0|            0|            0|  0.00%|           [3, 4, 5]])\n",
      "   606|         0|            0|            0|  0.00%|    >>> np.argwhere(x>1)\n",
      "   607|         0|            0|            0|  0.00%|    array([[0, 2],\n",
      "   608|         0|            0|            0|  0.00%|           [1, 0],\n",
      "   609|         0|            0|            0|  0.00%|           [1, 1],\n",
      "   610|         0|            0|            0|  0.00%|           [1, 2]])\n",
      "   611|         0|            0|            0|  0.00%|\n",
      "   612|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   613|         0|            0|            0|  0.00%|    # nonzero does not behave well on 0d, so promote to 1d\n",
      "   614|         0|            0|            0|  0.00%|    if np.ndim(a) == 0:\n",
      "   615|         0|            0|            0|  0.00%|        a = shape_base.atleast_1d(a)\n",
      "   616|         0|            0|            0|  0.00%|        # then remove the added dimension\n",
      "   617|         0|            0|            0|  0.00%|        return argwhere(a)[:,:0]\n",
      "   618|         0|            0|            0|  0.00%|    return transpose(nonzero(a))\n",
      "   619|         0|            0|            0|  0.00%|\n",
      "   620|         0|            0|            0|  0.00%|\n",
      "   621|         0|            0|            0|  0.00%|def _flatnonzero_dispatcher(a):\n",
      "   622|         0|            0|            0|  0.00%|    return (a,)\n",
      "   623|         0|            0|            0|  0.00%|\n",
      "   624|         0|            0|            0|  0.00%|\n",
      "   625|         0|            0|            0|  0.00%|@array_function_dispatch(_flatnonzero_dispatcher)\n",
      "   626|         0|            0|            0|  0.00%|def flatnonzero(a):\n",
      "   627|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   628|         0|            0|            0|  0.00%|    Return indices that are non-zero in the flattened version of a.\n",
      "   629|         0|            0|            0|  0.00%|\n",
      "   630|         0|            0|            0|  0.00%|    This is equivalent to np.nonzero(np.ravel(a))[0].\n",
      "   631|         0|            0|            0|  0.00%|\n",
      "   632|         0|            0|            0|  0.00%|    Parameters\n",
      "   633|         0|            0|            0|  0.00%|    ----------\n",
      "   634|         0|            0|            0|  0.00%|    a : array_like\n",
      "   635|         0|            0|            0|  0.00%|        Input data.\n",
      "   636|         0|            0|            0|  0.00%|\n",
      "   637|         0|            0|            0|  0.00%|    Returns\n",
      "   638|         0|            0|            0|  0.00%|    -------\n",
      "   639|         0|            0|            0|  0.00%|    res : ndarray\n",
      "   640|         0|            0|            0|  0.00%|        Output array, containing the indices of the elements of `a.ravel()`\n",
      "   641|         0|            0|            0|  0.00%|        that are non-zero.\n",
      "   642|         0|            0|            0|  0.00%|\n",
      "   643|         0|            0|            0|  0.00%|    See Also\n",
      "   644|         0|            0|            0|  0.00%|    --------\n",
      "   645|         0|            0|            0|  0.00%|    nonzero : Return the indices of the non-zero elements of the input array.\n",
      "   646|         0|            0|            0|  0.00%|    ravel : Return a 1-D array containing the elements of the input array.\n",
      "   647|         0|            0|            0|  0.00%|\n",
      "   648|         0|            0|            0|  0.00%|    Examples\n",
      "   649|         0|            0|            0|  0.00%|    --------\n",
      "   650|         0|            0|            0|  0.00%|    >>> x = np.arange(-2, 3)\n",
      "   651|         0|            0|            0|  0.00%|    >>> x\n",
      "   652|         0|            0|            0|  0.00%|    array([-2, -1,  0,  1,  2])\n",
      "   653|         0|            0|            0|  0.00%|    >>> np.flatnonzero(x)\n",
      "   654|         0|            0|            0|  0.00%|    array([0, 1, 3, 4])\n",
      "   655|         0|            0|            0|  0.00%|\n",
      "   656|         0|            0|            0|  0.00%|    Use the indices of the non-zero elements as an index array to extract\n",
      "   657|         0|            0|            0|  0.00%|    these elements:\n",
      "   658|         0|            0|            0|  0.00%|\n",
      "   659|         0|            0|            0|  0.00%|    >>> x.ravel()[np.flatnonzero(x)]\n",
      "   660|         0|            0|            0|  0.00%|    array([-2, -1,  1,  2])\n",
      "   661|         0|            0|            0|  0.00%|\n",
      "   662|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   663|         0|            0|            0|  0.00%|    return np.nonzero(np.ravel(a))[0]\n",
      "   664|         0|            0|            0|  0.00%|\n",
      "   665|         0|            0|            0|  0.00%|\n",
      "   666|         0|            0|            0|  0.00%|def _correlate_dispatcher(a, v, mode=None):\n",
      "   667|         0|            0|            0|  0.00%|    return (a, v)\n",
      "   668|         0|            0|            0|  0.00%|\n",
      "   669|         0|            0|            0|  0.00%|\n",
      "   670|         0|            0|            0|  0.00%|@array_function_dispatch(_correlate_dispatcher)\n",
      "   671|         0|            0|            0|  0.00%|def correlate(a, v, mode='valid'):\n",
      "   672|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   673|         0|            0|            0|  0.00%|    Cross-correlation of two 1-dimensional sequences.\n",
      "   674|         0|            0|            0|  0.00%|\n",
      "   675|         0|            0|            0|  0.00%|    This function computes the correlation as generally defined in signal\n",
      "   676|         0|            0|            0|  0.00%|    processing texts::\n",
      "   677|         0|            0|            0|  0.00%|\n",
      "   678|         0|            0|            0|  0.00%|        c_{av}[k] = sum_n a[n+k] * conj(v[n])\n",
      "   679|         0|            0|            0|  0.00%|\n",
      "   680|         0|            0|            0|  0.00%|    with a and v sequences being zero-padded where necessary and conj being\n",
      "   681|         0|            0|            0|  0.00%|    the conjugate.\n",
      "   682|         0|            0|            0|  0.00%|\n",
      "   683|         0|            0|            0|  0.00%|    Parameters\n",
      "   684|         0|            0|            0|  0.00%|    ----------\n",
      "   685|         0|            0|            0|  0.00%|    a, v : array_like\n",
      "   686|         0|            0|            0|  0.00%|        Input sequences.\n",
      "   687|         0|            0|            0|  0.00%|    mode : {'valid', 'same', 'full'}, optional\n",
      "   688|         0|            0|            0|  0.00%|        Refer to the `convolve` docstring.  Note that the default\n",
      "   689|         0|            0|            0|  0.00%|        is 'valid', unlike `convolve`, which uses 'full'.\n",
      "   690|         0|            0|            0|  0.00%|    old_behavior : bool\n",
      "   691|         0|            0|            0|  0.00%|        `old_behavior` was removed in NumPy 1.10. If you need the old\n",
      "   692|         0|            0|            0|  0.00%|        behavior, use `multiarray.correlate`.\n",
      "   693|         0|            0|            0|  0.00%|\n",
      "   694|         0|            0|            0|  0.00%|    Returns\n",
      "   695|         0|            0|            0|  0.00%|    -------\n",
      "   696|         0|            0|            0|  0.00%|    out : ndarray\n",
      "   697|         0|            0|            0|  0.00%|        Discrete cross-correlation of `a` and `v`.\n",
      "   698|         0|            0|            0|  0.00%|\n",
      "   699|         0|            0|            0|  0.00%|    See Also\n",
      "   700|         0|            0|            0|  0.00%|    --------\n",
      "   701|         0|            0|            0|  0.00%|    convolve : Discrete, linear convolution of two one-dimensional sequences.\n",
      "   702|         0|            0|            0|  0.00%|    multiarray.correlate : Old, no conjugate, version of correlate.\n",
      "   703|         0|            0|            0|  0.00%|    scipy.signal.correlate : uses FFT which has superior performance on large arrays.\n",
      "   704|         0|            0|            0|  0.00%|\n",
      "   705|         0|            0|            0|  0.00%|    Notes\n",
      "   706|         0|            0|            0|  0.00%|    -----\n",
      "   707|         0|            0|            0|  0.00%|    The definition of correlation above is not unique and sometimes correlation\n",
      "   708|         0|            0|            0|  0.00%|    may be defined differently. Another common definition is::\n",
      "   709|         0|            0|            0|  0.00%|\n",
      "   710|         0|            0|            0|  0.00%|        c'_{av}[k] = sum_n a[n] conj(v[n+k])\n",
      "   711|         0|            0|            0|  0.00%|\n",
      "   712|         0|            0|            0|  0.00%|    which is related to ``c_{av}[k]`` by ``c'_{av}[k] = c_{av}[-k]``.\n",
      "   713|         0|            0|            0|  0.00%|\n",
      "   714|         0|            0|            0|  0.00%|    `numpy.correlate` may perform slowly in large arrays (i.e. n = 1e5) because it does\n",
      "   715|         0|            0|            0|  0.00%|    not use the FFT to compute the convolution; in that case, `scipy.signal.correlate` might\n",
      "   716|         0|            0|            0|  0.00%|    be preferable.\n",
      "   717|         0|            0|            0|  0.00%|\n",
      "   718|         0|            0|            0|  0.00%|\n",
      "   719|         0|            0|            0|  0.00%|    Examples\n",
      "   720|         0|            0|            0|  0.00%|    --------\n",
      "   721|         0|            0|            0|  0.00%|    >>> np.correlate([1, 2, 3], [0, 1, 0.5])\n",
      "   722|         0|            0|            0|  0.00%|    array([3.5])\n",
      "   723|         0|            0|            0|  0.00%|    >>> np.correlate([1, 2, 3], [0, 1, 0.5], \"same\")\n",
      "   724|         0|            0|            0|  0.00%|    array([2. ,  3.5,  3. ])\n",
      "   725|         0|            0|            0|  0.00%|    >>> np.correlate([1, 2, 3], [0, 1, 0.5], \"full\")\n",
      "   726|         0|            0|            0|  0.00%|    array([0.5,  2. ,  3.5,  3. ,  0. ])\n",
      "   727|         0|            0|            0|  0.00%|\n",
      "   728|         0|            0|            0|  0.00%|    Using complex sequences:\n",
      "   729|         0|            0|            0|  0.00%|\n",
      "   730|         0|            0|            0|  0.00%|    >>> np.correlate([1+1j, 2, 3-1j], [0, 1, 0.5j], 'full')\n",
      "   731|         0|            0|            0|  0.00%|    array([ 0.5-0.5j,  1.0+0.j ,  1.5-1.5j,  3.0-1.j ,  0.0+0.j ])\n",
      "   732|         0|            0|            0|  0.00%|\n",
      "   733|         0|            0|            0|  0.00%|    Note that you get the time reversed, complex conjugated result\n",
      "   734|         0|            0|            0|  0.00%|    when the two input sequences change places, i.e.,\n",
      "   735|         0|            0|            0|  0.00%|    ``c_{va}[k] = c^{*}_{av}[-k]``:\n",
      "   736|         0|            0|            0|  0.00%|\n",
      "   737|         0|            0|            0|  0.00%|    >>> np.correlate([0, 1, 0.5j], [1+1j, 2, 3-1j], 'full')\n",
      "   738|         0|            0|            0|  0.00%|    array([ 0.0+0.j ,  3.0+1.j ,  1.5+1.5j,  1.0+0.j ,  0.5+0.5j])\n",
      "   739|         0|            0|            0|  0.00%|\n",
      "   740|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   741|         0|            0|            0|  0.00%|    return multiarray.correlate2(a, v, mode)\n",
      "   742|         0|            0|            0|  0.00%|\n",
      "   743|         0|            0|            0|  0.00%|\n",
      "   744|         0|            0|            0|  0.00%|def _convolve_dispatcher(a, v, mode=None):\n",
      "   745|         0|            0|            0|  0.00%|    return (a, v)\n",
      "   746|         0|            0|            0|  0.00%|\n",
      "   747|         0|            0|            0|  0.00%|\n",
      "   748|         0|            0|            0|  0.00%|@array_function_dispatch(_convolve_dispatcher)\n",
      "   749|         0|            0|            0|  0.00%|def convolve(a, v, mode='full'):\n",
      "   750|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   751|         0|            0|            0|  0.00%|    Returns the discrete, linear convolution of two one-dimensional sequences.\n",
      "   752|         0|            0|            0|  0.00%|\n",
      "   753|         0|            0|            0|  0.00%|    The convolution operator is often seen in signal processing, where it\n",
      "   754|         0|            0|            0|  0.00%|    models the effect of a linear time-invariant system on a signal [1]_.  In\n",
      "   755|         0|            0|            0|  0.00%|    probability theory, the sum of two independent random variables is\n",
      "   756|         0|            0|            0|  0.00%|    distributed according to the convolution of their individual\n",
      "   757|         0|            0|            0|  0.00%|    distributions.\n",
      "   758|         0|            0|            0|  0.00%|\n",
      "   759|         0|            0|            0|  0.00%|    If `v` is longer than `a`, the arrays are swapped before computation.\n",
      "   760|         0|            0|            0|  0.00%|\n",
      "   761|         0|            0|            0|  0.00%|    Parameters\n",
      "   762|         0|            0|            0|  0.00%|    ----------\n",
      "   763|         0|            0|            0|  0.00%|    a : (N,) array_like\n",
      "   764|         0|            0|            0|  0.00%|        First one-dimensional input array.\n",
      "   765|         0|            0|            0|  0.00%|    v : (M,) array_like\n",
      "   766|         0|            0|            0|  0.00%|        Second one-dimensional input array.\n",
      "   767|         0|            0|            0|  0.00%|    mode : {'full', 'valid', 'same'}, optional\n",
      "   768|         0|            0|            0|  0.00%|        'full':\n",
      "   769|         0|            0|            0|  0.00%|          By default, mode is 'full'.  This returns the convolution\n",
      "   770|         0|            0|            0|  0.00%|          at each point of overlap, with an output shape of (N+M-1,). At\n",
      "   771|         0|            0|            0|  0.00%|          the end-points of the convolution, the signals do not overlap\n",
      "   772|         0|            0|            0|  0.00%|          completely, and boundary effects may be seen.\n",
      "   773|         0|            0|            0|  0.00%|\n",
      "   774|         0|            0|            0|  0.00%|        'same':\n",
      "   775|         0|            0|            0|  0.00%|          Mode 'same' returns output of length ``max(M, N)``.  Boundary\n",
      "   776|         0|            0|            0|  0.00%|          effects are still visible.\n",
      "   777|         0|            0|            0|  0.00%|\n",
      "   778|         0|            0|            0|  0.00%|        'valid':\n",
      "   779|         0|            0|            0|  0.00%|          Mode 'valid' returns output of length\n",
      "   780|         0|            0|            0|  0.00%|          ``max(M, N) - min(M, N) + 1``.  The convolution product is only given\n",
      "   781|         0|            0|            0|  0.00%|          for points where the signals overlap completely.  Values outside\n",
      "   782|         0|            0|            0|  0.00%|          the signal boundary have no effect.\n",
      "   783|         0|            0|            0|  0.00%|\n",
      "   784|         0|            0|            0|  0.00%|    Returns\n",
      "   785|         0|            0|            0|  0.00%|    -------\n",
      "   786|         0|            0|            0|  0.00%|    out : ndarray\n",
      "   787|         0|            0|            0|  0.00%|        Discrete, linear convolution of `a` and `v`.\n",
      "   788|         0|            0|            0|  0.00%|\n",
      "   789|         0|            0|            0|  0.00%|    See Also\n",
      "   790|         0|            0|            0|  0.00%|    --------\n",
      "   791|         0|            0|            0|  0.00%|    scipy.signal.fftconvolve : Convolve two arrays using the Fast Fourier\n",
      "   792|         0|            0|            0|  0.00%|                               Transform.\n",
      "   793|         0|            0|            0|  0.00%|    scipy.linalg.toeplitz : Used to construct the convolution operator.\n",
      "   794|         0|            0|            0|  0.00%|    polymul : Polynomial multiplication. Same output as convolve, but also\n",
      "   795|         0|            0|            0|  0.00%|              accepts poly1d objects as input.\n",
      "   796|         0|            0|            0|  0.00%|\n",
      "   797|         0|            0|            0|  0.00%|    Notes\n",
      "   798|         0|            0|            0|  0.00%|    -----\n",
      "   799|         0|            0|            0|  0.00%|    The discrete convolution operation is defined as\n",
      "   800|         0|            0|            0|  0.00%|\n",
      "   801|         0|            0|            0|  0.00%|    .. math:: (a * v)[n] = \\\\sum_{m = -\\\\infty}^{\\\\infty} a[m] v[n - m]\n",
      "   802|         0|            0|            0|  0.00%|\n",
      "   803|         0|            0|            0|  0.00%|    It can be shown that a convolution :math:`x(t) * y(t)` in time/space\n",
      "   804|         0|            0|            0|  0.00%|    is equivalent to the multiplication :math:`X(f) Y(f)` in the Fourier\n",
      "   805|         0|            0|            0|  0.00%|    domain, after appropriate padding (padding is necessary to prevent\n",
      "   806|         0|            0|            0|  0.00%|    circular convolution).  Since multiplication is more efficient (faster)\n",
      "   807|         0|            0|            0|  0.00%|    than convolution, the function `scipy.signal.fftconvolve` exploits the\n",
      "   808|         0|            0|            0|  0.00%|    FFT to calculate the convolution of large data-sets.\n",
      "   809|         0|            0|            0|  0.00%|\n",
      "   810|         0|            0|            0|  0.00%|    References\n",
      "   811|         0|            0|            0|  0.00%|    ----------\n",
      "   812|         0|            0|            0|  0.00%|    .. [1] Wikipedia, \"Convolution\",\n",
      "   813|         0|            0|            0|  0.00%|        https://en.wikipedia.org/wiki/Convolution\n",
      "   814|         0|            0|            0|  0.00%|\n",
      "   815|         0|            0|            0|  0.00%|    Examples\n",
      "   816|         0|            0|            0|  0.00%|    --------\n",
      "   817|         0|            0|            0|  0.00%|    Note how the convolution operator flips the second array\n",
      "   818|         0|            0|            0|  0.00%|    before \"sliding\" the two across one another:\n",
      "   819|         0|            0|            0|  0.00%|\n",
      "   820|         0|            0|            0|  0.00%|    >>> np.convolve([1, 2, 3], [0, 1, 0.5])\n",
      "   821|         0|            0|            0|  0.00%|    array([0. , 1. , 2.5, 4. , 1.5])\n",
      "   822|         0|            0|            0|  0.00%|\n",
      "   823|         0|            0|            0|  0.00%|    Only return the middle values of the convolution.\n",
      "   824|         0|            0|            0|  0.00%|    Contains boundary effects, where zeros are taken\n",
      "   825|         0|            0|            0|  0.00%|    into account:\n",
      "   826|         0|            0|            0|  0.00%|\n",
      "   827|         0|            0|            0|  0.00%|    >>> np.convolve([1,2,3],[0,1,0.5], 'same')\n",
      "   828|         0|            0|            0|  0.00%|    array([1. ,  2.5,  4. ])\n",
      "   829|         0|            0|            0|  0.00%|\n",
      "   830|         0|            0|            0|  0.00%|    The two arrays are of the same length, so there\n",
      "   831|         0|            0|            0|  0.00%|    is only one position where they completely overlap:\n",
      "   832|         0|            0|            0|  0.00%|\n",
      "   833|         0|            0|            0|  0.00%|    >>> np.convolve([1,2,3],[0,1,0.5], 'valid')\n",
      "   834|         0|            0|            0|  0.00%|    array([2.5])\n",
      "   835|         0|            0|            0|  0.00%|\n",
      "   836|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   837|         0|            0|            0|  0.00%|    a, v = array(a, copy=False, ndmin=1), array(v, copy=False, ndmin=1)\n",
      "   838|         0|            0|            0|  0.00%|    if (len(v) > len(a)):\n",
      "   839|         0|            0|            0|  0.00%|        a, v = v, a\n",
      "   840|         0|            0|            0|  0.00%|    if len(a) == 0:\n",
      "   841|         0|            0|            0|  0.00%|        raise ValueError('a cannot be empty')\n",
      "   842|         0|            0|            0|  0.00%|    if len(v) == 0:\n",
      "   843|         0|            0|            0|  0.00%|        raise ValueError('v cannot be empty')\n",
      "   844|         0|            0|            0|  0.00%|    return multiarray.correlate(a, v[::-1], mode)\n",
      "   845|         0|            0|            0|  0.00%|\n",
      "   846|         0|            0|            0|  0.00%|\n",
      "   847|         0|            0|            0|  0.00%|def _outer_dispatcher(a, b, out=None):\n",
      "   848|         0|            0|            0|  0.00%|    return (a, b, out)\n",
      "   849|         0|            0|            0|  0.00%|\n",
      "   850|         0|            0|            0|  0.00%|\n",
      "   851|         0|            0|            0|  0.00%|@array_function_dispatch(_outer_dispatcher)\n",
      "   852|         0|            0|            0|  0.00%|def outer(a, b, out=None):\n",
      "   853|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   854|         0|            0|            0|  0.00%|    Compute the outer product of two vectors.\n",
      "   855|         0|            0|            0|  0.00%|\n",
      "   856|         0|            0|            0|  0.00%|    Given two vectors, ``a = [a0, a1, ..., aM]`` and\n",
      "   857|         0|            0|            0|  0.00%|    ``b = [b0, b1, ..., bN]``,\n",
      "   858|         0|            0|            0|  0.00%|    the outer product [1]_ is::\n",
      "   859|         0|            0|            0|  0.00%|\n",
      "   860|         0|            0|            0|  0.00%|      [[a0*b0  a0*b1 ... a0*bN ]\n",
      "   861|         0|            0|            0|  0.00%|       [a1*b0    .\n",
      "   862|         0|            0|            0|  0.00%|       [ ...          .\n",
      "   863|         0|            0|            0|  0.00%|       [aM*b0            aM*bN ]]\n",
      "   864|         0|            0|            0|  0.00%|\n",
      "   865|         0|            0|            0|  0.00%|    Parameters\n",
      "   866|         0|            0|            0|  0.00%|    ----------\n",
      "   867|         0|            0|            0|  0.00%|    a : (M,) array_like\n",
      "   868|         0|            0|            0|  0.00%|        First input vector.  Input is flattened if\n",
      "   869|         0|            0|            0|  0.00%|        not already 1-dimensional.\n",
      "   870|         0|            0|            0|  0.00%|    b : (N,) array_like\n",
      "   871|         0|            0|            0|  0.00%|        Second input vector.  Input is flattened if\n",
      "   872|         0|            0|            0|  0.00%|        not already 1-dimensional.\n",
      "   873|         0|            0|            0|  0.00%|    out : (M, N) ndarray, optional\n",
      "   874|         0|            0|            0|  0.00%|        A location where the result is stored\n",
      "   875|         0|            0|            0|  0.00%|\n",
      "   876|         0|            0|            0|  0.00%|        .. versionadded:: 1.9.0\n",
      "   877|         0|            0|            0|  0.00%|\n",
      "   878|         0|            0|            0|  0.00%|    Returns\n",
      "   879|         0|            0|            0|  0.00%|    -------\n",
      "   880|         0|            0|            0|  0.00%|    out : (M, N) ndarray\n",
      "   881|         0|            0|            0|  0.00%|        ``out[i, j] = a[i] * b[j]``\n",
      "   882|         0|            0|            0|  0.00%|\n",
      "   883|         0|            0|            0|  0.00%|    See also\n",
      "   884|         0|            0|            0|  0.00%|    --------\n",
      "   885|         0|            0|            0|  0.00%|    inner\n",
      "   886|         0|            0|            0|  0.00%|    einsum : ``einsum('i,j->ij', a.ravel(), b.ravel())`` is the equivalent.\n",
      "   887|         0|            0|            0|  0.00%|    ufunc.outer : A generalization to dimensions other than 1D and other\n",
      "   888|         0|            0|            0|  0.00%|                  operations. ``np.multiply.outer(a.ravel(), b.ravel())``\n",
      "   889|         0|            0|            0|  0.00%|                  is the equivalent.\n",
      "   890|         0|            0|            0|  0.00%|    tensordot : ``np.tensordot(a.ravel(), b.ravel(), axes=((), ()))``\n",
      "   891|         0|            0|            0|  0.00%|                is the equivalent.\n",
      "   892|         0|            0|            0|  0.00%|\n",
      "   893|         0|            0|            0|  0.00%|    References\n",
      "   894|         0|            0|            0|  0.00%|    ----------\n",
      "   895|         0|            0|            0|  0.00%|    .. [1] : G. H. Golub and C. F. Van Loan, *Matrix Computations*, 3rd\n",
      "   896|         0|            0|            0|  0.00%|             ed., Baltimore, MD, Johns Hopkins University Press, 1996,\n",
      "   897|         0|            0|            0|  0.00%|             pg. 8.\n",
      "   898|         0|            0|            0|  0.00%|\n",
      "   899|         0|            0|            0|  0.00%|    Examples\n",
      "   900|         0|            0|            0|  0.00%|    --------\n",
      "   901|         0|            0|            0|  0.00%|    Make a (*very* coarse) grid for computing a Mandelbrot set:\n",
      "   902|         0|            0|            0|  0.00%|\n",
      "   903|         0|            0|            0|  0.00%|    >>> rl = np.outer(np.ones((5,)), np.linspace(-2, 2, 5))\n",
      "   904|         0|            0|            0|  0.00%|    >>> rl\n",
      "   905|         0|            0|            0|  0.00%|    array([[-2., -1.,  0.,  1.,  2.],\n",
      "   906|         0|            0|            0|  0.00%|           [-2., -1.,  0.,  1.,  2.],\n",
      "   907|         0|            0|            0|  0.00%|           [-2., -1.,  0.,  1.,  2.],\n",
      "   908|         0|            0|            0|  0.00%|           [-2., -1.,  0.,  1.,  2.],\n",
      "   909|         0|            0|            0|  0.00%|           [-2., -1.,  0.,  1.,  2.]])\n",
      "   910|         0|            0|            0|  0.00%|    >>> im = np.outer(1j*np.linspace(2, -2, 5), np.ones((5,)))\n",
      "   911|         0|            0|            0|  0.00%|    >>> im\n",
      "   912|         0|            0|            0|  0.00%|    array([[0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j],\n",
      "   913|         0|            0|            0|  0.00%|           [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\n",
      "   914|         0|            0|            0|  0.00%|           [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "   915|         0|            0|            0|  0.00%|           [0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j],\n",
      "   916|         0|            0|            0|  0.00%|           [0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j]])\n",
      "   917|         0|            0|            0|  0.00%|    >>> grid = rl + im\n",
      "   918|         0|            0|            0|  0.00%|    >>> grid\n",
      "   919|         0|            0|            0|  0.00%|    array([[-2.+2.j, -1.+2.j,  0.+2.j,  1.+2.j,  2.+2.j],\n",
      "   920|         0|            0|            0|  0.00%|           [-2.+1.j, -1.+1.j,  0.+1.j,  1.+1.j,  2.+1.j],\n",
      "   921|         0|            0|            0|  0.00%|           [-2.+0.j, -1.+0.j,  0.+0.j,  1.+0.j,  2.+0.j],\n",
      "   922|         0|            0|            0|  0.00%|           [-2.-1.j, -1.-1.j,  0.-1.j,  1.-1.j,  2.-1.j],\n",
      "   923|         0|            0|            0|  0.00%|           [-2.-2.j, -1.-2.j,  0.-2.j,  1.-2.j,  2.-2.j]])\n",
      "   924|         0|            0|            0|  0.00%|\n",
      "   925|         0|            0|            0|  0.00%|    An example using a \"vector\" of letters:\n",
      "   926|         0|            0|            0|  0.00%|\n",
      "   927|         0|            0|            0|  0.00%|    >>> x = np.array(['a', 'b', 'c'], dtype=object)\n",
      "   928|         0|            0|            0|  0.00%|    >>> np.outer(x, [1, 2, 3])\n",
      "   929|         0|            0|            0|  0.00%|    array([['a', 'aa', 'aaa'],\n",
      "   930|         0|            0|            0|  0.00%|           ['b', 'bb', 'bbb'],\n",
      "   931|         0|            0|            0|  0.00%|           ['c', 'cc', 'ccc']], dtype=object)\n",
      "   932|         0|            0|            0|  0.00%|\n",
      "   933|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   934|         0|            0|            0|  0.00%|    a = asarray(a)\n",
      "   935|         0|            0|            0|  0.00%|    b = asarray(b)\n",
      "   936|         0|            0|            0|  0.00%|    return multiply(a.ravel()[:, newaxis], b.ravel()[newaxis, :], out)\n",
      "   937|         0|            0|            0|  0.00%|\n",
      "   938|         0|            0|            0|  0.00%|\n",
      "   939|         0|            0|            0|  0.00%|def _tensordot_dispatcher(a, b, axes=None):\n",
      "   940|         0|            0|            0|  0.00%|    return (a, b)\n",
      "   941|         0|            0|            0|  0.00%|\n",
      "   942|         0|            0|            0|  0.00%|\n",
      "   943|         0|            0|            0|  0.00%|@array_function_dispatch(_tensordot_dispatcher)\n",
      "   944|         0|            0|            0|  0.00%|def tensordot(a, b, axes=2):\n",
      "   945|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   946|         0|            0|            0|  0.00%|    Compute tensor dot product along specified axes.\n",
      "   947|         0|            0|            0|  0.00%|\n",
      "   948|         0|            0|            0|  0.00%|    Given two tensors, `a` and `b`, and an array_like object containing\n",
      "   949|         0|            0|            0|  0.00%|    two array_like objects, ``(a_axes, b_axes)``, sum the products of\n",
      "   950|         0|            0|            0|  0.00%|    `a`'s and `b`'s elements (components) over the axes specified by\n",
      "   951|         0|            0|            0|  0.00%|    ``a_axes`` and ``b_axes``. The third argument can be a single non-negative\n",
      "   952|         0|            0|            0|  0.00%|    integer_like scalar, ``N``; if it is such, then the last ``N`` dimensions\n",
      "   953|         0|            0|            0|  0.00%|    of `a` and the first ``N`` dimensions of `b` are summed over.\n",
      "   954|         0|            0|            0|  0.00%|\n",
      "   955|         0|            0|            0|  0.00%|    Parameters\n",
      "   956|         0|            0|            0|  0.00%|    ----------\n",
      "   957|         0|            0|            0|  0.00%|    a, b : array_like\n",
      "   958|         0|            0|            0|  0.00%|        Tensors to \"dot\".\n",
      "   959|         0|            0|            0|  0.00%|\n",
      "   960|         0|            0|            0|  0.00%|    axes : int or (2,) array_like\n",
      "   961|         0|            0|            0|  0.00%|        * integer_like\n",
      "   962|         0|            0|            0|  0.00%|          If an int N, sum over the last N axes of `a` and the first N axes\n",
      "   963|         0|            0|            0|  0.00%|          of `b` in order. The sizes of the corresponding axes must match.\n",
      "   964|         0|            0|            0|  0.00%|        * (2,) array_like\n",
      "   965|         0|            0|            0|  0.00%|          Or, a list of axes to be summed over, first sequence applying to `a`,\n",
      "   966|         0|            0|            0|  0.00%|          second to `b`. Both elements array_like must be of the same length.\n",
      "   967|         0|            0|            0|  0.00%|\n",
      "   968|         0|            0|            0|  0.00%|    Returns\n",
      "   969|         0|            0|            0|  0.00%|    -------\n",
      "   970|         0|            0|            0|  0.00%|    output : ndarray\n",
      "   971|         0|            0|            0|  0.00%|        The tensor dot product of the input.\n",
      "   972|         0|            0|            0|  0.00%|\n",
      "   973|         0|            0|            0|  0.00%|    See Also\n",
      "   974|         0|            0|            0|  0.00%|    --------\n",
      "   975|         0|            0|            0|  0.00%|    dot, einsum\n",
      "   976|         0|            0|            0|  0.00%|\n",
      "   977|         0|            0|            0|  0.00%|    Notes\n",
      "   978|         0|            0|            0|  0.00%|    -----\n",
      "   979|         0|            0|            0|  0.00%|    Three common use cases are:\n",
      "   980|         0|            0|            0|  0.00%|        * ``axes = 0`` : tensor product :math:`a\\\\otimes b`\n",
      "   981|         0|            0|            0|  0.00%|        * ``axes = 1`` : tensor dot product :math:`a\\\\cdot b`\n",
      "   982|         0|            0|            0|  0.00%|        * ``axes = 2`` : (default) tensor double contraction :math:`a:b`\n",
      "   983|         0|            0|            0|  0.00%|\n",
      "   984|         0|            0|            0|  0.00%|    When `axes` is integer_like, the sequence for evaluation will be: first\n",
      "   985|         0|            0|            0|  0.00%|    the -Nth axis in `a` and 0th axis in `b`, and the -1th axis in `a` and\n",
      "   986|         0|            0|            0|  0.00%|    Nth axis in `b` last.\n",
      "   987|         0|            0|            0|  0.00%|\n",
      "   988|         0|            0|            0|  0.00%|    When there is more than one axis to sum over - and they are not the last\n",
      "   989|         0|            0|            0|  0.00%|    (first) axes of `a` (`b`) - the argument `axes` should consist of\n",
      "   990|         0|            0|            0|  0.00%|    two sequences of the same length, with the first axis to sum over given\n",
      "   991|         0|            0|            0|  0.00%|    first in both sequences, the second axis second, and so forth.\n",
      "   992|         0|            0|            0|  0.00%|\n",
      "   993|         0|            0|            0|  0.00%|    The shape of the result consists of the non-contracted axes of the\n",
      "   994|         0|            0|            0|  0.00%|    first tensor, followed by the non-contracted axes of the second.\n",
      "   995|         0|            0|            0|  0.00%|\n",
      "   996|         0|            0|            0|  0.00%|    Examples\n",
      "   997|         0|            0|            0|  0.00%|    --------\n",
      "   998|         0|            0|            0|  0.00%|    A \"traditional\" example:\n",
      "   999|         0|            0|            0|  0.00%|\n",
      "  1000|         0|            0|            0|  0.00%|    >>> a = np.arange(60.).reshape(3,4,5)\n",
      "  1001|         0|            0|            0|  0.00%|    >>> b = np.arange(24.).reshape(4,3,2)\n",
      "  1002|         0|            0|            0|  0.00%|    >>> c = np.tensordot(a,b, axes=([1,0],[0,1]))\n",
      "  1003|         0|            0|            0|  0.00%|    >>> c.shape\n",
      "  1004|         0|            0|            0|  0.00%|    (5, 2)\n",
      "  1005|         0|            0|            0|  0.00%|    >>> c\n",
      "  1006|         0|            0|            0|  0.00%|    array([[4400., 4730.],\n",
      "  1007|         0|            0|            0|  0.00%|           [4532., 4874.],\n",
      "  1008|         0|            0|            0|  0.00%|           [4664., 5018.],\n",
      "  1009|         0|            0|            0|  0.00%|           [4796., 5162.],\n",
      "  1010|         0|            0|            0|  0.00%|           [4928., 5306.]])\n",
      "  1011|         0|            0|            0|  0.00%|    >>> # A slower but equivalent way of computing the same...\n",
      "  1012|         0|            0|            0|  0.00%|    >>> d = np.zeros((5,2))\n",
      "  1013|         0|            0|            0|  0.00%|    >>> for i in range(5):\n",
      "  1014|         0|            0|            0|  0.00%|    ...   for j in range(2):\n",
      "  1015|         0|            0|            0|  0.00%|    ...     for k in range(3):\n",
      "  1016|         0|            0|            0|  0.00%|    ...       for n in range(4):\n",
      "  1017|         0|            0|            0|  0.00%|    ...         d[i,j] += a[k,n,i] * b[n,k,j]\n",
      "  1018|         0|            0|            0|  0.00%|    >>> c == d\n",
      "  1019|         0|            0|            0|  0.00%|    array([[ True,  True],\n",
      "  1020|         0|            0|            0|  0.00%|           [ True,  True],\n",
      "  1021|         0|            0|            0|  0.00%|           [ True,  True],\n",
      "  1022|         0|            0|            0|  0.00%|           [ True,  True],\n",
      "  1023|         0|            0|            0|  0.00%|           [ True,  True]])\n",
      "  1024|         0|            0|            0|  0.00%|\n",
      "  1025|         0|            0|            0|  0.00%|    An extended example taking advantage of the overloading of + and \\\\*:\n",
      "  1026|         0|            0|            0|  0.00%|\n",
      "  1027|         0|            0|            0|  0.00%|    >>> a = np.array(range(1, 9))\n",
      "  1028|         0|            0|            0|  0.00%|    >>> a.shape = (2, 2, 2)\n",
      "  1029|         0|            0|            0|  0.00%|    >>> A = np.array(('a', 'b', 'c', 'd'), dtype=object)\n",
      "  1030|         0|            0|            0|  0.00%|    >>> A.shape = (2, 2)\n",
      "  1031|         0|            0|            0|  0.00%|    >>> a; A\n",
      "  1032|         0|            0|            0|  0.00%|    array([[[1, 2],\n",
      "  1033|         0|            0|            0|  0.00%|            [3, 4]],\n",
      "  1034|         0|            0|            0|  0.00%|           [[5, 6],\n",
      "  1035|         0|            0|            0|  0.00%|            [7, 8]]])\n",
      "  1036|         0|            0|            0|  0.00%|    array([['a', 'b'],\n",
      "  1037|         0|            0|            0|  0.00%|           ['c', 'd']], dtype=object)\n",
      "  1038|         0|            0|            0|  0.00%|\n",
      "  1039|         0|            0|            0|  0.00%|    >>> np.tensordot(a, A) # third argument default is 2 for double-contraction\n",
      "  1040|         0|            0|            0|  0.00%|    array(['abbcccdddd', 'aaaaabbbbbbcccccccdddddddd'], dtype=object)\n",
      "  1041|         0|            0|            0|  0.00%|\n",
      "  1042|         0|            0|            0|  0.00%|    >>> np.tensordot(a, A, 1)\n",
      "  1043|         0|            0|            0|  0.00%|    array([[['acc', 'bdd'],\n",
      "  1044|         0|            0|            0|  0.00%|            ['aaacccc', 'bbbdddd']],\n",
      "  1045|         0|            0|            0|  0.00%|           [['aaaaacccccc', 'bbbbbdddddd'],\n",
      "  1046|         0|            0|            0|  0.00%|            ['aaaaaaacccccccc', 'bbbbbbbdddddddd']]], dtype=object)\n",
      "  1047|         0|            0|            0|  0.00%|\n",
      "  1048|         0|            0|            0|  0.00%|    >>> np.tensordot(a, A, 0) # tensor product (result too long to incl.)\n",
      "  1049|         0|            0|            0|  0.00%|    array([[[[['a', 'b'],\n",
      "  1050|         0|            0|            0|  0.00%|              ['c', 'd']],\n",
      "  1051|         0|            0|            0|  0.00%|              ...\n",
      "  1052|         0|            0|            0|  0.00%|\n",
      "  1053|         0|            0|            0|  0.00%|    >>> np.tensordot(a, A, (0, 1))\n",
      "  1054|         0|            0|            0|  0.00%|    array([[['abbbbb', 'cddddd'],\n",
      "  1055|         0|            0|            0|  0.00%|            ['aabbbbbb', 'ccdddddd']],\n",
      "  1056|         0|            0|            0|  0.00%|           [['aaabbbbbbb', 'cccddddddd'],\n",
      "  1057|         0|            0|            0|  0.00%|            ['aaaabbbbbbbb', 'ccccdddddddd']]], dtype=object)\n",
      "  1058|         0|            0|            0|  0.00%|\n",
      "  1059|         0|            0|            0|  0.00%|    >>> np.tensordot(a, A, (2, 1))\n",
      "  1060|         0|            0|            0|  0.00%|    array([[['abb', 'cdd'],\n",
      "  1061|         0|            0|            0|  0.00%|            ['aaabbbb', 'cccdddd']],\n",
      "  1062|         0|            0|            0|  0.00%|           [['aaaaabbbbbb', 'cccccdddddd'],\n",
      "  1063|         0|            0|            0|  0.00%|            ['aaaaaaabbbbbbbb', 'cccccccdddddddd']]], dtype=object)\n",
      "  1064|         0|            0|            0|  0.00%|\n",
      "  1065|         0|            0|            0|  0.00%|    >>> np.tensordot(a, A, ((0, 1), (0, 1)))\n",
      "  1066|         0|            0|            0|  0.00%|    array(['abbbcccccddddddd', 'aabbbbccccccdddddddd'], dtype=object)\n",
      "  1067|         0|            0|            0|  0.00%|\n",
      "  1068|         0|            0|            0|  0.00%|    >>> np.tensordot(a, A, ((2, 1), (1, 0)))\n",
      "  1069|         0|            0|            0|  0.00%|    array(['acccbbdddd', 'aaaaacccccccbbbbbbdddddddd'], dtype=object)\n",
      "  1070|         0|            0|            0|  0.00%|\n",
      "  1071|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1072|         0|            0|            0|  0.00%|    try:\n",
      "  1073|         0|            0|            0|  0.00%|        iter(axes)\n",
      "  1074|         0|            0|            0|  0.00%|    except Exception:\n",
      "  1075|         0|            0|            0|  0.00%|        axes_a = list(range(-axes, 0))\n",
      "  1076|         0|            0|            0|  0.00%|        axes_b = list(range(0, axes))\n",
      "  1077|         0|            0|            0|  0.00%|    else:\n",
      "  1078|         0|            0|            0|  0.00%|        axes_a, axes_b = axes\n",
      "  1079|         0|            0|            0|  0.00%|    try:\n",
      "  1080|         0|            0|            0|  0.00%|        na = len(axes_a)\n",
      "  1081|         0|            0|            0|  0.00%|        axes_a = list(axes_a)\n",
      "  1082|         0|            0|            0|  0.00%|    except TypeError:\n",
      "  1083|         0|            0|            0|  0.00%|        axes_a = [axes_a]\n",
      "  1084|         0|            0|            0|  0.00%|        na = 1\n",
      "  1085|         0|            0|            0|  0.00%|    try:\n",
      "  1086|         0|            0|            0|  0.00%|        nb = len(axes_b)\n",
      "  1087|         0|            0|            0|  0.00%|        axes_b = list(axes_b)\n",
      "  1088|         0|            0|            0|  0.00%|    except TypeError:\n",
      "  1089|         0|            0|            0|  0.00%|        axes_b = [axes_b]\n",
      "  1090|         0|            0|            0|  0.00%|        nb = 1\n",
      "  1091|         0|            0|            0|  0.00%|\n",
      "  1092|         0|            0|            0|  0.00%|    a, b = asarray(a), asarray(b)\n",
      "  1093|         0|            0|            0|  0.00%|    as_ = a.shape\n",
      "  1094|         0|            0|            0|  0.00%|    nda = a.ndim\n",
      "  1095|         0|            0|            0|  0.00%|    bs = b.shape\n",
      "  1096|         0|            0|            0|  0.00%|    ndb = b.ndim\n",
      "  1097|         0|            0|            0|  0.00%|    equal = True\n",
      "  1098|         0|            0|            0|  0.00%|    if na != nb:\n",
      "  1099|         0|            0|            0|  0.00%|        equal = False\n",
      "  1100|         0|            0|            0|  0.00%|    else:\n",
      "  1101|         0|            0|            0|  0.00%|        for k in range(na):\n",
      "  1102|         0|            0|            0|  0.00%|            if as_[axes_a[k]] != bs[axes_b[k]]:\n",
      "  1103|         0|            0|            0|  0.00%|                equal = False\n",
      "  1104|         0|            0|            0|  0.00%|                break\n",
      "  1105|         0|            0|            0|  0.00%|            if axes_a[k] < 0:\n",
      "  1106|         0|            0|            0|  0.00%|                axes_a[k] += nda\n",
      "  1107|         0|            0|            0|  0.00%|            if axes_b[k] < 0:\n",
      "  1108|         0|            0|            0|  0.00%|                axes_b[k] += ndb\n",
      "  1109|         0|            0|            0|  0.00%|    if not equal:\n",
      "  1110|         0|            0|            0|  0.00%|        raise ValueError(\"shape-mismatch for sum\")\n",
      "  1111|         0|            0|            0|  0.00%|\n",
      "  1112|         0|            0|            0|  0.00%|    # Move the axes to sum over to the end of \"a\"\n",
      "  1113|         0|            0|            0|  0.00%|    # and to the front of \"b\"\n",
      "  1114|         0|            0|            0|  0.00%|    notin = [k for k in range(nda) if k not in axes_a]\n",
      "  1115|         0|            0|            0|  0.00%|    newaxes_a = notin + axes_a\n",
      "  1116|         0|            0|            0|  0.00%|    N2 = 1\n",
      "  1117|         0|            0|            0|  0.00%|    for axis in axes_a:\n",
      "  1118|         0|            0|            0|  0.00%|        N2 *= as_[axis]\n",
      "  1119|         0|            0|            0|  0.00%|    newshape_a = (int(multiply.reduce([as_[ax] for ax in notin])), N2)\n",
      "  1120|         0|            0|            0|  0.00%|    olda = [as_[axis] for axis in notin]\n",
      "  1121|         0|            0|            0|  0.00%|\n",
      "  1122|         0|            0|            0|  0.00%|    notin = [k for k in range(ndb) if k not in axes_b]\n",
      "  1123|         0|            0|            0|  0.00%|    newaxes_b = axes_b + notin\n",
      "  1124|         0|            0|            0|  0.00%|    N2 = 1\n",
      "  1125|         0|            0|            0|  0.00%|    for axis in axes_b:\n",
      "  1126|         0|            0|            0|  0.00%|        N2 *= bs[axis]\n",
      "  1127|         0|            0|            0|  0.00%|    newshape_b = (N2, int(multiply.reduce([bs[ax] for ax in notin])))\n",
      "  1128|         0|            0|            0|  0.00%|    oldb = [bs[axis] for axis in notin]\n",
      "  1129|         0|            0|            0|  0.00%|\n",
      "  1130|         0|            0|            0|  0.00%|    at = a.transpose(newaxes_a).reshape(newshape_a)\n",
      "  1131|         0|            0|            0|  0.00%|    bt = b.transpose(newaxes_b).reshape(newshape_b)\n",
      "  1132|         0|            0|            0|  0.00%|    res = dot(at, bt)\n",
      "  1133|         0|            0|            0|  0.00%|    return res.reshape(olda + oldb)\n",
      "  1134|         0|            0|            0|  0.00%|\n",
      "  1135|         0|            0|            0|  0.00%|\n",
      "  1136|         0|            0|            0|  0.00%|def _roll_dispatcher(a, shift, axis=None):\n",
      "  1137|         0|            0|            0|  0.00%|    return (a,)\n",
      "  1138|         0|            0|            0|  0.00%|\n",
      "  1139|         0|            0|            0|  0.00%|\n",
      "  1140|         0|            0|            0|  0.00%|@array_function_dispatch(_roll_dispatcher)\n",
      "  1141|         0|            0|            0|  0.00%|def roll(a, shift, axis=None):\n",
      "  1142|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1143|         0|            0|            0|  0.00%|    Roll array elements along a given axis.\n",
      "  1144|         0|            0|            0|  0.00%|\n",
      "  1145|         0|            0|            0|  0.00%|    Elements that roll beyond the last position are re-introduced at\n",
      "  1146|         0|            0|            0|  0.00%|    the first.\n",
      "  1147|         0|            0|            0|  0.00%|\n",
      "  1148|         0|            0|            0|  0.00%|    Parameters\n",
      "  1149|         0|            0|            0|  0.00%|    ----------\n",
      "  1150|         0|            0|            0|  0.00%|    a : array_like\n",
      "  1151|         0|            0|            0|  0.00%|        Input array.\n",
      "  1152|         0|            0|            0|  0.00%|    shift : int or tuple of ints\n",
      "  1153|         0|            0|            0|  0.00%|        The number of places by which elements are shifted.  If a tuple,\n",
      "  1154|         0|            0|            0|  0.00%|        then `axis` must be a tuple of the same size, and each of the\n",
      "  1155|         0|            0|            0|  0.00%|        given axes is shifted by the corresponding number.  If an int\n",
      "  1156|         0|            0|            0|  0.00%|        while `axis` is a tuple of ints, then the same value is used for\n",
      "  1157|         0|            0|            0|  0.00%|        all given axes.\n",
      "  1158|         0|            0|            0|  0.00%|    axis : int or tuple of ints, optional\n",
      "  1159|         0|            0|            0|  0.00%|        Axis or axes along which elements are shifted.  By default, the\n",
      "  1160|         0|            0|            0|  0.00%|        array is flattened before shifting, after which the original\n",
      "  1161|         0|            0|            0|  0.00%|        shape is restored.\n",
      "  1162|         0|            0|            0|  0.00%|\n",
      "  1163|         0|            0|            0|  0.00%|    Returns\n",
      "  1164|         0|            0|            0|  0.00%|    -------\n",
      "  1165|         0|            0|            0|  0.00%|    res : ndarray\n",
      "  1166|         0|            0|            0|  0.00%|        Output array, with the same shape as `a`.\n",
      "  1167|         0|            0|            0|  0.00%|\n",
      "  1168|         0|            0|            0|  0.00%|    See Also\n",
      "  1169|         0|            0|            0|  0.00%|    --------\n",
      "  1170|         0|            0|            0|  0.00%|    rollaxis : Roll the specified axis backwards, until it lies in a\n",
      "  1171|         0|            0|            0|  0.00%|               given position.\n",
      "  1172|         0|            0|            0|  0.00%|\n",
      "  1173|         0|            0|            0|  0.00%|    Notes\n",
      "  1174|         0|            0|            0|  0.00%|    -----\n",
      "  1175|         0|            0|            0|  0.00%|    .. versionadded:: 1.12.0\n",
      "  1176|         0|            0|            0|  0.00%|\n",
      "  1177|         0|            0|            0|  0.00%|    Supports rolling over multiple dimensions simultaneously.\n",
      "  1178|         0|            0|            0|  0.00%|\n",
      "  1179|         0|            0|            0|  0.00%|    Examples\n",
      "  1180|         0|            0|            0|  0.00%|    --------\n",
      "  1181|         0|            0|            0|  0.00%|    >>> x = np.arange(10)\n",
      "  1182|         0|            0|            0|  0.00%|    >>> np.roll(x, 2)\n",
      "  1183|         0|            0|            0|  0.00%|    array([8, 9, 0, 1, 2, 3, 4, 5, 6, 7])\n",
      "  1184|         0|            0|            0|  0.00%|    >>> np.roll(x, -2)\n",
      "  1185|         0|            0|            0|  0.00%|    array([2, 3, 4, 5, 6, 7, 8, 9, 0, 1])\n",
      "  1186|         0|            0|            0|  0.00%|\n",
      "  1187|         0|            0|            0|  0.00%|    >>> x2 = np.reshape(x, (2, 5))\n",
      "  1188|         0|            0|            0|  0.00%|    >>> x2\n",
      "  1189|         0|            0|            0|  0.00%|    array([[0, 1, 2, 3, 4],\n",
      "  1190|         0|            0|            0|  0.00%|           [5, 6, 7, 8, 9]])\n",
      "  1191|         0|            0|            0|  0.00%|    >>> np.roll(x2, 1)\n",
      "  1192|         0|            0|            0|  0.00%|    array([[9, 0, 1, 2, 3],\n",
      "  1193|         0|            0|            0|  0.00%|           [4, 5, 6, 7, 8]])\n",
      "  1194|         0|            0|            0|  0.00%|    >>> np.roll(x2, -1)\n",
      "  1195|         0|            0|            0|  0.00%|    array([[1, 2, 3, 4, 5],\n",
      "  1196|         0|            0|            0|  0.00%|           [6, 7, 8, 9, 0]])\n",
      "  1197|         0|            0|            0|  0.00%|    >>> np.roll(x2, 1, axis=0)\n",
      "  1198|         0|            0|            0|  0.00%|    array([[5, 6, 7, 8, 9],\n",
      "  1199|         0|            0|            0|  0.00%|           [0, 1, 2, 3, 4]])\n",
      "  1200|         0|            0|            0|  0.00%|    >>> np.roll(x2, -1, axis=0)\n",
      "  1201|         0|            0|            0|  0.00%|    array([[5, 6, 7, 8, 9],\n",
      "  1202|         0|            0|            0|  0.00%|           [0, 1, 2, 3, 4]])\n",
      "  1203|         0|            0|            0|  0.00%|    >>> np.roll(x2, 1, axis=1)\n",
      "  1204|         0|            0|            0|  0.00%|    array([[4, 0, 1, 2, 3],\n",
      "  1205|         0|            0|            0|  0.00%|           [9, 5, 6, 7, 8]])\n",
      "  1206|         0|            0|            0|  0.00%|    >>> np.roll(x2, -1, axis=1)\n",
      "  1207|         0|            0|            0|  0.00%|    array([[1, 2, 3, 4, 0],\n",
      "  1208|         0|            0|            0|  0.00%|           [6, 7, 8, 9, 5]])\n",
      "  1209|         0|            0|            0|  0.00%|    >>> np.roll(x2, (1, 1), axis=(1, 0))\n",
      "  1210|         0|            0|            0|  0.00%|    array([[9, 5, 6, 7, 8],\n",
      "  1211|         0|            0|            0|  0.00%|           [4, 0, 1, 2, 3]])\n",
      "  1212|         0|            0|            0|  0.00%|    >>> np.roll(x2, (2, 1), axis=(1, 0))\n",
      "  1213|         0|            0|            0|  0.00%|    array([[8, 9, 5, 6, 7],\n",
      "  1214|         0|            0|            0|  0.00%|           [3, 4, 0, 1, 2]])\n",
      "  1215|         0|            0|            0|  0.00%|\n",
      "  1216|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1217|         0|            0|            0|  0.00%|    a = asanyarray(a)\n",
      "  1218|         0|            0|            0|  0.00%|    if axis is None:\n",
      "  1219|         0|            0|            0|  0.00%|        return roll(a.ravel(), shift, 0).reshape(a.shape)\n",
      "  1220|         0|            0|            0|  0.00%|\n",
      "  1221|         0|            0|            0|  0.00%|    else:\n",
      "  1222|         0|            0|            0|  0.00%|        axis = normalize_axis_tuple(axis, a.ndim, allow_duplicate=True)\n",
      "  1223|         0|            0|            0|  0.00%|        broadcasted = broadcast(shift, axis)\n",
      "  1224|         0|            0|            0|  0.00%|        if broadcasted.ndim > 1:\n",
      "  1225|         0|            0|            0|  0.00%|            raise ValueError(\n",
      "  1226|         0|            0|            0|  0.00%|                \"'shift' and 'axis' should be scalars or 1D sequences\")\n",
      "  1227|         0|            0|            0|  0.00%|        shifts = {ax: 0 for ax in range(a.ndim)}\n",
      "  1228|         0|            0|            0|  0.00%|        for sh, ax in broadcasted:\n",
      "  1229|         0|            0|            0|  0.00%|            shifts[ax] += sh\n",
      "  1230|         0|            0|            0|  0.00%|\n",
      "  1231|         0|            0|            0|  0.00%|        rolls = [((slice(None), slice(None)),)] * a.ndim\n",
      "  1232|         0|            0|            0|  0.00%|        for ax, offset in shifts.items():\n",
      "  1233|         0|            0|            0|  0.00%|            offset %= a.shape[ax] or 1  # If `a` is empty, nothing matters.\n",
      "  1234|         0|            0|            0|  0.00%|            if offset:\n",
      "  1235|         0|            0|            0|  0.00%|                # (original, result), (original, result)\n",
      "  1236|         0|            0|            0|  0.00%|                rolls[ax] = ((slice(None, -offset), slice(offset, None)),\n",
      "  1237|         0|            0|            0|  0.00%|                             (slice(-offset, None), slice(None, offset)))\n",
      "  1238|         0|            0|            0|  0.00%|\n",
      "  1239|         0|            0|            0|  0.00%|        result = empty_like(a)\n",
      "  1240|         0|            0|            0|  0.00%|        for indices in itertools.product(*rolls):\n",
      "  1241|         0|            0|            0|  0.00%|            arr_index, res_index = zip(*indices)\n",
      "  1242|         0|            0|            0|  0.00%|            result[res_index] = a[arr_index]\n",
      "  1243|         0|            0|            0|  0.00%|\n",
      "  1244|         0|            0|            0|  0.00%|        return result\n",
      "  1245|         0|            0|            0|  0.00%|\n",
      "  1246|         0|            0|            0|  0.00%|\n",
      "  1247|         0|            0|            0|  0.00%|def _rollaxis_dispatcher(a, axis, start=None):\n",
      "  1248|         0|            0|            0|  0.00%|    return (a,)\n",
      "  1249|         0|            0|            0|  0.00%|\n",
      "  1250|         0|            0|            0|  0.00%|\n",
      "  1251|         0|            0|            0|  0.00%|@array_function_dispatch(_rollaxis_dispatcher)\n",
      "  1252|         0|            0|            0|  0.00%|def rollaxis(a, axis, start=0):\n",
      "  1253|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1254|         0|            0|            0|  0.00%|    Roll the specified axis backwards, until it lies in a given position.\n",
      "  1255|         0|            0|            0|  0.00%|\n",
      "  1256|         0|            0|            0|  0.00%|    This function continues to be supported for backward compatibility, but you\n",
      "  1257|         0|            0|            0|  0.00%|    should prefer `moveaxis`. The `moveaxis` function was added in NumPy\n",
      "  1258|         0|            0|            0|  0.00%|    1.11.\n",
      "  1259|         0|            0|            0|  0.00%|\n",
      "  1260|         0|            0|            0|  0.00%|    Parameters\n",
      "  1261|         0|            0|            0|  0.00%|    ----------\n",
      "  1262|         0|            0|            0|  0.00%|    a : ndarray\n",
      "  1263|         0|            0|            0|  0.00%|        Input array.\n",
      "  1264|         0|            0|            0|  0.00%|    axis : int\n",
      "  1265|         0|            0|            0|  0.00%|        The axis to be rolled. The positions of the other axes do not\n",
      "  1266|         0|            0|            0|  0.00%|        change relative to one another.\n",
      "  1267|         0|            0|            0|  0.00%|    start : int, optional\n",
      "  1268|         0|            0|            0|  0.00%|        When ``start <= axis``, the axis is rolled back until it lies in\n",
      "  1269|         0|            0|            0|  0.00%|        this position. When ``start > axis``, the axis is rolled until it\n",
      "  1270|         0|            0|            0|  0.00%|        lies before this position. The default, 0, results in a \"complete\"\n",
      "  1271|         0|            0|            0|  0.00%|        roll. The following table describes how negative values of ``start``\n",
      "  1272|         0|            0|            0|  0.00%|        are interpreted:\n",
      "  1273|         0|            0|            0|  0.00%|\n",
      "  1274|         0|            0|            0|  0.00%|        .. table::\n",
      "  1275|         0|            0|            0|  0.00%|           :align: left\n",
      "  1276|         0|            0|            0|  0.00%|\n",
      "  1277|         0|            0|            0|  0.00%|           +-------------------+----------------------+\n",
      "  1278|         0|            0|            0|  0.00%|           |     ``start``     | Normalized ``start`` |\n",
      "  1279|         0|            0|            0|  0.00%|           +===================+======================+\n",
      "  1280|         0|            0|            0|  0.00%|           | ``-(arr.ndim+1)`` | raise ``AxisError``  |\n",
      "  1281|         0|            0|            0|  0.00%|           +-------------------+----------------------+\n",
      "  1282|         0|            0|            0|  0.00%|           | ``-arr.ndim``     | 0                    |\n",
      "  1283|         0|            0|            0|  0.00%|           +-------------------+----------------------+\n",
      "  1284|         0|            0|            0|  0.00%|           | |vdots|           | |vdots|              |\n",
      "  1285|         0|            0|            0|  0.00%|           +-------------------+----------------------+\n",
      "  1286|         0|            0|            0|  0.00%|           | ``-1``            | ``arr.ndim-1``       |\n",
      "  1287|         0|            0|            0|  0.00%|           +-------------------+----------------------+\n",
      "  1288|         0|            0|            0|  0.00%|           | ``0``             | ``0``                |\n",
      "  1289|         0|            0|            0|  0.00%|           +-------------------+----------------------+\n",
      "  1290|         0|            0|            0|  0.00%|           | |vdots|           | |vdots|              |\n",
      "  1291|         0|            0|            0|  0.00%|           +-------------------+----------------------+\n",
      "  1292|         0|            0|            0|  0.00%|           | ``arr.ndim``      | ``arr.ndim``         |\n",
      "  1293|         0|            0|            0|  0.00%|           +-------------------+----------------------+\n",
      "  1294|         0|            0|            0|  0.00%|           | ``arr.ndim + 1``  | raise ``AxisError``  |\n",
      "  1295|         0|            0|            0|  0.00%|           +-------------------+----------------------+\n",
      "  1296|         0|            0|            0|  0.00%|\n",
      "  1297|         0|            0|            0|  0.00%|        .. |vdots|   unicode:: U+22EE .. Vertical Ellipsis\n",
      "  1298|         0|            0|            0|  0.00%|\n",
      "  1299|         0|            0|            0|  0.00%|    Returns\n",
      "  1300|         0|            0|            0|  0.00%|    -------\n",
      "  1301|         0|            0|            0|  0.00%|    res : ndarray\n",
      "  1302|         0|            0|            0|  0.00%|        For NumPy >= 1.10.0 a view of `a` is always returned. For earlier\n",
      "  1303|         0|            0|            0|  0.00%|        NumPy versions a view of `a` is returned only if the order of the\n",
      "  1304|         0|            0|            0|  0.00%|        axes is changed, otherwise the input array is returned.\n",
      "  1305|         0|            0|            0|  0.00%|\n",
      "  1306|         0|            0|            0|  0.00%|    See Also\n",
      "  1307|         0|            0|            0|  0.00%|    --------\n",
      "  1308|         0|            0|            0|  0.00%|    moveaxis : Move array axes to new positions.\n",
      "  1309|         0|            0|            0|  0.00%|    roll : Roll the elements of an array by a number of positions along a\n",
      "  1310|         0|            0|            0|  0.00%|        given axis.\n",
      "  1311|         0|            0|            0|  0.00%|\n",
      "  1312|         0|            0|            0|  0.00%|    Examples\n",
      "  1313|         0|            0|            0|  0.00%|    --------\n",
      "  1314|         0|            0|            0|  0.00%|    >>> a = np.ones((3,4,5,6))\n",
      "  1315|         0|            0|            0|  0.00%|    >>> np.rollaxis(a, 3, 1).shape\n",
      "  1316|         0|            0|            0|  0.00%|    (3, 6, 4, 5)\n",
      "  1317|         0|            0|            0|  0.00%|    >>> np.rollaxis(a, 2).shape\n",
      "  1318|         0|            0|            0|  0.00%|    (5, 3, 4, 6)\n",
      "  1319|         0|            0|            0|  0.00%|    >>> np.rollaxis(a, 1, 4).shape\n",
      "  1320|         0|            0|            0|  0.00%|    (3, 5, 6, 4)\n",
      "  1321|         0|            0|            0|  0.00%|\n",
      "  1322|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1323|         0|            0|            0|  0.00%|    n = a.ndim\n",
      "  1324|         0|            0|            0|  0.00%|    axis = normalize_axis_index(axis, n)\n",
      "  1325|         0|            0|            0|  0.00%|    if start < 0:\n",
      "  1326|         0|            0|            0|  0.00%|        start += n\n",
      "  1327|         0|            0|            0|  0.00%|    msg = \"'%s' arg requires %d <= %s < %d, but %d was passed in\"\n",
      "  1328|         0|            0|            0|  0.00%|    if not (0 <= start < n + 1):\n",
      "  1329|         0|            0|            0|  0.00%|        raise AxisError(msg % ('start', -n, 'start', n + 1, start))\n",
      "  1330|         0|            0|            0|  0.00%|    if axis < start:\n",
      "  1331|         0|            0|            0|  0.00%|        # it's been removed\n",
      "  1332|         0|            0|            0|  0.00%|        start -= 1\n",
      "  1333|         0|            0|            0|  0.00%|    if axis == start:\n",
      "  1334|         0|            0|            0|  0.00%|        return a[...]\n",
      "  1335|         0|            0|            0|  0.00%|    axes = list(range(0, n))\n",
      "  1336|         0|            0|            0|  0.00%|    axes.remove(axis)\n",
      "  1337|         0|            0|            0|  0.00%|    axes.insert(start, axis)\n",
      "  1338|         0|            0|            0|  0.00%|    return a.transpose(axes)\n",
      "  1339|         0|            0|            0|  0.00%|\n",
      "  1340|         0|            0|            0|  0.00%|\n",
      "  1341|         0|            0|            0|  0.00%|def normalize_axis_tuple(axis, ndim, argname=None, allow_duplicate=False):\n",
      "  1342|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1343|         0|            0|            0|  0.00%|    Normalizes an axis argument into a tuple of non-negative integer axes.\n",
      "  1344|         0|            0|            0|  0.00%|\n",
      "  1345|         0|            0|            0|  0.00%|    This handles shorthands such as ``1`` and converts them to ``(1,)``,\n",
      "  1346|         0|            0|            0|  0.00%|    as well as performing the handling of negative indices covered by\n",
      "  1347|         0|            0|            0|  0.00%|    `normalize_axis_index`.\n",
      "  1348|         0|            0|            0|  0.00%|\n",
      "  1349|         0|            0|            0|  0.00%|    By default, this forbids axes from being specified multiple times.\n",
      "  1350|         0|            0|            0|  0.00%|\n",
      "  1351|         0|            0|            0|  0.00%|    Used internally by multi-axis-checking logic.\n",
      "  1352|         0|            0|            0|  0.00%|\n",
      "  1353|         0|            0|            0|  0.00%|    .. versionadded:: 1.13.0\n",
      "  1354|         0|            0|            0|  0.00%|\n",
      "  1355|         0|            0|            0|  0.00%|    Parameters\n",
      "  1356|         0|            0|            0|  0.00%|    ----------\n",
      "  1357|         0|            0|            0|  0.00%|    axis : int, iterable of int\n",
      "  1358|         0|            0|            0|  0.00%|        The un-normalized index or indices of the axis.\n",
      "  1359|         0|            0|            0|  0.00%|    ndim : int\n",
      "  1360|         0|            0|            0|  0.00%|        The number of dimensions of the array that `axis` should be normalized\n",
      "  1361|         0|            0|            0|  0.00%|        against.\n",
      "  1362|         0|            0|            0|  0.00%|    argname : str, optional\n",
      "  1363|         0|            0|            0|  0.00%|        A prefix to put before the error message, typically the name of the\n",
      "  1364|         0|            0|            0|  0.00%|        argument.\n",
      "  1365|         0|            0|            0|  0.00%|    allow_duplicate : bool, optional\n",
      "  1366|         0|            0|            0|  0.00%|        If False, the default, disallow an axis from being specified twice.\n",
      "  1367|         0|            0|            0|  0.00%|\n",
      "  1368|         0|            0|            0|  0.00%|    Returns\n",
      "  1369|         0|            0|            0|  0.00%|    -------\n",
      "  1370|         0|            0|            0|  0.00%|    normalized_axes : tuple of int\n",
      "  1371|         0|            0|            0|  0.00%|        The normalized axis index, such that `0 <= normalized_axis < ndim`\n",
      "  1372|         0|            0|            0|  0.00%|\n",
      "  1373|         0|            0|            0|  0.00%|    Raises\n",
      "  1374|         0|            0|            0|  0.00%|    ------\n",
      "  1375|         0|            0|            0|  0.00%|    AxisError\n",
      "  1376|         0|            0|            0|  0.00%|        If any axis provided is out of range\n",
      "  1377|         0|            0|            0|  0.00%|    ValueError\n",
      "  1378|         0|            0|            0|  0.00%|        If an axis is repeated\n",
      "  1379|         0|            0|            0|  0.00%|\n",
      "  1380|         0|            0|            0|  0.00%|    See also\n",
      "  1381|         0|            0|            0|  0.00%|    --------\n",
      "  1382|         0|            0|            0|  0.00%|    normalize_axis_index : normalizing a single scalar axis\n",
      "  1383|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1384|         0|            0|            0|  0.00%|    # Optimization to speed-up the most common cases.\n",
      "  1385|         0|            0|            0|  0.00%|    if type(axis) not in (tuple, list):\n",
      "  1386|         0|            0|            0|  0.00%|        try:\n",
      "  1387|         0|            0|            0|  0.00%|            axis = [operator.index(axis)]\n",
      "  1388|         0|            0|            0|  0.00%|        except TypeError:\n",
      "  1389|         0|            0|            0|  0.00%|            pass\n",
      "  1390|         0|            0|            0|  0.00%|    # Going via an iterator directly is slower than via list comprehension.\n",
      "  1391|         0|            0|            0|  0.00%|    axis = tuple([normalize_axis_index(ax, ndim, argname) for ax in axis])\n",
      "  1392|         0|            0|            0|  0.00%|    if not allow_duplicate and len(set(axis)) != len(axis):\n",
      "  1393|         0|            0|            0|  0.00%|        if argname:\n",
      "  1394|         0|            0|            0|  0.00%|            raise ValueError('repeated axis in `{}` argument'.format(argname))\n",
      "  1395|         0|            0|            0|  0.00%|        else:\n",
      "  1396|         0|            0|            0|  0.00%|            raise ValueError('repeated axis')\n",
      "  1397|         0|            0|            0|  0.00%|    return axis\n",
      "  1398|         0|            0|            0|  0.00%|\n",
      "  1399|         0|            0|            0|  0.00%|\n",
      "  1400|         0|            0|            0|  0.00%|def _moveaxis_dispatcher(a, source, destination):\n",
      "  1401|         0|            0|            0|  0.00%|    return (a,)\n",
      "  1402|         0|            0|            0|  0.00%|\n",
      "  1403|         0|            0|            0|  0.00%|\n",
      "  1404|         0|            0|            0|  0.00%|@array_function_dispatch(_moveaxis_dispatcher)\n",
      "  1405|         0|            0|            0|  0.00%|def moveaxis(a, source, destination):\n",
      "  1406|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1407|         0|            0|            0|  0.00%|    Move axes of an array to new positions.\n",
      "  1408|         0|            0|            0|  0.00%|\n",
      "  1409|         0|            0|            0|  0.00%|    Other axes remain in their original order.\n",
      "  1410|         0|            0|            0|  0.00%|\n",
      "  1411|         0|            0|            0|  0.00%|    .. versionadded:: 1.11.0\n",
      "  1412|         0|            0|            0|  0.00%|\n",
      "  1413|         0|            0|            0|  0.00%|    Parameters\n",
      "  1414|         0|            0|            0|  0.00%|    ----------\n",
      "  1415|         0|            0|            0|  0.00%|    a : np.ndarray\n",
      "  1416|         0|            0|            0|  0.00%|        The array whose axes should be reordered.\n",
      "  1417|         0|            0|            0|  0.00%|    source : int or sequence of int\n",
      "  1418|         0|            0|            0|  0.00%|        Original positions of the axes to move. These must be unique.\n",
      "  1419|         0|            0|            0|  0.00%|    destination : int or sequence of int\n",
      "  1420|         0|            0|            0|  0.00%|        Destination positions for each of the original axes. These must also be\n",
      "  1421|         0|            0|            0|  0.00%|        unique.\n",
      "  1422|         0|            0|            0|  0.00%|\n",
      "  1423|         0|            0|            0|  0.00%|    Returns\n",
      "  1424|         0|            0|            0|  0.00%|    -------\n",
      "  1425|         0|            0|            0|  0.00%|    result : np.ndarray\n",
      "  1426|         0|            0|            0|  0.00%|        Array with moved axes. This array is a view of the input array.\n",
      "  1427|         0|            0|            0|  0.00%|\n",
      "  1428|         0|            0|            0|  0.00%|    See Also\n",
      "  1429|         0|            0|            0|  0.00%|    --------\n",
      "  1430|         0|            0|            0|  0.00%|    transpose : Permute the dimensions of an array.\n",
      "  1431|         0|            0|            0|  0.00%|    swapaxes : Interchange two axes of an array.\n",
      "  1432|         0|            0|            0|  0.00%|\n",
      "  1433|         0|            0|            0|  0.00%|    Examples\n",
      "  1434|         0|            0|            0|  0.00%|    --------\n",
      "  1435|         0|            0|            0|  0.00%|    >>> x = np.zeros((3, 4, 5))\n",
      "  1436|         0|            0|            0|  0.00%|    >>> np.moveaxis(x, 0, -1).shape\n",
      "  1437|         0|            0|            0|  0.00%|    (4, 5, 3)\n",
      "  1438|         0|            0|            0|  0.00%|    >>> np.moveaxis(x, -1, 0).shape\n",
      "  1439|         0|            0|            0|  0.00%|    (5, 3, 4)\n",
      "  1440|         0|            0|            0|  0.00%|\n",
      "  1441|         0|            0|            0|  0.00%|    These all achieve the same result:\n",
      "  1442|         0|            0|            0|  0.00%|\n",
      "  1443|         0|            0|            0|  0.00%|    >>> np.transpose(x).shape\n",
      "  1444|         0|            0|            0|  0.00%|    (5, 4, 3)\n",
      "  1445|         0|            0|            0|  0.00%|    >>> np.swapaxes(x, 0, -1).shape\n",
      "  1446|         0|            0|            0|  0.00%|    (5, 4, 3)\n",
      "  1447|         0|            0|            0|  0.00%|    >>> np.moveaxis(x, [0, 1], [-1, -2]).shape\n",
      "  1448|         0|            0|            0|  0.00%|    (5, 4, 3)\n",
      "  1449|         0|            0|            0|  0.00%|    >>> np.moveaxis(x, [0, 1, 2], [-1, -2, -3]).shape\n",
      "  1450|         0|            0|            0|  0.00%|    (5, 4, 3)\n",
      "  1451|         0|            0|            0|  0.00%|\n",
      "  1452|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1453|         0|            0|            0|  0.00%|    try:\n",
      "  1454|         0|            0|            0|  0.00%|        # allow duck-array types if they define transpose\n",
      "  1455|         0|            0|            0|  0.00%|        transpose = a.transpose\n",
      "  1456|         0|            0|            0|  0.00%|    except AttributeError:\n",
      "  1457|         0|            0|            0|  0.00%|        a = asarray(a)\n",
      "  1458|         0|            0|            0|  0.00%|        transpose = a.transpose\n",
      "  1459|         0|            0|            0|  0.00%|\n",
      "  1460|         0|            0|            0|  0.00%|    source = normalize_axis_tuple(source, a.ndim, 'source')\n",
      "  1461|         0|            0|            0|  0.00%|    destination = normalize_axis_tuple(destination, a.ndim, 'destination')\n",
      "  1462|         0|            0|            0|  0.00%|    if len(source) != len(destination):\n",
      "  1463|         0|            0|            0|  0.00%|        raise ValueError('`source` and `destination` arguments must have '\n",
      "  1464|         0|            0|            0|  0.00%|                         'the same number of elements')\n",
      "  1465|         0|            0|            0|  0.00%|\n",
      "  1466|         0|            0|            0|  0.00%|    order = [n for n in range(a.ndim) if n not in source]\n",
      "  1467|         0|            0|            0|  0.00%|\n",
      "  1468|         0|            0|            0|  0.00%|    for dest, src in sorted(zip(destination, source)):\n",
      "  1469|         0|            0|            0|  0.00%|        order.insert(dest, src)\n",
      "  1470|         0|            0|            0|  0.00%|\n",
      "  1471|         0|            0|            0|  0.00%|    result = transpose(order)\n",
      "  1472|         0|            0|            0|  0.00%|    return result\n",
      "  1473|         0|            0|            0|  0.00%|\n",
      "  1474|         0|            0|            0|  0.00%|\n",
      "  1475|         0|            0|            0|  0.00%|def _cross_dispatcher(a, b, axisa=None, axisb=None, axisc=None, axis=None):\n",
      "  1476|         0|            0|            0|  0.00%|    return (a, b)\n",
      "  1477|         0|            0|            0|  0.00%|\n",
      "  1478|         0|            0|            0|  0.00%|\n",
      "  1479|         0|            0|            0|  0.00%|@array_function_dispatch(_cross_dispatcher)\n",
      "  1480|         0|            0|            0|  0.00%|def cross(a, b, axisa=-1, axisb=-1, axisc=-1, axis=None):\n",
      "  1481|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1482|         0|            0|            0|  0.00%|    Return the cross product of two (arrays of) vectors.\n",
      "  1483|         0|            0|            0|  0.00%|\n",
      "  1484|         0|            0|            0|  0.00%|    The cross product of `a` and `b` in :math:`R^3` is a vector perpendicular\n",
      "  1485|         0|            0|            0|  0.00%|    to both `a` and `b`.  If `a` and `b` are arrays of vectors, the vectors\n",
      "  1486|         0|            0|            0|  0.00%|    are defined by the last axis of `a` and `b` by default, and these axes\n",
      "  1487|         0|            0|            0|  0.00%|    can have dimensions 2 or 3.  Where the dimension of either `a` or `b` is\n",
      "  1488|         0|            0|            0|  0.00%|    2, the third component of the input vector is assumed to be zero and the\n",
      "  1489|         0|            0|            0|  0.00%|    cross product calculated accordingly.  In cases where both input vectors\n",
      "  1490|         0|            0|            0|  0.00%|    have dimension 2, the z-component of the cross product is returned.\n",
      "  1491|         0|            0|            0|  0.00%|\n",
      "  1492|         0|            0|            0|  0.00%|    Parameters\n",
      "  1493|         0|            0|            0|  0.00%|    ----------\n",
      "  1494|         0|            0|            0|  0.00%|    a : array_like\n",
      "  1495|         0|            0|            0|  0.00%|        Components of the first vector(s).\n",
      "  1496|         0|            0|            0|  0.00%|    b : array_like\n",
      "  1497|         0|            0|            0|  0.00%|        Components of the second vector(s).\n",
      "  1498|         0|            0|            0|  0.00%|    axisa : int, optional\n",
      "  1499|         0|            0|            0|  0.00%|        Axis of `a` that defines the vector(s).  By default, the last axis.\n",
      "  1500|         0|            0|            0|  0.00%|    axisb : int, optional\n",
      "  1501|         0|            0|            0|  0.00%|        Axis of `b` that defines the vector(s).  By default, the last axis.\n",
      "  1502|         0|            0|            0|  0.00%|    axisc : int, optional\n",
      "  1503|         0|            0|            0|  0.00%|        Axis of `c` containing the cross product vector(s).  Ignored if\n",
      "  1504|         0|            0|            0|  0.00%|        both input vectors have dimension 2, as the return is scalar.\n",
      "  1505|         0|            0|            0|  0.00%|        By default, the last axis.\n",
      "  1506|         0|            0|            0|  0.00%|    axis : int, optional\n",
      "  1507|         0|            0|            0|  0.00%|        If defined, the axis of `a`, `b` and `c` that defines the vector(s)\n",
      "  1508|         0|            0|            0|  0.00%|        and cross product(s).  Overrides `axisa`, `axisb` and `axisc`.\n",
      "  1509|         0|            0|            0|  0.00%|\n",
      "  1510|         0|            0|            0|  0.00%|    Returns\n",
      "  1511|         0|            0|            0|  0.00%|    -------\n",
      "  1512|         0|            0|            0|  0.00%|    c : ndarray\n",
      "  1513|         0|            0|            0|  0.00%|        Vector cross product(s).\n",
      "  1514|         0|            0|            0|  0.00%|\n",
      "  1515|         0|            0|            0|  0.00%|    Raises\n",
      "  1516|         0|            0|            0|  0.00%|    ------\n",
      "  1517|         0|            0|            0|  0.00%|    ValueError\n",
      "  1518|         0|            0|            0|  0.00%|        When the dimension of the vector(s) in `a` and/or `b` does not\n",
      "  1519|         0|            0|            0|  0.00%|        equal 2 or 3.\n",
      "  1520|         0|            0|            0|  0.00%|\n",
      "  1521|         0|            0|            0|  0.00%|    See Also\n",
      "  1522|         0|            0|            0|  0.00%|    --------\n",
      "  1523|         0|            0|            0|  0.00%|    inner : Inner product\n",
      "  1524|         0|            0|            0|  0.00%|    outer : Outer product.\n",
      "  1525|         0|            0|            0|  0.00%|    ix_ : Construct index arrays.\n",
      "  1526|         0|            0|            0|  0.00%|\n",
      "  1527|         0|            0|            0|  0.00%|    Notes\n",
      "  1528|         0|            0|            0|  0.00%|    -----\n",
      "  1529|         0|            0|            0|  0.00%|    .. versionadded:: 1.9.0\n",
      "  1530|         0|            0|            0|  0.00%|\n",
      "  1531|         0|            0|            0|  0.00%|    Supports full broadcasting of the inputs.\n",
      "  1532|         0|            0|            0|  0.00%|\n",
      "  1533|         0|            0|            0|  0.00%|    Examples\n",
      "  1534|         0|            0|            0|  0.00%|    --------\n",
      "  1535|         0|            0|            0|  0.00%|    Vector cross-product.\n",
      "  1536|         0|            0|            0|  0.00%|\n",
      "  1537|         0|            0|            0|  0.00%|    >>> x = [1, 2, 3]\n",
      "  1538|         0|            0|            0|  0.00%|    >>> y = [4, 5, 6]\n",
      "  1539|         0|            0|            0|  0.00%|    >>> np.cross(x, y)\n",
      "  1540|         0|            0|            0|  0.00%|    array([-3,  6, -3])\n",
      "  1541|         0|            0|            0|  0.00%|\n",
      "  1542|         0|            0|            0|  0.00%|    One vector with dimension 2.\n",
      "  1543|         0|            0|            0|  0.00%|\n",
      "  1544|         0|            0|            0|  0.00%|    >>> x = [1, 2]\n",
      "  1545|         0|            0|            0|  0.00%|    >>> y = [4, 5, 6]\n",
      "  1546|         0|            0|            0|  0.00%|    >>> np.cross(x, y)\n",
      "  1547|         0|            0|            0|  0.00%|    array([12, -6, -3])\n",
      "  1548|         0|            0|            0|  0.00%|\n",
      "  1549|         0|            0|            0|  0.00%|    Equivalently:\n",
      "  1550|         0|            0|            0|  0.00%|\n",
      "  1551|         0|            0|            0|  0.00%|    >>> x = [1, 2, 0]\n",
      "  1552|         0|            0|            0|  0.00%|    >>> y = [4, 5, 6]\n",
      "  1553|         0|            0|            0|  0.00%|    >>> np.cross(x, y)\n",
      "  1554|         0|            0|            0|  0.00%|    array([12, -6, -3])\n",
      "  1555|         0|            0|            0|  0.00%|\n",
      "  1556|         0|            0|            0|  0.00%|    Both vectors with dimension 2.\n",
      "  1557|         0|            0|            0|  0.00%|\n",
      "  1558|         0|            0|            0|  0.00%|    >>> x = [1,2]\n",
      "  1559|         0|            0|            0|  0.00%|    >>> y = [4,5]\n",
      "  1560|         0|            0|            0|  0.00%|    >>> np.cross(x, y)\n",
      "  1561|         0|            0|            0|  0.00%|    array(-3)\n",
      "  1562|         0|            0|            0|  0.00%|\n",
      "  1563|         0|            0|            0|  0.00%|    Multiple vector cross-products. Note that the direction of the cross\n",
      "  1564|         0|            0|            0|  0.00%|    product vector is defined by the `right-hand rule`.\n",
      "  1565|         0|            0|            0|  0.00%|\n",
      "  1566|         0|            0|            0|  0.00%|    >>> x = np.array([[1,2,3], [4,5,6]])\n",
      "  1567|         0|            0|            0|  0.00%|    >>> y = np.array([[4,5,6], [1,2,3]])\n",
      "  1568|         0|            0|            0|  0.00%|    >>> np.cross(x, y)\n",
      "  1569|         0|            0|            0|  0.00%|    array([[-3,  6, -3],\n",
      "  1570|         0|            0|            0|  0.00%|           [ 3, -6,  3]])\n",
      "  1571|         0|            0|            0|  0.00%|\n",
      "  1572|         0|            0|            0|  0.00%|    The orientation of `c` can be changed using the `axisc` keyword.\n",
      "  1573|         0|            0|            0|  0.00%|\n",
      "  1574|         0|            0|            0|  0.00%|    >>> np.cross(x, y, axisc=0)\n",
      "  1575|         0|            0|            0|  0.00%|    array([[-3,  3],\n",
      "  1576|         0|            0|            0|  0.00%|           [ 6, -6],\n",
      "  1577|         0|            0|            0|  0.00%|           [-3,  3]])\n",
      "  1578|         0|            0|            0|  0.00%|\n",
      "  1579|         0|            0|            0|  0.00%|    Change the vector definition of `x` and `y` using `axisa` and `axisb`.\n",
      "  1580|         0|            0|            0|  0.00%|\n",
      "  1581|         0|            0|            0|  0.00%|    >>> x = np.array([[1,2,3], [4,5,6], [7, 8, 9]])\n",
      "  1582|         0|            0|            0|  0.00%|    >>> y = np.array([[7, 8, 9], [4,5,6], [1,2,3]])\n",
      "  1583|         0|            0|            0|  0.00%|    >>> np.cross(x, y)\n",
      "  1584|         0|            0|            0|  0.00%|    array([[ -6,  12,  -6],\n",
      "  1585|         0|            0|            0|  0.00%|           [  0,   0,   0],\n",
      "  1586|         0|            0|            0|  0.00%|           [  6, -12,   6]])\n",
      "  1587|         0|            0|            0|  0.00%|    >>> np.cross(x, y, axisa=0, axisb=0)\n",
      "  1588|         0|            0|            0|  0.00%|    array([[-24,  48, -24],\n",
      "  1589|         0|            0|            0|  0.00%|           [-30,  60, -30],\n",
      "  1590|         0|            0|            0|  0.00%|           [-36,  72, -36]])\n",
      "  1591|         0|            0|            0|  0.00%|\n",
      "  1592|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1593|         0|            0|            0|  0.00%|    if axis is not None:\n",
      "  1594|         0|            0|            0|  0.00%|        axisa, axisb, axisc = (axis,) * 3\n",
      "  1595|         0|            0|            0|  0.00%|    a = asarray(a)\n",
      "  1596|         0|            0|            0|  0.00%|    b = asarray(b)\n",
      "  1597|         0|            0|            0|  0.00%|    # Check axisa and axisb are within bounds\n",
      "  1598|         0|            0|            0|  0.00%|    axisa = normalize_axis_index(axisa, a.ndim, msg_prefix='axisa')\n",
      "  1599|         0|            0|            0|  0.00%|    axisb = normalize_axis_index(axisb, b.ndim, msg_prefix='axisb')\n",
      "  1600|         0|            0|            0|  0.00%|\n",
      "  1601|         0|            0|            0|  0.00%|    # Move working axis to the end of the shape\n",
      "  1602|         0|            0|            0|  0.00%|    a = moveaxis(a, axisa, -1)\n",
      "  1603|         0|            0|            0|  0.00%|    b = moveaxis(b, axisb, -1)\n",
      "  1604|         0|            0|            0|  0.00%|    msg = (\"incompatible dimensions for cross product\\n\"\n",
      "  1605|         0|            0|            0|  0.00%|           \"(dimension must be 2 or 3)\")\n",
      "  1606|         0|            0|            0|  0.00%|    if a.shape[-1] not in (2, 3) or b.shape[-1] not in (2, 3):\n",
      "  1607|         0|            0|            0|  0.00%|        raise ValueError(msg)\n",
      "  1608|         0|            0|            0|  0.00%|\n",
      "  1609|         0|            0|            0|  0.00%|    # Create the output array\n",
      "  1610|         0|            0|            0|  0.00%|    shape = broadcast(a[..., 0], b[..., 0]).shape\n",
      "  1611|         0|            0|            0|  0.00%|    if a.shape[-1] == 3 or b.shape[-1] == 3:\n",
      "  1612|         0|            0|            0|  0.00%|        shape += (3,)\n",
      "  1613|         0|            0|            0|  0.00%|        # Check axisc is within bounds\n",
      "  1614|         0|            0|            0|  0.00%|        axisc = normalize_axis_index(axisc, len(shape), msg_prefix='axisc')\n",
      "  1615|         0|            0|            0|  0.00%|    dtype = promote_types(a.dtype, b.dtype)\n",
      "  1616|         0|            0|            0|  0.00%|    cp = empty(shape, dtype)\n",
      "  1617|         0|            0|            0|  0.00%|\n",
      "  1618|         0|            0|            0|  0.00%|    # create local aliases for readability\n",
      "  1619|         0|            0|            0|  0.00%|    a0 = a[..., 0]\n",
      "  1620|         0|            0|            0|  0.00%|    a1 = a[..., 1]\n",
      "  1621|         0|            0|            0|  0.00%|    if a.shape[-1] == 3:\n",
      "  1622|         0|            0|            0|  0.00%|        a2 = a[..., 2]\n",
      "  1623|         0|            0|            0|  0.00%|    b0 = b[..., 0]\n",
      "  1624|         0|            0|            0|  0.00%|    b1 = b[..., 1]\n",
      "  1625|         0|            0|            0|  0.00%|    if b.shape[-1] == 3:\n",
      "  1626|         0|            0|            0|  0.00%|        b2 = b[..., 2]\n",
      "  1627|         0|            0|            0|  0.00%|    if cp.ndim != 0 and cp.shape[-1] == 3:\n",
      "  1628|         0|            0|            0|  0.00%|        cp0 = cp[..., 0]\n",
      "  1629|         0|            0|            0|  0.00%|        cp1 = cp[..., 1]\n",
      "  1630|         0|            0|            0|  0.00%|        cp2 = cp[..., 2]\n",
      "  1631|         0|            0|            0|  0.00%|\n",
      "  1632|         0|            0|            0|  0.00%|    if a.shape[-1] == 2:\n",
      "  1633|         0|            0|            0|  0.00%|        if b.shape[-1] == 2:\n",
      "  1634|         0|            0|            0|  0.00%|            # a0 * b1 - a1 * b0\n",
      "  1635|         0|            0|            0|  0.00%|            multiply(a0, b1, out=cp)\n",
      "  1636|         0|            0|            0|  0.00%|            cp -= a1 * b0\n",
      "  1637|         0|            0|            0|  0.00%|            return cp\n",
      "  1638|         0|            0|            0|  0.00%|        else:\n",
      "  1639|         0|            0|            0|  0.00%|            assert b.shape[-1] == 3\n",
      "  1640|         0|            0|            0|  0.00%|            # cp0 = a1 * b2 - 0  (a2 = 0)\n",
      "  1641|         0|            0|            0|  0.00%|            # cp1 = 0 - a0 * b2  (a2 = 0)\n",
      "  1642|         0|            0|            0|  0.00%|            # cp2 = a0 * b1 - a1 * b0\n",
      "  1643|         0|            0|            0|  0.00%|            multiply(a1, b2, out=cp0)\n",
      "  1644|         0|            0|            0|  0.00%|            multiply(a0, b2, out=cp1)\n",
      "  1645|         0|            0|            0|  0.00%|            negative(cp1, out=cp1)\n",
      "  1646|         0|            0|            0|  0.00%|            multiply(a0, b1, out=cp2)\n",
      "  1647|         0|            0|            0|  0.00%|            cp2 -= a1 * b0\n",
      "  1648|         0|            0|            0|  0.00%|    else:\n",
      "  1649|         0|            0|            0|  0.00%|        assert a.shape[-1] == 3\n",
      "  1650|         0|            0|            0|  0.00%|        if b.shape[-1] == 3:\n",
      "  1651|         0|            0|            0|  0.00%|            # cp0 = a1 * b2 - a2 * b1\n",
      "  1652|         0|            0|            0|  0.00%|            # cp1 = a2 * b0 - a0 * b2\n",
      "  1653|         0|            0|            0|  0.00%|            # cp2 = a0 * b1 - a1 * b0\n",
      "  1654|         0|            0|            0|  0.00%|            multiply(a1, b2, out=cp0)\n",
      "  1655|         0|            0|            0|  0.00%|            tmp = array(a2 * b1)\n",
      "  1656|         0|            0|            0|  0.00%|            cp0 -= tmp\n",
      "  1657|         0|            0|            0|  0.00%|            multiply(a2, b0, out=cp1)\n",
      "  1658|         0|            0|            0|  0.00%|            multiply(a0, b2, out=tmp)\n",
      "  1659|         0|            0|            0|  0.00%|            cp1 -= tmp\n",
      "  1660|         0|            0|            0|  0.00%|            multiply(a0, b1, out=cp2)\n",
      "  1661|         0|            0|            0|  0.00%|            multiply(a1, b0, out=tmp)\n",
      "  1662|         0|            0|            0|  0.00%|            cp2 -= tmp\n",
      "  1663|         0|            0|            0|  0.00%|        else:\n",
      "  1664|         0|            0|            0|  0.00%|            assert b.shape[-1] == 2\n",
      "  1665|         0|            0|            0|  0.00%|            # cp0 = 0 - a2 * b1  (b2 = 0)\n",
      "  1666|         0|            0|            0|  0.00%|            # cp1 = a2 * b0 - 0  (b2 = 0)\n",
      "  1667|         0|            0|            0|  0.00%|            # cp2 = a0 * b1 - a1 * b0\n",
      "  1668|         0|            0|            0|  0.00%|            multiply(a2, b1, out=cp0)\n",
      "  1669|         0|            0|            0|  0.00%|            negative(cp0, out=cp0)\n",
      "  1670|         0|            0|            0|  0.00%|            multiply(a2, b0, out=cp1)\n",
      "  1671|         0|            0|            0|  0.00%|            multiply(a0, b1, out=cp2)\n",
      "  1672|         0|            0|            0|  0.00%|            cp2 -= a1 * b0\n",
      "  1673|         0|            0|            0|  0.00%|\n",
      "  1674|         0|            0|            0|  0.00%|    return moveaxis(cp, -1, axisc)\n",
      "  1675|         0|            0|            0|  0.00%|\n",
      "  1676|         0|            0|            0|  0.00%|\n",
      "  1677|         0|            0|            0|  0.00%|little_endian = (sys.byteorder == 'little')\n",
      "  1678|         0|            0|            0|  0.00%|\n",
      "  1679|         0|            0|            0|  0.00%|\n",
      "  1680|         0|            0|            0|  0.00%|@set_module('numpy')\n",
      "  1681|         0|            0|            0|  0.00%|def indices(dimensions, dtype=int, sparse=False):\n",
      "  1682|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1683|         0|            0|            0|  0.00%|    Return an array representing the indices of a grid.\n",
      "  1684|         0|            0|            0|  0.00%|\n",
      "  1685|         0|            0|            0|  0.00%|    Compute an array where the subarrays contain index values 0, 1, ...\n",
      "  1686|         0|            0|            0|  0.00%|    varying only along the corresponding axis.\n",
      "  1687|         0|            0|            0|  0.00%|\n",
      "  1688|         0|            0|            0|  0.00%|    Parameters\n",
      "  1689|         0|            0|            0|  0.00%|    ----------\n",
      "  1690|         0|            0|            0|  0.00%|    dimensions : sequence of ints\n",
      "  1691|         0|            0|            0|  0.00%|        The shape of the grid.\n",
      "  1692|         0|            0|            0|  0.00%|    dtype : dtype, optional\n",
      "  1693|         0|            0|            0|  0.00%|        Data type of the result.\n",
      "  1694|         0|            0|            0|  0.00%|    sparse : boolean, optional\n",
      "  1695|         0|            0|            0|  0.00%|        Return a sparse representation of the grid instead of a dense\n",
      "  1696|         0|            0|            0|  0.00%|        representation. Default is False.\n",
      "  1697|         0|            0|            0|  0.00%|\n",
      "  1698|         0|            0|            0|  0.00%|        .. versionadded:: 1.17\n",
      "  1699|         0|            0|            0|  0.00%|\n",
      "  1700|         0|            0|            0|  0.00%|    Returns\n",
      "  1701|         0|            0|            0|  0.00%|    -------\n",
      "  1702|         0|            0|            0|  0.00%|    grid : one ndarray or tuple of ndarrays\n",
      "  1703|         0|            0|            0|  0.00%|        If sparse is False:\n",
      "  1704|         0|            0|            0|  0.00%|            Returns one array of grid indices,\n",
      "  1705|         0|            0|            0|  0.00%|            ``grid.shape = (len(dimensions),) + tuple(dimensions)``.\n",
      "  1706|         0|            0|            0|  0.00%|        If sparse is True:\n",
      "  1707|         0|            0|            0|  0.00%|            Returns a tuple of arrays, with\n",
      "  1708|         0|            0|            0|  0.00%|            ``grid[i].shape = (1, ..., 1, dimensions[i], 1, ..., 1)`` with\n",
      "  1709|         0|            0|            0|  0.00%|            dimensions[i] in the ith place\n",
      "  1710|         0|            0|            0|  0.00%|\n",
      "  1711|         0|            0|            0|  0.00%|    See Also\n",
      "  1712|         0|            0|            0|  0.00%|    --------\n",
      "  1713|         0|            0|            0|  0.00%|    mgrid, ogrid, meshgrid\n",
      "  1714|         0|            0|            0|  0.00%|\n",
      "  1715|         0|            0|            0|  0.00%|    Notes\n",
      "  1716|         0|            0|            0|  0.00%|    -----\n",
      "  1717|         0|            0|            0|  0.00%|    The output shape in the dense case is obtained by prepending the number\n",
      "  1718|         0|            0|            0|  0.00%|    of dimensions in front of the tuple of dimensions, i.e. if `dimensions`\n",
      "  1719|         0|            0|            0|  0.00%|    is a tuple ``(r0, ..., rN-1)`` of length ``N``, the output shape is\n",
      "  1720|         0|            0|            0|  0.00%|    ``(N, r0, ..., rN-1)``.\n",
      "  1721|         0|            0|            0|  0.00%|\n",
      "  1722|         0|            0|            0|  0.00%|    The subarrays ``grid[k]`` contains the N-D array of indices along the\n",
      "  1723|         0|            0|            0|  0.00%|    ``k-th`` axis. Explicitly::\n",
      "  1724|         0|            0|            0|  0.00%|\n",
      "  1725|         0|            0|            0|  0.00%|        grid[k, i0, i1, ..., iN-1] = ik\n",
      "  1726|         0|            0|            0|  0.00%|\n",
      "  1727|         0|            0|            0|  0.00%|    Examples\n",
      "  1728|         0|            0|            0|  0.00%|    --------\n",
      "  1729|         0|            0|            0|  0.00%|    >>> grid = np.indices((2, 3))\n",
      "  1730|         0|            0|            0|  0.00%|    >>> grid.shape\n",
      "  1731|         0|            0|            0|  0.00%|    (2, 2, 3)\n",
      "  1732|         0|            0|            0|  0.00%|    >>> grid[0]        # row indices\n",
      "  1733|         0|            0|            0|  0.00%|    array([[0, 0, 0],\n",
      "  1734|         0|            0|            0|  0.00%|           [1, 1, 1]])\n",
      "  1735|         0|            0|            0|  0.00%|    >>> grid[1]        # column indices\n",
      "  1736|         0|            0|            0|  0.00%|    array([[0, 1, 2],\n",
      "  1737|         0|            0|            0|  0.00%|           [0, 1, 2]])\n",
      "  1738|         0|            0|            0|  0.00%|\n",
      "  1739|         0|            0|            0|  0.00%|    The indices can be used as an index into an array.\n",
      "  1740|         0|            0|            0|  0.00%|\n",
      "  1741|         0|            0|            0|  0.00%|    >>> x = np.arange(20).reshape(5, 4)\n",
      "  1742|         0|            0|            0|  0.00%|    >>> row, col = np.indices((2, 3))\n",
      "  1743|         0|            0|            0|  0.00%|    >>> x[row, col]\n",
      "  1744|         0|            0|            0|  0.00%|    array([[0, 1, 2],\n",
      "  1745|         0|            0|            0|  0.00%|           [4, 5, 6]])\n",
      "  1746|         0|            0|            0|  0.00%|\n",
      "  1747|         0|            0|            0|  0.00%|    Note that it would be more straightforward in the above example to\n",
      "  1748|         0|            0|            0|  0.00%|    extract the required elements directly with ``x[:2, :3]``.\n",
      "  1749|         0|            0|            0|  0.00%|\n",
      "  1750|         0|            0|            0|  0.00%|    If sparse is set to true, the grid will be returned in a sparse\n",
      "  1751|         0|            0|            0|  0.00%|    representation.\n",
      "  1752|         0|            0|            0|  0.00%|\n",
      "  1753|         0|            0|            0|  0.00%|    >>> i, j = np.indices((2, 3), sparse=True)\n",
      "  1754|         0|            0|            0|  0.00%|    >>> i.shape\n",
      "  1755|         0|            0|            0|  0.00%|    (2, 1)\n",
      "  1756|         0|            0|            0|  0.00%|    >>> j.shape\n",
      "  1757|         0|            0|            0|  0.00%|    (1, 3)\n",
      "  1758|         0|            0|            0|  0.00%|    >>> i        # row indices\n",
      "  1759|         0|            0|            0|  0.00%|    array([[0],\n",
      "  1760|         0|            0|            0|  0.00%|           [1]])\n",
      "  1761|         0|            0|            0|  0.00%|    >>> j        # column indices\n",
      "  1762|         0|            0|            0|  0.00%|    array([[0, 1, 2]])\n",
      "  1763|         0|            0|            0|  0.00%|\n",
      "  1764|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1765|         0|            0|            0|  0.00%|    dimensions = tuple(dimensions)\n",
      "  1766|         0|            0|            0|  0.00%|    N = len(dimensions)\n",
      "  1767|         0|            0|            0|  0.00%|    shape = (1,)*N\n",
      "  1768|         0|            0|            0|  0.00%|    if sparse:\n",
      "  1769|         0|            0|            0|  0.00%|        res = tuple()\n",
      "  1770|         0|            0|            0|  0.00%|    else:\n",
      "  1771|         0|            0|            0|  0.00%|        res = empty((N,)+dimensions, dtype=dtype)\n",
      "  1772|         0|            0|            0|  0.00%|    for i, dim in enumerate(dimensions):\n",
      "  1773|         0|            0|            0|  0.00%|        idx = arange(dim, dtype=dtype).reshape(\n",
      "  1774|         0|            0|            0|  0.00%|            shape[:i] + (dim,) + shape[i+1:]\n",
      "  1775|         0|            0|            0|  0.00%|        )\n",
      "  1776|         0|            0|            0|  0.00%|        if sparse:\n",
      "  1777|         0|            0|            0|  0.00%|            res = res + (idx,)\n",
      "  1778|         0|            0|            0|  0.00%|        else:\n",
      "  1779|         0|            0|            0|  0.00%|            res[i] = idx\n",
      "  1780|         0|            0|            0|  0.00%|    return res\n",
      "  1781|         0|            0|            0|  0.00%|\n",
      "  1782|         0|            0|            0|  0.00%|\n",
      "  1783|         0|            0|            0|  0.00%|def _fromfunction_dispatcher(function, shape, *, dtype=None, like=None, **kwargs):\n",
      "  1784|         0|            0|            0|  0.00%|    return (like,)\n",
      "  1785|         0|            0|            0|  0.00%|\n",
      "  1786|         0|            0|            0|  0.00%|\n",
      "  1787|         0|            0|            0|  0.00%|@set_array_function_like_doc\n",
      "  1788|         0|            0|            0|  0.00%|@set_module('numpy')\n",
      "  1789|         0|            0|            0|  0.00%|def fromfunction(function, shape, *, dtype=float, like=None, **kwargs):\n",
      "  1790|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1791|         0|            0|            0|  0.00%|    Construct an array by executing a function over each coordinate.\n",
      "  1792|         0|            0|            0|  0.00%|\n",
      "  1793|         0|            0|            0|  0.00%|    The resulting array therefore has a value ``fn(x, y, z)`` at\n",
      "  1794|         0|            0|            0|  0.00%|    coordinate ``(x, y, z)``.\n",
      "  1795|         0|            0|            0|  0.00%|\n",
      "  1796|         0|            0|            0|  0.00%|    Parameters\n",
      "  1797|         0|            0|            0|  0.00%|    ----------\n",
      "  1798|         0|            0|            0|  0.00%|    function : callable\n",
      "  1799|         0|            0|            0|  0.00%|        The function is called with N parameters, where N is the rank of\n",
      "  1800|         0|            0|            0|  0.00%|        `shape`.  Each parameter represents the coordinates of the array\n",
      "  1801|         0|            0|            0|  0.00%|        varying along a specific axis.  For example, if `shape`\n",
      "  1802|         0|            0|            0|  0.00%|        were ``(2, 2)``, then the parameters would be\n",
      "  1803|         0|            0|            0|  0.00%|        ``array([[0, 0], [1, 1]])`` and ``array([[0, 1], [0, 1]])``\n",
      "  1804|         0|            0|            0|  0.00%|    shape : (N,) tuple of ints\n",
      "  1805|         0|            0|            0|  0.00%|        Shape of the output array, which also determines the shape of\n",
      "  1806|         0|            0|            0|  0.00%|        the coordinate arrays passed to `function`.\n",
      "  1807|         0|            0|            0|  0.00%|    dtype : data-type, optional\n",
      "  1808|         0|            0|            0|  0.00%|        Data-type of the coordinate arrays passed to `function`.\n",
      "  1809|         0|            0|            0|  0.00%|        By default, `dtype` is float.\n",
      "  1810|         0|            0|            0|  0.00%|    ${ARRAY_FUNCTION_LIKE}\n",
      "  1811|         0|            0|            0|  0.00%|\n",
      "  1812|         0|            0|            0|  0.00%|        .. versionadded:: 1.20.0\n",
      "  1813|         0|            0|            0|  0.00%|\n",
      "  1814|         0|            0|            0|  0.00%|    Returns\n",
      "  1815|         0|            0|            0|  0.00%|    -------\n",
      "  1816|         0|            0|            0|  0.00%|    fromfunction : any\n",
      "  1817|         0|            0|            0|  0.00%|        The result of the call to `function` is passed back directly.\n",
      "  1818|         0|            0|            0|  0.00%|        Therefore the shape of `fromfunction` is completely determined by\n",
      "  1819|         0|            0|            0|  0.00%|        `function`.  If `function` returns a scalar value, the shape of\n",
      "  1820|         0|            0|            0|  0.00%|        `fromfunction` would not match the `shape` parameter.\n",
      "  1821|         0|            0|            0|  0.00%|\n",
      "  1822|         0|            0|            0|  0.00%|    See Also\n",
      "  1823|         0|            0|            0|  0.00%|    --------\n",
      "  1824|         0|            0|            0|  0.00%|    indices, meshgrid\n",
      "  1825|         0|            0|            0|  0.00%|\n",
      "  1826|         0|            0|            0|  0.00%|    Notes\n",
      "  1827|         0|            0|            0|  0.00%|    -----\n",
      "  1828|         0|            0|            0|  0.00%|    Keywords other than `dtype` are passed to `function`.\n",
      "  1829|         0|            0|            0|  0.00%|\n",
      "  1830|         0|            0|            0|  0.00%|    Examples\n",
      "  1831|         0|            0|            0|  0.00%|    --------\n",
      "  1832|         0|            0|            0|  0.00%|    >>> np.fromfunction(lambda i, j: i == j, (3, 3), dtype=int)\n",
      "  1833|         0|            0|            0|  0.00%|    array([[ True, False, False],\n",
      "  1834|         0|            0|            0|  0.00%|           [False,  True, False],\n",
      "  1835|         0|            0|            0|  0.00%|           [False, False,  True]])\n",
      "  1836|         0|            0|            0|  0.00%|\n",
      "  1837|         0|            0|            0|  0.00%|    >>> np.fromfunction(lambda i, j: i + j, (3, 3), dtype=int)\n",
      "  1838|         0|            0|            0|  0.00%|    array([[0, 1, 2],\n",
      "  1839|         0|            0|            0|  0.00%|           [1, 2, 3],\n",
      "  1840|         0|            0|            0|  0.00%|           [2, 3, 4]])\n",
      "  1841|         0|            0|            0|  0.00%|\n",
      "  1842|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1843|         0|            0|            0|  0.00%|    if like is not None:\n",
      "  1844|         0|            0|            0|  0.00%|        return _fromfunction_with_like(function, shape, dtype=dtype, like=like, **kwargs)\n",
      "  1845|         0|            0|            0|  0.00%|\n",
      "  1846|         0|            0|            0|  0.00%|    args = indices(shape, dtype=dtype)\n",
      "  1847|         0|            0|            0|  0.00%|    return function(*args, **kwargs)\n",
      "  1848|         0|            0|            0|  0.00%|\n",
      "  1849|         0|            0|            0|  0.00%|\n",
      "  1850|         0|            0|            0|  0.00%|_fromfunction_with_like = array_function_dispatch(\n",
      "  1851|         0|            0|            0|  0.00%|    _fromfunction_dispatcher\n",
      "  1852|         0|            0|            0|  0.00%|)(fromfunction)\n",
      "  1853|         0|            0|            0|  0.00%|\n",
      "  1854|         0|            0|            0|  0.00%|\n",
      "  1855|         0|            0|            0|  0.00%|def _frombuffer(buf, dtype, shape, order):\n",
      "  1856|         0|            0|            0|  0.00%|    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)\n",
      "  1857|         0|            0|            0|  0.00%|\n",
      "  1858|         0|            0|            0|  0.00%|\n",
      "  1859|         0|            0|            0|  0.00%|@set_module('numpy')\n",
      "  1860|         0|            0|            0|  0.00%|def isscalar(element):\n",
      "  1861|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1862|         0|            0|            0|  0.00%|    Returns True if the type of `element` is a scalar type.\n",
      "  1863|         0|            0|            0|  0.00%|\n",
      "  1864|         0|            0|            0|  0.00%|    Parameters\n",
      "  1865|         0|            0|            0|  0.00%|    ----------\n",
      "  1866|         0|            0|            0|  0.00%|    element : any\n",
      "  1867|         0|            0|            0|  0.00%|        Input argument, can be of any type and shape.\n",
      "  1868|         0|            0|            0|  0.00%|\n",
      "  1869|         0|            0|            0|  0.00%|    Returns\n",
      "  1870|         0|            0|            0|  0.00%|    -------\n",
      "  1871|         0|            0|            0|  0.00%|    val : bool\n",
      "  1872|         0|            0|            0|  0.00%|        True if `element` is a scalar type, False if it is not.\n",
      "  1873|         0|            0|            0|  0.00%|\n",
      "  1874|         0|            0|            0|  0.00%|    See Also\n",
      "  1875|         0|            0|            0|  0.00%|    --------\n",
      "  1876|         0|            0|            0|  0.00%|    ndim : Get the number of dimensions of an array\n",
      "  1877|         0|            0|            0|  0.00%|\n",
      "  1878|         0|            0|            0|  0.00%|    Notes\n",
      "  1879|         0|            0|            0|  0.00%|    -----\n",
      "  1880|         0|            0|            0|  0.00%|    If you need a stricter way to identify a *numerical* scalar, use\n",
      "  1881|         0|            0|            0|  0.00%|    ``isinstance(x, numbers.Number)``, as that returns ``False`` for most\n",
      "  1882|         0|            0|            0|  0.00%|    non-numerical elements such as strings.\n",
      "  1883|         0|            0|            0|  0.00%|\n",
      "  1884|         0|            0|            0|  0.00%|    In most cases ``np.ndim(x) == 0`` should be used instead of this function,\n",
      "  1885|         0|            0|            0|  0.00%|    as that will also return true for 0d arrays. This is how numpy overloads\n",
      "  1886|         0|            0|            0|  0.00%|    functions in the style of the ``dx`` arguments to `gradient` and the ``bins``\n",
      "  1887|         0|            0|            0|  0.00%|    argument to `histogram`. Some key differences:\n",
      "  1888|         0|            0|            0|  0.00%|\n",
      "  1889|         0|            0|            0|  0.00%|    +--------------------------------------+---------------+-------------------+\n",
      "  1890|         0|            0|            0|  0.00%|    | x                                    |``isscalar(x)``|``np.ndim(x) == 0``|\n",
      "  1891|         0|            0|            0|  0.00%|    +======================================+===============+===================+\n",
      "  1892|         0|            0|            0|  0.00%|    | PEP 3141 numeric objects (including  | ``True``      | ``True``          |\n",
      "  1893|         0|            0|            0|  0.00%|    | builtins)                            |               |                   |\n",
      "  1894|         0|            0|            0|  0.00%|    +--------------------------------------+---------------+-------------------+\n",
      "  1895|         0|            0|            0|  0.00%|    | builtin string and buffer objects    | ``True``      | ``True``          |\n",
      "  1896|         0|            0|            0|  0.00%|    +--------------------------------------+---------------+-------------------+\n",
      "  1897|         0|            0|            0|  0.00%|    | other builtin objects, like          | ``False``     | ``True``          |\n",
      "  1898|         0|            0|            0|  0.00%|    | `pathlib.Path`, `Exception`,         |               |                   |\n",
      "  1899|         0|            0|            0|  0.00%|    | the result of `re.compile`           |               |                   |\n",
      "  1900|         0|            0|            0|  0.00%|    +--------------------------------------+---------------+-------------------+\n",
      "  1901|         0|            0|            0|  0.00%|    | third-party objects like             | ``False``     | ``True``          |\n",
      "  1902|         0|            0|            0|  0.00%|    | `matplotlib.figure.Figure`           |               |                   |\n",
      "  1903|         0|            0|            0|  0.00%|    +--------------------------------------+---------------+-------------------+\n",
      "  1904|         0|            0|            0|  0.00%|    | zero-dimensional numpy arrays        | ``False``     | ``True``          |\n",
      "  1905|         0|            0|            0|  0.00%|    +--------------------------------------+---------------+-------------------+\n",
      "  1906|         0|            0|            0|  0.00%|    | other numpy arrays                   | ``False``     | ``False``         |\n",
      "  1907|         0|            0|            0|  0.00%|    +--------------------------------------+---------------+-------------------+\n",
      "  1908|         0|            0|            0|  0.00%|    | `list`, `tuple`, and other sequence  | ``False``     | ``False``         |\n",
      "  1909|         0|            0|            0|  0.00%|    | objects                              |               |                   |\n",
      "  1910|         0|            0|            0|  0.00%|    +--------------------------------------+---------------+-------------------+\n",
      "  1911|         0|            0|            0|  0.00%|\n",
      "  1912|         0|            0|            0|  0.00%|    Examples\n",
      "  1913|         0|            0|            0|  0.00%|    --------\n",
      "  1914|         0|            0|            0|  0.00%|    >>> np.isscalar(3.1)\n",
      "  1915|         0|            0|            0|  0.00%|    True\n",
      "  1916|         0|            0|            0|  0.00%|    >>> np.isscalar(np.array(3.1))\n",
      "  1917|         0|            0|            0|  0.00%|    False\n",
      "  1918|         0|            0|            0|  0.00%|    >>> np.isscalar([3.1])\n",
      "  1919|         0|            0|            0|  0.00%|    False\n",
      "  1920|         0|            0|            0|  0.00%|    >>> np.isscalar(False)\n",
      "  1921|         0|            0|            0|  0.00%|    True\n",
      "  1922|         0|            0|            0|  0.00%|    >>> np.isscalar('numpy')\n",
      "  1923|         0|            0|            0|  0.00%|    True\n",
      "  1924|         0|            0|            0|  0.00%|\n",
      "  1925|         0|            0|            0|  0.00%|    NumPy supports PEP 3141 numbers:\n",
      "  1926|         0|            0|            0|  0.00%|\n",
      "  1927|         0|            0|            0|  0.00%|    >>> from fractions import Fraction\n",
      "  1928|         0|            0|            0|  0.00%|    >>> np.isscalar(Fraction(5, 17))\n",
      "  1929|         0|            0|            0|  0.00%|    True\n",
      "  1930|         0|            0|            0|  0.00%|    >>> from numbers import Number\n",
      "  1931|         0|            0|            0|  0.00%|    >>> np.isscalar(Number())\n",
      "  1932|         0|            0|            0|  0.00%|    True\n",
      "  1933|         0|            0|            0|  0.00%|\n",
      "  1934|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1935|         0|            0|            0|  0.00%|    return (isinstance(element, generic)\n",
      "  1936|         0|            0|            0|  0.00%|            or type(element) in ScalarType\n",
      "  1937|         0|            0|            0|  0.00%|            or isinstance(element, numbers.Number))\n",
      "  1938|         0|            0|            0|  0.00%|\n",
      "  1939|         0|            0|            0|  0.00%|\n",
      "  1940|         0|            0|            0|  0.00%|@set_module('numpy')\n",
      "  1941|         0|            0|            0|  0.00%|def binary_repr(num, width=None):\n",
      "  1942|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1943|         0|            0|            0|  0.00%|    Return the binary representation of the input number as a string.\n",
      "  1944|         0|            0|            0|  0.00%|\n",
      "  1945|         0|            0|            0|  0.00%|    For negative numbers, if width is not given, a minus sign is added to the\n",
      "  1946|         0|            0|            0|  0.00%|    front. If width is given, the two's complement of the number is\n",
      "  1947|         0|            0|            0|  0.00%|    returned, with respect to that width.\n",
      "  1948|         0|            0|            0|  0.00%|\n",
      "  1949|         0|            0|            0|  0.00%|    In a two's-complement system negative numbers are represented by the two's\n",
      "  1950|         0|            0|            0|  0.00%|    complement of the absolute value. This is the most common method of\n",
      "  1951|         0|            0|            0|  0.00%|    representing signed integers on computers [1]_. A N-bit two's-complement\n",
      "  1952|         0|            0|            0|  0.00%|    system can represent every integer in the range\n",
      "  1953|         0|            0|            0|  0.00%|    :math:`-2^{N-1}` to :math:`+2^{N-1}-1`.\n",
      "  1954|         0|            0|            0|  0.00%|\n",
      "  1955|         0|            0|            0|  0.00%|    Parameters\n",
      "  1956|         0|            0|            0|  0.00%|    ----------\n",
      "  1957|         0|            0|            0|  0.00%|    num : int\n",
      "  1958|         0|            0|            0|  0.00%|        Only an integer decimal number can be used.\n",
      "  1959|         0|            0|            0|  0.00%|    width : int, optional\n",
      "  1960|         0|            0|            0|  0.00%|        The length of the returned string if `num` is positive, or the length\n",
      "  1961|         0|            0|            0|  0.00%|        of the two's complement if `num` is negative, provided that `width` is\n",
      "  1962|         0|            0|            0|  0.00%|        at least a sufficient number of bits for `num` to be represented in the\n",
      "  1963|         0|            0|            0|  0.00%|        designated form.\n",
      "  1964|         0|            0|            0|  0.00%|\n",
      "  1965|         0|            0|            0|  0.00%|        If the `width` value is insufficient, it will be ignored, and `num` will\n",
      "  1966|         0|            0|            0|  0.00%|        be returned in binary (`num` > 0) or two's complement (`num` < 0) form\n",
      "  1967|         0|            0|            0|  0.00%|        with its width equal to the minimum number of bits needed to represent\n",
      "  1968|         0|            0|            0|  0.00%|        the number in the designated form. This behavior is deprecated and will\n",
      "  1969|         0|            0|            0|  0.00%|        later raise an error.\n",
      "  1970|         0|            0|            0|  0.00%|\n",
      "  1971|         0|            0|            0|  0.00%|        .. deprecated:: 1.12.0\n",
      "  1972|         0|            0|            0|  0.00%|\n",
      "  1973|         0|            0|            0|  0.00%|    Returns\n",
      "  1974|         0|            0|            0|  0.00%|    -------\n",
      "  1975|         0|            0|            0|  0.00%|    bin : str\n",
      "  1976|         0|            0|            0|  0.00%|        Binary representation of `num` or two's complement of `num`.\n",
      "  1977|         0|            0|            0|  0.00%|\n",
      "  1978|         0|            0|            0|  0.00%|    See Also\n",
      "  1979|         0|            0|            0|  0.00%|    --------\n",
      "  1980|         0|            0|            0|  0.00%|    base_repr: Return a string representation of a number in the given base\n",
      "  1981|         0|            0|            0|  0.00%|               system.\n",
      "  1982|         0|            0|            0|  0.00%|    bin: Python's built-in binary representation generator of an integer.\n",
      "  1983|         0|            0|            0|  0.00%|\n",
      "  1984|         0|            0|            0|  0.00%|    Notes\n",
      "  1985|         0|            0|            0|  0.00%|    -----\n",
      "  1986|         0|            0|            0|  0.00%|    `binary_repr` is equivalent to using `base_repr` with base 2, but about 25x\n",
      "  1987|         0|            0|            0|  0.00%|    faster.\n",
      "  1988|         0|            0|            0|  0.00%|\n",
      "  1989|         0|            0|            0|  0.00%|    References\n",
      "  1990|         0|            0|            0|  0.00%|    ----------\n",
      "  1991|         0|            0|            0|  0.00%|    .. [1] Wikipedia, \"Two's complement\",\n",
      "  1992|         0|            0|            0|  0.00%|        https://en.wikipedia.org/wiki/Two's_complement\n",
      "  1993|         0|            0|            0|  0.00%|\n",
      "  1994|         0|            0|            0|  0.00%|    Examples\n",
      "  1995|         0|            0|            0|  0.00%|    --------\n",
      "  1996|         0|            0|            0|  0.00%|    >>> np.binary_repr(3)\n",
      "  1997|         0|            0|            0|  0.00%|    '11'\n",
      "  1998|         0|            0|            0|  0.00%|    >>> np.binary_repr(-3)\n",
      "  1999|         0|            0|            0|  0.00%|    '-11'\n",
      "  2000|         0|            0|            0|  0.00%|    >>> np.binary_repr(3, width=4)\n",
      "  2001|         0|            0|            0|  0.00%|    '0011'\n",
      "  2002|         0|            0|            0|  0.00%|\n",
      "  2003|         0|            0|            0|  0.00%|    The two's complement is returned when the input number is negative and\n",
      "  2004|         0|            0|            0|  0.00%|    width is specified:\n",
      "  2005|         0|            0|            0|  0.00%|\n",
      "  2006|         0|            0|            0|  0.00%|    >>> np.binary_repr(-3, width=3)\n",
      "  2007|         0|            0|            0|  0.00%|    '101'\n",
      "  2008|         0|            0|            0|  0.00%|    >>> np.binary_repr(-3, width=5)\n",
      "  2009|         0|            0|            0|  0.00%|    '11101'\n",
      "  2010|         0|            0|            0|  0.00%|\n",
      "  2011|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2012|         0|            0|            0|  0.00%|    def warn_if_insufficient(width, binwidth):\n",
      "  2013|         0|            0|            0|  0.00%|        if width is not None and width < binwidth:\n",
      "  2014|         0|            0|            0|  0.00%|            warnings.warn(\n",
      "  2015|         0|            0|            0|  0.00%|                \"Insufficient bit width provided. This behavior \"\n",
      "  2016|         0|            0|            0|  0.00%|                \"will raise an error in the future.\", DeprecationWarning,\n",
      "  2017|         0|            0|            0|  0.00%|                stacklevel=3)\n",
      "  2018|         0|            0|            0|  0.00%|\n",
      "  2019|         0|            0|            0|  0.00%|    # Ensure that num is a Python integer to avoid overflow or unwanted\n",
      "  2020|         0|            0|            0|  0.00%|    # casts to floating point.\n",
      "  2021|         0|            0|            0|  0.00%|    num = operator.index(num)\n",
      "  2022|         0|            0|            0|  0.00%|\n",
      "  2023|         0|            0|            0|  0.00%|    if num == 0:\n",
      "  2024|         0|            0|            0|  0.00%|        return '0' * (width or 1)\n",
      "  2025|         0|            0|            0|  0.00%|\n",
      "  2026|         0|            0|            0|  0.00%|    elif num > 0:\n",
      "  2027|         0|            0|            0|  0.00%|        binary = bin(num)[2:]\n",
      "  2028|         0|            0|            0|  0.00%|        binwidth = len(binary)\n",
      "  2029|         0|            0|            0|  0.00%|        outwidth = (binwidth if width is None\n",
      "  2030|         0|            0|            0|  0.00%|                    else max(binwidth, width))\n",
      "  2031|         0|            0|            0|  0.00%|        warn_if_insufficient(width, binwidth)\n",
      "  2032|         0|            0|            0|  0.00%|        return binary.zfill(outwidth)\n",
      "  2033|         0|            0|            0|  0.00%|\n",
      "  2034|         0|            0|            0|  0.00%|    else:\n",
      "  2035|         0|            0|            0|  0.00%|        if width is None:\n",
      "  2036|         0|            0|            0|  0.00%|            return '-' + bin(-num)[2:]\n",
      "  2037|         0|            0|            0|  0.00%|\n",
      "  2038|         0|            0|            0|  0.00%|        else:\n",
      "  2039|         0|            0|            0|  0.00%|            poswidth = len(bin(-num)[2:])\n",
      "  2040|         0|            0|            0|  0.00%|\n",
      "  2041|         0|            0|            0|  0.00%|            # See gh-8679: remove extra digit\n",
      "  2042|         0|            0|            0|  0.00%|            # for numbers at boundaries.\n",
      "  2043|         0|            0|            0|  0.00%|            if 2**(poswidth - 1) == -num:\n",
      "  2044|         0|            0|            0|  0.00%|                poswidth -= 1\n",
      "  2045|         0|            0|            0|  0.00%|\n",
      "  2046|         0|            0|            0|  0.00%|            twocomp = 2**(poswidth + 1) + num\n",
      "  2047|         0|            0|            0|  0.00%|            binary = bin(twocomp)[2:]\n",
      "  2048|         0|            0|            0|  0.00%|            binwidth = len(binary)\n",
      "  2049|         0|            0|            0|  0.00%|\n",
      "  2050|         0|            0|            0|  0.00%|            outwidth = max(binwidth, width)\n",
      "  2051|         0|            0|            0|  0.00%|            warn_if_insufficient(width, binwidth)\n",
      "  2052|         0|            0|            0|  0.00%|            return '1' * (outwidth - binwidth) + binary\n",
      "  2053|         0|            0|            0|  0.00%|\n",
      "  2054|         0|            0|            0|  0.00%|\n",
      "  2055|         0|            0|            0|  0.00%|@set_module('numpy')\n",
      "  2056|         0|            0|            0|  0.00%|def base_repr(number, base=2, padding=0):\n",
      "  2057|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2058|         0|            0|            0|  0.00%|    Return a string representation of a number in the given base system.\n",
      "  2059|         0|            0|            0|  0.00%|\n",
      "  2060|         0|            0|            0|  0.00%|    Parameters\n",
      "  2061|         0|            0|            0|  0.00%|    ----------\n",
      "  2062|         0|            0|            0|  0.00%|    number : int\n",
      "  2063|         0|            0|            0|  0.00%|        The value to convert. Positive and negative values are handled.\n",
      "  2064|         0|            0|            0|  0.00%|    base : int, optional\n",
      "  2065|         0|            0|            0|  0.00%|        Convert `number` to the `base` number system. The valid range is 2-36,\n",
      "  2066|         0|            0|            0|  0.00%|        the default value is 2.\n",
      "  2067|         0|            0|            0|  0.00%|    padding : int, optional\n",
      "  2068|         0|            0|            0|  0.00%|        Number of zeros padded on the left. Default is 0 (no padding).\n",
      "  2069|         0|            0|            0|  0.00%|\n",
      "  2070|         0|            0|            0|  0.00%|    Returns\n",
      "  2071|         0|            0|            0|  0.00%|    -------\n",
      "  2072|         0|            0|            0|  0.00%|    out : str\n",
      "  2073|         0|            0|            0|  0.00%|        String representation of `number` in `base` system.\n",
      "  2074|         0|            0|            0|  0.00%|\n",
      "  2075|         0|            0|            0|  0.00%|    See Also\n",
      "  2076|         0|            0|            0|  0.00%|    --------\n",
      "  2077|         0|            0|            0|  0.00%|    binary_repr : Faster version of `base_repr` for base 2.\n",
      "  2078|         0|            0|            0|  0.00%|\n",
      "  2079|         0|            0|            0|  0.00%|    Examples\n",
      "  2080|         0|            0|            0|  0.00%|    --------\n",
      "  2081|         0|            0|            0|  0.00%|    >>> np.base_repr(5)\n",
      "  2082|         0|            0|            0|  0.00%|    '101'\n",
      "  2083|         0|            0|            0|  0.00%|    >>> np.base_repr(6, 5)\n",
      "  2084|         0|            0|            0|  0.00%|    '11'\n",
      "  2085|         0|            0|            0|  0.00%|    >>> np.base_repr(7, base=5, padding=3)\n",
      "  2086|         0|            0|            0|  0.00%|    '00012'\n",
      "  2087|         0|            0|            0|  0.00%|\n",
      "  2088|         0|            0|            0|  0.00%|    >>> np.base_repr(10, base=16)\n",
      "  2089|         0|            0|            0|  0.00%|    'A'\n",
      "  2090|         0|            0|            0|  0.00%|    >>> np.base_repr(32, base=16)\n",
      "  2091|         0|            0|            0|  0.00%|    '20'\n",
      "  2092|         0|            0|            0|  0.00%|\n",
      "  2093|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2094|         0|            0|            0|  0.00%|    digits = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
      "  2095|         0|            0|            0|  0.00%|    if base > len(digits):\n",
      "  2096|         0|            0|            0|  0.00%|        raise ValueError(\"Bases greater than 36 not handled in base_repr.\")\n",
      "  2097|         0|            0|            0|  0.00%|    elif base < 2:\n",
      "  2098|         0|            0|            0|  0.00%|        raise ValueError(\"Bases less than 2 not handled in base_repr.\")\n",
      "  2099|         0|            0|            0|  0.00%|\n",
      "  2100|         0|            0|            0|  0.00%|    num = abs(number)\n",
      "  2101|         0|            0|            0|  0.00%|    res = []\n",
      "  2102|         0|            0|            0|  0.00%|    while num:\n",
      "  2103|         0|            0|            0|  0.00%|        res.append(digits[num % base])\n",
      "  2104|         0|            0|            0|  0.00%|        num //= base\n",
      "  2105|         0|            0|            0|  0.00%|    if padding:\n",
      "  2106|         0|            0|            0|  0.00%|        res.append('0' * padding)\n",
      "  2107|         0|            0|            0|  0.00%|    if number < 0:\n",
      "  2108|         0|            0|            0|  0.00%|        res.append('-')\n",
      "  2109|         0|            0|            0|  0.00%|    return ''.join(reversed(res or '0'))\n",
      "  2110|         0|            0|            0|  0.00%|\n",
      "  2111|         0|            0|            0|  0.00%|\n",
      "  2112|         0|            0|            0|  0.00%|# These are all essentially abbreviations\n",
      "  2113|         0|            0|            0|  0.00%|# These might wind up in a special abbreviations module\n",
      "  2114|         0|            0|            0|  0.00%|\n",
      "  2115|         0|            0|            0|  0.00%|\n",
      "  2116|         0|            0|            0|  0.00%|def _maketup(descr, val):\n",
      "  2117|         0|            0|            0|  0.00%|    dt = dtype(descr)\n",
      "  2118|         0|            0|            0|  0.00%|    # Place val in all scalar tuples:\n",
      "  2119|         0|            0|            0|  0.00%|    fields = dt.fields\n",
      "  2120|         0|            0|            0|  0.00%|    if fields is None:\n",
      "  2121|         0|            0|            0|  0.00%|        return val\n",
      "  2122|         0|            0|            0|  0.00%|    else:\n",
      "  2123|         0|            0|            0|  0.00%|        res = [_maketup(fields[name][0], val) for name in dt.names]\n",
      "  2124|         0|            0|            0|  0.00%|        return tuple(res)\n",
      "  2125|         0|            0|            0|  0.00%|\n",
      "  2126|         0|            0|            0|  0.00%|\n",
      "  2127|         0|            0|            0|  0.00%|def _identity_dispatcher(n, dtype=None, *, like=None):\n",
      "  2128|         0|            0|            0|  0.00%|    return (like,)\n",
      "  2129|         0|            0|            0|  0.00%|\n",
      "  2130|         0|            0|            0|  0.00%|\n",
      "  2131|         0|            0|            0|  0.00%|@set_array_function_like_doc\n",
      "  2132|         0|            0|            0|  0.00%|@set_module('numpy')\n",
      "  2133|         0|            0|            0|  0.00%|def identity(n, dtype=None, *, like=None):\n",
      "  2134|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2135|         0|            0|            0|  0.00%|    Return the identity array.\n",
      "  2136|         0|            0|            0|  0.00%|\n",
      "  2137|         0|            0|            0|  0.00%|    The identity array is a square array with ones on\n",
      "  2138|         0|            0|            0|  0.00%|    the main diagonal.\n",
      "  2139|         0|            0|            0|  0.00%|\n",
      "  2140|         0|            0|            0|  0.00%|    Parameters\n",
      "  2141|         0|            0|            0|  0.00%|    ----------\n",
      "  2142|         0|            0|            0|  0.00%|    n : int\n",
      "  2143|         0|            0|            0|  0.00%|        Number of rows (and columns) in `n` x `n` output.\n",
      "  2144|         0|            0|            0|  0.00%|    dtype : data-type, optional\n",
      "  2145|         0|            0|            0|  0.00%|        Data-type of the output.  Defaults to ``float``.\n",
      "  2146|         0|            0|            0|  0.00%|    ${ARRAY_FUNCTION_LIKE}\n",
      "  2147|         0|            0|            0|  0.00%|\n",
      "  2148|         0|            0|            0|  0.00%|        .. versionadded:: 1.20.0\n",
      "  2149|         0|            0|            0|  0.00%|\n",
      "  2150|         0|            0|            0|  0.00%|    Returns\n",
      "  2151|         0|            0|            0|  0.00%|    -------\n",
      "  2152|         0|            0|            0|  0.00%|    out : ndarray\n",
      "  2153|         0|            0|            0|  0.00%|        `n` x `n` array with its main diagonal set to one,\n",
      "  2154|         0|            0|            0|  0.00%|        and all other elements 0.\n",
      "  2155|         0|            0|            0|  0.00%|\n",
      "  2156|         0|            0|            0|  0.00%|    Examples\n",
      "  2157|         0|            0|            0|  0.00%|    --------\n",
      "  2158|         0|            0|            0|  0.00%|    >>> np.identity(3)\n",
      "  2159|         0|            0|            0|  0.00%|    array([[1.,  0.,  0.],\n",
      "  2160|         0|            0|            0|  0.00%|           [0.,  1.,  0.],\n",
      "  2161|         0|            0|            0|  0.00%|           [0.,  0.,  1.]])\n",
      "  2162|         0|            0|            0|  0.00%|\n",
      "  2163|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2164|         0|            0|            0|  0.00%|    if like is not None:\n",
      "  2165|         0|            0|            0|  0.00%|        return _identity_with_like(n, dtype=dtype, like=like)\n",
      "  2166|         0|            0|            0|  0.00%|\n",
      "  2167|         0|            0|            0|  0.00%|    from numpy import eye\n",
      "  2168|         0|            0|            0|  0.00%|    return eye(n, dtype=dtype, like=like)\n",
      "  2169|         0|            0|            0|  0.00%|\n",
      "  2170|         0|            0|            0|  0.00%|\n",
      "  2171|         0|            0|            0|  0.00%|_identity_with_like = array_function_dispatch(\n",
      "  2172|         0|            0|            0|  0.00%|    _identity_dispatcher\n",
      "  2173|         0|            0|            0|  0.00%|)(identity)\n",
      "  2174|         0|            0|            0|  0.00%|\n",
      "  2175|         0|            0|            0|  0.00%|\n",
      "  2176|         0|            0|            0|  0.00%|def _allclose_dispatcher(a, b, rtol=None, atol=None, equal_nan=None):\n",
      "  2177|         0|            0|            0|  0.00%|    return (a, b)\n",
      "  2178|         0|            0|            0|  0.00%|\n",
      "  2179|         0|            0|            0|  0.00%|\n",
      "  2180|         0|            0|            0|  0.00%|@array_function_dispatch(_allclose_dispatcher)\n",
      "  2181|         0|            0|            0|  0.00%|def allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n",
      "  2182|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2183|         0|            0|            0|  0.00%|    Returns True if two arrays are element-wise equal within a tolerance.\n",
      "  2184|         0|            0|            0|  0.00%|\n",
      "  2185|         0|            0|            0|  0.00%|    The tolerance values are positive, typically very small numbers.  The\n",
      "  2186|         0|            0|            0|  0.00%|    relative difference (`rtol` * abs(`b`)) and the absolute difference\n",
      "  2187|         0|            0|            0|  0.00%|    `atol` are added together to compare against the absolute difference\n",
      "  2188|         0|            0|            0|  0.00%|    between `a` and `b`.\n",
      "  2189|         0|            0|            0|  0.00%|\n",
      "  2190|         0|            0|            0|  0.00%|    NaNs are treated as equal if they are in the same place and if\n",
      "  2191|         0|            0|            0|  0.00%|    ``equal_nan=True``.  Infs are treated as equal if they are in the same\n",
      "  2192|         0|            0|            0|  0.00%|    place and of the same sign in both arrays.\n",
      "  2193|         0|            0|            0|  0.00%|\n",
      "  2194|         0|            0|            0|  0.00%|    Parameters\n",
      "  2195|         0|            0|            0|  0.00%|    ----------\n",
      "  2196|         0|            0|            0|  0.00%|    a, b : array_like\n",
      "  2197|         0|            0|            0|  0.00%|        Input arrays to compare.\n",
      "  2198|         0|            0|            0|  0.00%|    rtol : float\n",
      "  2199|         0|            0|            0|  0.00%|        The relative tolerance parameter (see Notes).\n",
      "  2200|         0|            0|            0|  0.00%|    atol : float\n",
      "  2201|         0|            0|            0|  0.00%|        The absolute tolerance parameter (see Notes).\n",
      "  2202|         0|            0|            0|  0.00%|    equal_nan : bool\n",
      "  2203|         0|            0|            0|  0.00%|        Whether to compare NaN's as equal.  If True, NaN's in `a` will be\n",
      "  2204|         0|            0|            0|  0.00%|        considered equal to NaN's in `b` in the output array.\n",
      "  2205|         0|            0|            0|  0.00%|\n",
      "  2206|         0|            0|            0|  0.00%|        .. versionadded:: 1.10.0\n",
      "  2207|         0|            0|            0|  0.00%|\n",
      "  2208|         0|            0|            0|  0.00%|    Returns\n",
      "  2209|         0|            0|            0|  0.00%|    -------\n",
      "  2210|         0|            0|            0|  0.00%|    allclose : bool\n",
      "  2211|         0|            0|            0|  0.00%|        Returns True if the two arrays are equal within the given\n",
      "  2212|         0|            0|            0|  0.00%|        tolerance; False otherwise.\n",
      "  2213|         0|            0|            0|  0.00%|\n",
      "  2214|         0|            0|            0|  0.00%|    See Also\n",
      "  2215|         0|            0|            0|  0.00%|    --------\n",
      "  2216|         0|            0|            0|  0.00%|    isclose, all, any, equal\n",
      "  2217|         0|            0|            0|  0.00%|\n",
      "  2218|         0|            0|            0|  0.00%|    Notes\n",
      "  2219|         0|            0|            0|  0.00%|    -----\n",
      "  2220|         0|            0|            0|  0.00%|    If the following equation is element-wise True, then allclose returns\n",
      "  2221|         0|            0|            0|  0.00%|    True.\n",
      "  2222|         0|            0|            0|  0.00%|\n",
      "  2223|         0|            0|            0|  0.00%|     absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))\n",
      "  2224|         0|            0|            0|  0.00%|\n",
      "  2225|         0|            0|            0|  0.00%|    The above equation is not symmetric in `a` and `b`, so that\n",
      "  2226|         0|            0|            0|  0.00%|    ``allclose(a, b)`` might be different from ``allclose(b, a)`` in\n",
      "  2227|         0|            0|            0|  0.00%|    some rare cases.\n",
      "  2228|         0|            0|            0|  0.00%|\n",
      "  2229|         0|            0|            0|  0.00%|    The comparison of `a` and `b` uses standard broadcasting, which\n",
      "  2230|         0|            0|            0|  0.00%|    means that `a` and `b` need not have the same shape in order for\n",
      "  2231|         0|            0|            0|  0.00%|    ``allclose(a, b)`` to evaluate to True.  The same is true for\n",
      "  2232|         0|            0|            0|  0.00%|    `equal` but not `array_equal`.\n",
      "  2233|         0|            0|            0|  0.00%|\n",
      "  2234|         0|            0|            0|  0.00%|    `allclose` is not defined for non-numeric data types.\n",
      "  2235|         0|            0|            0|  0.00%|    `bool` is considered a numeric data-type for this purpose.\n",
      "  2236|         0|            0|            0|  0.00%|\n",
      "  2237|         0|            0|            0|  0.00%|    Examples\n",
      "  2238|         0|            0|            0|  0.00%|    --------\n",
      "  2239|         0|            0|            0|  0.00%|    >>> np.allclose([1e10,1e-7], [1.00001e10,1e-8])\n",
      "  2240|         0|            0|            0|  0.00%|    False\n",
      "  2241|         0|            0|            0|  0.00%|    >>> np.allclose([1e10,1e-8], [1.00001e10,1e-9])\n",
      "  2242|         0|            0|            0|  0.00%|    True\n",
      "  2243|         0|            0|            0|  0.00%|    >>> np.allclose([1e10,1e-8], [1.0001e10,1e-9])\n",
      "  2244|         0|            0|            0|  0.00%|    False\n",
      "  2245|         0|            0|            0|  0.00%|    >>> np.allclose([1.0, np.nan], [1.0, np.nan])\n",
      "  2246|         0|            0|            0|  0.00%|    False\n",
      "  2247|         0|            0|            0|  0.00%|    >>> np.allclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)\n",
      "  2248|         0|            0|            0|  0.00%|    True\n",
      "  2249|         0|            0|            0|  0.00%|\n",
      "  2250|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2251|         0|            0|            0|  0.00%|    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n",
      "  2252|         0|            0|            0|  0.00%|    return bool(res)\n",
      "  2253|         0|            0|            0|  0.00%|\n",
      "  2254|         0|            0|            0|  0.00%|\n",
      "  2255|         0|            0|            0|  0.00%|def _isclose_dispatcher(a, b, rtol=None, atol=None, equal_nan=None):\n",
      "  2256|         0|            0|            0|  0.00%|    return (a, b)\n",
      "  2257|         0|            0|            0|  0.00%|\n",
      "  2258|         0|            0|            0|  0.00%|\n",
      "  2259|         0|            0|            0|  0.00%|@array_function_dispatch(_isclose_dispatcher)\n",
      "  2260|         0|            0|            0|  0.00%|def isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n",
      "  2261|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2262|         0|            0|            0|  0.00%|    Returns a boolean array where two arrays are element-wise equal within a\n",
      "  2263|         0|            0|            0|  0.00%|    tolerance.\n",
      "  2264|         0|            0|            0|  0.00%|\n",
      "  2265|         0|            0|            0|  0.00%|    The tolerance values are positive, typically very small numbers.  The\n",
      "  2266|         0|            0|            0|  0.00%|    relative difference (`rtol` * abs(`b`)) and the absolute difference\n",
      "  2267|         0|            0|            0|  0.00%|    `atol` are added together to compare against the absolute difference\n",
      "  2268|         0|            0|            0|  0.00%|    between `a` and `b`.\n",
      "  2269|         0|            0|            0|  0.00%|\n",
      "  2270|         0|            0|            0|  0.00%|    .. warning:: The default `atol` is not appropriate for comparing numbers\n",
      "  2271|         0|            0|            0|  0.00%|                 that are much smaller than one (see Notes).\n",
      "  2272|         0|            0|            0|  0.00%|\n",
      "  2273|         0|            0|            0|  0.00%|    Parameters\n",
      "  2274|         0|            0|            0|  0.00%|    ----------\n",
      "  2275|         0|            0|            0|  0.00%|    a, b : array_like\n",
      "  2276|         0|            0|            0|  0.00%|        Input arrays to compare.\n",
      "  2277|         0|            0|            0|  0.00%|    rtol : float\n",
      "  2278|         0|            0|            0|  0.00%|        The relative tolerance parameter (see Notes).\n",
      "  2279|         0|            0|            0|  0.00%|    atol : float\n",
      "  2280|         0|            0|            0|  0.00%|        The absolute tolerance parameter (see Notes).\n",
      "  2281|         0|            0|            0|  0.00%|    equal_nan : bool\n",
      "  2282|         0|            0|            0|  0.00%|        Whether to compare NaN's as equal.  If True, NaN's in `a` will be\n",
      "  2283|         0|            0|            0|  0.00%|        considered equal to NaN's in `b` in the output array.\n",
      "  2284|         0|            0|            0|  0.00%|\n",
      "  2285|         0|            0|            0|  0.00%|    Returns\n",
      "  2286|         0|            0|            0|  0.00%|    -------\n",
      "  2287|         0|            0|            0|  0.00%|    y : array_like\n",
      "  2288|         0|            0|            0|  0.00%|        Returns a boolean array of where `a` and `b` are equal within the\n",
      "  2289|         0|            0|            0|  0.00%|        given tolerance. If both `a` and `b` are scalars, returns a single\n",
      "  2290|         0|            0|            0|  0.00%|        boolean value.\n",
      "  2291|         0|            0|            0|  0.00%|\n",
      "  2292|         0|            0|            0|  0.00%|    See Also\n",
      "  2293|         0|            0|            0|  0.00%|    --------\n",
      "  2294|         0|            0|            0|  0.00%|    allclose\n",
      "  2295|         0|            0|            0|  0.00%|    math.isclose\n",
      "  2296|         0|            0|            0|  0.00%|\n",
      "  2297|         0|            0|            0|  0.00%|    Notes\n",
      "  2298|         0|            0|            0|  0.00%|    -----\n",
      "  2299|         0|            0|            0|  0.00%|    .. versionadded:: 1.7.0\n",
      "  2300|         0|            0|            0|  0.00%|\n",
      "  2301|         0|            0|            0|  0.00%|    For finite values, isclose uses the following equation to test whether\n",
      "  2302|         0|            0|            0|  0.00%|    two floating point values are equivalent.\n",
      "  2303|         0|            0|            0|  0.00%|\n",
      "  2304|         0|            0|            0|  0.00%|     absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))\n",
      "  2305|         0|            0|            0|  0.00%|\n",
      "  2306|         0|            0|            0|  0.00%|    Unlike the built-in `math.isclose`, the above equation is not symmetric\n",
      "  2307|         0|            0|            0|  0.00%|    in `a` and `b` -- it assumes `b` is the reference value -- so that\n",
      "  2308|         0|            0|            0|  0.00%|    `isclose(a, b)` might be different from `isclose(b, a)`. Furthermore,\n",
      "  2309|         0|            0|            0|  0.00%|    the default value of atol is not zero, and is used to determine what\n",
      "  2310|         0|            0|            0|  0.00%|    small values should be considered close to zero. The default value is\n",
      "  2311|         0|            0|            0|  0.00%|    appropriate for expected values of order unity: if the expected values\n",
      "  2312|         0|            0|            0|  0.00%|    are significantly smaller than one, it can result in false positives.\n",
      "  2313|         0|            0|            0|  0.00%|    `atol` should be carefully selected for the use case at hand. A zero value\n",
      "  2314|         0|            0|            0|  0.00%|    for `atol` will result in `False` if either `a` or `b` is zero.\n",
      "  2315|         0|            0|            0|  0.00%|\n",
      "  2316|         0|            0|            0|  0.00%|    `isclose` is not defined for non-numeric data types.\n",
      "  2317|         0|            0|            0|  0.00%|    `bool` is considered a numeric data-type for this purpose.\n",
      "  2318|         0|            0|            0|  0.00%|\n",
      "  2319|         0|            0|            0|  0.00%|    Examples\n",
      "  2320|         0|            0|            0|  0.00%|    --------\n",
      "  2321|         0|            0|            0|  0.00%|    >>> np.isclose([1e10,1e-7], [1.00001e10,1e-8])\n",
      "  2322|         0|            0|            0|  0.00%|    array([ True, False])\n",
      "  2323|         0|            0|            0|  0.00%|    >>> np.isclose([1e10,1e-8], [1.00001e10,1e-9])\n",
      "  2324|         0|            0|            0|  0.00%|    array([ True, True])\n",
      "  2325|         0|            0|            0|  0.00%|    >>> np.isclose([1e10,1e-8], [1.0001e10,1e-9])\n",
      "  2326|         0|            0|            0|  0.00%|    array([False,  True])\n",
      "  2327|         0|            0|            0|  0.00%|    >>> np.isclose([1.0, np.nan], [1.0, np.nan])\n",
      "  2328|         0|            0|            0|  0.00%|    array([ True, False])\n",
      "  2329|         0|            0|            0|  0.00%|    >>> np.isclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)\n",
      "  2330|         0|            0|            0|  0.00%|    array([ True, True])\n",
      "  2331|         0|            0|            0|  0.00%|    >>> np.isclose([1e-8, 1e-7], [0.0, 0.0])\n",
      "  2332|         0|            0|            0|  0.00%|    array([ True, False])\n",
      "  2333|         0|            0|            0|  0.00%|    >>> np.isclose([1e-100, 1e-7], [0.0, 0.0], atol=0.0)\n",
      "  2334|         0|            0|            0|  0.00%|    array([False, False])\n",
      "  2335|         0|            0|            0|  0.00%|    >>> np.isclose([1e-10, 1e-10], [1e-20, 0.0])\n",
      "  2336|         0|            0|            0|  0.00%|    array([ True,  True])\n",
      "  2337|         0|            0|            0|  0.00%|    >>> np.isclose([1e-10, 1e-10], [1e-20, 0.999999e-10], atol=0.0)\n",
      "  2338|         0|            0|            0|  0.00%|    array([False,  True])\n",
      "  2339|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2340|         0|            0|            0|  0.00%|    def within_tol(x, y, atol, rtol):\n",
      "  2341|         0|            0|            0|  0.00%|        with errstate(invalid='ignore'):\n",
      "  2342|         0|            0|            0|  0.00%|            return less_equal(abs(x-y), atol + rtol * abs(y))\n",
      "  2343|         0|            0|            0|  0.00%|\n",
      "  2344|         0|            0|            0|  0.00%|    x = asanyarray(a)\n",
      "  2345|         0|            0|            0|  0.00%|    y = asanyarray(b)\n",
      "  2346|         0|            0|            0|  0.00%|\n",
      "  2347|         0|            0|            0|  0.00%|    # Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).\n",
      "  2348|         0|            0|            0|  0.00%|    # This will cause casting of x later. Also, make sure to allow subclasses\n",
      "  2349|         0|            0|            0|  0.00%|    # (e.g., for numpy.ma).\n",
      "  2350|         0|            0|            0|  0.00%|    # NOTE: We explicitly allow timedelta, which used to work. This could\n",
      "  2351|         0|            0|            0|  0.00%|    #       possibly be deprecated. See also gh-18286.\n",
      "  2352|         0|            0|            0|  0.00%|    #       timedelta works if `atol` is an integer or also a timedelta.\n",
      "  2353|         0|            0|            0|  0.00%|    #       Although, the default tolerances are unlikely to be useful\n",
      "  2354|         0|            0|            0|  0.00%|    if y.dtype.kind != \"m\":\n",
      "  2355|         0|            0|            0|  0.00%|        dt = multiarray.result_type(y, 1.)\n",
      "  2356|         0|            0|            0|  0.00%|        y = asanyarray(y, dtype=dt)\n",
      "  2357|         0|            0|            0|  0.00%|\n",
      "  2358|         0|            0|            0|  0.00%|    xfin = isfinite(x)\n",
      "  2359|         0|            0|            0|  0.00%|    yfin = isfinite(y)\n",
      "  2360|         0|            0|            0|  0.00%|    if all(xfin) and all(yfin):\n",
      "  2361|         0|            0|            0|  0.00%|        return within_tol(x, y, atol, rtol)\n",
      "  2362|         0|            0|            0|  0.00%|    else:\n",
      "  2363|         0|            0|            0|  0.00%|        finite = xfin & yfin\n",
      "  2364|         0|            0|            0|  0.00%|        cond = zeros_like(finite, subok=True)\n",
      "  2365|         0|            0|            0|  0.00%|        # Because we're using boolean indexing, x & y must be the same shape.\n",
      "  2366|         0|            0|            0|  0.00%|        # Ideally, we'd just do x, y = broadcast_arrays(x, y). It's in\n",
      "  2367|         0|            0|            0|  0.00%|        # lib.stride_tricks, though, so we can't import it here.\n",
      "  2368|         0|            0|            0|  0.00%|        x = x * ones_like(cond)\n",
      "  2369|         0|            0|            0|  0.00%|        y = y * ones_like(cond)\n",
      "  2370|         0|            0|            0|  0.00%|        # Avoid subtraction with infinite/nan values...\n",
      "  2371|         0|            0|            0|  0.00%|        cond[finite] = within_tol(x[finite], y[finite], atol, rtol)\n",
      "  2372|         0|            0|            0|  0.00%|        # Check for equality of infinite values...\n",
      "  2373|         0|            0|            0|  0.00%|        cond[~finite] = (x[~finite] == y[~finite])\n",
      "  2374|         0|            0|            0|  0.00%|        if equal_nan:\n",
      "  2375|         0|            0|            0|  0.00%|            # Make NaN == NaN\n",
      "  2376|         0|            0|            0|  0.00%|            both_nan = isnan(x) & isnan(y)\n",
      "  2377|         0|            0|            0|  0.00%|\n",
      "  2378|         0|            0|            0|  0.00%|            # Needed to treat masked arrays correctly. = True would not work.\n",
      "  2379|         0|            0|            0|  0.00%|            cond[both_nan] = both_nan[both_nan]\n",
      "  2380|         0|            0|            0|  0.00%|\n",
      "  2381|         0|            0|            0|  0.00%|        return cond[()]  # Flatten 0d arrays to scalars\n",
      "  2382|         0|            0|            0|  0.00%|\n",
      "  2383|         0|            0|            0|  0.00%|\n",
      "  2384|         0|            0|            0|  0.00%|def _array_equal_dispatcher(a1, a2, equal_nan=None):\n",
      "  2385|         0|            0|            0|  0.00%|    return (a1, a2)\n",
      "  2386|         0|            0|            0|  0.00%|\n",
      "  2387|         0|            0|            0|  0.00%|\n",
      "  2388|         0|            0|            0|  0.00%|@array_function_dispatch(_array_equal_dispatcher)\n",
      "  2389|         0|            0|            0|  0.00%|def array_equal(a1, a2, equal_nan=False):\n",
      "  2390|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2391|         0|            0|            0|  0.00%|    True if two arrays have the same shape and elements, False otherwise.\n",
      "  2392|         0|            0|            0|  0.00%|\n",
      "  2393|         0|            0|            0|  0.00%|    Parameters\n",
      "  2394|         0|            0|            0|  0.00%|    ----------\n",
      "  2395|         0|            0|            0|  0.00%|    a1, a2 : array_like\n",
      "  2396|         0|            0|            0|  0.00%|        Input arrays.\n",
      "  2397|         0|            0|            0|  0.00%|    equal_nan : bool\n",
      "  2398|         0|            0|            0|  0.00%|        Whether to compare NaN's as equal. If the dtype of a1 and a2 is\n",
      "  2399|         0|            0|            0|  0.00%|        complex, values will be considered equal if either the real or the\n",
      "  2400|         0|            0|            0|  0.00%|        imaginary component of a given value is ``nan``.\n",
      "  2401|         0|            0|            0|  0.00%|\n",
      "  2402|         0|            0|            0|  0.00%|        .. versionadded:: 1.19.0\n",
      "  2403|         0|            0|            0|  0.00%|\n",
      "  2404|         0|            0|            0|  0.00%|    Returns\n",
      "  2405|         0|            0|            0|  0.00%|    -------\n",
      "  2406|         0|            0|            0|  0.00%|    b : bool\n",
      "  2407|         0|            0|            0|  0.00%|        Returns True if the arrays are equal.\n",
      "  2408|         0|            0|            0|  0.00%|\n",
      "  2409|         0|            0|            0|  0.00%|    See Also\n",
      "  2410|         0|            0|            0|  0.00%|    --------\n",
      "  2411|         0|            0|            0|  0.00%|    allclose: Returns True if two arrays are element-wise equal within a\n",
      "  2412|         0|            0|            0|  0.00%|              tolerance.\n",
      "  2413|         0|            0|            0|  0.00%|    array_equiv: Returns True if input arrays are shape consistent and all\n",
      "  2414|         0|            0|            0|  0.00%|                 elements equal.\n",
      "  2415|         0|            0|            0|  0.00%|\n",
      "  2416|         0|            0|            0|  0.00%|    Examples\n",
      "  2417|         0|            0|            0|  0.00%|    --------\n",
      "  2418|         0|            0|            0|  0.00%|    >>> np.array_equal([1, 2], [1, 2])\n",
      "  2419|         0|            0|            0|  0.00%|    True\n",
      "  2420|         0|            0|            0|  0.00%|    >>> np.array_equal(np.array([1, 2]), np.array([1, 2]))\n",
      "  2421|         0|            0|            0|  0.00%|    True\n",
      "  2422|         0|            0|            0|  0.00%|    >>> np.array_equal([1, 2], [1, 2, 3])\n",
      "  2423|         0|            0|            0|  0.00%|    False\n",
      "  2424|         0|            0|            0|  0.00%|    >>> np.array_equal([1, 2], [1, 4])\n",
      "  2425|         0|            0|            0|  0.00%|    False\n",
      "  2426|         0|            0|            0|  0.00%|    >>> a = np.array([1, np.nan])\n",
      "  2427|         0|            0|            0|  0.00%|    >>> np.array_equal(a, a)\n",
      "  2428|         0|            0|            0|  0.00%|    False\n",
      "  2429|         0|            0|            0|  0.00%|    >>> np.array_equal(a, a, equal_nan=True)\n",
      "  2430|         0|            0|            0|  0.00%|    True\n",
      "  2431|         0|            0|            0|  0.00%|\n",
      "  2432|         0|            0|            0|  0.00%|    When ``equal_nan`` is True, complex values with nan components are\n",
      "  2433|         0|            0|            0|  0.00%|    considered equal if either the real *or* the imaginary components are nan.\n",
      "  2434|         0|            0|            0|  0.00%|\n",
      "  2435|         0|            0|            0|  0.00%|    >>> a = np.array([1 + 1j])\n",
      "  2436|         0|            0|            0|  0.00%|    >>> b = a.copy()\n",
      "  2437|         0|            0|            0|  0.00%|    >>> a.real = np.nan\n",
      "  2438|         0|            0|            0|  0.00%|    >>> b.imag = np.nan\n",
      "  2439|         0|            0|            0|  0.00%|    >>> np.array_equal(a, b, equal_nan=True)\n",
      "  2440|         0|            0|            0|  0.00%|    True\n",
      "  2441|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2442|         0|            0|            0|  0.00%|    try:\n",
      "  2443|         0|            0|            0|  0.00%|        a1, a2 = asarray(a1), asarray(a2)\n",
      "  2444|         0|            0|            0|  0.00%|    except Exception:\n",
      "  2445|         0|            0|            0|  0.00%|        return False\n",
      "  2446|         0|            0|            0|  0.00%|    if a1.shape != a2.shape:\n",
      "  2447|         0|            0|            0|  0.00%|        return False\n",
      "  2448|         0|            0|            0|  0.00%|    if not equal_nan:\n",
      "  2449|         0|            0|            0|  0.00%|        return bool(asarray(a1 == a2).all())\n",
      "  2450|         0|            0|            0|  0.00%|    # Handling NaN values if equal_nan is True\n",
      "  2451|         0|            0|            0|  0.00%|    a1nan, a2nan = isnan(a1), isnan(a2)\n",
      "  2452|         0|            0|            0|  0.00%|    # NaN's occur at different locations\n",
      "  2453|         0|            0|            0|  0.00%|    if not (a1nan == a2nan).all():\n",
      "  2454|         0|            0|            0|  0.00%|        return False\n",
      "  2455|         0|            0|            0|  0.00%|    # Shapes of a1, a2 and masks are guaranteed to be consistent by this point\n",
      "  2456|         0|            0|            0|  0.00%|    return bool(asarray(a1[~a1nan] == a2[~a1nan]).all())\n",
      "  2457|         0|            0|            0|  0.00%|\n",
      "  2458|         0|            0|            0|  0.00%|\n",
      "  2459|         0|            0|            0|  0.00%|def _array_equiv_dispatcher(a1, a2):\n",
      "  2460|         0|            0|            0|  0.00%|    return (a1, a2)\n",
      "  2461|         0|            0|            0|  0.00%|\n",
      "  2462|         0|            0|            0|  0.00%|\n",
      "  2463|         0|            0|            0|  0.00%|@array_function_dispatch(_array_equiv_dispatcher)\n",
      "  2464|         0|            0|            0|  0.00%|def array_equiv(a1, a2):\n",
      "  2465|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2466|         0|            0|            0|  0.00%|    Returns True if input arrays are shape consistent and all elements equal.\n",
      "  2467|         0|            0|            0|  0.00%|\n",
      "  2468|         0|            0|            0|  0.00%|    Shape consistent means they are either the same shape, or one input array\n",
      "  2469|         0|            0|            0|  0.00%|    can be broadcasted to create the same shape as the other one.\n",
      "  2470|         0|            0|            0|  0.00%|\n",
      "  2471|         0|            0|            0|  0.00%|    Parameters\n",
      "  2472|         0|            0|            0|  0.00%|    ----------\n",
      "  2473|         0|            0|            0|  0.00%|    a1, a2 : array_like\n",
      "  2474|         0|            0|            0|  0.00%|        Input arrays.\n",
      "  2475|         0|            0|            0|  0.00%|\n",
      "  2476|         0|            0|            0|  0.00%|    Returns\n",
      "  2477|         0|            0|            0|  0.00%|    -------\n",
      "  2478|         0|            0|            0|  0.00%|    out : bool\n",
      "  2479|         0|            0|            0|  0.00%|        True if equivalent, False otherwise.\n",
      "  2480|         0|            0|            0|  0.00%|\n",
      "  2481|         0|            0|            0|  0.00%|    Examples\n",
      "  2482|         0|            0|            0|  0.00%|    --------\n",
      "  2483|         0|            0|            0|  0.00%|    >>> np.array_equiv([1, 2], [1, 2])\n",
      "  2484|         0|            0|            0|  0.00%|    True\n",
      "  2485|         0|            0|            0|  0.00%|    >>> np.array_equiv([1, 2], [1, 3])\n",
      "  2486|         0|            0|            0|  0.00%|    False\n",
      "  2487|         0|            0|            0|  0.00%|\n",
      "  2488|         0|            0|            0|  0.00%|    Showing the shape equivalence:\n",
      "  2489|         0|            0|            0|  0.00%|\n",
      "  2490|         0|            0|            0|  0.00%|    >>> np.array_equiv([1, 2], [[1, 2], [1, 2]])\n",
      "  2491|         0|            0|            0|  0.00%|    True\n",
      "  2492|         0|            0|            0|  0.00%|    >>> np.array_equiv([1, 2], [[1, 2, 1, 2], [1, 2, 1, 2]])\n",
      "  2493|         0|            0|            0|  0.00%|    False\n",
      "  2494|         0|            0|            0|  0.00%|\n",
      "  2495|         0|            0|            0|  0.00%|    >>> np.array_equiv([1, 2], [[1, 2], [1, 3]])\n",
      "  2496|         0|            0|            0|  0.00%|    False\n",
      "  2497|         0|            0|            0|  0.00%|\n",
      "  2498|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2499|         0|            0|            0|  0.00%|    try:\n",
      "  2500|         0|            0|            0|  0.00%|        a1, a2 = asarray(a1), asarray(a2)\n",
      "  2501|         0|            0|            0|  0.00%|    except Exception:\n",
      "  2502|         0|            0|            0|  0.00%|        return False\n",
      "  2503|         0|            0|            0|  0.00%|    try:\n",
      "  2504|         0|            0|            0|  0.00%|        multiarray.broadcast(a1, a2)\n",
      "  2505|         0|            0|            0|  0.00%|    except Exception:\n",
      "  2506|         0|            0|            0|  0.00%|        return False\n",
      "  2507|         0|            0|            0|  0.00%|\n",
      "  2508|         0|            0|            0|  0.00%|    return bool(asarray(a1 == a2).all())\n",
      "  2509|         0|            0|            0|  0.00%|\n",
      "  2510|         0|            0|            0|  0.00%|\n",
      "  2511|         0|            0|            0|  0.00%|Inf = inf = infty = Infinity = PINF\n",
      "  2512|         0|            0|            0|  0.00%|nan = NaN = NAN\n",
      "  2513|         0|            0|            0|  0.00%|False_ = bool_(False)\n",
      "  2514|         0|            0|            0|  0.00%|True_ = bool_(True)\n",
      "  2515|         0|            0|            0|  0.00%|\n",
      "  2516|         0|            0|            0|  0.00%|\n",
      "  2517|         0|            0|            0|  0.00%|def extend_all(module):\n",
      "  2518|         0|            0|            0|  0.00%|    existing = set(__all__)\n",
      "  2519|         0|            0|            0|  0.00%|    mall = getattr(module, '__all__')\n",
      "  2520|         0|            0|            0|  0.00%|    for a in mall:\n",
      "  2521|         0|            0|            0|  0.00%|        if a not in existing:\n",
      "  2522|         0|            0|            0|  0.00%|            __all__.append(a)\n",
      "  2523|         0|            0|            0|  0.00%|\n",
      "  2524|         0|            0|            0|  0.00%|\n",
      "  2525|         0|            0|            0|  0.00%|from .umath import *\n",
      "  2526|         0|            0|            0|  0.00%|from .numerictypes import *\n",
      "  2527|         0|            0|            0|  0.00%|from . import fromnumeric\n",
      "  2528|         0|            0|            0|  0.00%|from .fromnumeric import *\n",
      "  2529|         0|            0|            0|  0.00%|from . import arrayprint\n",
      "  2530|         0|            0|            0|  0.00%|from .arrayprint import *\n",
      "  2531|         0|            0|            0|  0.00%|from . import _asarray\n",
      "  2532|         0|            0|            0|  0.00%|from ._asarray import *\n",
      "  2533|         0|            0|            0|  0.00%|from . import _ufunc_config\n",
      "  2534|         0|            0|            0|  0.00%|from ._ufunc_config import *\n",
      "  2535|         0|            0|            0|  0.00%|extend_all(fromnumeric)\n",
      "  2536|         0|            0|            0|  0.00%|extend_all(umath)\n",
      "  2537|         0|            0|            0|  0.00%|extend_all(numerictypes)\n",
      "  2538|         0|            0|            0|  0.00%|extend_all(arrayprint)\n",
      "  2539|         0|            0|            0|  0.00%|extend_all(_asarray)\n",
      "  2540|         0|            0|            0|  0.00%|extend_all(_ufunc_config)\n",
      "File: /apps/open_spiel/open_spiel/python/examples/ubc_math_utils.py\n",
      "File duration: 2.09075s (0.34%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|import numpy as np\n",
      "     2|         0|            0|            0|  0.00%|\n",
      "     3|         0|            0|            0|  0.00%|# Much faster than np.random.choice, at least for our current version of numpy and our distribution over the arguments\n",
      "     4|     74880|     0.150542|  2.01045e-06|  0.02%|def fast_choice(options, probs, rng=None):\n",
      "     5|     74880|     0.485871|  6.48867e-06|  0.08%|    cdf = np.cumsum(probs)\n",
      "(call)|     74880|      5.25236|  7.01437e-05|  0.85%|# <__array_function__ internals>:177 cumsum\n",
      "     6|     74880|     0.347424|  4.63974e-06|  0.06%|    randomness = rng.rand() if rng is not None else np.random.rand()\n",
      "     7|     74880|     0.958365|  1.27987e-05|  0.15%|    i = np.searchsorted(cdf / cdf[-1], randomness)\n",
      "(call)|     74880|      2.91737|  3.89606e-05|  0.47%|# <__array_function__ internals>:177 searchsorted\n",
      "     8|     74880|     0.148551|  1.98385e-06|  0.02%|    return options[i]\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/functional.py\n",
      "File duration: 1.89087s (0.30%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|r\"\"\"Functional interface\"\"\"\n",
      "     2|         0|            0|            0|  0.00%|from typing import Callable, List, Optional, Tuple, Union\n",
      "     3|         0|            0|            0|  0.00%|import math\n",
      "     4|         0|            0|            0|  0.00%|import warnings\n",
      "     5|         0|            0|            0|  0.00%|\n",
      "     6|         0|            0|            0|  0.00%|import torch\n",
      "     7|         0|            0|            0|  0.00%|from torch import _VF\n",
      "     8|         0|            0|            0|  0.00%|from torch._C import _infer_size, _add_docstr\n",
      "     9|         0|            0|            0|  0.00%|from torch._torch_docs import reproducibility_notes, tf32_notes\n",
      "    10|         0|            0|            0|  0.00%|# A workaround to support both TorchScript and MyPy:\n",
      "    11|         0|            0|            0|  0.00%|from typing import TYPE_CHECKING\n",
      "    12|         0|            0|            0|  0.00%|if TYPE_CHECKING:\n",
      "    13|         0|            0|            0|  0.00%|    from torch.types import _dtype as DType\n",
      "    14|         0|            0|            0|  0.00%|else:\n",
      "    15|         0|            0|            0|  0.00%|    # The JIT doesn't understand Union, nor torch.dtype here\n",
      "    16|         0|            0|            0|  0.00%|    DType = int\n",
      "    17|         0|            0|            0|  0.00%|\n",
      "    18|         0|            0|            0|  0.00%|from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3\n",
      "    19|         0|            0|            0|  0.00%|from ..overrides import (\n",
      "    20|         0|            0|            0|  0.00%|    has_torch_function, has_torch_function_unary, has_torch_function_variadic,\n",
      "    21|         0|            0|            0|  0.00%|    handle_torch_function)\n",
      "    22|         0|            0|            0|  0.00%|from . import _reduction as _Reduction\n",
      "    23|         0|            0|            0|  0.00%|from . import grad  # noqa: F401\n",
      "    24|         0|            0|            0|  0.00%|from .modules import utils\n",
      "    25|         0|            0|            0|  0.00%|from .modules.utils import _single, _pair, _triple, _list_with_default\n",
      "    26|         0|            0|            0|  0.00%|\n",
      "    27|         0|            0|            0|  0.00%|\n",
      "    28|         0|            0|            0|  0.00%|Tensor = torch.Tensor\n",
      "    29|         0|            0|            0|  0.00%|\n",
      "    30|         0|            0|            0|  0.00%|conv1d = _add_docstr(\n",
      "    31|         0|            0|            0|  0.00%|    torch.conv1d,\n",
      "    32|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "    33|         0|            0|            0|  0.00%|conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n",
      "    34|         0|            0|            0|  0.00%|\n",
      "    35|         0|            0|            0|  0.00%|Applies a 1D convolution over an input signal composed of several input\n",
      "    36|         0|            0|            0|  0.00%|planes.\n",
      "    37|         0|            0|            0|  0.00%|\n",
      "    38|         0|            0|            0|  0.00%|{tf32_note}\n",
      "    39|         0|            0|            0|  0.00%|\n",
      "    40|         0|            0|            0|  0.00%|See :class:`~torch.nn.Conv1d` for details and output shape.\n",
      "    41|         0|            0|            0|  0.00%|\n",
      "    42|         0|            0|            0|  0.00%|Note:\n",
      "    43|         0|            0|            0|  0.00%|    {cudnn_reproducibility_note}\n",
      "    44|         0|            0|            0|  0.00%|\n",
      "    45|         0|            0|            0|  0.00%|Note:\n",
      "    46|         0|            0|            0|  0.00%|    This operator supports complex data types i.e. ``complex32, complex64, complex128``.\n",
      "    47|         0|            0|            0|  0.00%|\"\"\".format(\n",
      "    48|         0|            0|            0|  0.00%|        **reproducibility_notes, **tf32_notes\n",
      "    49|         0|            0|            0|  0.00%|    )\n",
      "    50|         0|            0|            0|  0.00%|    + r\"\"\"\n",
      "    51|         0|            0|            0|  0.00%|\n",
      "    52|         0|            0|            0|  0.00%|Args:\n",
      "    53|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n",
      "    54|         0|            0|            0|  0.00%|    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kW)`\n",
      "    55|         0|            0|            0|  0.00%|    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n",
      "    56|         0|            0|            0|  0.00%|    stride: the stride of the convolving kernel. Can be a single number or\n",
      "    57|         0|            0|            0|  0.00%|      a one-element tuple `(sW,)`. Default: 1\n",
      "    58|         0|            0|            0|  0.00%|    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n",
      "    59|         0|            0|            0|  0.00%|      single number or a one-element tuple `(padW,)`. Default: 0\n",
      "    60|         0|            0|            0|  0.00%|      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n",
      "    61|         0|            0|            0|  0.00%|      the input so the output has the same shape as the input. However, this mode\n",
      "    62|         0|            0|            0|  0.00%|      doesn't support any stride values other than 1.\n",
      "    63|         0|            0|            0|  0.00%|\n",
      "    64|         0|            0|            0|  0.00%|      .. warning::\n",
      "    65|         0|            0|            0|  0.00%|          For ``padding='same'``, if the ``weight`` is even-length and\n",
      "    66|         0|            0|            0|  0.00%|          ``dilation`` is odd in any dimension, a full :func:`pad` operation\n",
      "    67|         0|            0|            0|  0.00%|          may be needed internally. Lowering performance.\n",
      "    68|         0|            0|            0|  0.00%|    dilation: the spacing between kernel elements. Can be a single number or\n",
      "    69|         0|            0|            0|  0.00%|      a one-element tuple `(dW,)`. Default: 1\n",
      "    70|         0|            0|            0|  0.00%|    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n",
      "    71|         0|            0|            0|  0.00%|      the number of groups. Default: 1\n",
      "    72|         0|            0|            0|  0.00%|\n",
      "    73|         0|            0|            0|  0.00%|Examples::\n",
      "    74|         0|            0|            0|  0.00%|\n",
      "    75|         0|            0|            0|  0.00%|    >>> inputs = torch.randn(33, 16, 30)\n",
      "    76|         0|            0|            0|  0.00%|    >>> filters = torch.randn(20, 16, 5)\n",
      "    77|         0|            0|            0|  0.00%|    >>> F.conv1d(inputs, filters)\n",
      "    78|         0|            0|            0|  0.00%|\"\"\",\n",
      "    79|         0|            0|            0|  0.00%|)\n",
      "    80|         0|            0|            0|  0.00%|\n",
      "    81|         0|            0|            0|  0.00%|conv2d = _add_docstr(\n",
      "    82|         0|            0|            0|  0.00%|    torch.conv2d,\n",
      "    83|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "    84|         0|            0|            0|  0.00%|conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n",
      "    85|         0|            0|            0|  0.00%|\n",
      "    86|         0|            0|            0|  0.00%|Applies a 2D convolution over an input image composed of several input\n",
      "    87|         0|            0|            0|  0.00%|planes.\n",
      "    88|         0|            0|            0|  0.00%|\n",
      "    89|         0|            0|            0|  0.00%|{tf32_note}\n",
      "    90|         0|            0|            0|  0.00%|\n",
      "    91|         0|            0|            0|  0.00%|See :class:`~torch.nn.Conv2d` for details and output shape.\n",
      "    92|         0|            0|            0|  0.00%|\n",
      "    93|         0|            0|            0|  0.00%|Note:\n",
      "    94|         0|            0|            0|  0.00%|    {cudnn_reproducibility_note}\n",
      "    95|         0|            0|            0|  0.00%|\n",
      "    96|         0|            0|            0|  0.00%|Note:\n",
      "    97|         0|            0|            0|  0.00%|    This operator supports complex data types i.e. ``complex32, complex64, complex128``.\n",
      "    98|         0|            0|            0|  0.00%|\"\"\".format(\n",
      "    99|         0|            0|            0|  0.00%|        **reproducibility_notes, **tf32_notes\n",
      "   100|         0|            0|            0|  0.00%|    )\n",
      "   101|         0|            0|            0|  0.00%|    + r\"\"\"\n",
      "   102|         0|            0|            0|  0.00%|\n",
      "   103|         0|            0|            0|  0.00%|Args:\n",
      "   104|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n",
      "   105|         0|            0|            0|  0.00%|    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)`\n",
      "   106|         0|            0|            0|  0.00%|    bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n",
      "   107|         0|            0|            0|  0.00%|    stride: the stride of the convolving kernel. Can be a single number or a\n",
      "   108|         0|            0|            0|  0.00%|      tuple `(sH, sW)`. Default: 1\n",
      "   109|         0|            0|            0|  0.00%|    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n",
      "   110|         0|            0|            0|  0.00%|      single number or a tuple `(padH, padW)`. Default: 0\n",
      "   111|         0|            0|            0|  0.00%|      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n",
      "   112|         0|            0|            0|  0.00%|      the input so the output has the same shape as the input. However, this mode\n",
      "   113|         0|            0|            0|  0.00%|      doesn't support any stride values other than 1.\n",
      "   114|         0|            0|            0|  0.00%|\n",
      "   115|         0|            0|            0|  0.00%|      .. warning::\n",
      "   116|         0|            0|            0|  0.00%|          For ``padding='same'``, if the ``weight`` is even-length and\n",
      "   117|         0|            0|            0|  0.00%|          ``dilation`` is odd in any dimension, a full :func:`pad` operation\n",
      "   118|         0|            0|            0|  0.00%|          may be needed internally. Lowering performance.\n",
      "   119|         0|            0|            0|  0.00%|\n",
      "   120|         0|            0|            0|  0.00%|    dilation: the spacing between kernel elements. Can be a single number or\n",
      "   121|         0|            0|            0|  0.00%|      a tuple `(dH, dW)`. Default: 1\n",
      "   122|         0|            0|            0|  0.00%|    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n",
      "   123|         0|            0|            0|  0.00%|      number of groups. Default: 1\n",
      "   124|         0|            0|            0|  0.00%|\n",
      "   125|         0|            0|            0|  0.00%|Examples::\n",
      "   126|         0|            0|            0|  0.00%|\n",
      "   127|         0|            0|            0|  0.00%|    >>> # With square kernels and equal stride\n",
      "   128|         0|            0|            0|  0.00%|    >>> filters = torch.randn(8, 4, 3, 3)\n",
      "   129|         0|            0|            0|  0.00%|    >>> inputs = torch.randn(1, 4, 5, 5)\n",
      "   130|         0|            0|            0|  0.00%|    >>> F.conv2d(inputs, filters, padding=1)\n",
      "   131|         0|            0|            0|  0.00%|\"\"\",\n",
      "   132|         0|            0|            0|  0.00%|)  # noqa: E501\n",
      "   133|         0|            0|            0|  0.00%|\n",
      "   134|         0|            0|            0|  0.00%|conv3d = _add_docstr(\n",
      "   135|         0|            0|            0|  0.00%|    torch.conv3d,\n",
      "   136|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   137|         0|            0|            0|  0.00%|conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n",
      "   138|         0|            0|            0|  0.00%|\n",
      "   139|         0|            0|            0|  0.00%|Applies a 3D convolution over an input image composed of several input\n",
      "   140|         0|            0|            0|  0.00%|planes.\n",
      "   141|         0|            0|            0|  0.00%|\n",
      "   142|         0|            0|            0|  0.00%|{tf32_note}\n",
      "   143|         0|            0|            0|  0.00%|\n",
      "   144|         0|            0|            0|  0.00%|See :class:`~torch.nn.Conv3d` for details and output shape.\n",
      "   145|         0|            0|            0|  0.00%|\n",
      "   146|         0|            0|            0|  0.00%|Note:\n",
      "   147|         0|            0|            0|  0.00%|    {cudnn_reproducibility_note}\n",
      "   148|         0|            0|            0|  0.00%|\n",
      "   149|         0|            0|            0|  0.00%|Note:\n",
      "   150|         0|            0|            0|  0.00%|    This operator supports complex data types i.e. ``complex32, complex64, complex128``.\n",
      "   151|         0|            0|            0|  0.00%|\"\"\".format(\n",
      "   152|         0|            0|            0|  0.00%|        **reproducibility_notes, **tf32_notes\n",
      "   153|         0|            0|            0|  0.00%|    )\n",
      "   154|         0|            0|            0|  0.00%|    + r\"\"\"\n",
      "   155|         0|            0|            0|  0.00%|\n",
      "   156|         0|            0|            0|  0.00%|Args:\n",
      "   157|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)`\n",
      "   158|         0|            0|            0|  0.00%|    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kT , kH , kW)`\n",
      "   159|         0|            0|            0|  0.00%|    bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: None\n",
      "   160|         0|            0|            0|  0.00%|    stride: the stride of the convolving kernel. Can be a single number or a\n",
      "   161|         0|            0|            0|  0.00%|      tuple `(sT, sH, sW)`. Default: 1\n",
      "   162|         0|            0|            0|  0.00%|    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n",
      "   163|         0|            0|            0|  0.00%|      single number or a tuple `(padT, padH, padW)`. Default: 0\n",
      "   164|         0|            0|            0|  0.00%|      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n",
      "   165|         0|            0|            0|  0.00%|      the input so the output has the same shape as the input. However, this mode\n",
      "   166|         0|            0|            0|  0.00%|      doesn't support any stride values other than 1.\n",
      "   167|         0|            0|            0|  0.00%|\n",
      "   168|         0|            0|            0|  0.00%|      .. warning::\n",
      "   169|         0|            0|            0|  0.00%|          For ``padding='same'``, if the ``weight`` is even-length and\n",
      "   170|         0|            0|            0|  0.00%|          ``dilation`` is odd in any dimension, a full :func:`pad` operation\n",
      "   171|         0|            0|            0|  0.00%|          may be needed internally. Lowering performance.\n",
      "   172|         0|            0|            0|  0.00%|\n",
      "   173|         0|            0|            0|  0.00%|    dilation: the spacing between kernel elements. Can be a single number or\n",
      "   174|         0|            0|            0|  0.00%|      a tuple `(dT, dH, dW)`. Default: 1\n",
      "   175|         0|            0|            0|  0.00%|    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n",
      "   176|         0|            0|            0|  0.00%|      the number of groups. Default: 1\n",
      "   177|         0|            0|            0|  0.00%|\n",
      "   178|         0|            0|            0|  0.00%|Examples::\n",
      "   179|         0|            0|            0|  0.00%|\n",
      "   180|         0|            0|            0|  0.00%|    >>> filters = torch.randn(33, 16, 3, 3, 3)\n",
      "   181|         0|            0|            0|  0.00%|    >>> inputs = torch.randn(20, 16, 50, 10, 20)\n",
      "   182|         0|            0|            0|  0.00%|    >>> F.conv3d(inputs, filters)\n",
      "   183|         0|            0|            0|  0.00%|\"\"\",\n",
      "   184|         0|            0|            0|  0.00%|)  # noqa: E501\n",
      "   185|         0|            0|            0|  0.00%|\n",
      "   186|         0|            0|            0|  0.00%|conv_transpose1d = _add_docstr(\n",
      "   187|         0|            0|            0|  0.00%|    torch.conv_transpose1d,\n",
      "   188|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   189|         0|            0|            0|  0.00%|conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n",
      "   190|         0|            0|            0|  0.00%|\n",
      "   191|         0|            0|            0|  0.00%|Applies a 1D transposed convolution operator over an input signal\n",
      "   192|         0|            0|            0|  0.00%|composed of several input planes, sometimes also called \"deconvolution\".\n",
      "   193|         0|            0|            0|  0.00%|\n",
      "   194|         0|            0|            0|  0.00%|{tf32_note}\n",
      "   195|         0|            0|            0|  0.00%|\n",
      "   196|         0|            0|            0|  0.00%|See :class:`~torch.nn.ConvTranspose1d` for details and output shape.\n",
      "   197|         0|            0|            0|  0.00%|\n",
      "   198|         0|            0|            0|  0.00%|Note:\n",
      "   199|         0|            0|            0|  0.00%|    {cudnn_reproducibility_note}\n",
      "   200|         0|            0|            0|  0.00%|\"\"\".format(\n",
      "   201|         0|            0|            0|  0.00%|        **reproducibility_notes, **tf32_notes\n",
      "   202|         0|            0|            0|  0.00%|    )\n",
      "   203|         0|            0|            0|  0.00%|    + r\"\"\"\n",
      "   204|         0|            0|            0|  0.00%|\n",
      "   205|         0|            0|            0|  0.00%|Args:\n",
      "   206|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n",
      "   207|         0|            0|            0|  0.00%|    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kW)`\n",
      "   208|         0|            0|            0|  0.00%|    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n",
      "   209|         0|            0|            0|  0.00%|    stride: the stride of the convolving kernel. Can be a single number or a\n",
      "   210|         0|            0|            0|  0.00%|      tuple ``(sW,)``. Default: 1\n",
      "   211|         0|            0|            0|  0.00%|    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n",
      "   212|         0|            0|            0|  0.00%|      sides of each dimension in the input. Can be a single number or a tuple\n",
      "   213|         0|            0|            0|  0.00%|      ``(padW,)``. Default: 0\n",
      "   214|         0|            0|            0|  0.00%|    output_padding: additional size added to one side of each dimension in the\n",
      "   215|         0|            0|            0|  0.00%|      output shape. Can be a single number or a tuple ``(out_padW)``. Default: 0\n",
      "   216|         0|            0|            0|  0.00%|    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n",
      "   217|         0|            0|            0|  0.00%|      number of groups. Default: 1\n",
      "   218|         0|            0|            0|  0.00%|    dilation: the spacing between kernel elements. Can be a single number or\n",
      "   219|         0|            0|            0|  0.00%|      a tuple ``(dW,)``. Default: 1\n",
      "   220|         0|            0|            0|  0.00%|\n",
      "   221|         0|            0|            0|  0.00%|Examples::\n",
      "   222|         0|            0|            0|  0.00%|\n",
      "   223|         0|            0|            0|  0.00%|    >>> inputs = torch.randn(20, 16, 50)\n",
      "   224|         0|            0|            0|  0.00%|    >>> weights = torch.randn(16, 33, 5)\n",
      "   225|         0|            0|            0|  0.00%|    >>> F.conv_transpose1d(inputs, weights)\n",
      "   226|         0|            0|            0|  0.00%|\"\"\",\n",
      "   227|         0|            0|            0|  0.00%|)\n",
      "   228|         0|            0|            0|  0.00%|\n",
      "   229|         0|            0|            0|  0.00%|conv_transpose2d = _add_docstr(\n",
      "   230|         0|            0|            0|  0.00%|    torch.conv_transpose2d,\n",
      "   231|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   232|         0|            0|            0|  0.00%|conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n",
      "   233|         0|            0|            0|  0.00%|\n",
      "   234|         0|            0|            0|  0.00%|Applies a 2D transposed convolution operator over an input image\n",
      "   235|         0|            0|            0|  0.00%|composed of several input planes, sometimes also called \"deconvolution\".\n",
      "   236|         0|            0|            0|  0.00%|\n",
      "   237|         0|            0|            0|  0.00%|{tf32_note}\n",
      "   238|         0|            0|            0|  0.00%|\n",
      "   239|         0|            0|            0|  0.00%|See :class:`~torch.nn.ConvTranspose2d` for details and output shape.\n",
      "   240|         0|            0|            0|  0.00%|\n",
      "   241|         0|            0|            0|  0.00%|Note:\n",
      "   242|         0|            0|            0|  0.00%|    {cudnn_reproducibility_note}\n",
      "   243|         0|            0|            0|  0.00%|\"\"\".format(\n",
      "   244|         0|            0|            0|  0.00%|        **reproducibility_notes, **tf32_notes\n",
      "   245|         0|            0|            0|  0.00%|    )\n",
      "   246|         0|            0|            0|  0.00%|    + r\"\"\"\n",
      "   247|         0|            0|            0|  0.00%|\n",
      "   248|         0|            0|            0|  0.00%|Args:\n",
      "   249|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n",
      "   250|         0|            0|            0|  0.00%|    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kH , kW)`\n",
      "   251|         0|            0|            0|  0.00%|    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n",
      "   252|         0|            0|            0|  0.00%|    stride: the stride of the convolving kernel. Can be a single number or a\n",
      "   253|         0|            0|            0|  0.00%|      tuple ``(sH, sW)``. Default: 1\n",
      "   254|         0|            0|            0|  0.00%|    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n",
      "   255|         0|            0|            0|  0.00%|      sides of each dimension in the input. Can be a single number or a tuple\n",
      "   256|         0|            0|            0|  0.00%|      ``(padH, padW)``. Default: 0\n",
      "   257|         0|            0|            0|  0.00%|    output_padding: additional size added to one side of each dimension in the\n",
      "   258|         0|            0|            0|  0.00%|      output shape. Can be a single number or a tuple ``(out_padH, out_padW)``.\n",
      "   259|         0|            0|            0|  0.00%|      Default: 0\n",
      "   260|         0|            0|            0|  0.00%|    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n",
      "   261|         0|            0|            0|  0.00%|      number of groups. Default: 1\n",
      "   262|         0|            0|            0|  0.00%|    dilation: the spacing between kernel elements. Can be a single number or\n",
      "   263|         0|            0|            0|  0.00%|      a tuple ``(dH, dW)``. Default: 1\n",
      "   264|         0|            0|            0|  0.00%|\n",
      "   265|         0|            0|            0|  0.00%|Examples::\n",
      "   266|         0|            0|            0|  0.00%|\n",
      "   267|         0|            0|            0|  0.00%|    >>> # With square kernels and equal stride\n",
      "   268|         0|            0|            0|  0.00%|    >>> inputs = torch.randn(1, 4, 5, 5)\n",
      "   269|         0|            0|            0|  0.00%|    >>> weights = torch.randn(4, 8, 3, 3)\n",
      "   270|         0|            0|            0|  0.00%|    >>> F.conv_transpose2d(inputs, weights, padding=1)\n",
      "   271|         0|            0|            0|  0.00%|\"\"\",\n",
      "   272|         0|            0|            0|  0.00%|)  # noqa: E501\n",
      "   273|         0|            0|            0|  0.00%|\n",
      "   274|         0|            0|            0|  0.00%|conv_transpose3d = _add_docstr(\n",
      "   275|         0|            0|            0|  0.00%|    torch.conv_transpose3d,\n",
      "   276|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   277|         0|            0|            0|  0.00%|conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n",
      "   278|         0|            0|            0|  0.00%|\n",
      "   279|         0|            0|            0|  0.00%|Applies a 3D transposed convolution operator over an input image\n",
      "   280|         0|            0|            0|  0.00%|composed of several input planes, sometimes also called \"deconvolution\"\n",
      "   281|         0|            0|            0|  0.00%|\n",
      "   282|         0|            0|            0|  0.00%|{tf32_note}\n",
      "   283|         0|            0|            0|  0.00%|\n",
      "   284|         0|            0|            0|  0.00%|See :class:`~torch.nn.ConvTranspose3d` for details and output shape.\n",
      "   285|         0|            0|            0|  0.00%|\n",
      "   286|         0|            0|            0|  0.00%|Note:\n",
      "   287|         0|            0|            0|  0.00%|    {cudnn_reproducibility_note}\n",
      "   288|         0|            0|            0|  0.00%|\"\"\".format(\n",
      "   289|         0|            0|            0|  0.00%|        **reproducibility_notes, **tf32_notes\n",
      "   290|         0|            0|            0|  0.00%|    )\n",
      "   291|         0|            0|            0|  0.00%|    + r\"\"\"\n",
      "   292|         0|            0|            0|  0.00%|\n",
      "   293|         0|            0|            0|  0.00%|Args:\n",
      "   294|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)`\n",
      "   295|         0|            0|            0|  0.00%|    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kT , kH , kW)`\n",
      "   296|         0|            0|            0|  0.00%|    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n",
      "   297|         0|            0|            0|  0.00%|    stride: the stride of the convolving kernel. Can be a single number or a\n",
      "   298|         0|            0|            0|  0.00%|      tuple ``(sT, sH, sW)``. Default: 1\n",
      "   299|         0|            0|            0|  0.00%|    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n",
      "   300|         0|            0|            0|  0.00%|      sides of each dimension in the input. Can be a single number or a tuple\n",
      "   301|         0|            0|            0|  0.00%|      ``(padT, padH, padW)``. Default: 0\n",
      "   302|         0|            0|            0|  0.00%|    output_padding: additional size added to one side of each dimension in the\n",
      "   303|         0|            0|            0|  0.00%|      output shape. Can be a single number or a tuple\n",
      "   304|         0|            0|            0|  0.00%|      ``(out_padT, out_padH, out_padW)``. Default: 0\n",
      "   305|         0|            0|            0|  0.00%|    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n",
      "   306|         0|            0|            0|  0.00%|      number of groups. Default: 1\n",
      "   307|         0|            0|            0|  0.00%|    dilation: the spacing between kernel elements. Can be a single number or\n",
      "   308|         0|            0|            0|  0.00%|      a tuple `(dT, dH, dW)`. Default: 1\n",
      "   309|         0|            0|            0|  0.00%|\n",
      "   310|         0|            0|            0|  0.00%|Examples::\n",
      "   311|         0|            0|            0|  0.00%|\n",
      "   312|         0|            0|            0|  0.00%|    >>> inputs = torch.randn(20, 16, 50, 10, 20)\n",
      "   313|         0|            0|            0|  0.00%|    >>> weights = torch.randn(16, 33, 3, 3, 3)\n",
      "   314|         0|            0|            0|  0.00%|    >>> F.conv_transpose3d(inputs, weights)\n",
      "   315|         0|            0|            0|  0.00%|\"\"\",\n",
      "   316|         0|            0|            0|  0.00%|)  # noqa: E501\n",
      "   317|         0|            0|            0|  0.00%|\n",
      "   318|         0|            0|            0|  0.00%|conv_tbc = _add_docstr(\n",
      "   319|         0|            0|            0|  0.00%|    torch.conv_tbc,\n",
      "   320|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   321|         0|            0|            0|  0.00%|Applies a 1-dimensional sequence convolution over an input sequence.\n",
      "   322|         0|            0|            0|  0.00%|Input and output dimensions are (Time, Batch, Channels) - hence TBC.\n",
      "   323|         0|            0|            0|  0.00%|\n",
      "   324|         0|            0|            0|  0.00%|Args:\n",
      "   325|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\\text{sequence length} \\times batch \\times \\text{in\\_channels})`\n",
      "   326|         0|            0|            0|  0.00%|    weight: filter of shape (:math:`\\text{kernel width} \\times \\text{in\\_channels} \\times \\text{out\\_channels}`)\n",
      "   327|         0|            0|            0|  0.00%|    bias: bias of shape (:math:`\\text{out\\_channels}`)\n",
      "   328|         0|            0|            0|  0.00%|    pad: number of timesteps to pad. Default: 0\n",
      "   329|         0|            0|            0|  0.00%|\"\"\",\n",
      "   330|         0|            0|            0|  0.00%|)\n",
      "   331|         0|            0|            0|  0.00%|\n",
      "   332|         0|            0|            0|  0.00%|\n",
      "   333|         0|            0|            0|  0.00%|# Pooling\n",
      "   334|         0|            0|            0|  0.00%|avg_pool1d = _add_docstr(\n",
      "   335|         0|            0|            0|  0.00%|    torch.avg_pool1d,\n",
      "   336|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   337|         0|            0|            0|  0.00%|avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor\n",
      "   338|         0|            0|            0|  0.00%|\n",
      "   339|         0|            0|            0|  0.00%|Applies a 1D average pooling over an input signal composed of several\n",
      "   340|         0|            0|            0|  0.00%|input planes.\n",
      "   341|         0|            0|            0|  0.00%|\n",
      "   342|         0|            0|            0|  0.00%|See :class:`~torch.nn.AvgPool1d` for details and output shape.\n",
      "   343|         0|            0|            0|  0.00%|\n",
      "   344|         0|            0|            0|  0.00%|Args:\n",
      "   345|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n",
      "   346|         0|            0|            0|  0.00%|    kernel_size: the size of the window. Can be a single number or a\n",
      "   347|         0|            0|            0|  0.00%|      tuple `(kW,)`\n",
      "   348|         0|            0|            0|  0.00%|    stride: the stride of the window. Can be a single number or a tuple\n",
      "   349|         0|            0|            0|  0.00%|      `(sW,)`. Default: :attr:`kernel_size`\n",
      "   350|         0|            0|            0|  0.00%|    padding: implicit zero paddings on both sides of the input. Can be a\n",
      "   351|         0|            0|            0|  0.00%|      single number or a tuple `(padW,)`. Default: 0\n",
      "   352|         0|            0|            0|  0.00%|    ceil_mode: when True, will use `ceil` instead of `floor` to compute the\n",
      "   353|         0|            0|            0|  0.00%|        output shape. Default: ``False``\n",
      "   354|         0|            0|            0|  0.00%|    count_include_pad: when True, will include the zero-padding in the\n",
      "   355|         0|            0|            0|  0.00%|        averaging calculation. Default: ``True``\n",
      "   356|         0|            0|            0|  0.00%|\n",
      "   357|         0|            0|            0|  0.00%|Examples::\n",
      "   358|         0|            0|            0|  0.00%|\n",
      "   359|         0|            0|            0|  0.00%|    >>> # pool of square window of size=3, stride=2\n",
      "   360|         0|            0|            0|  0.00%|    >>> input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32)\n",
      "   361|         0|            0|            0|  0.00%|    >>> F.avg_pool1d(input, kernel_size=3, stride=2)\n",
      "   362|         0|            0|            0|  0.00%|    tensor([[[ 2.,  4.,  6.]]])\n",
      "   363|         0|            0|            0|  0.00%|\n",
      "   364|         0|            0|            0|  0.00%|\"\"\",\n",
      "   365|         0|            0|            0|  0.00%|)\n",
      "   366|         0|            0|            0|  0.00%|\n",
      "   367|         0|            0|            0|  0.00%|\n",
      "   368|         0|            0|            0|  0.00%|avg_pool2d = _add_docstr(\n",
      "   369|         0|            0|            0|  0.00%|    torch._C._nn.avg_pool2d,\n",
      "   370|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   371|         0|            0|            0|  0.00%|avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n",
      "   372|         0|            0|            0|  0.00%|\n",
      "   373|         0|            0|            0|  0.00%|Applies 2D average-pooling operation in :math:`kH \\times kW` regions by step size\n",
      "   374|         0|            0|            0|  0.00%|:math:`sH \\times sW` steps. The number of output features is equal to the number of\n",
      "   375|         0|            0|            0|  0.00%|input planes.\n",
      "   376|         0|            0|            0|  0.00%|\n",
      "   377|         0|            0|            0|  0.00%|See :class:`~torch.nn.AvgPool2d` for details and output shape.\n",
      "   378|         0|            0|            0|  0.00%|\n",
      "   379|         0|            0|            0|  0.00%|Args:\n",
      "   380|         0|            0|            0|  0.00%|    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n",
      "   381|         0|            0|            0|  0.00%|    kernel_size: size of the pooling region. Can be a single number or a\n",
      "   382|         0|            0|            0|  0.00%|      tuple `(kH, kW)`\n",
      "   383|         0|            0|            0|  0.00%|    stride: stride of the pooling operation. Can be a single number or a\n",
      "   384|         0|            0|            0|  0.00%|      tuple `(sH, sW)`. Default: :attr:`kernel_size`\n",
      "   385|         0|            0|            0|  0.00%|    padding: implicit zero paddings on both sides of the input. Can be a\n",
      "   386|         0|            0|            0|  0.00%|      single number or a tuple `(padH, padW)`. Default: 0\n",
      "   387|         0|            0|            0|  0.00%|    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n",
      "   388|         0|            0|            0|  0.00%|        to compute the output shape. Default: ``False``\n",
      "   389|         0|            0|            0|  0.00%|    count_include_pad: when True, will include the zero-padding in the\n",
      "   390|         0|            0|            0|  0.00%|        averaging calculation. Default: ``True``\n",
      "   391|         0|            0|            0|  0.00%|    divisor_override: if specified, it will be used as divisor, otherwise\n",
      "   392|         0|            0|            0|  0.00%|         size of the pooling region will be used. Default: None\n",
      "   393|         0|            0|            0|  0.00%|\"\"\",\n",
      "   394|         0|            0|            0|  0.00%|)\n",
      "   395|         0|            0|            0|  0.00%|\n",
      "   396|         0|            0|            0|  0.00%|avg_pool3d = _add_docstr(\n",
      "   397|         0|            0|            0|  0.00%|    torch._C._nn.avg_pool3d,\n",
      "   398|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   399|         0|            0|            0|  0.00%|avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n",
      "   400|         0|            0|            0|  0.00%|\n",
      "   401|         0|            0|            0|  0.00%|Applies 3D average-pooling operation in :math:`kT \\times kH \\times kW` regions by step\n",
      "   402|         0|            0|            0|  0.00%|size :math:`sT \\times sH \\times sW` steps. The number of output features is equal to\n",
      "   403|         0|            0|            0|  0.00%|:math:`\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor`.\n",
      "   404|         0|            0|            0|  0.00%|\n",
      "   405|         0|            0|            0|  0.00%|See :class:`~torch.nn.AvgPool3d` for details and output shape.\n",
      "   406|         0|            0|            0|  0.00%|\n",
      "   407|         0|            0|            0|  0.00%|Args:\n",
      "   408|         0|            0|            0|  0.00%|    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW)`\n",
      "   409|         0|            0|            0|  0.00%|    kernel_size: size of the pooling region. Can be a single number or a\n",
      "   410|         0|            0|            0|  0.00%|      tuple `(kT, kH, kW)`\n",
      "   411|         0|            0|            0|  0.00%|    stride: stride of the pooling operation. Can be a single number or a\n",
      "   412|         0|            0|            0|  0.00%|      tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`\n",
      "   413|         0|            0|            0|  0.00%|    padding: implicit zero paddings on both sides of the input. Can be a\n",
      "   414|         0|            0|            0|  0.00%|      single number or a tuple `(padT, padH, padW)`, Default: 0\n",
      "   415|         0|            0|            0|  0.00%|    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n",
      "   416|         0|            0|            0|  0.00%|        to compute the output shape\n",
      "   417|         0|            0|            0|  0.00%|    count_include_pad: when True, will include the zero-padding in the\n",
      "   418|         0|            0|            0|  0.00%|        averaging calculation\n",
      "   419|         0|            0|            0|  0.00%|    divisor_override: if specified, it will be used as divisor, otherwise\n",
      "   420|         0|            0|            0|  0.00%|        size of the pooling region will be used. Default: None\n",
      "   421|         0|            0|            0|  0.00%|\"\"\",\n",
      "   422|         0|            0|            0|  0.00%|)\n",
      "   423|         0|            0|            0|  0.00%|\n",
      "   424|         0|            0|            0|  0.00%|\n",
      "   425|         0|            0|            0|  0.00%|def fractional_max_pool2d_with_indices(\n",
      "   426|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList2[int],\n",
      "   427|         0|            0|            0|  0.00%|    output_size: Optional[BroadcastingList2[int]] = None,\n",
      "   428|         0|            0|            0|  0.00%|    output_ratio: Optional[BroadcastingList2[float]] = None,\n",
      "   429|         0|            0|            0|  0.00%|    return_indices: bool = False,\n",
      "   430|         0|            0|            0|  0.00%|    _random_samples: Optional[Tensor] = None\n",
      "   431|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:\n",
      "   432|         0|            0|            0|  0.00%|    r\"\"\"Applies 2D fractional max pooling over an input signal composed of several input planes.\n",
      "   433|         0|            0|            0|  0.00%|\n",
      "   434|         0|            0|            0|  0.00%|    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham\n",
      "   435|         0|            0|            0|  0.00%|\n",
      "   436|         0|            0|            0|  0.00%|    The max-pooling operation is applied in :math:`kH \\times kW` regions by a stochastic\n",
      "   437|         0|            0|            0|  0.00%|    step size determined by the target output size.\n",
      "   438|         0|            0|            0|  0.00%|    The number of output features is equal to the number of input planes.\n",
      "   439|         0|            0|            0|  0.00%|\n",
      "   440|         0|            0|            0|  0.00%|    Args:\n",
      "   441|         0|            0|            0|  0.00%|        kernel_size: the size of the window to take a max over.\n",
      "   442|         0|            0|            0|  0.00%|                     Can be a single number :math:`k` (for a square kernel of :math:`k \\times k`)\n",
      "   443|         0|            0|            0|  0.00%|                     or a tuple `(kH, kW)`\n",
      "   444|         0|            0|            0|  0.00%|        output_size: the target output size of the image of the form :math:`oH \\times oW`.\n",
      "   445|         0|            0|            0|  0.00%|                     Can be a tuple `(oH, oW)` or a single number :math:`oH` for a square image :math:`oH \\times oH`\n",
      "   446|         0|            0|            0|  0.00%|        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.\n",
      "   447|         0|            0|            0|  0.00%|                      This has to be a number or tuple in the range (0, 1)\n",
      "   448|         0|            0|            0|  0.00%|        return_indices: if ``True``, will return the indices along with the outputs.\n",
      "   449|         0|            0|            0|  0.00%|                        Useful to pass to :func:`~torch.nn.functional.max_unpool2d`.\n",
      "   450|         0|            0|            0|  0.00%|\n",
      "   451|         0|            0|            0|  0.00%|    Examples::\n",
      "   452|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50, 32)\n",
      "   453|         0|            0|            0|  0.00%|        >>> # pool of square window of size=3, and target output size 13x12\n",
      "   454|         0|            0|            0|  0.00%|        >>> F.fractional_max_pool2d(input, 3, output_size=(13, 12))\n",
      "   455|         0|            0|            0|  0.00%|        >>> # pool of square window and target output size being half of input image size\n",
      "   456|         0|            0|            0|  0.00%|        >>> F.fractional_max_pool2d(input, 3, output_ratio=(0.5, 0.5))\n",
      "   457|         0|            0|            0|  0.00%|\n",
      "   458|         0|            0|            0|  0.00%|    .. _Fractional MaxPooling:\n",
      "   459|         0|            0|            0|  0.00%|        http://arxiv.org/abs/1412.6071\n",
      "   460|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   461|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, _random_samples):\n",
      "   462|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   463|         0|            0|            0|  0.00%|            fractional_max_pool2d_with_indices,\n",
      "   464|         0|            0|            0|  0.00%|            (input, _random_samples),\n",
      "   465|         0|            0|            0|  0.00%|            input,\n",
      "   466|         0|            0|            0|  0.00%|            kernel_size,\n",
      "   467|         0|            0|            0|  0.00%|            output_size=output_size,\n",
      "   468|         0|            0|            0|  0.00%|            output_ratio=output_ratio,\n",
      "   469|         0|            0|            0|  0.00%|            return_indices=return_indices,\n",
      "   470|         0|            0|            0|  0.00%|            _random_samples=_random_samples,\n",
      "   471|         0|            0|            0|  0.00%|        )\n",
      "   472|         0|            0|            0|  0.00%|    if output_size is None and output_ratio is None:\n",
      "   473|         0|            0|            0|  0.00%|        raise ValueError(\"fractional_max_pool2d requires specifying either \" \"an output_size or an output_ratio\")\n",
      "   474|         0|            0|            0|  0.00%|    if output_size is None:\n",
      "   475|         0|            0|            0|  0.00%|        assert output_ratio is not None\n",
      "   476|         0|            0|            0|  0.00%|        _output_ratio = _pair(output_ratio)\n",
      "   477|         0|            0|            0|  0.00%|        output_size = [int(input.size(-2) * _output_ratio[0]), int(input.size(-1) * _output_ratio[1])]\n",
      "   478|         0|            0|            0|  0.00%|\n",
      "   479|         0|            0|            0|  0.00%|    if _random_samples is None:\n",
      "   480|         0|            0|            0|  0.00%|        n_batch = 1 if input.dim() == 3 else input.size(0)\n",
      "   481|         0|            0|            0|  0.00%|        _random_samples = torch.rand(n_batch, input.size(-3), 2, dtype=input.dtype, device=input.device)\n",
      "   482|         0|            0|            0|  0.00%|    return torch._C._nn.fractional_max_pool2d(input, kernel_size, output_size, _random_samples)\n",
      "   483|         0|            0|            0|  0.00%|\n",
      "   484|         0|            0|            0|  0.00%|\n",
      "   485|         0|            0|            0|  0.00%|def _fractional_max_pool2d(\n",
      "   486|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList2[int],\n",
      "   487|         0|            0|            0|  0.00%|    output_size: Optional[BroadcastingList2[int]] = None,\n",
      "   488|         0|            0|            0|  0.00%|    output_ratio: Optional[BroadcastingList2[float]] = None,\n",
      "   489|         0|            0|            0|  0.00%|    return_indices: bool = False,\n",
      "   490|         0|            0|            0|  0.00%|    _random_samples: Optional[Tensor] = None\n",
      "   491|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "   492|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, _random_samples):\n",
      "   493|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   494|         0|            0|            0|  0.00%|            fractional_max_pool2d,\n",
      "   495|         0|            0|            0|  0.00%|            (input, _random_samples),\n",
      "   496|         0|            0|            0|  0.00%|            input,\n",
      "   497|         0|            0|            0|  0.00%|            kernel_size,\n",
      "   498|         0|            0|            0|  0.00%|            output_size=output_size,\n",
      "   499|         0|            0|            0|  0.00%|            output_ratio=output_ratio,\n",
      "   500|         0|            0|            0|  0.00%|            return_indices=return_indices,\n",
      "   501|         0|            0|            0|  0.00%|            _random_samples=_random_samples,\n",
      "   502|         0|            0|            0|  0.00%|        )\n",
      "   503|         0|            0|            0|  0.00%|    return fractional_max_pool2d_with_indices(\n",
      "   504|         0|            0|            0|  0.00%|        input, kernel_size, output_size, output_ratio, return_indices, _random_samples\n",
      "   505|         0|            0|            0|  0.00%|    )[0]\n",
      "   506|         0|            0|            0|  0.00%|\n",
      "   507|         0|            0|            0|  0.00%|\n",
      "   508|         0|            0|            0|  0.00%|fractional_max_pool2d = boolean_dispatch(\n",
      "   509|         0|            0|            0|  0.00%|    arg_name=\"return_indices\",\n",
      "   510|         0|            0|            0|  0.00%|    arg_index=4,\n",
      "   511|         0|            0|            0|  0.00%|    default=False,\n",
      "   512|         0|            0|            0|  0.00%|    if_true=fractional_max_pool2d_with_indices,\n",
      "   513|         0|            0|            0|  0.00%|    if_false=_fractional_max_pool2d,\n",
      "   514|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "   515|         0|            0|            0|  0.00%|    func_name=\"fractional_max_pool2d\",\n",
      "   516|         0|            0|            0|  0.00%|)\n",
      "   517|         0|            0|            0|  0.00%|\n",
      "   518|         0|            0|            0|  0.00%|\n",
      "   519|         0|            0|            0|  0.00%|def fractional_max_pool3d_with_indices(\n",
      "   520|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList3[int],\n",
      "   521|         0|            0|            0|  0.00%|    output_size: Optional[BroadcastingList3[int]] = None,\n",
      "   522|         0|            0|            0|  0.00%|    output_ratio: Optional[BroadcastingList3[float]] = None,\n",
      "   523|         0|            0|            0|  0.00%|    return_indices: bool = False,\n",
      "   524|         0|            0|            0|  0.00%|    _random_samples: Optional[Tensor] = None\n",
      "   525|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:\n",
      "   526|         0|            0|            0|  0.00%|    r\"\"\"Applies 3D fractional max pooling over an input signal composed of several input planes.\n",
      "   527|         0|            0|            0|  0.00%|\n",
      "   528|         0|            0|            0|  0.00%|    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham\n",
      "   529|         0|            0|            0|  0.00%|\n",
      "   530|         0|            0|            0|  0.00%|    The max-pooling operation is applied in :math:`kT \\times kH \\times kW` regions by a stochastic\n",
      "   531|         0|            0|            0|  0.00%|    step size determined by the target output size.\n",
      "   532|         0|            0|            0|  0.00%|    The number of output features is equal to the number of input planes.\n",
      "   533|         0|            0|            0|  0.00%|\n",
      "   534|         0|            0|            0|  0.00%|    Args:\n",
      "   535|         0|            0|            0|  0.00%|        kernel_size: the size of the window to take a max over.\n",
      "   536|         0|            0|            0|  0.00%|                     Can be a single number :math:`k` (for a square kernel of :math:`k \\times k \\times k`)\n",
      "   537|         0|            0|            0|  0.00%|                     or a tuple `(kT, kH, kW)`\n",
      "   538|         0|            0|            0|  0.00%|        output_size: the target output size of the form :math:`oT \\times oH \\times oW`.\n",
      "   539|         0|            0|            0|  0.00%|                     Can be a tuple `(oT, oH, oW)` or a single number :math:`oH` for a cubic output\n",
      "   540|         0|            0|            0|  0.00%|                     :math:`oH \\times oH \\times oH`\n",
      "   541|         0|            0|            0|  0.00%|        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.\n",
      "   542|         0|            0|            0|  0.00%|                      This has to be a number or tuple in the range (0, 1)\n",
      "   543|         0|            0|            0|  0.00%|        return_indices: if ``True``, will return the indices along with the outputs.\n",
      "   544|         0|            0|            0|  0.00%|                        Useful to pass to :func:`~torch.nn.functional.max_unpool3d`.\n",
      "   545|         0|            0|            0|  0.00%|\n",
      "   546|         0|            0|            0|  0.00%|    Shape:\n",
      "   547|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, T_{in}, H_{in}, W_{in})` or :math:`(C, T_{in}, H_{in}, W_{in})`.\n",
      "   548|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, T_{out}, H_{out}, W_{out})` or :math:`(C, T_{out}, H_{out}, W_{out})`, where\n",
      "   549|         0|            0|            0|  0.00%|          :math:`(T_{out}, H_{out}, W_{out})=\\text{output\\_size}` or\n",
      "   550|         0|            0|            0|  0.00%|          :math:`(T_{out}, H_{out}, W_{out})=\\text{output\\_ratio} \\times (T_{in}, H_{in}, W_{in})`\n",
      "   551|         0|            0|            0|  0.00%|\n",
      "   552|         0|            0|            0|  0.00%|    Examples::\n",
      "   553|         0|            0|            0|  0.00%|        >>> input = torch.randn(20, 16, 50, 32, 16)\n",
      "   554|         0|            0|            0|  0.00%|        >>> # pool of cubic window of size=3, and target output size 13x12x11\n",
      "   555|         0|            0|            0|  0.00%|        >>> F.fractional_max_pool3d(input, 3, output_size=(13, 12, 11))\n",
      "   556|         0|            0|            0|  0.00%|        >>> # pool of cubic window and target output size being half of input size\n",
      "   557|         0|            0|            0|  0.00%|        >>> F.fractional_max_pool3d(input, 3, output_ratio=(0.5, 0.5, 0.5))\n",
      "   558|         0|            0|            0|  0.00%|\n",
      "   559|         0|            0|            0|  0.00%|    .. _Fractional MaxPooling:\n",
      "   560|         0|            0|            0|  0.00%|        http://arxiv.org/abs/1412.6071\n",
      "   561|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   562|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, _random_samples):\n",
      "   563|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   564|         0|            0|            0|  0.00%|            fractional_max_pool3d_with_indices,\n",
      "   565|         0|            0|            0|  0.00%|            (input, _random_samples),\n",
      "   566|         0|            0|            0|  0.00%|            input,\n",
      "   567|         0|            0|            0|  0.00%|            kernel_size,\n",
      "   568|         0|            0|            0|  0.00%|            output_size=output_size,\n",
      "   569|         0|            0|            0|  0.00%|            output_ratio=output_ratio,\n",
      "   570|         0|            0|            0|  0.00%|            return_indices=return_indices,\n",
      "   571|         0|            0|            0|  0.00%|            _random_samples=_random_samples,\n",
      "   572|         0|            0|            0|  0.00%|        )\n",
      "   573|         0|            0|            0|  0.00%|    if output_size is None and output_ratio is None:\n",
      "   574|         0|            0|            0|  0.00%|        raise ValueError(\"fractional_max_pool3d requires specifying either \" \"an output_size or an output_ratio\")\n",
      "   575|         0|            0|            0|  0.00%|    if output_size is None:\n",
      "   576|         0|            0|            0|  0.00%|        assert output_ratio is not None\n",
      "   577|         0|            0|            0|  0.00%|        _output_ratio = _triple(output_ratio)\n",
      "   578|         0|            0|            0|  0.00%|        output_size = [\n",
      "   579|         0|            0|            0|  0.00%|            int(input.size(-3) * _output_ratio[0]),\n",
      "   580|         0|            0|            0|  0.00%|            int(input.size(-2) * _output_ratio[1]),\n",
      "   581|         0|            0|            0|  0.00%|            int(input.size(-1) * _output_ratio[2]),\n",
      "   582|         0|            0|            0|  0.00%|        ]\n",
      "   583|         0|            0|            0|  0.00%|\n",
      "   584|         0|            0|            0|  0.00%|    if _random_samples is None:\n",
      "   585|         0|            0|            0|  0.00%|        n_batch = 1 if input.dim() == 4 else input.size(0)\n",
      "   586|         0|            0|            0|  0.00%|        _random_samples = torch.rand(n_batch, input.size(-4), 3, dtype=input.dtype, device=input.device)\n",
      "   587|         0|            0|            0|  0.00%|    return torch._C._nn.fractional_max_pool3d(input, kernel_size, output_size, _random_samples)\n",
      "   588|         0|            0|            0|  0.00%|\n",
      "   589|         0|            0|            0|  0.00%|\n",
      "   590|         0|            0|            0|  0.00%|def _fractional_max_pool3d(\n",
      "   591|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList3[int],\n",
      "   592|         0|            0|            0|  0.00%|    output_size: Optional[BroadcastingList3[int]] = None,\n",
      "   593|         0|            0|            0|  0.00%|    output_ratio: Optional[BroadcastingList3[float]] = None,\n",
      "   594|         0|            0|            0|  0.00%|    return_indices: bool = False,\n",
      "   595|         0|            0|            0|  0.00%|    _random_samples: Optional[Tensor] = None\n",
      "   596|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "   597|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, _random_samples):\n",
      "   598|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   599|         0|            0|            0|  0.00%|            fractional_max_pool3d,\n",
      "   600|         0|            0|            0|  0.00%|            (input, _random_samples),\n",
      "   601|         0|            0|            0|  0.00%|            input,\n",
      "   602|         0|            0|            0|  0.00%|            kernel_size,\n",
      "   603|         0|            0|            0|  0.00%|            output_size=output_size,\n",
      "   604|         0|            0|            0|  0.00%|            output_ratio=output_ratio,\n",
      "   605|         0|            0|            0|  0.00%|            return_indices=return_indices,\n",
      "   606|         0|            0|            0|  0.00%|            _random_samples=_random_samples,\n",
      "   607|         0|            0|            0|  0.00%|        )\n",
      "   608|         0|            0|            0|  0.00%|    return fractional_max_pool3d_with_indices(\n",
      "   609|         0|            0|            0|  0.00%|        input, kernel_size, output_size, output_ratio, return_indices, _random_samples\n",
      "   610|         0|            0|            0|  0.00%|    )[0]\n",
      "   611|         0|            0|            0|  0.00%|\n",
      "   612|         0|            0|            0|  0.00%|\n",
      "   613|         0|            0|            0|  0.00%|fractional_max_pool3d = boolean_dispatch(\n",
      "   614|         0|            0|            0|  0.00%|    arg_name=\"return_indices\",\n",
      "   615|         0|            0|            0|  0.00%|    arg_index=4,\n",
      "   616|         0|            0|            0|  0.00%|    default=False,\n",
      "   617|         0|            0|            0|  0.00%|    if_true=fractional_max_pool3d_with_indices,\n",
      "   618|         0|            0|            0|  0.00%|    if_false=_fractional_max_pool3d,\n",
      "   619|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "   620|         0|            0|            0|  0.00%|    func_name=\"fractional_max_pool3d\",\n",
      "   621|         0|            0|            0|  0.00%|)\n",
      "   622|         0|            0|            0|  0.00%|\n",
      "   623|         0|            0|            0|  0.00%|\n",
      "   624|         0|            0|            0|  0.00%|def max_pool1d_with_indices(\n",
      "   625|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList1[int],\n",
      "   626|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList1[int]] = None,\n",
      "   627|         0|            0|            0|  0.00%|    padding: BroadcastingList1[int] = 0,\n",
      "   628|         0|            0|            0|  0.00%|    dilation: BroadcastingList1[int] = 1,\n",
      "   629|         0|            0|            0|  0.00%|    ceil_mode: bool = False,\n",
      "   630|         0|            0|            0|  0.00%|    return_indices: bool = False\n",
      "   631|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:\n",
      "   632|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   633|         0|            0|            0|  0.00%|    max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n",
      "   634|         0|            0|            0|  0.00%|\n",
      "   635|         0|            0|            0|  0.00%|    Applies a 1D max pooling over an input signal composed of several input\n",
      "   636|         0|            0|            0|  0.00%|    planes.\n",
      "   637|         0|            0|            0|  0.00%|\n",
      "   638|         0|            0|            0|  0.00%|    .. note::\n",
      "   639|         0|            0|            0|  0.00%|        The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n",
      "   640|         0|            0|            0|  0.00%|        what seen in :class:`~torch.nn.MaxPool1d`, and will change in a future release.\n",
      "   641|         0|            0|            0|  0.00%|\n",
      "   642|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MaxPool1d` for details.\n",
      "   643|         0|            0|            0|  0.00%|\n",
      "   644|         0|            0|            0|  0.00%|    Args:\n",
      "   645|         0|            0|            0|  0.00%|        input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`, minibatch dim optional.\n",
      "   646|         0|            0|            0|  0.00%|        kernel_size: the size of the window. Can be a single number or a\n",
      "   647|         0|            0|            0|  0.00%|            tuple `(kW,)`\n",
      "   648|         0|            0|            0|  0.00%|        stride: the stride of the window. Can be a single number or a tuple\n",
      "   649|         0|            0|            0|  0.00%|            `(sW,)`. Default: :attr:`kernel_size`\n",
      "   650|         0|            0|            0|  0.00%|        padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n",
      "   651|         0|            0|            0|  0.00%|        dilation: The stride between elements within a sliding window, must be > 0.\n",
      "   652|         0|            0|            0|  0.00%|        ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n",
      "   653|         0|            0|            0|  0.00%|                   ensures that every element in the input tensor is covered by a sliding window.\n",
      "   654|         0|            0|            0|  0.00%|        return_indices: If ``True``, will return the argmax along with the max values.\n",
      "   655|         0|            0|            0|  0.00%|                        Useful for :class:`torch.nn.functional.max_unpool1d` later\n",
      "   656|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   657|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   658|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   659|         0|            0|            0|  0.00%|            max_pool1d_with_indices,\n",
      "   660|         0|            0|            0|  0.00%|            (input,),\n",
      "   661|         0|            0|            0|  0.00%|            input,\n",
      "   662|         0|            0|            0|  0.00%|            kernel_size,\n",
      "   663|         0|            0|            0|  0.00%|            stride=stride,\n",
      "   664|         0|            0|            0|  0.00%|            padding=padding,\n",
      "   665|         0|            0|            0|  0.00%|            dilation=dilation,\n",
      "   666|         0|            0|            0|  0.00%|            ceil_mode=ceil_mode,\n",
      "   667|         0|            0|            0|  0.00%|            return_indices=return_indices,\n",
      "   668|         0|            0|            0|  0.00%|        )\n",
      "   669|         0|            0|            0|  0.00%|    if stride is None:\n",
      "   670|         0|            0|            0|  0.00%|        stride = torch.jit.annotate(List[int], [])\n",
      "   671|         0|            0|            0|  0.00%|    return torch.max_pool1d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "   672|         0|            0|            0|  0.00%|\n",
      "   673|         0|            0|            0|  0.00%|\n",
      "   674|         0|            0|            0|  0.00%|def _max_pool1d(\n",
      "   675|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList1[int],\n",
      "   676|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList1[int]] = None,\n",
      "   677|         0|            0|            0|  0.00%|    padding: BroadcastingList1[int] = 0,\n",
      "   678|         0|            0|            0|  0.00%|    dilation: BroadcastingList1[int] = 1,\n",
      "   679|         0|            0|            0|  0.00%|    ceil_mode: bool = False,\n",
      "   680|         0|            0|            0|  0.00%|    return_indices: bool = False\n",
      "   681|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "   682|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   683|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   684|         0|            0|            0|  0.00%|            max_pool1d,\n",
      "   685|         0|            0|            0|  0.00%|            (input,),\n",
      "   686|         0|            0|            0|  0.00%|            input,\n",
      "   687|         0|            0|            0|  0.00%|            kernel_size,\n",
      "   688|         0|            0|            0|  0.00%|            stride=stride,\n",
      "   689|         0|            0|            0|  0.00%|            padding=padding,\n",
      "   690|         0|            0|            0|  0.00%|            dilation=dilation,\n",
      "   691|         0|            0|            0|  0.00%|            ceil_mode=ceil_mode,\n",
      "   692|         0|            0|            0|  0.00%|            return_indices=return_indices,\n",
      "   693|         0|            0|            0|  0.00%|        )\n",
      "   694|         0|            0|            0|  0.00%|    if stride is None:\n",
      "   695|         0|            0|            0|  0.00%|        stride = torch.jit.annotate(List[int], [])\n",
      "   696|         0|            0|            0|  0.00%|    return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "   697|         0|            0|            0|  0.00%|\n",
      "   698|         0|            0|            0|  0.00%|\n",
      "   699|         0|            0|            0|  0.00%|max_pool1d = boolean_dispatch(\n",
      "   700|         0|            0|            0|  0.00%|    arg_name=\"return_indices\",\n",
      "   701|         0|            0|            0|  0.00%|    arg_index=6,\n",
      "   702|         0|            0|            0|  0.00%|    default=False,\n",
      "   703|         0|            0|            0|  0.00%|    if_true=max_pool1d_with_indices,\n",
      "   704|         0|            0|            0|  0.00%|    if_false=_max_pool1d,\n",
      "   705|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "   706|         0|            0|            0|  0.00%|    func_name=\"max_pool1d\",\n",
      "   707|         0|            0|            0|  0.00%|)\n",
      "   708|         0|            0|            0|  0.00%|\n",
      "   709|         0|            0|            0|  0.00%|\n",
      "   710|         0|            0|            0|  0.00%|def max_pool2d_with_indices(\n",
      "   711|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList2[int],\n",
      "   712|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList2[int]] = None,\n",
      "   713|         0|            0|            0|  0.00%|    padding: BroadcastingList2[int] = 0,\n",
      "   714|         0|            0|            0|  0.00%|    dilation: BroadcastingList2[int] = 1,\n",
      "   715|         0|            0|            0|  0.00%|    ceil_mode: bool = False,\n",
      "   716|         0|            0|            0|  0.00%|    return_indices: bool = False\n",
      "   717|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:\n",
      "   718|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   719|         0|            0|            0|  0.00%|    max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n",
      "   720|         0|            0|            0|  0.00%|\n",
      "   721|         0|            0|            0|  0.00%|    Applies a 2D max pooling over an input signal composed of several input\n",
      "   722|         0|            0|            0|  0.00%|    planes.\n",
      "   723|         0|            0|            0|  0.00%|\n",
      "   724|         0|            0|            0|  0.00%|    .. note::\n",
      "   725|         0|            0|            0|  0.00%|        The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n",
      "   726|         0|            0|            0|  0.00%|        what seen in :class:`~torch.nn.MaxPool2d`, and will change in a future release.\n",
      "   727|         0|            0|            0|  0.00%|\n",
      "   728|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MaxPool2d` for details.\n",
      "   729|         0|            0|            0|  0.00%|\n",
      "   730|         0|            0|            0|  0.00%|    Args:\n",
      "   731|         0|            0|            0|  0.00%|        input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`, minibatch dim optional.\n",
      "   732|         0|            0|            0|  0.00%|        kernel_size: size of the pooling region. Can be a single number or a\n",
      "   733|         0|            0|            0|  0.00%|            tuple `(kH, kW)`\n",
      "   734|         0|            0|            0|  0.00%|        stride: stride of the pooling operation. Can be a single number or a\n",
      "   735|         0|            0|            0|  0.00%|            tuple `(sH, sW)`. Default: :attr:`kernel_size`\n",
      "   736|         0|            0|            0|  0.00%|        padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n",
      "   737|         0|            0|            0|  0.00%|        dilation: The stride between elements within a sliding window, must be > 0.\n",
      "   738|         0|            0|            0|  0.00%|        ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n",
      "   739|         0|            0|            0|  0.00%|                   ensures that every element in the input tensor is covered by a sliding window.\n",
      "   740|         0|            0|            0|  0.00%|        return_indices: If ``True``, will return the argmax along with the max values.\n",
      "   741|         0|            0|            0|  0.00%|                        Useful for :class:`torch.nn.functional.max_unpool2d` later\n",
      "   742|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   743|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   744|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   745|         0|            0|            0|  0.00%|            max_pool2d_with_indices,\n",
      "   746|         0|            0|            0|  0.00%|            (input,),\n",
      "   747|         0|            0|            0|  0.00%|            input,\n",
      "   748|         0|            0|            0|  0.00%|            kernel_size,\n",
      "   749|         0|            0|            0|  0.00%|            stride=stride,\n",
      "   750|         0|            0|            0|  0.00%|            padding=padding,\n",
      "   751|         0|            0|            0|  0.00%|            dilation=dilation,\n",
      "   752|         0|            0|            0|  0.00%|            ceil_mode=ceil_mode,\n",
      "   753|         0|            0|            0|  0.00%|            return_indices=return_indices,\n",
      "   754|         0|            0|            0|  0.00%|        )\n",
      "   755|         0|            0|            0|  0.00%|    if stride is None:\n",
      "   756|         0|            0|            0|  0.00%|        stride = torch.jit.annotate(List[int], [])\n",
      "   757|         0|            0|            0|  0.00%|    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "   758|         0|            0|            0|  0.00%|\n",
      "   759|         0|            0|            0|  0.00%|\n",
      "   760|         0|            0|            0|  0.00%|def _max_pool2d(\n",
      "   761|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList2[int],\n",
      "   762|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList2[int]] = None,\n",
      "   763|         0|            0|            0|  0.00%|    padding: BroadcastingList2[int] = 0,\n",
      "   764|         0|            0|            0|  0.00%|    dilation: BroadcastingList2[int] = 1,\n",
      "   765|         0|            0|            0|  0.00%|    ceil_mode: bool = False,\n",
      "   766|         0|            0|            0|  0.00%|    return_indices: bool = False\n",
      "   767|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "   768|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   769|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   770|         0|            0|            0|  0.00%|            max_pool2d,\n",
      "   771|         0|            0|            0|  0.00%|            (input,),\n",
      "   772|         0|            0|            0|  0.00%|            input,\n",
      "   773|         0|            0|            0|  0.00%|            kernel_size,\n",
      "   774|         0|            0|            0|  0.00%|            stride=stride,\n",
      "   775|         0|            0|            0|  0.00%|            padding=padding,\n",
      "   776|         0|            0|            0|  0.00%|            dilation=dilation,\n",
      "   777|         0|            0|            0|  0.00%|            ceil_mode=ceil_mode,\n",
      "   778|         0|            0|            0|  0.00%|            return_indices=return_indices,\n",
      "   779|         0|            0|            0|  0.00%|        )\n",
      "   780|         0|            0|            0|  0.00%|    if stride is None:\n",
      "   781|         0|            0|            0|  0.00%|        stride = torch.jit.annotate(List[int], [])\n",
      "   782|         0|            0|            0|  0.00%|    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "   783|         0|            0|            0|  0.00%|\n",
      "   784|         0|            0|            0|  0.00%|\n",
      "   785|         0|            0|            0|  0.00%|max_pool2d = boolean_dispatch(\n",
      "   786|         0|            0|            0|  0.00%|    arg_name=\"return_indices\",\n",
      "   787|         0|            0|            0|  0.00%|    arg_index=6,\n",
      "   788|         0|            0|            0|  0.00%|    default=False,\n",
      "   789|         0|            0|            0|  0.00%|    if_true=max_pool2d_with_indices,\n",
      "   790|         0|            0|            0|  0.00%|    if_false=_max_pool2d,\n",
      "   791|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "   792|         0|            0|            0|  0.00%|    func_name=\"max_pool2d\",\n",
      "   793|         0|            0|            0|  0.00%|)\n",
      "   794|         0|            0|            0|  0.00%|\n",
      "   795|         0|            0|            0|  0.00%|\n",
      "   796|         0|            0|            0|  0.00%|def max_pool3d_with_indices(\n",
      "   797|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList3[int],\n",
      "   798|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList3[int]] = None,\n",
      "   799|         0|            0|            0|  0.00%|    padding: BroadcastingList3[int] = 0,\n",
      "   800|         0|            0|            0|  0.00%|    dilation: BroadcastingList3[int] = 1,\n",
      "   801|         0|            0|            0|  0.00%|    ceil_mode: bool = False,\n",
      "   802|         0|            0|            0|  0.00%|    return_indices: bool = False\n",
      "   803|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:\n",
      "   804|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   805|         0|            0|            0|  0.00%|    max_pool3d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n",
      "   806|         0|            0|            0|  0.00%|\n",
      "   807|         0|            0|            0|  0.00%|    Applies a 3D max pooling over an input signal composed of several input\n",
      "   808|         0|            0|            0|  0.00%|    planes.\n",
      "   809|         0|            0|            0|  0.00%|\n",
      "   810|         0|            0|            0|  0.00%|    .. note::\n",
      "   811|         0|            0|            0|  0.00%|        The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n",
      "   812|         0|            0|            0|  0.00%|        what seen in :class:`~torch.nn.MaxPool3d`, and will change in a future release.\n",
      "   813|         0|            0|            0|  0.00%|\n",
      "   814|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MaxPool3d` for details.\n",
      "   815|         0|            0|            0|  0.00%|\n",
      "   816|         0|            0|            0|  0.00%|    Args:\n",
      "   817|         0|            0|            0|  0.00%|        input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iD, iH , iW)`, minibatch dim optional.\n",
      "   818|         0|            0|            0|  0.00%|        kernel_size: size of the pooling region. Can be a single number or a\n",
      "   819|         0|            0|            0|  0.00%|                     tuple `(kT, kH, kW)`\n",
      "   820|         0|            0|            0|  0.00%|        stride: stride of the pooling operation. Can be a single number or a\n",
      "   821|         0|            0|            0|  0.00%|                tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`\n",
      "   822|         0|            0|            0|  0.00%|        padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n",
      "   823|         0|            0|            0|  0.00%|        dilation: The stride between elements within a sliding window, must be > 0.\n",
      "   824|         0|            0|            0|  0.00%|        ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n",
      "   825|         0|            0|            0|  0.00%|                   ensures that every element in the input tensor is covered by a sliding window.\n",
      "   826|         0|            0|            0|  0.00%|        return_indices: If ``True``, will return the argmax along with the max values.\n",
      "   827|         0|            0|            0|  0.00%|                        Useful for :class:`torch.nn.functional.max_unpool3d` later\n",
      "   828|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   829|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   830|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   831|         0|            0|            0|  0.00%|            max_pool3d_with_indices,\n",
      "   832|         0|            0|            0|  0.00%|            (input,),\n",
      "   833|         0|            0|            0|  0.00%|            input,\n",
      "   834|         0|            0|            0|  0.00%|            kernel_size,\n",
      "   835|         0|            0|            0|  0.00%|            stride=stride,\n",
      "   836|         0|            0|            0|  0.00%|            padding=padding,\n",
      "   837|         0|            0|            0|  0.00%|            dilation=dilation,\n",
      "   838|         0|            0|            0|  0.00%|            ceil_mode=ceil_mode,\n",
      "   839|         0|            0|            0|  0.00%|            return_indices=return_indices,\n",
      "   840|         0|            0|            0|  0.00%|        )\n",
      "   841|         0|            0|            0|  0.00%|    if stride is None:\n",
      "   842|         0|            0|            0|  0.00%|        stride = torch.jit.annotate(List[int], [])\n",
      "   843|         0|            0|            0|  0.00%|    return torch._C._nn.max_pool3d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "   844|         0|            0|            0|  0.00%|\n",
      "   845|         0|            0|            0|  0.00%|\n",
      "   846|         0|            0|            0|  0.00%|def _max_pool3d(\n",
      "   847|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList3[int],\n",
      "   848|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList3[int]] = None,\n",
      "   849|         0|            0|            0|  0.00%|    padding: BroadcastingList3[int] = 0,\n",
      "   850|         0|            0|            0|  0.00%|    dilation: BroadcastingList3[int] = 1,\n",
      "   851|         0|            0|            0|  0.00%|    ceil_mode: bool = False,\n",
      "   852|         0|            0|            0|  0.00%|    return_indices: bool = False\n",
      "   853|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "   854|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   855|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   856|         0|            0|            0|  0.00%|            max_pool3d,\n",
      "   857|         0|            0|            0|  0.00%|            (input,),\n",
      "   858|         0|            0|            0|  0.00%|            input,\n",
      "   859|         0|            0|            0|  0.00%|            kernel_size,\n",
      "   860|         0|            0|            0|  0.00%|            stride=stride,\n",
      "   861|         0|            0|            0|  0.00%|            padding=padding,\n",
      "   862|         0|            0|            0|  0.00%|            dilation=dilation,\n",
      "   863|         0|            0|            0|  0.00%|            ceil_mode=ceil_mode,\n",
      "   864|         0|            0|            0|  0.00%|            return_indices=return_indices,\n",
      "   865|         0|            0|            0|  0.00%|        )\n",
      "   866|         0|            0|            0|  0.00%|    if stride is None:\n",
      "   867|         0|            0|            0|  0.00%|        stride = torch.jit.annotate(List[int], [])\n",
      "   868|         0|            0|            0|  0.00%|    return torch.max_pool3d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "   869|         0|            0|            0|  0.00%|\n",
      "   870|         0|            0|            0|  0.00%|\n",
      "   871|         0|            0|            0|  0.00%|max_pool3d = boolean_dispatch(\n",
      "   872|         0|            0|            0|  0.00%|    arg_name=\"return_indices\",\n",
      "   873|         0|            0|            0|  0.00%|    arg_index=6,\n",
      "   874|         0|            0|            0|  0.00%|    default=False,\n",
      "   875|         0|            0|            0|  0.00%|    if_true=max_pool3d_with_indices,\n",
      "   876|         0|            0|            0|  0.00%|    if_false=_max_pool3d,\n",
      "   877|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "   878|         0|            0|            0|  0.00%|    func_name=\"max_pool3d\",\n",
      "   879|         0|            0|            0|  0.00%|)\n",
      "   880|         0|            0|            0|  0.00%|\n",
      "   881|         0|            0|            0|  0.00%|\n",
      "   882|         0|            0|            0|  0.00%|def _unpool_output_size(\n",
      "   883|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: List[int], stride: List[int], padding: List[int], output_size: Optional[List[int]]\n",
      "   884|         0|            0|            0|  0.00%|) -> List[int]:\n",
      "   885|         0|            0|            0|  0.00%|    input_size = input.size()\n",
      "   886|         0|            0|            0|  0.00%|    default_size = torch.jit.annotate(List[int], [])\n",
      "   887|         0|            0|            0|  0.00%|    for d in range(len(kernel_size)):\n",
      "   888|         0|            0|            0|  0.00%|        default_size.append((input_size[-len(kernel_size) + d] - 1) * stride[d] + kernel_size[d] - 2 * padding[d])\n",
      "   889|         0|            0|            0|  0.00%|    if output_size is None:\n",
      "   890|         0|            0|            0|  0.00%|        ret = default_size\n",
      "   891|         0|            0|            0|  0.00%|    else:\n",
      "   892|         0|            0|            0|  0.00%|        if len(output_size) == len(kernel_size) + 2:\n",
      "   893|         0|            0|            0|  0.00%|            output_size = output_size[2:]\n",
      "   894|         0|            0|            0|  0.00%|        if len(output_size) != len(kernel_size):\n",
      "   895|         0|            0|            0|  0.00%|            raise ValueError(\n",
      "   896|         0|            0|            0|  0.00%|                \"output_size should be a sequence containing \"\n",
      "   897|         0|            0|            0|  0.00%|                \"{} or {} elements, but it has a length of '{}'\".format(\n",
      "   898|         0|            0|            0|  0.00%|                    len(kernel_size), len(kernel_size) + 2, len(output_size)\n",
      "   899|         0|            0|            0|  0.00%|                )\n",
      "   900|         0|            0|            0|  0.00%|            )\n",
      "   901|         0|            0|            0|  0.00%|        for d in range(len(kernel_size)):\n",
      "   902|         0|            0|            0|  0.00%|            min_size = default_size[d] - stride[d]\n",
      "   903|         0|            0|            0|  0.00%|            max_size = default_size[d] + stride[d]\n",
      "   904|         0|            0|            0|  0.00%|            if not (min_size < output_size[d] < max_size):\n",
      "   905|         0|            0|            0|  0.00%|                raise ValueError(\n",
      "   906|         0|            0|            0|  0.00%|                    'invalid output_size \"{}\" (dim {} must be between {} and {})'.format(\n",
      "   907|         0|            0|            0|  0.00%|                        output_size, d, min_size, max_size\n",
      "   908|         0|            0|            0|  0.00%|                    )\n",
      "   909|         0|            0|            0|  0.00%|                )\n",
      "   910|         0|            0|            0|  0.00%|\n",
      "   911|         0|            0|            0|  0.00%|        ret = output_size\n",
      "   912|         0|            0|            0|  0.00%|    return ret\n",
      "   913|         0|            0|            0|  0.00%|\n",
      "   914|         0|            0|            0|  0.00%|\n",
      "   915|         0|            0|            0|  0.00%|def max_unpool1d(\n",
      "   916|         0|            0|            0|  0.00%|    input: Tensor, indices: Tensor,\n",
      "   917|         0|            0|            0|  0.00%|    kernel_size: BroadcastingList1[int],\n",
      "   918|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList1[int]] = None,\n",
      "   919|         0|            0|            0|  0.00%|    padding: BroadcastingList1[int] = 0,\n",
      "   920|         0|            0|            0|  0.00%|    output_size: Optional[BroadcastingList1[int]] = None\n",
      "   921|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "   922|         0|            0|            0|  0.00%|    r\"\"\"Computes a partial inverse of :class:`MaxPool1d`.\n",
      "   923|         0|            0|            0|  0.00%|\n",
      "   924|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MaxUnpool1d` for details.\n",
      "   925|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   926|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   927|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   928|         0|            0|            0|  0.00%|            max_unpool1d,\n",
      "   929|         0|            0|            0|  0.00%|            (input,),\n",
      "   930|         0|            0|            0|  0.00%|            input,\n",
      "   931|         0|            0|            0|  0.00%|            indices,\n",
      "   932|         0|            0|            0|  0.00%|            kernel_size,\n",
      "   933|         0|            0|            0|  0.00%|            stride=stride,\n",
      "   934|         0|            0|            0|  0.00%|            padding=padding,\n",
      "   935|         0|            0|            0|  0.00%|            output_size=output_size,\n",
      "   936|         0|            0|            0|  0.00%|        )\n",
      "   937|         0|            0|            0|  0.00%|    kernel_size = _single(kernel_size)\n",
      "   938|         0|            0|            0|  0.00%|    if stride is not None:\n",
      "   939|         0|            0|            0|  0.00%|        _stride = _single(stride)\n",
      "   940|         0|            0|            0|  0.00%|    else:\n",
      "   941|         0|            0|            0|  0.00%|        _stride = kernel_size\n",
      "   942|         0|            0|            0|  0.00%|    padding = _single(padding)\n",
      "   943|         0|            0|            0|  0.00%|    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)\n",
      "   944|         0|            0|            0|  0.00%|    if isinstance(output_size, list):\n",
      "   945|         0|            0|            0|  0.00%|        output_size = output_size + [1]\n",
      "   946|         0|            0|            0|  0.00%|    else:\n",
      "   947|         0|            0|            0|  0.00%|        output_size = output_size + (1,)\n",
      "   948|         0|            0|            0|  0.00%|    return torch._C._nn.max_unpool2d(input.unsqueeze(-1), indices.unsqueeze(-1), output_size).squeeze(-1)\n",
      "   949|         0|            0|            0|  0.00%|\n",
      "   950|         0|            0|            0|  0.00%|\n",
      "   951|         0|            0|            0|  0.00%|def max_unpool2d(\n",
      "   952|         0|            0|            0|  0.00%|    input: Tensor, indices: Tensor,\n",
      "   953|         0|            0|            0|  0.00%|    kernel_size: BroadcastingList2[int],\n",
      "   954|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList2[int]] = None,\n",
      "   955|         0|            0|            0|  0.00%|    padding: BroadcastingList2[int] = 0,\n",
      "   956|         0|            0|            0|  0.00%|    output_size: Optional[BroadcastingList2[int]] = None\n",
      "   957|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "   958|         0|            0|            0|  0.00%|    r\"\"\"Computes a partial inverse of :class:`MaxPool2d`.\n",
      "   959|         0|            0|            0|  0.00%|\n",
      "   960|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MaxUnpool2d` for details.\n",
      "   961|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   962|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   963|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   964|         0|            0|            0|  0.00%|            max_unpool2d,\n",
      "   965|         0|            0|            0|  0.00%|            (input,),\n",
      "   966|         0|            0|            0|  0.00%|            input,\n",
      "   967|         0|            0|            0|  0.00%|            indices,\n",
      "   968|         0|            0|            0|  0.00%|            kernel_size,\n",
      "   969|         0|            0|            0|  0.00%|            stride=stride,\n",
      "   970|         0|            0|            0|  0.00%|            padding=padding,\n",
      "   971|         0|            0|            0|  0.00%|            output_size=output_size,\n",
      "   972|         0|            0|            0|  0.00%|        )\n",
      "   973|         0|            0|            0|  0.00%|    kernel_size = _pair(kernel_size)\n",
      "   974|         0|            0|            0|  0.00%|    if stride is not None:\n",
      "   975|         0|            0|            0|  0.00%|        _stride = _pair(stride)\n",
      "   976|         0|            0|            0|  0.00%|    else:\n",
      "   977|         0|            0|            0|  0.00%|        _stride = kernel_size\n",
      "   978|         0|            0|            0|  0.00%|    padding = _pair(padding)\n",
      "   979|         0|            0|            0|  0.00%|    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)\n",
      "   980|         0|            0|            0|  0.00%|    return torch._C._nn.max_unpool2d(input, indices, output_size)\n",
      "   981|         0|            0|            0|  0.00%|\n",
      "   982|         0|            0|            0|  0.00%|\n",
      "   983|         0|            0|            0|  0.00%|def max_unpool3d(\n",
      "   984|         0|            0|            0|  0.00%|    input: Tensor, indices: Tensor,\n",
      "   985|         0|            0|            0|  0.00%|    kernel_size: BroadcastingList3[int],\n",
      "   986|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList3[int]] = None,\n",
      "   987|         0|            0|            0|  0.00%|    padding: BroadcastingList3[int] = 0,\n",
      "   988|         0|            0|            0|  0.00%|    output_size: Optional[BroadcastingList3[int]] = None\n",
      "   989|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "   990|         0|            0|            0|  0.00%|    r\"\"\"Computes a partial inverse of :class:`MaxPool3d`.\n",
      "   991|         0|            0|            0|  0.00%|\n",
      "   992|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MaxUnpool3d` for details.\n",
      "   993|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   994|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "   995|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "   996|         0|            0|            0|  0.00%|            max_unpool3d,\n",
      "   997|         0|            0|            0|  0.00%|            (input,),\n",
      "   998|         0|            0|            0|  0.00%|            input,\n",
      "   999|         0|            0|            0|  0.00%|            indices,\n",
      "  1000|         0|            0|            0|  0.00%|            kernel_size,\n",
      "  1001|         0|            0|            0|  0.00%|            stride=stride,\n",
      "  1002|         0|            0|            0|  0.00%|            padding=padding,\n",
      "  1003|         0|            0|            0|  0.00%|            output_size=output_size,\n",
      "  1004|         0|            0|            0|  0.00%|        )\n",
      "  1005|         0|            0|            0|  0.00%|    kernel_size = _triple(kernel_size)\n",
      "  1006|         0|            0|            0|  0.00%|    if stride is not None:\n",
      "  1007|         0|            0|            0|  0.00%|        _stride = _triple(stride)\n",
      "  1008|         0|            0|            0|  0.00%|    else:\n",
      "  1009|         0|            0|            0|  0.00%|        _stride = kernel_size\n",
      "  1010|         0|            0|            0|  0.00%|    padding = _triple(padding)\n",
      "  1011|         0|            0|            0|  0.00%|    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)\n",
      "  1012|         0|            0|            0|  0.00%|    return torch._C._nn.max_unpool3d(input, indices, output_size, _stride, padding)\n",
      "  1013|         0|            0|            0|  0.00%|\n",
      "  1014|         0|            0|            0|  0.00%|\n",
      "  1015|         0|            0|            0|  0.00%|def lp_pool2d(\n",
      "  1016|         0|            0|            0|  0.00%|    input: Tensor, norm_type: Union[int, float],\n",
      "  1017|         0|            0|            0|  0.00%|    kernel_size: BroadcastingList2[int],\n",
      "  1018|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList2[int]] = None,\n",
      "  1019|         0|            0|            0|  0.00%|    ceil_mode: bool = False\n",
      "  1020|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  1021|         0|            0|            0|  0.00%|    r\"\"\"Applies a 2D power-average pooling over an input signal composed of\n",
      "  1022|         0|            0|            0|  0.00%|    several input planes. If the sum of all inputs to the power of `p` is\n",
      "  1023|         0|            0|            0|  0.00%|    zero, the gradient is set to zero as well.\n",
      "  1024|         0|            0|            0|  0.00%|\n",
      "  1025|         0|            0|            0|  0.00%|    See :class:`~torch.nn.LPPool2d` for details.\n",
      "  1026|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1027|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1028|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  1029|         0|            0|            0|  0.00%|            lp_pool2d, (input,), input, norm_type, kernel_size, stride=stride, ceil_mode=ceil_mode\n",
      "  1030|         0|            0|            0|  0.00%|        )\n",
      "  1031|         0|            0|            0|  0.00%|    kw, kh = utils._pair(kernel_size)\n",
      "  1032|         0|            0|            0|  0.00%|    if stride is not None:\n",
      "  1033|         0|            0|            0|  0.00%|        out = avg_pool2d(input.pow(norm_type), kernel_size, stride, 0, ceil_mode)\n",
      "  1034|         0|            0|            0|  0.00%|    else:\n",
      "  1035|         0|            0|            0|  0.00%|        out = avg_pool2d(input.pow(norm_type), kernel_size, padding=0, ceil_mode=ceil_mode)\n",
      "  1036|         0|            0|            0|  0.00%|\n",
      "  1037|         0|            0|            0|  0.00%|    return (torch.sign(out) * relu(torch.abs(out))).mul(kw * kh).pow(1.0 / norm_type)\n",
      "  1038|         0|            0|            0|  0.00%|\n",
      "  1039|         0|            0|            0|  0.00%|\n",
      "  1040|         0|            0|            0|  0.00%|def lp_pool1d(\n",
      "  1041|         0|            0|            0|  0.00%|    input: Tensor, norm_type: Union[int, float],\n",
      "  1042|         0|            0|            0|  0.00%|    kernel_size: int,\n",
      "  1043|         0|            0|            0|  0.00%|    stride: Optional[BroadcastingList1[int]] = None,\n",
      "  1044|         0|            0|            0|  0.00%|    ceil_mode: bool = False\n",
      "  1045|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  1046|         0|            0|            0|  0.00%|    r\"\"\"Applies a 1D power-average pooling over an input signal composed of\n",
      "  1047|         0|            0|            0|  0.00%|    several input planes. If the sum of all inputs to the power of `p` is\n",
      "  1048|         0|            0|            0|  0.00%|    zero, the gradient is set to zero as well.\n",
      "  1049|         0|            0|            0|  0.00%|\n",
      "  1050|         0|            0|            0|  0.00%|    See :class:`~torch.nn.LPPool1d` for details.\n",
      "  1051|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1052|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1053|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  1054|         0|            0|            0|  0.00%|            lp_pool1d, (input,), input, norm_type, kernel_size, stride=stride, ceil_mode=ceil_mode\n",
      "  1055|         0|            0|            0|  0.00%|        )\n",
      "  1056|         0|            0|            0|  0.00%|    if stride is not None:\n",
      "  1057|         0|            0|            0|  0.00%|        out = avg_pool1d(input.pow(norm_type), kernel_size, stride, 0, ceil_mode)\n",
      "  1058|         0|            0|            0|  0.00%|    else:\n",
      "  1059|         0|            0|            0|  0.00%|        out = avg_pool1d(input.pow(norm_type), kernel_size, padding=0, ceil_mode=ceil_mode)\n",
      "  1060|         0|            0|            0|  0.00%|\n",
      "  1061|         0|            0|            0|  0.00%|    return (torch.sign(out) * relu(torch.abs(out))).mul(kernel_size).pow(1.0 / norm_type)\n",
      "  1062|         0|            0|            0|  0.00%|\n",
      "  1063|         0|            0|            0|  0.00%|\n",
      "  1064|         0|            0|            0|  0.00%|def adaptive_max_pool1d_with_indices(\n",
      "  1065|         0|            0|            0|  0.00%|    input: Tensor, output_size: BroadcastingList1[int], return_indices: bool = False\n",
      "  1066|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:\n",
      "  1067|         0|            0|            0|  0.00%|    r\"\"\"Applies a 1D adaptive max pooling over an input signal composed of\n",
      "  1068|         0|            0|            0|  0.00%|    several input planes.\n",
      "  1069|         0|            0|            0|  0.00%|\n",
      "  1070|         0|            0|            0|  0.00%|    See :class:`~torch.nn.AdaptiveMaxPool1d` for details and output shape.\n",
      "  1071|         0|            0|            0|  0.00%|\n",
      "  1072|         0|            0|            0|  0.00%|    Args:\n",
      "  1073|         0|            0|            0|  0.00%|        output_size: the target output size (single integer)\n",
      "  1074|         0|            0|            0|  0.00%|        return_indices: whether to return pooling indices. Default: ``False``\n",
      "  1075|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1076|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1077|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  1078|         0|            0|            0|  0.00%|            adaptive_max_pool1d_with_indices, (input,), input, output_size, return_indices=return_indices\n",
      "  1079|         0|            0|            0|  0.00%|        )\n",
      "  1080|         0|            0|            0|  0.00%|    return torch.adaptive_max_pool1d(input, output_size)\n",
      "  1081|         0|            0|            0|  0.00%|\n",
      "  1082|         0|            0|            0|  0.00%|\n",
      "  1083|         0|            0|            0|  0.00%|def _adaptive_max_pool1d(input: Tensor, output_size: BroadcastingList1[int], return_indices: bool = False) -> Tensor:\n",
      "  1084|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1085|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  1086|         0|            0|            0|  0.00%|            adaptive_max_pool1d, (input,), input, output_size, return_indices=return_indices\n",
      "  1087|         0|            0|            0|  0.00%|        )\n",
      "  1088|         0|            0|            0|  0.00%|    return adaptive_max_pool1d_with_indices(input, output_size)[0]\n",
      "  1089|         0|            0|            0|  0.00%|\n",
      "  1090|         0|            0|            0|  0.00%|\n",
      "  1091|         0|            0|            0|  0.00%|adaptive_max_pool1d = boolean_dispatch(\n",
      "  1092|         0|            0|            0|  0.00%|    arg_name=\"return_indices\",\n",
      "  1093|         0|            0|            0|  0.00%|    arg_index=2,\n",
      "  1094|         0|            0|            0|  0.00%|    default=False,\n",
      "  1095|         0|            0|            0|  0.00%|    if_true=adaptive_max_pool1d_with_indices,\n",
      "  1096|         0|            0|            0|  0.00%|    if_false=_adaptive_max_pool1d,\n",
      "  1097|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "  1098|         0|            0|            0|  0.00%|    func_name=\"adaptive_max_pool1d\",\n",
      "  1099|         0|            0|            0|  0.00%|)\n",
      "  1100|         0|            0|            0|  0.00%|\n",
      "  1101|         0|            0|            0|  0.00%|\n",
      "  1102|         0|            0|            0|  0.00%|def adaptive_max_pool2d_with_indices(\n",
      "  1103|         0|            0|            0|  0.00%|    input: Tensor, output_size: BroadcastingList2[int],\n",
      "  1104|         0|            0|            0|  0.00%|    return_indices: bool = False\n",
      "  1105|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:\n",
      "  1106|         0|            0|            0|  0.00%|    r\"\"\"Applies a 2D adaptive max pooling over an input signal composed of\n",
      "  1107|         0|            0|            0|  0.00%|    several input planes.\n",
      "  1108|         0|            0|            0|  0.00%|\n",
      "  1109|         0|            0|            0|  0.00%|    See :class:`~torch.nn.AdaptiveMaxPool2d` for details and output shape.\n",
      "  1110|         0|            0|            0|  0.00%|\n",
      "  1111|         0|            0|            0|  0.00%|    Args:\n",
      "  1112|         0|            0|            0|  0.00%|        output_size: the target output size (single integer or\n",
      "  1113|         0|            0|            0|  0.00%|            double-integer tuple)\n",
      "  1114|         0|            0|            0|  0.00%|        return_indices: whether to return pooling indices. Default: ``False``\n",
      "  1115|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1116|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1117|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  1118|         0|            0|            0|  0.00%|            adaptive_max_pool2d_with_indices, (input,), input, output_size, return_indices=return_indices\n",
      "  1119|         0|            0|            0|  0.00%|        )\n",
      "  1120|         0|            0|            0|  0.00%|    output_size = _list_with_default(output_size, input.size())\n",
      "  1121|         0|            0|            0|  0.00%|    return torch._C._nn.adaptive_max_pool2d(input, output_size)\n",
      "  1122|         0|            0|            0|  0.00%|\n",
      "  1123|         0|            0|            0|  0.00%|\n",
      "  1124|         0|            0|            0|  0.00%|def _adaptive_max_pool2d(input: Tensor, output_size: BroadcastingList2[int], return_indices: bool = False) -> Tensor:\n",
      "  1125|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1126|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  1127|         0|            0|            0|  0.00%|            adaptive_max_pool2d, (input,), input, output_size, return_indices=return_indices\n",
      "  1128|         0|            0|            0|  0.00%|        )\n",
      "  1129|         0|            0|            0|  0.00%|    return adaptive_max_pool2d_with_indices(input, output_size)[0]\n",
      "  1130|         0|            0|            0|  0.00%|\n",
      "  1131|         0|            0|            0|  0.00%|\n",
      "  1132|         0|            0|            0|  0.00%|adaptive_max_pool2d = boolean_dispatch(\n",
      "  1133|         0|            0|            0|  0.00%|    arg_name=\"return_indices\",\n",
      "  1134|         0|            0|            0|  0.00%|    arg_index=2,\n",
      "  1135|         0|            0|            0|  0.00%|    default=False,\n",
      "  1136|         0|            0|            0|  0.00%|    if_true=adaptive_max_pool2d_with_indices,\n",
      "  1137|         0|            0|            0|  0.00%|    if_false=_adaptive_max_pool2d,\n",
      "  1138|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "  1139|         0|            0|            0|  0.00%|    func_name=\"adaptive_max_pool2d\",\n",
      "  1140|         0|            0|            0|  0.00%|)\n",
      "  1141|         0|            0|            0|  0.00%|\n",
      "  1142|         0|            0|            0|  0.00%|\n",
      "  1143|         0|            0|            0|  0.00%|def adaptive_max_pool3d_with_indices(\n",
      "  1144|         0|            0|            0|  0.00%|    input: Tensor, output_size: BroadcastingList3[int],\n",
      "  1145|         0|            0|            0|  0.00%|    return_indices: bool = False\n",
      "  1146|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:\n",
      "  1147|         0|            0|            0|  0.00%|    r\"\"\"Applies a 3D adaptive max pooling over an input signal composed of\n",
      "  1148|         0|            0|            0|  0.00%|    several input planes.\n",
      "  1149|         0|            0|            0|  0.00%|\n",
      "  1150|         0|            0|            0|  0.00%|    See :class:`~torch.nn.AdaptiveMaxPool3d` for details and output shape.\n",
      "  1151|         0|            0|            0|  0.00%|\n",
      "  1152|         0|            0|            0|  0.00%|    Args:\n",
      "  1153|         0|            0|            0|  0.00%|        output_size: the target output size (single integer or\n",
      "  1154|         0|            0|            0|  0.00%|            triple-integer tuple)\n",
      "  1155|         0|            0|            0|  0.00%|        return_indices: whether to return pooling indices. Default: ``False``\n",
      "  1156|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1157|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1158|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  1159|         0|            0|            0|  0.00%|            adaptive_max_pool3d_with_indices, (input,), input, output_size, return_indices=return_indices\n",
      "  1160|         0|            0|            0|  0.00%|        )\n",
      "  1161|         0|            0|            0|  0.00%|    output_size = _list_with_default(output_size, input.size())\n",
      "  1162|         0|            0|            0|  0.00%|    return torch._C._nn.adaptive_max_pool3d(input, output_size)\n",
      "  1163|         0|            0|            0|  0.00%|\n",
      "  1164|         0|            0|            0|  0.00%|\n",
      "  1165|         0|            0|            0|  0.00%|def _adaptive_max_pool3d(input: Tensor, output_size: BroadcastingList3[int], return_indices: bool = False) -> Tensor:\n",
      "  1166|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1167|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  1168|         0|            0|            0|  0.00%|            adaptive_max_pool3d, (input,), input, output_size, return_indices=return_indices\n",
      "  1169|         0|            0|            0|  0.00%|        )\n",
      "  1170|         0|            0|            0|  0.00%|    return adaptive_max_pool3d_with_indices(input, output_size)[0]\n",
      "  1171|         0|            0|            0|  0.00%|\n",
      "  1172|         0|            0|            0|  0.00%|\n",
      "  1173|         0|            0|            0|  0.00%|adaptive_max_pool3d = boolean_dispatch(\n",
      "  1174|         0|            0|            0|  0.00%|    arg_name=\"return_indices\",\n",
      "  1175|         0|            0|            0|  0.00%|    arg_index=2,\n",
      "  1176|         0|            0|            0|  0.00%|    default=False,\n",
      "  1177|         0|            0|            0|  0.00%|    if_true=adaptive_max_pool3d_with_indices,\n",
      "  1178|         0|            0|            0|  0.00%|    if_false=_adaptive_max_pool3d,\n",
      "  1179|         0|            0|            0|  0.00%|    module_name=__name__,\n",
      "  1180|         0|            0|            0|  0.00%|    func_name=\"adaptive_max_pool3d\",\n",
      "  1181|         0|            0|            0|  0.00%|)\n",
      "  1182|         0|            0|            0|  0.00%|\n",
      "  1183|         0|            0|            0|  0.00%|\n",
      "  1184|         0|            0|            0|  0.00%|adaptive_avg_pool1d = _add_docstr(\n",
      "  1185|         0|            0|            0|  0.00%|    torch.adaptive_avg_pool1d,\n",
      "  1186|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1187|         0|            0|            0|  0.00%|adaptive_avg_pool1d(input, output_size) -> Tensor\n",
      "  1188|         0|            0|            0|  0.00%|\n",
      "  1189|         0|            0|            0|  0.00%|Applies a 1D adaptive average pooling over an input signal composed of\n",
      "  1190|         0|            0|            0|  0.00%|several input planes.\n",
      "  1191|         0|            0|            0|  0.00%|\n",
      "  1192|         0|            0|            0|  0.00%|See :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.\n",
      "  1193|         0|            0|            0|  0.00%|\n",
      "  1194|         0|            0|            0|  0.00%|Args:\n",
      "  1195|         0|            0|            0|  0.00%|    output_size: the target output size (single integer)\n",
      "  1196|         0|            0|            0|  0.00%|\"\"\",\n",
      "  1197|         0|            0|            0|  0.00%|)\n",
      "  1198|         0|            0|            0|  0.00%|\n",
      "  1199|         0|            0|            0|  0.00%|\n",
      "  1200|         0|            0|            0|  0.00%|def adaptive_avg_pool2d(input: Tensor, output_size: BroadcastingList2[int]) -> Tensor:\n",
      "  1201|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1202|         0|            0|            0|  0.00%|    Applies a 2D adaptive average pooling over an input signal composed of\n",
      "  1203|         0|            0|            0|  0.00%|    several input planes.\n",
      "  1204|         0|            0|            0|  0.00%|\n",
      "  1205|         0|            0|            0|  0.00%|    See :class:`~torch.nn.AdaptiveAvgPool2d` for details and output shape.\n",
      "  1206|         0|            0|            0|  0.00%|\n",
      "  1207|         0|            0|            0|  0.00%|    Args:\n",
      "  1208|         0|            0|            0|  0.00%|        output_size: the target output size (single integer or\n",
      "  1209|         0|            0|            0|  0.00%|            double-integer tuple)\n",
      "  1210|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1211|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1212|         0|            0|            0|  0.00%|        return handle_torch_function(adaptive_avg_pool2d, (input,), input, output_size)\n",
      "  1213|         0|            0|            0|  0.00%|    _output_size = _list_with_default(output_size, input.size())\n",
      "  1214|         0|            0|            0|  0.00%|    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)\n",
      "  1215|         0|            0|            0|  0.00%|\n",
      "  1216|         0|            0|            0|  0.00%|\n",
      "  1217|         0|            0|            0|  0.00%|def adaptive_avg_pool3d(input: Tensor, output_size: BroadcastingList3[int]) -> Tensor:\n",
      "  1218|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1219|         0|            0|            0|  0.00%|    Applies a 3D adaptive average pooling over an input signal composed of\n",
      "  1220|         0|            0|            0|  0.00%|    several input planes.\n",
      "  1221|         0|            0|            0|  0.00%|\n",
      "  1222|         0|            0|            0|  0.00%|    See :class:`~torch.nn.AdaptiveAvgPool3d` for details and output shape.\n",
      "  1223|         0|            0|            0|  0.00%|\n",
      "  1224|         0|            0|            0|  0.00%|    Args:\n",
      "  1225|         0|            0|            0|  0.00%|        output_size: the target output size (single integer or\n",
      "  1226|         0|            0|            0|  0.00%|            triple-integer tuple)\n",
      "  1227|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1228|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1229|         0|            0|            0|  0.00%|        return handle_torch_function(adaptive_avg_pool3d, (input,), input, output_size)\n",
      "  1230|         0|            0|            0|  0.00%|    _output_size = _list_with_default(output_size, input.size())\n",
      "  1231|         0|            0|            0|  0.00%|    return torch._C._nn.adaptive_avg_pool3d(input, _output_size)\n",
      "  1232|         0|            0|            0|  0.00%|\n",
      "  1233|         0|            0|            0|  0.00%|\n",
      "  1234|         0|            0|            0|  0.00%|# Activation functions\n",
      "  1235|         0|            0|            0|  0.00%|def dropout(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:\n",
      "  1236|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1237|         0|            0|            0|  0.00%|    During training, randomly zeroes some of the elements of the input\n",
      "  1238|         0|            0|            0|  0.00%|    tensor with probability :attr:`p` using samples from a Bernoulli\n",
      "  1239|         0|            0|            0|  0.00%|    distribution.\n",
      "  1240|         0|            0|            0|  0.00%|\n",
      "  1241|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Dropout` for details.\n",
      "  1242|         0|            0|            0|  0.00%|\n",
      "  1243|         0|            0|            0|  0.00%|    Args:\n",
      "  1244|         0|            0|            0|  0.00%|        p: probability of an element to be zeroed. Default: 0.5\n",
      "  1245|         0|            0|            0|  0.00%|        training: apply dropout if is ``True``. Default: ``True``\n",
      "  1246|         0|            0|            0|  0.00%|        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
      "  1247|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1248|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1249|         0|            0|            0|  0.00%|        return handle_torch_function(dropout, (input,), input, p=p, training=training, inplace=inplace)\n",
      "  1250|         0|            0|            0|  0.00%|    if p < 0.0 or p > 1.0:\n",
      "  1251|         0|            0|            0|  0.00%|        raise ValueError(\"dropout probability has to be between 0 and 1, \" \"but got {}\".format(p))\n",
      "  1252|         0|            0|            0|  0.00%|    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "  1253|         0|            0|            0|  0.00%|\n",
      "  1254|         0|            0|            0|  0.00%|\n",
      "  1255|         0|            0|            0|  0.00%|def alpha_dropout(input: Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> Tensor:\n",
      "  1256|         0|            0|            0|  0.00%|    r\"\"\"Applies alpha dropout to the input.\n",
      "  1257|         0|            0|            0|  0.00%|\n",
      "  1258|         0|            0|            0|  0.00%|    See :class:`~torch.nn.AlphaDropout` for details.\n",
      "  1259|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1260|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1261|         0|            0|            0|  0.00%|        return handle_torch_function(alpha_dropout, (input,), input, p=p, training=training, inplace=inplace)\n",
      "  1262|         0|            0|            0|  0.00%|    if p < 0.0 or p > 1.0:\n",
      "  1263|         0|            0|            0|  0.00%|        raise ValueError(\"dropout probability has to be between 0 and 1, \" \"but got {}\".format(p))\n",
      "  1264|         0|            0|            0|  0.00%|    return _VF.alpha_dropout_(input, p, training) if inplace else _VF.alpha_dropout(input, p, training)\n",
      "  1265|         0|            0|            0|  0.00%|\n",
      "  1266|         0|            0|            0|  0.00%|\n",
      "  1267|         0|            0|            0|  0.00%|def dropout1d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:\n",
      "  1268|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1269|         0|            0|            0|  0.00%|    Randomly zero out entire channels (a channel is a 1D feature map,\n",
      "  1270|         0|            0|            0|  0.00%|    e.g., the :math:`j`-th channel of the :math:`i`-th sample in the\n",
      "  1271|         0|            0|            0|  0.00%|    batched input is a 1D tensor :math:`\\text{input}[i, j]`) of the input tensor).\n",
      "  1272|         0|            0|            0|  0.00%|    Each channel will be zeroed out independently on every forward call with\n",
      "  1273|         0|            0|            0|  0.00%|    probability :attr:`p` using samples from a Bernoulli distribution.\n",
      "  1274|         0|            0|            0|  0.00%|\n",
      "  1275|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Dropout1d` for details.\n",
      "  1276|         0|            0|            0|  0.00%|\n",
      "  1277|         0|            0|            0|  0.00%|    Args:\n",
      "  1278|         0|            0|            0|  0.00%|        p: probability of a channel to be zeroed. Default: 0.5\n",
      "  1279|         0|            0|            0|  0.00%|        training: apply dropout if is ``True``. Default: ``True``\n",
      "  1280|         0|            0|            0|  0.00%|        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
      "  1281|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1282|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1283|         0|            0|            0|  0.00%|        return handle_torch_function(dropout1d, (input,), input, p=p, training=training, inplace=inplace)\n",
      "  1284|         0|            0|            0|  0.00%|    if p < 0.0 or p > 1.0:\n",
      "  1285|         0|            0|            0|  0.00%|        raise ValueError(\"dropout probability has to be between 0 and 1, \" \"but got {}\".format(p))\n",
      "  1286|         0|            0|            0|  0.00%|    inp_dim = input.dim()\n",
      "  1287|         0|            0|            0|  0.00%|    if inp_dim not in (2, 3):\n",
      "  1288|         0|            0|            0|  0.00%|        raise RuntimeError(f\"dropout1d: Expected 2D or 3D input, but received a {inp_dim}D input. \"\n",
      "  1289|         0|            0|            0|  0.00%|                           \"Note that dropout1d exists to provide channel-wise dropout on inputs with 1 \"\n",
      "  1290|         0|            0|            0|  0.00%|                           \"spatial dimension, a channel dimension, and an optional batch dimension \"\n",
      "  1291|         0|            0|            0|  0.00%|                           \"(i.e. 2D or 3D inputs).\")\n",
      "  1292|         0|            0|            0|  0.00%|\n",
      "  1293|         0|            0|            0|  0.00%|    is_batched = inp_dim == 3\n",
      "  1294|         0|            0|            0|  0.00%|    if not is_batched:\n",
      "  1295|         0|            0|            0|  0.00%|        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)\n",
      "  1296|         0|            0|            0|  0.00%|\n",
      "  1297|         0|            0|            0|  0.00%|    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)\n",
      "  1298|         0|            0|            0|  0.00%|\n",
      "  1299|         0|            0|            0|  0.00%|    if not is_batched:\n",
      "  1300|         0|            0|            0|  0.00%|        result = result.squeeze_(0) if inplace else result.squeeze(0)\n",
      "  1301|         0|            0|            0|  0.00%|\n",
      "  1302|         0|            0|            0|  0.00%|    return result\n",
      "  1303|         0|            0|            0|  0.00%|\n",
      "  1304|         0|            0|            0|  0.00%|\n",
      "  1305|         0|            0|            0|  0.00%|def dropout2d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:\n",
      "  1306|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1307|         0|            0|            0|  0.00%|    Randomly zero out entire channels (a channel is a 2D feature map,\n",
      "  1308|         0|            0|            0|  0.00%|    e.g., the :math:`j`-th channel of the :math:`i`-th sample in the\n",
      "  1309|         0|            0|            0|  0.00%|    batched input is a 2D tensor :math:`\\text{input}[i, j]`) of the input tensor).\n",
      "  1310|         0|            0|            0|  0.00%|    Each channel will be zeroed out independently on every forward call with\n",
      "  1311|         0|            0|            0|  0.00%|    probability :attr:`p` using samples from a Bernoulli distribution.\n",
      "  1312|         0|            0|            0|  0.00%|\n",
      "  1313|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Dropout2d` for details.\n",
      "  1314|         0|            0|            0|  0.00%|\n",
      "  1315|         0|            0|            0|  0.00%|    Args:\n",
      "  1316|         0|            0|            0|  0.00%|        p: probability of a channel to be zeroed. Default: 0.5\n",
      "  1317|         0|            0|            0|  0.00%|        training: apply dropout if is ``True``. Default: ``True``\n",
      "  1318|         0|            0|            0|  0.00%|        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
      "  1319|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1320|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1321|         0|            0|            0|  0.00%|        return handle_torch_function(dropout2d, (input,), input, p=p, training=training, inplace=inplace)\n",
      "  1322|         0|            0|            0|  0.00%|    if p < 0.0 or p > 1.0:\n",
      "  1323|         0|            0|            0|  0.00%|        raise ValueError(\"dropout probability has to be between 0 and 1, \" \"but got {}\".format(p))\n",
      "  1324|         0|            0|            0|  0.00%|    inp_dim = input.dim()\n",
      "  1325|         0|            0|            0|  0.00%|    if inp_dim not in (3, 4):\n",
      "  1326|         0|            0|            0|  0.00%|        warn_msg = (f\"dropout2d: Received a {inp_dim}-D input to dropout2d, which is deprecated \"\n",
      "  1327|         0|            0|            0|  0.00%|                    \"and will result in an error in a future release. To retain the behavior \"\n",
      "  1328|         0|            0|            0|  0.00%|                    \"and silence this warning, please use dropout instead. Note that dropout2d \"\n",
      "  1329|         0|            0|            0|  0.00%|                    \"exists to provide channel-wise dropout on inputs with 2 spatial dimensions, \"\n",
      "  1330|         0|            0|            0|  0.00%|                    \"a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\")\n",
      "  1331|         0|            0|            0|  0.00%|        warnings.warn(warn_msg)\n",
      "  1332|         0|            0|            0|  0.00%|\n",
      "  1333|         0|            0|            0|  0.00%|    # TODO: Properly support no-batch-dim inputs. For now, these are NOT supported; passing\n",
      "  1334|         0|            0|            0|  0.00%|    # a 3D input will perform dropout1d behavior instead. This was done historically and the\n",
      "  1335|         0|            0|            0|  0.00%|    # behavior is maintained here for now.\n",
      "  1336|         0|            0|            0|  0.00%|    # See https://github.com/pytorch/pytorch/issues/77081\n",
      "  1337|         0|            0|            0|  0.00%|    if inp_dim == 3:\n",
      "  1338|         0|            0|            0|  0.00%|        warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n",
      "  1339|         0|            0|            0|  0.00%|                      \"1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C \"\n",
      "  1340|         0|            0|            0|  0.00%|                      \"is the channel dim. This behavior will change in a future release to interpret the \"\n",
      "  1341|         0|            0|            0|  0.00%|                      \"input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D \"\n",
      "  1342|         0|            0|            0|  0.00%|                      \"channel-wise dropout behavior, please switch to using dropout1d instead.\")\n",
      "  1343|         0|            0|            0|  0.00%|\n",
      "  1344|         0|            0|            0|  0.00%|    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)\n",
      "  1345|         0|            0|            0|  0.00%|\n",
      "  1346|         0|            0|            0|  0.00%|    return result\n",
      "  1347|         0|            0|            0|  0.00%|\n",
      "  1348|         0|            0|            0|  0.00%|\n",
      "  1349|         0|            0|            0|  0.00%|def dropout3d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:\n",
      "  1350|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1351|         0|            0|            0|  0.00%|    Randomly zero out entire channels (a channel is a 3D feature map,\n",
      "  1352|         0|            0|            0|  0.00%|    e.g., the :math:`j`-th channel of the :math:`i`-th sample in the\n",
      "  1353|         0|            0|            0|  0.00%|    batched input is a 3D tensor :math:`\\text{input}[i, j]`) of the input tensor).\n",
      "  1354|         0|            0|            0|  0.00%|    Each channel will be zeroed out independently on every forward call with\n",
      "  1355|         0|            0|            0|  0.00%|    probability :attr:`p` using samples from a Bernoulli distribution.\n",
      "  1356|         0|            0|            0|  0.00%|\n",
      "  1357|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Dropout3d` for details.\n",
      "  1358|         0|            0|            0|  0.00%|\n",
      "  1359|         0|            0|            0|  0.00%|    Args:\n",
      "  1360|         0|            0|            0|  0.00%|        p: probability of a channel to be zeroed. Default: 0.5\n",
      "  1361|         0|            0|            0|  0.00%|        training: apply dropout if is ``True``. Default: ``True``\n",
      "  1362|         0|            0|            0|  0.00%|        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
      "  1363|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1364|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1365|         0|            0|            0|  0.00%|        return handle_torch_function(dropout3d, (input,), input, p=p, training=training, inplace=inplace)\n",
      "  1366|         0|            0|            0|  0.00%|    if p < 0.0 or p > 1.0:\n",
      "  1367|         0|            0|            0|  0.00%|        raise ValueError(\"dropout probability has to be between 0 and 1, \" \"but got {}\".format(p))\n",
      "  1368|         0|            0|            0|  0.00%|    inp_dim = input.dim()\n",
      "  1369|         0|            0|            0|  0.00%|    if inp_dim not in (4, 5):\n",
      "  1370|         0|            0|            0|  0.00%|        warn_msg = (f\"dropout3d: Received a {inp_dim}-D input to dropout3d, which is deprecated \"\n",
      "  1371|         0|            0|            0|  0.00%|                    \"and will result in an error in a future release. To retain the behavior \"\n",
      "  1372|         0|            0|            0|  0.00%|                    \"and silence this warning, please use dropout instead. Note that dropout3d \"\n",
      "  1373|         0|            0|            0|  0.00%|                    \"exists to provide channel-wise dropout on inputs with 3 spatial dimensions, \"\n",
      "  1374|         0|            0|            0|  0.00%|                    \"a channel dimension, and an optional batch dimension (i.e. 4D or 5D inputs).\")\n",
      "  1375|         0|            0|            0|  0.00%|        warnings.warn(warn_msg)\n",
      "  1376|         0|            0|            0|  0.00%|\n",
      "  1377|         0|            0|            0|  0.00%|    is_batched = inp_dim == 5\n",
      "  1378|         0|            0|            0|  0.00%|    if not is_batched:\n",
      "  1379|         0|            0|            0|  0.00%|        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)\n",
      "  1380|         0|            0|            0|  0.00%|\n",
      "  1381|         0|            0|            0|  0.00%|    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)\n",
      "  1382|         0|            0|            0|  0.00%|\n",
      "  1383|         0|            0|            0|  0.00%|    if not is_batched:\n",
      "  1384|         0|            0|            0|  0.00%|        result = result.squeeze_(0) if inplace else result.squeeze(0)\n",
      "  1385|         0|            0|            0|  0.00%|    return result\n",
      "  1386|         0|            0|            0|  0.00%|\n",
      "  1387|         0|            0|            0|  0.00%|\n",
      "  1388|         0|            0|            0|  0.00%|def feature_alpha_dropout(input: Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> Tensor:\n",
      "  1389|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1390|         0|            0|            0|  0.00%|    Randomly masks out entire channels (a channel is a feature map,\n",
      "  1391|         0|            0|            0|  0.00%|    e.g. the :math:`j`-th channel of the :math:`i`-th sample in the batch input\n",
      "  1392|         0|            0|            0|  0.00%|    is a tensor :math:`\\text{input}[i, j]`) of the input tensor). Instead of\n",
      "  1393|         0|            0|            0|  0.00%|    setting activations to zero, as in regular Dropout, the activations are set\n",
      "  1394|         0|            0|            0|  0.00%|    to the negative saturation value of the SELU activation function.\n",
      "  1395|         0|            0|            0|  0.00%|\n",
      "  1396|         0|            0|            0|  0.00%|    Each element will be masked independently on every forward call with\n",
      "  1397|         0|            0|            0|  0.00%|    probability :attr:`p` using samples from a Bernoulli distribution.\n",
      "  1398|         0|            0|            0|  0.00%|    The elements to be masked are randomized on every forward call, and scaled\n",
      "  1399|         0|            0|            0|  0.00%|    and shifted to maintain zero mean and unit variance.\n",
      "  1400|         0|            0|            0|  0.00%|\n",
      "  1401|         0|            0|            0|  0.00%|    See :class:`~torch.nn.FeatureAlphaDropout` for details.\n",
      "  1402|         0|            0|            0|  0.00%|\n",
      "  1403|         0|            0|            0|  0.00%|    Args:\n",
      "  1404|         0|            0|            0|  0.00%|        p: dropout probability of a channel to be zeroed. Default: 0.5\n",
      "  1405|         0|            0|            0|  0.00%|        training: apply dropout if is ``True``. Default: ``True``\n",
      "  1406|         0|            0|            0|  0.00%|        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
      "  1407|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1408|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1409|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  1410|         0|            0|            0|  0.00%|            feature_alpha_dropout, (input,), input, p=p, training=training, inplace=inplace\n",
      "  1411|         0|            0|            0|  0.00%|        )\n",
      "  1412|         0|            0|            0|  0.00%|    if p < 0.0 or p > 1.0:\n",
      "  1413|         0|            0|            0|  0.00%|        raise ValueError(\"dropout probability has to be between 0 and 1, \" \"but got {}\".format(p))\n",
      "  1414|         0|            0|            0|  0.00%|    return _VF.feature_alpha_dropout_(input, p, training) if inplace else _VF.feature_alpha_dropout(input, p, training)\n",
      "  1415|         0|            0|            0|  0.00%|\n",
      "  1416|         0|            0|            0|  0.00%|\n",
      "  1417|         0|            0|            0|  0.00%|def _threshold(input: Tensor, threshold: float, value: float, inplace: bool = False) -> Tensor:\n",
      "  1418|         0|            0|            0|  0.00%|    r\"\"\"Thresholds each element of the input Tensor.\n",
      "  1419|         0|            0|            0|  0.00%|\n",
      "  1420|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Threshold` for more details.\n",
      "  1421|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1422|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1423|         0|            0|            0|  0.00%|        return handle_torch_function(_threshold, (input,), input, threshold, value, inplace=inplace)\n",
      "  1424|         0|            0|            0|  0.00%|    if inplace:\n",
      "  1425|         0|            0|            0|  0.00%|        result = _VF.threshold_(input, threshold, value)\n",
      "  1426|         0|            0|            0|  0.00%|    else:\n",
      "  1427|         0|            0|            0|  0.00%|        result = _VF.threshold(input, threshold, value)\n",
      "  1428|         0|            0|            0|  0.00%|    return result\n",
      "  1429|         0|            0|            0|  0.00%|\n",
      "  1430|         0|            0|            0|  0.00%|\n",
      "  1431|         0|            0|            0|  0.00%|# We define this function as _threshold because it takes an argument\n",
      "  1432|         0|            0|            0|  0.00%|# named threshold, which clobbers the recursive reference to the\n",
      "  1433|         0|            0|            0|  0.00%|# function needed for __torch_function__ support\n",
      "  1434|         0|            0|            0|  0.00%|threshold = _threshold\n",
      "  1435|         0|            0|            0|  0.00%|\n",
      "  1436|         0|            0|            0|  0.00%|threshold_ = _add_docstr(\n",
      "  1437|         0|            0|            0|  0.00%|    _VF.threshold_,\n",
      "  1438|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1439|         0|            0|            0|  0.00%|threshold_(input, threshold, value) -> Tensor\n",
      "  1440|         0|            0|            0|  0.00%|\n",
      "  1441|         0|            0|            0|  0.00%|In-place version of :func:`~threshold`.\n",
      "  1442|         0|            0|            0|  0.00%|\"\"\",\n",
      "  1443|         0|            0|            0|  0.00%|)\n",
      "  1444|         0|            0|            0|  0.00%|\n",
      "  1445|         0|            0|            0|  0.00%|\n",
      "  1446|     63180|     0.110928|  1.75574e-06|  0.02%|def relu(input: Tensor, inplace: bool = False) -> Tensor:\n",
      "  1447|         0|            0|            0|  0.00%|    r\"\"\"relu(input, inplace=False) -> Tensor\n",
      "  1448|         0|            0|            0|  0.00%|\n",
      "  1449|         0|            0|            0|  0.00%|    Applies the rectified linear unit function element-wise. See\n",
      "  1450|         0|            0|            0|  0.00%|    :class:`~torch.nn.ReLU` for more details.\n",
      "  1451|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1452|     63180|     0.146109|  2.31258e-06|  0.02%|    if has_torch_function_unary(input):\n",
      "  1453|         0|            0|            0|  0.00%|        return handle_torch_function(relu, (input,), input, inplace=inplace)\n",
      "  1454|     63180|     0.121238|  1.91893e-06|  0.02%|    if inplace:\n",
      "  1455|         0|            0|            0|  0.00%|        result = torch.relu_(input)\n",
      "  1456|         0|            0|            0|  0.00%|    else:\n",
      "  1457|     63180|     0.597556|  9.45799e-06|  0.10%|        result = torch.relu(input)\n",
      "  1458|     63180|     0.132755|  2.10122e-06|  0.02%|    return result\n",
      "  1459|         0|            0|            0|  0.00%|\n",
      "  1460|         0|            0|            0|  0.00%|\n",
      "  1461|         0|            0|            0|  0.00%|relu_ = _add_docstr(\n",
      "  1462|         0|            0|            0|  0.00%|    torch.relu_,\n",
      "  1463|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1464|         0|            0|            0|  0.00%|relu_(input) -> Tensor\n",
      "  1465|         0|            0|            0|  0.00%|\n",
      "  1466|         0|            0|            0|  0.00%|In-place version of :func:`~relu`.\n",
      "  1467|         0|            0|            0|  0.00%|\"\"\",\n",
      "  1468|         0|            0|            0|  0.00%|)\n",
      "  1469|         0|            0|            0|  0.00%|\n",
      "  1470|         0|            0|            0|  0.00%|\n",
      "  1471|         0|            0|            0|  0.00%|def glu(input: Tensor, dim: int = -1) -> Tensor:\n",
      "  1472|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1473|         0|            0|            0|  0.00%|    glu(input, dim=-1) -> Tensor\n",
      "  1474|         0|            0|            0|  0.00%|\n",
      "  1475|         0|            0|            0|  0.00%|    The gated linear unit. Computes:\n",
      "  1476|         0|            0|            0|  0.00%|\n",
      "  1477|         0|            0|            0|  0.00%|    .. math ::\n",
      "  1478|         0|            0|            0|  0.00%|        \\text{GLU}(a, b) = a \\otimes \\sigma(b)\n",
      "  1479|         0|            0|            0|  0.00%|\n",
      "  1480|         0|            0|            0|  0.00%|    where `input` is split in half along `dim` to form `a` and `b`, :math:`\\sigma`\n",
      "  1481|         0|            0|            0|  0.00%|    is the sigmoid function and :math:`\\otimes` is the element-wise product between matrices.\n",
      "  1482|         0|            0|            0|  0.00%|\n",
      "  1483|         0|            0|            0|  0.00%|    See `Language Modeling with Gated Convolutional Networks <https://arxiv.org/abs/1612.08083>`_.\n",
      "  1484|         0|            0|            0|  0.00%|\n",
      "  1485|         0|            0|            0|  0.00%|    Args:\n",
      "  1486|         0|            0|            0|  0.00%|        input (Tensor): input tensor\n",
      "  1487|         0|            0|            0|  0.00%|        dim (int): dimension on which to split the input. Default: -1\n",
      "  1488|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1489|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1490|         0|            0|            0|  0.00%|        return handle_torch_function(glu, (input,), input, dim=dim)\n",
      "  1491|         0|            0|            0|  0.00%|    if input.dim() == 0:\n",
      "  1492|         0|            0|            0|  0.00%|        raise RuntimeError(\"glu does not support scalars because halving size must be even\")\n",
      "  1493|         0|            0|            0|  0.00%|    return torch._C._nn.glu(input, dim)\n",
      "  1494|         0|            0|            0|  0.00%|\n",
      "  1495|         0|            0|            0|  0.00%|\n",
      "  1496|         0|            0|            0|  0.00%|def hardtanh(input: Tensor, min_val: float = -1., max_val: float = 1., inplace: bool = False) -> Tensor:\n",
      "  1497|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1498|         0|            0|            0|  0.00%|    hardtanh(input, min_val=-1., max_val=1., inplace=False) -> Tensor\n",
      "  1499|         0|            0|            0|  0.00%|\n",
      "  1500|         0|            0|            0|  0.00%|    Applies the HardTanh function element-wise. See :class:`~torch.nn.Hardtanh` for more\n",
      "  1501|         0|            0|            0|  0.00%|    details.\n",
      "  1502|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1503|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1504|         0|            0|            0|  0.00%|        return handle_torch_function(hardtanh, (input,), input, min_val=min_val, max_val=max_val, inplace=inplace)\n",
      "  1505|         0|            0|            0|  0.00%|    if inplace:\n",
      "  1506|         0|            0|            0|  0.00%|        result = torch._C._nn.hardtanh_(input, min_val, max_val)\n",
      "  1507|         0|            0|            0|  0.00%|    else:\n",
      "  1508|         0|            0|            0|  0.00%|        result = torch._C._nn.hardtanh(input, min_val, max_val)\n",
      "  1509|         0|            0|            0|  0.00%|    return result\n",
      "  1510|         0|            0|            0|  0.00%|\n",
      "  1511|         0|            0|            0|  0.00%|\n",
      "  1512|         0|            0|            0|  0.00%|hardtanh_ = _add_docstr(\n",
      "  1513|         0|            0|            0|  0.00%|    torch._C._nn.hardtanh_,\n",
      "  1514|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1515|         0|            0|            0|  0.00%|hardtanh_(input, min_val=-1., max_val=1.) -> Tensor\n",
      "  1516|         0|            0|            0|  0.00%|\n",
      "  1517|         0|            0|            0|  0.00%|In-place version of :func:`~hardtanh`.\n",
      "  1518|         0|            0|            0|  0.00%|\"\"\",\n",
      "  1519|         0|            0|            0|  0.00%|)\n",
      "  1520|         0|            0|            0|  0.00%|\n",
      "  1521|         0|            0|            0|  0.00%|\n",
      "  1522|         0|            0|            0|  0.00%|def relu6(input: Tensor, inplace: bool = False) -> Tensor:\n",
      "  1523|         0|            0|            0|  0.00%|    r\"\"\"relu6(input, inplace=False) -> Tensor\n",
      "  1524|         0|            0|            0|  0.00%|\n",
      "  1525|         0|            0|            0|  0.00%|    Applies the element-wise function :math:`\\text{ReLU6}(x) = \\min(\\max(0,x), 6)`.\n",
      "  1526|         0|            0|            0|  0.00%|\n",
      "  1527|         0|            0|            0|  0.00%|    See :class:`~torch.nn.ReLU6` for more details.\n",
      "  1528|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1529|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1530|         0|            0|            0|  0.00%|        return handle_torch_function(relu6, (input,), input, inplace=inplace)\n",
      "  1531|         0|            0|            0|  0.00%|    if inplace:\n",
      "  1532|         0|            0|            0|  0.00%|        result = torch._C._nn.relu6_(input)\n",
      "  1533|         0|            0|            0|  0.00%|    else:\n",
      "  1534|         0|            0|            0|  0.00%|        result = torch._C._nn.relu6(input)\n",
      "  1535|         0|            0|            0|  0.00%|    return result\n",
      "  1536|         0|            0|            0|  0.00%|\n",
      "  1537|         0|            0|            0|  0.00%|\n",
      "  1538|         0|            0|            0|  0.00%|def elu(input: Tensor, alpha: float = 1.0, inplace: bool = False) -> Tensor:\n",
      "  1539|         0|            0|            0|  0.00%|    r\"\"\"Applies element-wise,\n",
      "  1540|         0|            0|            0|  0.00%|    :math:`\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))`.\n",
      "  1541|         0|            0|            0|  0.00%|\n",
      "  1542|         0|            0|            0|  0.00%|    See :class:`~torch.nn.ELU` for more details.\n",
      "  1543|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1544|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1545|         0|            0|            0|  0.00%|        return handle_torch_function(elu, (input,), input, alpha=alpha, inplace=inplace)\n",
      "  1546|         0|            0|            0|  0.00%|    if inplace:\n",
      "  1547|         0|            0|            0|  0.00%|        result = torch._C._nn.elu_(input, alpha)\n",
      "  1548|         0|            0|            0|  0.00%|    else:\n",
      "  1549|         0|            0|            0|  0.00%|        result = torch._C._nn.elu(input, alpha)\n",
      "  1550|         0|            0|            0|  0.00%|    return result\n",
      "  1551|         0|            0|            0|  0.00%|\n",
      "  1552|         0|            0|            0|  0.00%|\n",
      "  1553|         0|            0|            0|  0.00%|elu_ = _add_docstr(\n",
      "  1554|         0|            0|            0|  0.00%|    torch._C._nn.elu_,\n",
      "  1555|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1556|         0|            0|            0|  0.00%|elu_(input, alpha=1.) -> Tensor\n",
      "  1557|         0|            0|            0|  0.00%|\n",
      "  1558|         0|            0|            0|  0.00%|In-place version of :func:`~elu`.\n",
      "  1559|         0|            0|            0|  0.00%|\"\"\",\n",
      "  1560|         0|            0|            0|  0.00%|)\n",
      "  1561|         0|            0|            0|  0.00%|\n",
      "  1562|         0|            0|            0|  0.00%|\n",
      "  1563|         0|            0|            0|  0.00%|def selu(input: Tensor, inplace: bool = False) -> Tensor:\n",
      "  1564|         0|            0|            0|  0.00%|    r\"\"\"selu(input, inplace=False) -> Tensor\n",
      "  1565|         0|            0|            0|  0.00%|\n",
      "  1566|         0|            0|            0|  0.00%|    Applies element-wise,\n",
      "  1567|         0|            0|            0|  0.00%|    :math:`\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))`,\n",
      "  1568|         0|            0|            0|  0.00%|    with :math:`\\alpha=1.6732632423543772848170429916717` and\n",
      "  1569|         0|            0|            0|  0.00%|    :math:`scale=1.0507009873554804934193349852946`.\n",
      "  1570|         0|            0|            0|  0.00%|\n",
      "  1571|         0|            0|            0|  0.00%|    See :class:`~torch.nn.SELU` for more details.\n",
      "  1572|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1573|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1574|         0|            0|            0|  0.00%|        return handle_torch_function(selu, (input,), input, inplace=inplace)\n",
      "  1575|         0|            0|            0|  0.00%|    if inplace:\n",
      "  1576|         0|            0|            0|  0.00%|        result = torch.selu_(input)\n",
      "  1577|         0|            0|            0|  0.00%|    else:\n",
      "  1578|         0|            0|            0|  0.00%|        result = torch.selu(input)\n",
      "  1579|         0|            0|            0|  0.00%|    return result\n",
      "  1580|         0|            0|            0|  0.00%|\n",
      "  1581|         0|            0|            0|  0.00%|\n",
      "  1582|         0|            0|            0|  0.00%|selu_ = _add_docstr(\n",
      "  1583|         0|            0|            0|  0.00%|    torch.selu_,\n",
      "  1584|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1585|         0|            0|            0|  0.00%|selu_(input) -> Tensor\n",
      "  1586|         0|            0|            0|  0.00%|\n",
      "  1587|         0|            0|            0|  0.00%|In-place version of :func:`~selu`.\n",
      "  1588|         0|            0|            0|  0.00%|\"\"\",\n",
      "  1589|         0|            0|            0|  0.00%|)\n",
      "  1590|         0|            0|            0|  0.00%|\n",
      "  1591|         0|            0|            0|  0.00%|\n",
      "  1592|         0|            0|            0|  0.00%|def celu(input: Tensor, alpha: float = 1.0, inplace: bool = False) -> Tensor:\n",
      "  1593|         0|            0|            0|  0.00%|    r\"\"\"celu(input, alpha=1., inplace=False) -> Tensor\n",
      "  1594|         0|            0|            0|  0.00%|\n",
      "  1595|         0|            0|            0|  0.00%|    Applies element-wise,\n",
      "  1596|         0|            0|            0|  0.00%|    :math:`\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))`.\n",
      "  1597|         0|            0|            0|  0.00%|\n",
      "  1598|         0|            0|            0|  0.00%|    See :class:`~torch.nn.CELU` for more details.\n",
      "  1599|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1600|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1601|         0|            0|            0|  0.00%|        return handle_torch_function(celu, (input,), input, alpha=alpha, inplace=inplace)\n",
      "  1602|         0|            0|            0|  0.00%|    if inplace:\n",
      "  1603|         0|            0|            0|  0.00%|        result = torch.celu_(input, alpha)\n",
      "  1604|         0|            0|            0|  0.00%|    else:\n",
      "  1605|         0|            0|            0|  0.00%|        result = torch.celu(input, alpha)\n",
      "  1606|         0|            0|            0|  0.00%|    return result\n",
      "  1607|         0|            0|            0|  0.00%|\n",
      "  1608|         0|            0|            0|  0.00%|\n",
      "  1609|         0|            0|            0|  0.00%|celu_ = _add_docstr(\n",
      "  1610|         0|            0|            0|  0.00%|    torch.celu_,\n",
      "  1611|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1612|         0|            0|            0|  0.00%|celu_(input, alpha=1.) -> Tensor\n",
      "  1613|         0|            0|            0|  0.00%|\n",
      "  1614|         0|            0|            0|  0.00%|In-place version of :func:`~celu`.\n",
      "  1615|         0|            0|            0|  0.00%|\"\"\",\n",
      "  1616|         0|            0|            0|  0.00%|)\n",
      "  1617|         0|            0|            0|  0.00%|\n",
      "  1618|         0|            0|            0|  0.00%|\n",
      "  1619|         0|            0|            0|  0.00%|def leaky_relu(input: Tensor, negative_slope: float = 0.01, inplace: bool = False) -> Tensor:\n",
      "  1620|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1621|         0|            0|            0|  0.00%|    leaky_relu(input, negative_slope=0.01, inplace=False) -> Tensor\n",
      "  1622|         0|            0|            0|  0.00%|\n",
      "  1623|         0|            0|            0|  0.00%|    Applies element-wise,\n",
      "  1624|         0|            0|            0|  0.00%|    :math:`\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)`\n",
      "  1625|         0|            0|            0|  0.00%|\n",
      "  1626|         0|            0|            0|  0.00%|    See :class:`~torch.nn.LeakyReLU` for more details.\n",
      "  1627|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1628|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1629|         0|            0|            0|  0.00%|        return handle_torch_function(leaky_relu, (input,), input, negative_slope=negative_slope, inplace=inplace)\n",
      "  1630|         0|            0|            0|  0.00%|    if inplace:\n",
      "  1631|         0|            0|            0|  0.00%|        result = torch._C._nn.leaky_relu_(input, negative_slope)\n",
      "  1632|         0|            0|            0|  0.00%|    else:\n",
      "  1633|         0|            0|            0|  0.00%|        result = torch._C._nn.leaky_relu(input, negative_slope)\n",
      "  1634|         0|            0|            0|  0.00%|    return result\n",
      "  1635|         0|            0|            0|  0.00%|\n",
      "  1636|         0|            0|            0|  0.00%|\n",
      "  1637|         0|            0|            0|  0.00%|leaky_relu_ = _add_docstr(\n",
      "  1638|         0|            0|            0|  0.00%|    torch._C._nn.leaky_relu_,\n",
      "  1639|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1640|         0|            0|            0|  0.00%|leaky_relu_(input, negative_slope=0.01) -> Tensor\n",
      "  1641|         0|            0|            0|  0.00%|\n",
      "  1642|         0|            0|            0|  0.00%|In-place version of :func:`~leaky_relu`.\n",
      "  1643|         0|            0|            0|  0.00%|\"\"\",\n",
      "  1644|         0|            0|            0|  0.00%|)\n",
      "  1645|         0|            0|            0|  0.00%|\n",
      "  1646|         0|            0|            0|  0.00%|\n",
      "  1647|         0|            0|            0|  0.00%|prelu = _add_docstr(\n",
      "  1648|         0|            0|            0|  0.00%|    torch.prelu,\n",
      "  1649|         0|            0|            0|  0.00%|    r\"\"\"prelu(input, weight) -> Tensor\n",
      "  1650|         0|            0|            0|  0.00%|\n",
      "  1651|         0|            0|            0|  0.00%|Applies element-wise the function\n",
      "  1652|         0|            0|            0|  0.00%|:math:`\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)` where weight is a\n",
      "  1653|         0|            0|            0|  0.00%|learnable parameter.\n",
      "  1654|         0|            0|            0|  0.00%|\n",
      "  1655|         0|            0|            0|  0.00%|See :class:`~torch.nn.PReLU` for more details.\n",
      "  1656|         0|            0|            0|  0.00%|\"\"\")\n",
      "  1657|         0|            0|            0|  0.00%|\n",
      "  1658|         0|            0|            0|  0.00%|\n",
      "  1659|         0|            0|            0|  0.00%|def rrelu(\n",
      "  1660|         0|            0|            0|  0.00%|    input: Tensor, lower: float = 1.0 / 8, upper: float = 1.0 / 3, training: bool = False, inplace: bool = False\n",
      "  1661|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  1662|         0|            0|            0|  0.00%|    r\"\"\"rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) -> Tensor\n",
      "  1663|         0|            0|            0|  0.00%|\n",
      "  1664|         0|            0|            0|  0.00%|    Randomized leaky ReLU.\n",
      "  1665|         0|            0|            0|  0.00%|\n",
      "  1666|         0|            0|            0|  0.00%|    See :class:`~torch.nn.RReLU` for more details.\n",
      "  1667|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1668|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1669|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  1670|         0|            0|            0|  0.00%|            rrelu, (input,), input, lower=lower, upper=upper, training=training, inplace=inplace\n",
      "  1671|         0|            0|            0|  0.00%|        )\n",
      "  1672|         0|            0|            0|  0.00%|    if inplace:\n",
      "  1673|         0|            0|            0|  0.00%|        result = torch.rrelu_(input, lower, upper, training)\n",
      "  1674|         0|            0|            0|  0.00%|    else:\n",
      "  1675|         0|            0|            0|  0.00%|        result = torch.rrelu(input, lower, upper, training)\n",
      "  1676|         0|            0|            0|  0.00%|    return result\n",
      "  1677|         0|            0|            0|  0.00%|\n",
      "  1678|         0|            0|            0|  0.00%|\n",
      "  1679|         0|            0|            0|  0.00%|rrelu_ = _add_docstr(\n",
      "  1680|         0|            0|            0|  0.00%|    torch.rrelu_,\n",
      "  1681|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1682|         0|            0|            0|  0.00%|rrelu_(input, lower=1./8, upper=1./3, training=False) -> Tensor\n",
      "  1683|         0|            0|            0|  0.00%|\n",
      "  1684|         0|            0|            0|  0.00%|In-place version of :func:`~rrelu`.\n",
      "  1685|         0|            0|            0|  0.00%|\"\"\",\n",
      "  1686|         0|            0|            0|  0.00%|)\n",
      "  1687|         0|            0|            0|  0.00%|\n",
      "  1688|         0|            0|            0|  0.00%|logsigmoid = _add_docstr(\n",
      "  1689|         0|            0|            0|  0.00%|    torch._C._nn.log_sigmoid,\n",
      "  1690|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1691|         0|            0|            0|  0.00%|logsigmoid(input) -> Tensor\n",
      "  1692|         0|            0|            0|  0.00%|\n",
      "  1693|         0|            0|            0|  0.00%|Applies element-wise :math:`\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)`\n",
      "  1694|         0|            0|            0|  0.00%|\n",
      "  1695|         0|            0|            0|  0.00%|See :class:`~torch.nn.LogSigmoid` for more details.\n",
      "  1696|         0|            0|            0|  0.00%|\"\"\",\n",
      "  1697|         0|            0|            0|  0.00%|)\n",
      "  1698|         0|            0|            0|  0.00%|\n",
      "  1699|         0|            0|            0|  0.00%|gelu = _add_docstr(\n",
      "  1700|         0|            0|            0|  0.00%|    torch._C._nn.gelu,\n",
      "  1701|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1702|         0|            0|            0|  0.00%|gelu(input, approximate = 'none') -> Tensor\n",
      "  1703|         0|            0|            0|  0.00%|\n",
      "  1704|         0|            0|            0|  0.00%|When the approximate argument is 'none', it applies element-wise the function\n",
      "  1705|         0|            0|            0|  0.00%|:math:`\\text{GELU}(x) = x * \\Phi(x)`\n",
      "  1706|         0|            0|            0|  0.00%|\n",
      "  1707|         0|            0|            0|  0.00%|where :math:`\\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.\n",
      "  1708|         0|            0|            0|  0.00%|\n",
      "  1709|         0|            0|            0|  0.00%|When the approximate argument is 'tanh', Gelu is estimated with:\n",
      "  1710|         0|            0|            0|  0.00%|    :math::  \\text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt(2 / \\pi) * (x + 0.044715 * x^3)))\n",
      "  1711|         0|            0|            0|  0.00%|\n",
      "  1712|         0|            0|            0|  0.00%|See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_.\n",
      "  1713|         0|            0|            0|  0.00%|\"\"\")\n",
      "  1714|         0|            0|            0|  0.00%|\n",
      "  1715|         0|            0|            0|  0.00%|hardshrink = _add_docstr(\n",
      "  1716|         0|            0|            0|  0.00%|    torch.hardshrink,\n",
      "  1717|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1718|         0|            0|            0|  0.00%|hardshrink(input, lambd=0.5) -> Tensor\n",
      "  1719|         0|            0|            0|  0.00%|\n",
      "  1720|         0|            0|            0|  0.00%|Applies the hard shrinkage function element-wise\n",
      "  1721|         0|            0|            0|  0.00%|\n",
      "  1722|         0|            0|            0|  0.00%|See :class:`~torch.nn.Hardshrink` for more details.\n",
      "  1723|         0|            0|            0|  0.00%|\"\"\")\n",
      "  1724|         0|            0|            0|  0.00%|\n",
      "  1725|         0|            0|            0|  0.00%|\n",
      "  1726|         0|            0|            0|  0.00%|def tanhshrink(input):\n",
      "  1727|         0|            0|            0|  0.00%|    r\"\"\"tanhshrink(input) -> Tensor\n",
      "  1728|         0|            0|            0|  0.00%|\n",
      "  1729|         0|            0|            0|  0.00%|    Applies element-wise, :math:`\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)`\n",
      "  1730|         0|            0|            0|  0.00%|\n",
      "  1731|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Tanhshrink` for more details.\n",
      "  1732|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1733|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1734|         0|            0|            0|  0.00%|        return handle_torch_function(tanhshrink, (input,), input)\n",
      "  1735|         0|            0|            0|  0.00%|    return input - input.tanh()\n",
      "  1736|         0|            0|            0|  0.00%|\n",
      "  1737|         0|            0|            0|  0.00%|\n",
      "  1738|         0|            0|            0|  0.00%|def softsign(input):\n",
      "  1739|         0|            0|            0|  0.00%|    r\"\"\"softsign(input) -> Tensor\n",
      "  1740|         0|            0|            0|  0.00%|\n",
      "  1741|         0|            0|            0|  0.00%|    Applies element-wise, the function :math:`\\text{SoftSign}(x) = \\frac{x}{1 + |x|}`\n",
      "  1742|         0|            0|            0|  0.00%|\n",
      "  1743|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Softsign` for more details.\n",
      "  1744|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1745|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1746|         0|            0|            0|  0.00%|        return handle_torch_function(softsign, (input,), input)\n",
      "  1747|         0|            0|            0|  0.00%|    return input / (input.abs() + 1)\n",
      "  1748|         0|            0|            0|  0.00%|\n",
      "  1749|         0|            0|            0|  0.00%|\n",
      "  1750|         0|            0|            0|  0.00%|softplus = _add_docstr(\n",
      "  1751|         0|            0|            0|  0.00%|    torch._C._nn.softplus,\n",
      "  1752|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1753|         0|            0|            0|  0.00%|softplus(input, beta=1, threshold=20) -> Tensor\n",
      "  1754|         0|            0|            0|  0.00%|\n",
      "  1755|         0|            0|            0|  0.00%|Applies element-wise, the function :math:`\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))`.\n",
      "  1756|         0|            0|            0|  0.00%|\n",
      "  1757|         0|            0|            0|  0.00%|For numerical stability the implementation reverts to the linear function\n",
      "  1758|         0|            0|            0|  0.00%|when :math:`input \\times \\beta > threshold`.\n",
      "  1759|         0|            0|            0|  0.00%|\n",
      "  1760|         0|            0|            0|  0.00%|See :class:`~torch.nn.Softplus` for more details.\n",
      "  1761|         0|            0|            0|  0.00%|\"\"\",\n",
      "  1762|         0|            0|            0|  0.00%|)\n",
      "  1763|         0|            0|            0|  0.00%|\n",
      "  1764|         0|            0|            0|  0.00%|\n",
      "  1765|         0|            0|            0|  0.00%|def _get_softmax_dim(name: str, ndim: int, stacklevel: int) -> int:\n",
      "  1766|         0|            0|            0|  0.00%|    warnings.warn(\n",
      "  1767|         0|            0|            0|  0.00%|        \"Implicit dimension choice for {} has been deprecated. \"\n",
      "  1768|         0|            0|            0|  0.00%|        \"Change the call to include dim=X as an argument.\".format(name),\n",
      "  1769|         0|            0|            0|  0.00%|        stacklevel=stacklevel,\n",
      "  1770|         0|            0|            0|  0.00%|    )\n",
      "  1771|         0|            0|            0|  0.00%|    if ndim == 0 or ndim == 1 or ndim == 3:\n",
      "  1772|         0|            0|            0|  0.00%|        ret = 0\n",
      "  1773|         0|            0|            0|  0.00%|    else:\n",
      "  1774|         0|            0|            0|  0.00%|        ret = 1\n",
      "  1775|         0|            0|            0|  0.00%|    return ret\n",
      "  1776|         0|            0|            0|  0.00%|\n",
      "  1777|         0|            0|            0|  0.00%|\n",
      "  1778|         0|            0|            0|  0.00%|def softmin(input: Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[DType] = None) -> Tensor:\n",
      "  1779|         0|            0|            0|  0.00%|    r\"\"\"Applies a softmin function.\n",
      "  1780|         0|            0|            0|  0.00%|\n",
      "  1781|         0|            0|            0|  0.00%|    Note that :math:`\\text{Softmin}(x) = \\text{Softmax}(-x)`. See softmax definition for mathematical formula.\n",
      "  1782|         0|            0|            0|  0.00%|\n",
      "  1783|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Softmin` for more details.\n",
      "  1784|         0|            0|            0|  0.00%|\n",
      "  1785|         0|            0|            0|  0.00%|    Args:\n",
      "  1786|         0|            0|            0|  0.00%|        input (Tensor): input\n",
      "  1787|         0|            0|            0|  0.00%|        dim (int): A dimension along which softmin will be computed (so every slice\n",
      "  1788|         0|            0|            0|  0.00%|            along dim will sum to 1).\n",
      "  1789|         0|            0|            0|  0.00%|        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "  1790|         0|            0|            0|  0.00%|          If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
      "  1791|         0|            0|            0|  0.00%|          is performed. This is useful for preventing data type overflows. Default: None.\n",
      "  1792|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1793|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1794|         0|            0|            0|  0.00%|        return handle_torch_function(softmin, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype)\n",
      "  1795|         0|            0|            0|  0.00%|    if dim is None:\n",
      "  1796|         0|            0|            0|  0.00%|        dim = _get_softmax_dim(\"softmin\", input.dim(), _stacklevel)\n",
      "  1797|         0|            0|            0|  0.00%|    if dtype is None:\n",
      "  1798|         0|            0|            0|  0.00%|        ret = (-input).softmax(dim)\n",
      "  1799|         0|            0|            0|  0.00%|    else:\n",
      "  1800|         0|            0|            0|  0.00%|        ret = (-input).softmax(dim, dtype=dtype)\n",
      "  1801|         0|            0|            0|  0.00%|    return ret\n",
      "  1802|         0|            0|            0|  0.00%|\n",
      "  1803|         0|            0|            0|  0.00%|\n",
      "  1804|     31590|    0.0659461|  2.08756e-06|  0.01%|def softmax(input: Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[DType] = None) -> Tensor:\n",
      "  1805|         0|            0|            0|  0.00%|    r\"\"\"Applies a softmax function.\n",
      "  1806|         0|            0|            0|  0.00%|\n",
      "  1807|         0|            0|            0|  0.00%|    Softmax is defined as:\n",
      "  1808|         0|            0|            0|  0.00%|\n",
      "  1809|         0|            0|            0|  0.00%|    :math:`\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}`\n",
      "  1810|         0|            0|            0|  0.00%|\n",
      "  1811|         0|            0|            0|  0.00%|    It is applied to all slices along dim, and will re-scale them so that the elements\n",
      "  1812|         0|            0|            0|  0.00%|    lie in the range `[0, 1]` and sum to 1.\n",
      "  1813|         0|            0|            0|  0.00%|\n",
      "  1814|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Softmax` for more details.\n",
      "  1815|         0|            0|            0|  0.00%|\n",
      "  1816|         0|            0|            0|  0.00%|    Args:\n",
      "  1817|         0|            0|            0|  0.00%|        input (Tensor): input\n",
      "  1818|         0|            0|            0|  0.00%|        dim (int): A dimension along which softmax will be computed.\n",
      "  1819|         0|            0|            0|  0.00%|        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "  1820|         0|            0|            0|  0.00%|          If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
      "  1821|         0|            0|            0|  0.00%|          is performed. This is useful for preventing data type overflows. Default: None.\n",
      "  1822|         0|            0|            0|  0.00%|\n",
      "  1823|         0|            0|            0|  0.00%|    .. note::\n",
      "  1824|         0|            0|            0|  0.00%|        This function doesn't work directly with NLLLoss,\n",
      "  1825|         0|            0|            0|  0.00%|        which expects the Log to be computed between the Softmax and itself.\n",
      "  1826|         0|            0|            0|  0.00%|        Use log_softmax instead (it's faster and has better numerical properties).\n",
      "  1827|         0|            0|            0|  0.00%|\n",
      "  1828|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1829|     31590|     0.083986|  2.65863e-06|  0.01%|    if has_torch_function_unary(input):\n",
      "  1830|         0|            0|            0|  0.00%|        return handle_torch_function(softmax, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype)\n",
      "  1831|     31590|    0.0685916|  2.17131e-06|  0.01%|    if dim is None:\n",
      "  1832|         0|            0|            0|  0.00%|        dim = _get_softmax_dim(\"softmax\", input.dim(), _stacklevel)\n",
      "  1833|     31590|    0.0636156|  2.01379e-06|  0.01%|    if dtype is None:\n",
      "  1834|     31590|     0.429403|   1.3593e-05|  0.07%|        ret = input.softmax(dim)\n",
      "  1835|         0|            0|            0|  0.00%|    else:\n",
      "  1836|         0|            0|            0|  0.00%|        ret = input.softmax(dim, dtype=dtype)\n",
      "  1837|     31590|    0.0707426|   2.2394e-06|  0.01%|    return ret\n",
      "  1838|         0|            0|            0|  0.00%|\n",
      "  1839|         0|            0|            0|  0.00%|\n",
      "  1840|         0|            0|            0|  0.00%|def gumbel_softmax(logits: Tensor, tau: float = 1, hard: bool = False, eps: float = 1e-10, dim: int = -1) -> Tensor:\n",
      "  1841|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1842|         0|            0|            0|  0.00%|    Samples from the Gumbel-Softmax distribution (`Link 1`_  `Link 2`_) and optionally discretizes.\n",
      "  1843|         0|            0|            0|  0.00%|\n",
      "  1844|         0|            0|            0|  0.00%|    Args:\n",
      "  1845|         0|            0|            0|  0.00%|      logits: `[..., num_features]` unnormalized log probabilities\n",
      "  1846|         0|            0|            0|  0.00%|      tau: non-negative scalar temperature\n",
      "  1847|         0|            0|            0|  0.00%|      hard: if ``True``, the returned samples will be discretized as one-hot vectors,\n",
      "  1848|         0|            0|            0|  0.00%|            but will be differentiated as if it is the soft sample in autograd\n",
      "  1849|         0|            0|            0|  0.00%|      dim (int): A dimension along which softmax will be computed. Default: -1.\n",
      "  1850|         0|            0|            0|  0.00%|\n",
      "  1851|         0|            0|            0|  0.00%|    Returns:\n",
      "  1852|         0|            0|            0|  0.00%|      Sampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.\n",
      "  1853|         0|            0|            0|  0.00%|      If ``hard=True``, the returned samples will be one-hot, otherwise they will\n",
      "  1854|         0|            0|            0|  0.00%|      be probability distributions that sum to 1 across `dim`.\n",
      "  1855|         0|            0|            0|  0.00%|\n",
      "  1856|         0|            0|            0|  0.00%|    .. note::\n",
      "  1857|         0|            0|            0|  0.00%|      This function is here for legacy reasons, may be removed from nn.Functional in the future.\n",
      "  1858|         0|            0|            0|  0.00%|\n",
      "  1859|         0|            0|            0|  0.00%|    .. note::\n",
      "  1860|         0|            0|            0|  0.00%|      The main trick for `hard` is to do  `y_hard - y_soft.detach() + y_soft`\n",
      "  1861|         0|            0|            0|  0.00%|\n",
      "  1862|         0|            0|            0|  0.00%|      It achieves two things:\n",
      "  1863|         0|            0|            0|  0.00%|      - makes the output value exactly one-hot\n",
      "  1864|         0|            0|            0|  0.00%|      (since we add then subtract y_soft value)\n",
      "  1865|         0|            0|            0|  0.00%|      - makes the gradient equal to y_soft gradient\n",
      "  1866|         0|            0|            0|  0.00%|      (since we strip all other gradients)\n",
      "  1867|         0|            0|            0|  0.00%|\n",
      "  1868|         0|            0|            0|  0.00%|    Examples::\n",
      "  1869|         0|            0|            0|  0.00%|        >>> logits = torch.randn(20, 32)\n",
      "  1870|         0|            0|            0|  0.00%|        >>> # Sample soft categorical using reparametrization trick:\n",
      "  1871|         0|            0|            0|  0.00%|        >>> F.gumbel_softmax(logits, tau=1, hard=False)\n",
      "  1872|         0|            0|            0|  0.00%|        >>> # Sample hard categorical using \"Straight-through\" trick:\n",
      "  1873|         0|            0|            0|  0.00%|        >>> F.gumbel_softmax(logits, tau=1, hard=True)\n",
      "  1874|         0|            0|            0|  0.00%|\n",
      "  1875|         0|            0|            0|  0.00%|    .. _Link 1:\n",
      "  1876|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1611.00712\n",
      "  1877|         0|            0|            0|  0.00%|    .. _Link 2:\n",
      "  1878|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1611.01144\n",
      "  1879|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1880|         0|            0|            0|  0.00%|    if has_torch_function_unary(logits):\n",
      "  1881|         0|            0|            0|  0.00%|        return handle_torch_function(gumbel_softmax, (logits,), logits, tau=tau, hard=hard, eps=eps, dim=dim)\n",
      "  1882|         0|            0|            0|  0.00%|    if eps != 1e-10:\n",
      "  1883|         0|            0|            0|  0.00%|        warnings.warn(\"`eps` parameter is deprecated and has no effect.\")\n",
      "  1884|         0|            0|            0|  0.00%|\n",
      "  1885|         0|            0|            0|  0.00%|    gumbels = (\n",
      "  1886|         0|            0|            0|  0.00%|        -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
      "  1887|         0|            0|            0|  0.00%|    )  # ~Gumbel(0,1)\n",
      "  1888|         0|            0|            0|  0.00%|    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n",
      "  1889|         0|            0|            0|  0.00%|    y_soft = gumbels.softmax(dim)\n",
      "  1890|         0|            0|            0|  0.00%|\n",
      "  1891|         0|            0|            0|  0.00%|    if hard:\n",
      "  1892|         0|            0|            0|  0.00%|        # Straight through.\n",
      "  1893|         0|            0|            0|  0.00%|        index = y_soft.max(dim, keepdim=True)[1]\n",
      "  1894|         0|            0|            0|  0.00%|        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
      "  1895|         0|            0|            0|  0.00%|        ret = y_hard - y_soft.detach() + y_soft\n",
      "  1896|         0|            0|            0|  0.00%|    else:\n",
      "  1897|         0|            0|            0|  0.00%|        # Reparametrization trick.\n",
      "  1898|         0|            0|            0|  0.00%|        ret = y_soft\n",
      "  1899|         0|            0|            0|  0.00%|    return ret\n",
      "  1900|         0|            0|            0|  0.00%|\n",
      "  1901|         0|            0|            0|  0.00%|\n",
      "  1902|         0|            0|            0|  0.00%|def log_softmax(input: Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[DType] = None) -> Tensor:\n",
      "  1903|         0|            0|            0|  0.00%|    r\"\"\"Applies a softmax followed by a logarithm.\n",
      "  1904|         0|            0|            0|  0.00%|\n",
      "  1905|         0|            0|            0|  0.00%|    While mathematically equivalent to log(softmax(x)), doing these two\n",
      "  1906|         0|            0|            0|  0.00%|    operations separately is slower and numerically unstable. This function\n",
      "  1907|         0|            0|            0|  0.00%|    uses an alternative formulation to compute the output and gradient correctly.\n",
      "  1908|         0|            0|            0|  0.00%|\n",
      "  1909|         0|            0|            0|  0.00%|    See :class:`~torch.nn.LogSoftmax` for more details.\n",
      "  1910|         0|            0|            0|  0.00%|\n",
      "  1911|         0|            0|            0|  0.00%|    Args:\n",
      "  1912|         0|            0|            0|  0.00%|        input (Tensor): input\n",
      "  1913|         0|            0|            0|  0.00%|        dim (int): A dimension along which log_softmax will be computed.\n",
      "  1914|         0|            0|            0|  0.00%|        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "  1915|         0|            0|            0|  0.00%|          If specified, the input tensor is cast to :attr:`dtype` before the operation\n",
      "  1916|         0|            0|            0|  0.00%|          is performed. This is useful for preventing data type overflows. Default: None.\n",
      "  1917|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1918|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1919|         0|            0|            0|  0.00%|        return handle_torch_function(log_softmax, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype)\n",
      "  1920|         0|            0|            0|  0.00%|    if dim is None:\n",
      "  1921|         0|            0|            0|  0.00%|        dim = _get_softmax_dim(\"log_softmax\", input.dim(), _stacklevel)\n",
      "  1922|         0|            0|            0|  0.00%|    if dtype is None:\n",
      "  1923|         0|            0|            0|  0.00%|        ret = input.log_softmax(dim)\n",
      "  1924|         0|            0|            0|  0.00%|    else:\n",
      "  1925|         0|            0|            0|  0.00%|        ret = input.log_softmax(dim, dtype=dtype)\n",
      "  1926|         0|            0|            0|  0.00%|    return ret\n",
      "  1927|         0|            0|            0|  0.00%|\n",
      "  1928|         0|            0|            0|  0.00%|\n",
      "  1929|         0|            0|            0|  0.00%|softshrink = _add_docstr(\n",
      "  1930|         0|            0|            0|  0.00%|    torch._C._nn.softshrink,\n",
      "  1931|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1932|         0|            0|            0|  0.00%|softshrink(input, lambd=0.5) -> Tensor\n",
      "  1933|         0|            0|            0|  0.00%|\n",
      "  1934|         0|            0|            0|  0.00%|Applies the soft shrinkage function elementwise\n",
      "  1935|         0|            0|            0|  0.00%|\n",
      "  1936|         0|            0|            0|  0.00%|See :class:`~torch.nn.Softshrink` for more details.\n",
      "  1937|         0|            0|            0|  0.00%|\"\"\",\n",
      "  1938|         0|            0|            0|  0.00%|)\n",
      "  1939|         0|            0|            0|  0.00%|\n",
      "  1940|         0|            0|            0|  0.00%|\n",
      "  1941|         0|            0|            0|  0.00%|def tanh(input):\n",
      "  1942|         0|            0|            0|  0.00%|    r\"\"\"tanh(input) -> Tensor\n",
      "  1943|         0|            0|            0|  0.00%|\n",
      "  1944|         0|            0|            0|  0.00%|    Applies element-wise,\n",
      "  1945|         0|            0|            0|  0.00%|    :math:`\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}`\n",
      "  1946|         0|            0|            0|  0.00%|\n",
      "  1947|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Tanh` for more details.\n",
      "  1948|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1949|         0|            0|            0|  0.00%|    warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "  1950|         0|            0|            0|  0.00%|    return input.tanh()\n",
      "  1951|         0|            0|            0|  0.00%|\n",
      "  1952|         0|            0|            0|  0.00%|\n",
      "  1953|         0|            0|            0|  0.00%|def sigmoid(input):\n",
      "  1954|         0|            0|            0|  0.00%|    r\"\"\"sigmoid(input) -> Tensor\n",
      "  1955|         0|            0|            0|  0.00%|\n",
      "  1956|         0|            0|            0|  0.00%|    Applies the element-wise function :math:`\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}`\n",
      "  1957|         0|            0|            0|  0.00%|\n",
      "  1958|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Sigmoid` for more details.\n",
      "  1959|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1960|         0|            0|            0|  0.00%|    warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "  1961|         0|            0|            0|  0.00%|    return input.sigmoid()\n",
      "  1962|         0|            0|            0|  0.00%|\n",
      "  1963|         0|            0|            0|  0.00%|\n",
      "  1964|         0|            0|            0|  0.00%|def hardsigmoid(input: Tensor, inplace: bool = False) -> Tensor:\n",
      "  1965|         0|            0|            0|  0.00%|    r\"\"\"Applies the element-wise function\n",
      "  1966|         0|            0|            0|  0.00%|\n",
      "  1967|         0|            0|            0|  0.00%|    .. math::\n",
      "  1968|         0|            0|            0|  0.00%|        \\text{Hardsigmoid}(x) = \\begin{cases}\n",
      "  1969|         0|            0|            0|  0.00%|            0 & \\text{if~} x \\le -3, \\\\\n",
      "  1970|         0|            0|            0|  0.00%|            1 & \\text{if~} x \\ge +3, \\\\\n",
      "  1971|         0|            0|            0|  0.00%|            x / 6 + 1 / 2 & \\text{otherwise}\n",
      "  1972|         0|            0|            0|  0.00%|        \\end{cases}\n",
      "  1973|         0|            0|            0|  0.00%|\n",
      "  1974|         0|            0|            0|  0.00%|    Args:\n",
      "  1975|         0|            0|            0|  0.00%|        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
      "  1976|         0|            0|            0|  0.00%|\n",
      "  1977|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Hardsigmoid` for more details.\n",
      "  1978|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1979|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  1980|         0|            0|            0|  0.00%|        return handle_torch_function(hardsigmoid, (input,), input, inplace=inplace)\n",
      "  1981|         0|            0|            0|  0.00%|    if inplace:\n",
      "  1982|         0|            0|            0|  0.00%|        return torch._C._nn.hardsigmoid_(input)\n",
      "  1983|         0|            0|            0|  0.00%|    return torch._C._nn.hardsigmoid(input)\n",
      "  1984|         0|            0|            0|  0.00%|\n",
      "  1985|         0|            0|            0|  0.00%|\n",
      "  1986|         0|            0|            0|  0.00%|linear = _add_docstr(\n",
      "  1987|         0|            0|            0|  0.00%|    torch._C._nn.linear,\n",
      "  1988|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1989|         0|            0|            0|  0.00%|linear(input, weight, bias=None) -> Tensor\n",
      "  1990|         0|            0|            0|  0.00%|\n",
      "  1991|         0|            0|            0|  0.00%|Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
      "  1992|         0|            0|            0|  0.00%|\n",
      "  1993|         0|            0|            0|  0.00%|This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "  1994|         0|            0|            0|  0.00%|\n",
      "  1995|         0|            0|            0|  0.00%|Shape:\n",
      "  1996|         0|            0|            0|  0.00%|\n",
      "  1997|         0|            0|            0|  0.00%|    - Input: :math:`(*, in\\_features)` where `*` means any number of\n",
      "  1998|         0|            0|            0|  0.00%|      additional dimensions, including none\n",
      "  1999|         0|            0|            0|  0.00%|    - Weight: :math:`(out\\_features, in\\_features)` or :math:`(in\\_features)`\n",
      "  2000|         0|            0|            0|  0.00%|    - Bias: :math:`(out\\_features)` or :math:`()`\n",
      "  2001|         0|            0|            0|  0.00%|    - Output: :math:`(*, out\\_features)` or :math:`(*)`, based on the shape of the weight\n",
      "  2002|         0|            0|            0|  0.00%|\"\"\")\n",
      "  2003|         0|            0|            0|  0.00%|\n",
      "  2004|         0|            0|            0|  0.00%|\n",
      "  2005|         0|            0|            0|  0.00%|bilinear = _add_docstr(\n",
      "  2006|         0|            0|            0|  0.00%|    torch.bilinear,\n",
      "  2007|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  2008|         0|            0|            0|  0.00%|bilinear(input1, input2, weight, bias=None) -> Tensor\n",
      "  2009|         0|            0|            0|  0.00%|\n",
      "  2010|         0|            0|            0|  0.00%|Applies a bilinear transformation to the incoming data:\n",
      "  2011|         0|            0|            0|  0.00%|:math:`y = x_1^T A x_2 + b`\n",
      "  2012|         0|            0|            0|  0.00%|\n",
      "  2013|         0|            0|            0|  0.00%|Shape:\n",
      "  2014|         0|            0|            0|  0.00%|\n",
      "  2015|         0|            0|            0|  0.00%|    - input1: :math:`(N, *, H_{in1})` where :math:`H_{in1}=\\text{in1\\_features}`\n",
      "  2016|         0|            0|            0|  0.00%|      and :math:`*` means any number of additional dimensions.\n",
      "  2017|         0|            0|            0|  0.00%|      All but the last dimension of the inputs should be the same.\n",
      "  2018|         0|            0|            0|  0.00%|    - input2: :math:`(N, *, H_{in2})` where :math:`H_{in2}=\\text{in2\\_features}`\n",
      "  2019|         0|            0|            0|  0.00%|    - weight: :math:`(\\text{out\\_features}, \\text{in1\\_features},\n",
      "  2020|         0|            0|            0|  0.00%|      \\text{in2\\_features})`\n",
      "  2021|         0|            0|            0|  0.00%|    - bias: :math:`(\\text{out\\_features})`\n",
      "  2022|         0|            0|            0|  0.00%|    - output: :math:`(N, *, H_{out})` where :math:`H_{out}=\\text{out\\_features}`\n",
      "  2023|         0|            0|            0|  0.00%|      and all but the last dimension are the same shape as the input.\n",
      "  2024|         0|            0|            0|  0.00%|\"\"\")\n",
      "  2025|         0|            0|            0|  0.00%|\n",
      "  2026|         0|            0|            0|  0.00%|\n",
      "  2027|         0|            0|            0|  0.00%|def silu(input: Tensor, inplace: bool = False) -> Tensor:\n",
      "  2028|         0|            0|            0|  0.00%|    r\"\"\"Applies the Sigmoid Linear Unit (SiLU) function, element-wise.\n",
      "  2029|         0|            0|            0|  0.00%|    The SiLU function is also known as the swish function.\n",
      "  2030|         0|            0|            0|  0.00%|\n",
      "  2031|         0|            0|            0|  0.00%|    .. math::\n",
      "  2032|         0|            0|            0|  0.00%|        \\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.}\n",
      "  2033|         0|            0|            0|  0.00%|\n",
      "  2034|         0|            0|            0|  0.00%|    .. note::\n",
      "  2035|         0|            0|            0|  0.00%|        See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_\n",
      "  2036|         0|            0|            0|  0.00%|        where the SiLU (Sigmoid Linear Unit) was originally coined, and see\n",
      "  2037|         0|            0|            0|  0.00%|        `Sigmoid-Weighted Linear Units for Neural Network Function Approximation\n",
      "  2038|         0|            0|            0|  0.00%|        in Reinforcement Learning <https://arxiv.org/abs/1702.03118>`_ and `Swish:\n",
      "  2039|         0|            0|            0|  0.00%|        a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941v1>`_\n",
      "  2040|         0|            0|            0|  0.00%|        where the SiLU was experimented with later.\n",
      "  2041|         0|            0|            0|  0.00%|\n",
      "  2042|         0|            0|            0|  0.00%|    See :class:`~torch.nn.SiLU` for more details.\n",
      "  2043|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2044|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  2045|         0|            0|            0|  0.00%|        return handle_torch_function(silu, (input,), input, inplace=inplace)\n",
      "  2046|         0|            0|            0|  0.00%|    if inplace:\n",
      "  2047|         0|            0|            0|  0.00%|        return torch._C._nn.silu_(input)\n",
      "  2048|         0|            0|            0|  0.00%|    return torch._C._nn.silu(input)\n",
      "  2049|         0|            0|            0|  0.00%|\n",
      "  2050|         0|            0|            0|  0.00%|\n",
      "  2051|         0|            0|            0|  0.00%|def mish(input: Tensor, inplace: bool = False) -> Tensor:\n",
      "  2052|         0|            0|            0|  0.00%|    r\"\"\"Applies the Mish function, element-wise.\n",
      "  2053|         0|            0|            0|  0.00%|    Mish: A Self Regularized Non-Monotonic Neural Activation Function.\n",
      "  2054|         0|            0|            0|  0.00%|\n",
      "  2055|         0|            0|            0|  0.00%|    .. math::\n",
      "  2056|         0|            0|            0|  0.00%|        \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))\n",
      "  2057|         0|            0|            0|  0.00%|\n",
      "  2058|         0|            0|            0|  0.00%|    .. note::\n",
      "  2059|         0|            0|            0|  0.00%|        See `Mish: A Self Regularized Non-Monotonic Neural Activation Function <https://arxiv.org/abs/1908.08681>`_\n",
      "  2060|         0|            0|            0|  0.00%|\n",
      "  2061|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Mish` for more details.\n",
      "  2062|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2063|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  2064|         0|            0|            0|  0.00%|        return handle_torch_function(mish, (input,), input, inplace=inplace)\n",
      "  2065|         0|            0|            0|  0.00%|    if inplace:\n",
      "  2066|         0|            0|            0|  0.00%|        return torch._C._nn.mish_(input)\n",
      "  2067|         0|            0|            0|  0.00%|    return torch._C._nn.mish(input)\n",
      "  2068|         0|            0|            0|  0.00%|\n",
      "  2069|         0|            0|            0|  0.00%|\n",
      "  2070|         0|            0|            0|  0.00%|def hardswish(input: Tensor, inplace: bool = False) -> Tensor:\n",
      "  2071|         0|            0|            0|  0.00%|    r\"\"\"Applies the hardswish function, element-wise, as described in the paper:\n",
      "  2072|         0|            0|            0|  0.00%|\n",
      "  2073|         0|            0|            0|  0.00%|    `Searching for MobileNetV3`_.\n",
      "  2074|         0|            0|            0|  0.00%|\n",
      "  2075|         0|            0|            0|  0.00%|    .. math::\n",
      "  2076|         0|            0|            0|  0.00%|        \\text{Hardswish}(x) = \\begin{cases}\n",
      "  2077|         0|            0|            0|  0.00%|            0 & \\text{if~} x \\le -3, \\\\\n",
      "  2078|         0|            0|            0|  0.00%|            x & \\text{if~} x \\ge +3, \\\\\n",
      "  2079|         0|            0|            0|  0.00%|            x \\cdot (x + 3) /6 & \\text{otherwise}\n",
      "  2080|         0|            0|            0|  0.00%|        \\end{cases}\n",
      "  2081|         0|            0|            0|  0.00%|\n",
      "  2082|         0|            0|            0|  0.00%|    See :class:`~torch.nn.Hardswish` for more details.\n",
      "  2083|         0|            0|            0|  0.00%|\n",
      "  2084|         0|            0|            0|  0.00%|    .. _`Searching for MobileNetV3`:\n",
      "  2085|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1905.02244\n",
      "  2086|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2087|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  2088|         0|            0|            0|  0.00%|        return handle_torch_function(hardswish, (input,), input, inplace=inplace)\n",
      "  2089|         0|            0|            0|  0.00%|    if inplace:\n",
      "  2090|         0|            0|            0|  0.00%|        return torch._C._nn.hardswish_(input)\n",
      "  2091|         0|            0|            0|  0.00%|    return torch._C._nn.hardswish(input)\n",
      "  2092|         0|            0|            0|  0.00%|\n",
      "  2093|         0|            0|            0|  0.00%|\n",
      "  2094|         0|            0|            0|  0.00%|def _no_grad_embedding_renorm_(weight: Tensor, input: Tensor, max_norm: float, norm_type: float) -> Tensor:\n",
      "  2095|         0|            0|            0|  0.00%|    with torch.no_grad():\n",
      "  2096|         0|            0|            0|  0.00%|        torch.embedding_renorm_(weight, input, max_norm, norm_type)\n",
      "  2097|         0|            0|            0|  0.00%|\n",
      "  2098|         0|            0|            0|  0.00%|\n",
      "  2099|         0|            0|            0|  0.00%|def embedding(\n",
      "  2100|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  2101|         0|            0|            0|  0.00%|    weight: Tensor,\n",
      "  2102|         0|            0|            0|  0.00%|    padding_idx: Optional[int] = None,\n",
      "  2103|         0|            0|            0|  0.00%|    max_norm: Optional[float] = None,\n",
      "  2104|         0|            0|            0|  0.00%|    norm_type: float = 2.0,\n",
      "  2105|         0|            0|            0|  0.00%|    scale_grad_by_freq: bool = False,\n",
      "  2106|         0|            0|            0|  0.00%|    sparse: bool = False,\n",
      "  2107|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  2108|         0|            0|            0|  0.00%|    r\"\"\"A simple lookup table that looks up embeddings in a fixed dictionary and size.\n",
      "  2109|         0|            0|            0|  0.00%|\n",
      "  2110|         0|            0|            0|  0.00%|    This module is often used to retrieve word embeddings using indices.\n",
      "  2111|         0|            0|            0|  0.00%|    The input to the module is a list of indices, and the embedding matrix,\n",
      "  2112|         0|            0|            0|  0.00%|    and the output is the corresponding word embeddings.\n",
      "  2113|         0|            0|            0|  0.00%|\n",
      "  2114|         0|            0|            0|  0.00%|    See :class:`torch.nn.Embedding` for more details.\n",
      "  2115|         0|            0|            0|  0.00%|\n",
      "  2116|         0|            0|            0|  0.00%|    Args:\n",
      "  2117|         0|            0|            0|  0.00%|        input (LongTensor): Tensor containing indices into the embedding matrix\n",
      "  2118|         0|            0|            0|  0.00%|        weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,\n",
      "  2119|         0|            0|            0|  0.00%|            and number of columns equal to the embedding size\n",
      "  2120|         0|            0|            0|  0.00%|        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n",
      "  2121|         0|            0|            0|  0.00%|                                     therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n",
      "  2122|         0|            0|            0|  0.00%|                                     i.e. it remains as a fixed \"pad\".\n",
      "  2123|         0|            0|            0|  0.00%|        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n",
      "  2124|         0|            0|            0|  0.00%|                                    is renormalized to have norm :attr:`max_norm`.\n",
      "  2125|         0|            0|            0|  0.00%|                                    Note: this will modify :attr:`weight` in-place.\n",
      "  2126|         0|            0|            0|  0.00%|        norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n",
      "  2127|         0|            0|            0|  0.00%|        scale_grad_by_freq (boolean, optional): If given, this will scale gradients by the inverse of frequency of\n",
      "  2128|         0|            0|            0|  0.00%|                                                the words in the mini-batch. Default ``False``.\n",
      "  2129|         0|            0|            0|  0.00%|        sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under\n",
      "  2130|         0|            0|            0|  0.00%|                                 :class:`torch.nn.Embedding` for more details regarding sparse gradients.\n",
      "  2131|         0|            0|            0|  0.00%|\n",
      "  2132|         0|            0|            0|  0.00%|    Shape:\n",
      "  2133|         0|            0|            0|  0.00%|        - Input: LongTensor of arbitrary shape containing the indices to extract\n",
      "  2134|         0|            0|            0|  0.00%|        - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,\n",
      "  2135|         0|            0|            0|  0.00%|          where V = maximum index + 1 and embedding_dim = the embedding size\n",
      "  2136|         0|            0|            0|  0.00%|        - Output: `(*, embedding_dim)`, where `*` is the input shape\n",
      "  2137|         0|            0|            0|  0.00%|\n",
      "  2138|         0|            0|            0|  0.00%|    Examples::\n",
      "  2139|         0|            0|            0|  0.00%|\n",
      "  2140|         0|            0|            0|  0.00%|        >>> # a batch of 2 samples of 4 indices each\n",
      "  2141|         0|            0|            0|  0.00%|        >>> input = torch.tensor([[1,2,4,5],[4,3,2,9]])\n",
      "  2142|         0|            0|            0|  0.00%|        >>> # an embedding matrix containing 10 tensors of size 3\n",
      "  2143|         0|            0|            0|  0.00%|        >>> embedding_matrix = torch.rand(10, 3)\n",
      "  2144|         0|            0|            0|  0.00%|        >>> F.embedding(input, embedding_matrix)\n",
      "  2145|         0|            0|            0|  0.00%|        tensor([[[ 0.8490,  0.9625,  0.6753],\n",
      "  2146|         0|            0|            0|  0.00%|                 [ 0.9666,  0.7761,  0.6108],\n",
      "  2147|         0|            0|            0|  0.00%|                 [ 0.6246,  0.9751,  0.3618],\n",
      "  2148|         0|            0|            0|  0.00%|                 [ 0.4161,  0.2419,  0.7383]],\n",
      "  2149|         0|            0|            0|  0.00%|\n",
      "  2150|         0|            0|            0|  0.00%|                [[ 0.6246,  0.9751,  0.3618],\n",
      "  2151|         0|            0|            0|  0.00%|                 [ 0.0237,  0.7794,  0.0528],\n",
      "  2152|         0|            0|            0|  0.00%|                 [ 0.9666,  0.7761,  0.6108],\n",
      "  2153|         0|            0|            0|  0.00%|                 [ 0.3385,  0.8612,  0.1867]]])\n",
      "  2154|         0|            0|            0|  0.00%|\n",
      "  2155|         0|            0|            0|  0.00%|        >>> # example with padding_idx\n",
      "  2156|         0|            0|            0|  0.00%|        >>> weights = torch.rand(10, 3)\n",
      "  2157|         0|            0|            0|  0.00%|        >>> weights[0, :].zero_()\n",
      "  2158|         0|            0|            0|  0.00%|        >>> embedding_matrix = weights\n",
      "  2159|         0|            0|            0|  0.00%|        >>> input = torch.tensor([[0,2,0,5]])\n",
      "  2160|         0|            0|            0|  0.00%|        >>> F.embedding(input, embedding_matrix, padding_idx=0)\n",
      "  2161|         0|            0|            0|  0.00%|        tensor([[[ 0.0000,  0.0000,  0.0000],\n",
      "  2162|         0|            0|            0|  0.00%|                 [ 0.5609,  0.5384,  0.8720],\n",
      "  2163|         0|            0|            0|  0.00%|                 [ 0.0000,  0.0000,  0.0000],\n",
      "  2164|         0|            0|            0|  0.00%|                 [ 0.6262,  0.2438,  0.7471]]])\n",
      "  2165|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2166|         0|            0|            0|  0.00%|\n",
      "  2167|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, weight):\n",
      "  2168|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  2169|         0|            0|            0|  0.00%|            embedding,\n",
      "  2170|         0|            0|            0|  0.00%|            (input, weight),\n",
      "  2171|         0|            0|            0|  0.00%|            input,\n",
      "  2172|         0|            0|            0|  0.00%|            weight,\n",
      "  2173|         0|            0|            0|  0.00%|            padding_idx=padding_idx,\n",
      "  2174|         0|            0|            0|  0.00%|            max_norm=max_norm,\n",
      "  2175|         0|            0|            0|  0.00%|            norm_type=norm_type,\n",
      "  2176|         0|            0|            0|  0.00%|            scale_grad_by_freq=scale_grad_by_freq,\n",
      "  2177|         0|            0|            0|  0.00%|            sparse=sparse,\n",
      "  2178|         0|            0|            0|  0.00%|        )\n",
      "  2179|         0|            0|            0|  0.00%|    if padding_idx is not None:\n",
      "  2180|         0|            0|            0|  0.00%|        if padding_idx > 0:\n",
      "  2181|         0|            0|            0|  0.00%|            assert padding_idx < weight.size(0), \"Padding_idx must be within num_embeddings\"\n",
      "  2182|         0|            0|            0|  0.00%|        elif padding_idx < 0:\n",
      "  2183|         0|            0|            0|  0.00%|            assert padding_idx >= -weight.size(0), \"Padding_idx must be within num_embeddings\"\n",
      "  2184|         0|            0|            0|  0.00%|            padding_idx = weight.size(0) + padding_idx\n",
      "  2185|         0|            0|            0|  0.00%|    else:\n",
      "  2186|         0|            0|            0|  0.00%|        padding_idx = -1\n",
      "  2187|         0|            0|            0|  0.00%|    if max_norm is not None:\n",
      "  2188|         0|            0|            0|  0.00%|        # Note [embedding_renorm contiguous]\n",
      "  2189|         0|            0|            0|  0.00%|        # `embedding_renorm_` will call .contiguous() on input anyways, so we\n",
      "  2190|         0|            0|            0|  0.00%|        # call it here and take advantage of the improved locality in the\n",
      "  2191|         0|            0|            0|  0.00%|        # `embedding` call below too.\n",
      "  2192|         0|            0|            0|  0.00%|        input = input.contiguous()\n",
      "  2193|         0|            0|            0|  0.00%|        # Note [embedding_renorm set_grad_enabled]\n",
      "  2194|         0|            0|            0|  0.00%|        # XXX: equivalent to\n",
      "  2195|         0|            0|            0|  0.00%|        # with torch.no_grad():\n",
      "  2196|         0|            0|            0|  0.00%|        #   torch.embedding_renorm_\n",
      "  2197|         0|            0|            0|  0.00%|        # remove once script supports set_grad_enabled\n",
      "  2198|         0|            0|            0|  0.00%|        _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n",
      "  2199|         0|            0|            0|  0.00%|    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "  2200|         0|            0|            0|  0.00%|\n",
      "  2201|         0|            0|            0|  0.00%|\n",
      "  2202|         0|            0|            0|  0.00%|def embedding_bag(\n",
      "  2203|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  2204|         0|            0|            0|  0.00%|    weight: Tensor,\n",
      "  2205|         0|            0|            0|  0.00%|    offsets: Optional[Tensor] = None,\n",
      "  2206|         0|            0|            0|  0.00%|    max_norm: Optional[float] = None,\n",
      "  2207|         0|            0|            0|  0.00%|    norm_type: float = 2,\n",
      "  2208|         0|            0|            0|  0.00%|    scale_grad_by_freq: bool = False,\n",
      "  2209|         0|            0|            0|  0.00%|    mode: str = \"mean\",\n",
      "  2210|         0|            0|            0|  0.00%|    sparse: bool = False,\n",
      "  2211|         0|            0|            0|  0.00%|    per_sample_weights: Optional[Tensor] = None,\n",
      "  2212|         0|            0|            0|  0.00%|    include_last_offset: bool = False,\n",
      "  2213|         0|            0|            0|  0.00%|    padding_idx: Optional[int] = None,\n",
      "  2214|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  2215|         0|            0|            0|  0.00%|    r\"\"\"Computes sums, means or maxes of `bags` of embeddings, without instantiating the\n",
      "  2216|         0|            0|            0|  0.00%|    intermediate embeddings.\n",
      "  2217|         0|            0|            0|  0.00%|\n",
      "  2218|         0|            0|            0|  0.00%|    See :class:`torch.nn.EmbeddingBag` for more details.\n",
      "  2219|         0|            0|            0|  0.00%|\n",
      "  2220|         0|            0|            0|  0.00%|    Note:\n",
      "  2221|         0|            0|            0|  0.00%|        {backward_reproducibility_note}\n",
      "  2222|         0|            0|            0|  0.00%|\n",
      "  2223|         0|            0|            0|  0.00%|    Args:\n",
      "  2224|         0|            0|            0|  0.00%|        input (LongTensor): Tensor containing bags of indices into the embedding matrix\n",
      "  2225|         0|            0|            0|  0.00%|        weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,\n",
      "  2226|         0|            0|            0|  0.00%|            and number of columns equal to the embedding size\n",
      "  2227|         0|            0|            0|  0.00%|        offsets (LongTensor, optional): Only used when :attr:`input` is 1D. :attr:`offsets` determines\n",
      "  2228|         0|            0|            0|  0.00%|                             the starting index position of each bag (sequence) in :attr:`input`.\n",
      "  2229|         0|            0|            0|  0.00%|        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n",
      "  2230|         0|            0|            0|  0.00%|                                    is renormalized to have norm :attr:`max_norm`.\n",
      "  2231|         0|            0|            0|  0.00%|                                    Note: this will modify :attr:`weight` in-place.\n",
      "  2232|         0|            0|            0|  0.00%|        norm_type (float, optional): The ``p`` in the ``p``-norm to compute for the :attr:`max_norm` option.\n",
      "  2233|         0|            0|            0|  0.00%|                                     Default ``2``.\n",
      "  2234|         0|            0|            0|  0.00%|        scale_grad_by_freq (boolean, optional): if given, this will scale gradients by the inverse of frequency of\n",
      "  2235|         0|            0|            0|  0.00%|                                                the words in the mini-batch. Default ``False``.\n",
      "  2236|         0|            0|            0|  0.00%|                                                Note: this option is not supported when ``mode=\"max\"``.\n",
      "  2237|         0|            0|            0|  0.00%|        mode (string, optional): ``\"sum\"``, ``\"mean\"`` or ``\"max\"``. Specifies the way to reduce the bag.\n",
      "  2238|         0|            0|            0|  0.00%|                                 Default: ``\"mean\"``\n",
      "  2239|         0|            0|            0|  0.00%|        sparse (bool, optional): if ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under\n",
      "  2240|         0|            0|            0|  0.00%|                                 :class:`torch.nn.Embedding` for more details regarding sparse gradients.\n",
      "  2241|         0|            0|            0|  0.00%|                                 Note: this option is not supported when ``mode=\"max\"``.\n",
      "  2242|         0|            0|            0|  0.00%|        per_sample_weights (Tensor, optional): a tensor of float / double weights, or None\n",
      "  2243|         0|            0|            0|  0.00%|            to indicate all weights should be taken to be 1. If specified, :attr:`per_sample_weights`\n",
      "  2244|         0|            0|            0|  0.00%|            must have exactly the same shape as input and is treated as having the same\n",
      "  2245|         0|            0|            0|  0.00%|            :attr:`offsets`, if those are not None.\n",
      "  2246|         0|            0|            0|  0.00%|\n",
      "  2247|         0|            0|            0|  0.00%|        include_last_offset (bool, optional): if ``True``, the size of offsets is equal to the number of bags + 1.\n",
      "  2248|         0|            0|            0|  0.00%|            The last element is the size of the input, or the ending index position of the last bag (sequence).\n",
      "  2249|         0|            0|            0|  0.00%|\n",
      "  2250|         0|            0|            0|  0.00%|        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the\n",
      "  2251|         0|            0|            0|  0.00%|                                     gradient; therefore, the embedding vector at :attr:`padding_idx` is not updated\n",
      "  2252|         0|            0|            0|  0.00%|                                     during training, i.e. it remains as a fixed \"pad\". Note that the embedding\n",
      "  2253|         0|            0|            0|  0.00%|                                     vector at :attr:`padding_idx` is excluded from the reduction.\n",
      "  2254|         0|            0|            0|  0.00%|\n",
      "  2255|         0|            0|            0|  0.00%|    Shape:\n",
      "  2256|         0|            0|            0|  0.00%|        - :attr:`input` (LongTensor) and :attr:`offsets` (LongTensor, optional)\n",
      "  2257|         0|            0|            0|  0.00%|\n",
      "  2258|         0|            0|            0|  0.00%|          - If :attr:`input` is 2D of shape `(B, N)`, it will be treated as ``B`` bags (sequences)\n",
      "  2259|         0|            0|            0|  0.00%|            each of fixed length ``N``, and this will return ``B`` values aggregated in a way\n",
      "  2260|         0|            0|            0|  0.00%|            depending on the :attr:`mode`. :attr:`offsets` is ignored and required to be ``None`` in this case.\n",
      "  2261|         0|            0|            0|  0.00%|\n",
      "  2262|         0|            0|            0|  0.00%|          - If :attr:`input` is 1D of shape `(N)`, it will be treated as a concatenation of\n",
      "  2263|         0|            0|            0|  0.00%|            multiple bags (sequences). :attr:`offsets` is required to be a 1D tensor containing\n",
      "  2264|         0|            0|            0|  0.00%|            the starting index positions of each bag in :attr:`input`. Therefore, for :attr:`offsets`\n",
      "  2265|         0|            0|            0|  0.00%|            of shape `(B)`, :attr:`input` will be viewed as having ``B`` bags.\n",
      "  2266|         0|            0|            0|  0.00%|            Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.\n",
      "  2267|         0|            0|            0|  0.00%|\n",
      "  2268|         0|            0|            0|  0.00%|        - :attr:`weight` (Tensor): the learnable weights of the module of shape `(num_embeddings, embedding_dim)`\n",
      "  2269|         0|            0|            0|  0.00%|\n",
      "  2270|         0|            0|            0|  0.00%|        - :attr:`per_sample_weights` (Tensor, optional). Has the same shape as :attr:`input`.\n",
      "  2271|         0|            0|            0|  0.00%|\n",
      "  2272|         0|            0|            0|  0.00%|        - :attr:`output`: aggregated embedding values of shape `(B, embedding_dim)`\n",
      "  2273|         0|            0|            0|  0.00%|\n",
      "  2274|         0|            0|            0|  0.00%|    Examples::\n",
      "  2275|         0|            0|            0|  0.00%|\n",
      "  2276|         0|            0|            0|  0.00%|        >>> # an Embedding module containing 10 tensors of size 3\n",
      "  2277|         0|            0|            0|  0.00%|        >>> embedding_matrix = torch.rand(10, 3)\n",
      "  2278|         0|            0|            0|  0.00%|        >>> # a batch of 2 samples of 4 indices each\n",
      "  2279|         0|            0|            0|  0.00%|        >>> input = torch.tensor([1,2,4,5,4,3,2,9])\n",
      "  2280|         0|            0|            0|  0.00%|        >>> offsets = torch.tensor([0,4])\n",
      "  2281|         0|            0|            0|  0.00%|        >>> F.embedding_bag(input, embedding_matrix, offsets)\n",
      "  2282|         0|            0|            0|  0.00%|        tensor([[ 0.3397,  0.3552,  0.5545],\n",
      "  2283|         0|            0|            0|  0.00%|                [ 0.5893,  0.4386,  0.5882]])\n",
      "  2284|         0|            0|            0|  0.00%|\n",
      "  2285|         0|            0|            0|  0.00%|        >>> # example with padding_idx\n",
      "  2286|         0|            0|            0|  0.00%|        >>> embedding_matrix = torch.rand(10, 3)\n",
      "  2287|         0|            0|            0|  0.00%|        >>> input = torch.tensor([2, 2, 2, 2, 4, 3, 2, 9])\n",
      "  2288|         0|            0|            0|  0.00%|        >>> offsets = torch.tensor([0,4])\n",
      "  2289|         0|            0|            0|  0.00%|        >>> F.embedding_bag(input, embedding_matrix, offsets, padding_idx=2, mode='sum')\n",
      "  2290|         0|            0|            0|  0.00%|        tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "  2291|         0|            0|            0|  0.00%|                [-0.7082,  3.2145, -2.6251]])\n",
      "  2292|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2293|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, weight, offsets, per_sample_weights):\n",
      "  2294|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  2295|         0|            0|            0|  0.00%|            embedding_bag,\n",
      "  2296|         0|            0|            0|  0.00%|            (input, weight, offsets, per_sample_weights),\n",
      "  2297|         0|            0|            0|  0.00%|            input,\n",
      "  2298|         0|            0|            0|  0.00%|            weight,\n",
      "  2299|         0|            0|            0|  0.00%|            offsets=offsets,\n",
      "  2300|         0|            0|            0|  0.00%|            max_norm=max_norm,\n",
      "  2301|         0|            0|            0|  0.00%|            norm_type=norm_type,\n",
      "  2302|         0|            0|            0|  0.00%|            scale_grad_by_freq=scale_grad_by_freq,\n",
      "  2303|         0|            0|            0|  0.00%|            mode=mode,\n",
      "  2304|         0|            0|            0|  0.00%|            sparse=sparse,\n",
      "  2305|         0|            0|            0|  0.00%|            per_sample_weights=per_sample_weights,\n",
      "  2306|         0|            0|            0|  0.00%|            include_last_offset=include_last_offset,\n",
      "  2307|         0|            0|            0|  0.00%|            padding_idx=padding_idx,\n",
      "  2308|         0|            0|            0|  0.00%|        )\n",
      "  2309|         0|            0|            0|  0.00%|    # Check for backward compatibility.\n",
      "  2310|         0|            0|            0|  0.00%|    # Used to be embedding_bag(weight, input, ...)\n",
      "  2311|         0|            0|            0|  0.00%|    # Now is     embedding_bag(input, weight, ...)\n",
      "  2312|         0|            0|            0|  0.00%|    if weight.dtype == torch.long and input.is_floating_point():\n",
      "  2313|         0|            0|            0|  0.00%|        warnings.warn(\n",
      "  2314|         0|            0|            0|  0.00%|            \"Argument order of nn.functional.embedding_bag was changed. \"\n",
      "  2315|         0|            0|            0|  0.00%|            \"Usage `embedding_bag(weight, input, ...)` is deprecated, \"\n",
      "  2316|         0|            0|            0|  0.00%|            \"and should now be `embedding_bag(input, weight, ...)`.\"\n",
      "  2317|         0|            0|            0|  0.00%|        )\n",
      "  2318|         0|            0|            0|  0.00%|        weight, input = input, weight\n",
      "  2319|         0|            0|            0|  0.00%|\n",
      "  2320|         0|            0|            0|  0.00%|    if per_sample_weights is not None and input.size() != per_sample_weights.size():\n",
      "  2321|         0|            0|            0|  0.00%|        raise ValueError(\n",
      "  2322|         0|            0|            0|  0.00%|            \"embedding_bag: If per_sample_weights ({}) is not None, \"\n",
      "  2323|         0|            0|            0|  0.00%|            \"then it must have the same shape as the input ({})\".format(per_sample_weights.shape, input.shape)\n",
      "  2324|         0|            0|            0|  0.00%|        )\n",
      "  2325|         0|            0|            0|  0.00%|\n",
      "  2326|         0|            0|            0|  0.00%|    if input.dim() == 2:\n",
      "  2327|         0|            0|            0|  0.00%|        if offsets is not None:\n",
      "  2328|         0|            0|            0|  0.00%|            type_str = \"<unknown>\"\n",
      "  2329|         0|            0|            0|  0.00%|            # TODO: Remove this once script supports type() calls\n",
      "  2330|         0|            0|            0|  0.00%|            if not torch.jit.is_scripting():\n",
      "  2331|         0|            0|            0|  0.00%|                type_str = str(type(offsets))\n",
      "  2332|         0|            0|            0|  0.00%|            raise ValueError(\n",
      "  2333|         0|            0|            0|  0.00%|                \"if input is 2D, then offsets has to be None\"\n",
      "  2334|         0|            0|            0|  0.00%|                \", as input is treated is a mini-batch of\"\n",
      "  2335|         0|            0|            0|  0.00%|                \" fixed length sequences. However, found \"\n",
      "  2336|         0|            0|            0|  0.00%|                \"offsets of type {}\".format(type_str)\n",
      "  2337|         0|            0|            0|  0.00%|            )\n",
      "  2338|         0|            0|            0|  0.00%|        offsets = torch.arange(0, input.numel(), input.size(1), dtype=input.dtype, device=input.device)\n",
      "  2339|         0|            0|            0|  0.00%|\n",
      "  2340|         0|            0|            0|  0.00%|        input = input.reshape(-1)\n",
      "  2341|         0|            0|            0|  0.00%|        if per_sample_weights is not None:\n",
      "  2342|         0|            0|            0|  0.00%|            per_sample_weights = per_sample_weights.reshape(-1)\n",
      "  2343|         0|            0|            0|  0.00%|    elif input.dim() == 1:\n",
      "  2344|         0|            0|            0|  0.00%|        if offsets is None:\n",
      "  2345|         0|            0|            0|  0.00%|            raise ValueError(\"offsets has to be a 1D Tensor but got None\")\n",
      "  2346|         0|            0|            0|  0.00%|        if offsets.dim() != 1:\n",
      "  2347|         0|            0|            0|  0.00%|            raise ValueError(\"offsets has to be a 1D Tensor\")\n",
      "  2348|         0|            0|            0|  0.00%|    else:\n",
      "  2349|         0|            0|            0|  0.00%|        raise ValueError(\"input has to be 1D or 2D Tensor,\" \" but got Tensor of dimension {}\".format(input.dim()))\n",
      "  2350|         0|            0|            0|  0.00%|    if mode == \"sum\":\n",
      "  2351|         0|            0|            0|  0.00%|        mode_enum = 0\n",
      "  2352|         0|            0|            0|  0.00%|    elif mode == \"mean\":\n",
      "  2353|         0|            0|            0|  0.00%|        mode_enum = 1\n",
      "  2354|         0|            0|            0|  0.00%|    elif mode == \"max\":\n",
      "  2355|         0|            0|            0|  0.00%|        mode_enum = 2\n",
      "  2356|         0|            0|            0|  0.00%|\n",
      "  2357|         0|            0|            0|  0.00%|        if scale_grad_by_freq:\n",
      "  2358|         0|            0|            0|  0.00%|            raise ValueError(\"max mode does not support scaling the gradient by the frequency\")\n",
      "  2359|         0|            0|            0|  0.00%|\n",
      "  2360|         0|            0|            0|  0.00%|        if sparse:\n",
      "  2361|         0|            0|            0|  0.00%|            raise ValueError(\"max mode does not support sparse weights\")\n",
      "  2362|         0|            0|            0|  0.00%|\n",
      "  2363|         0|            0|            0|  0.00%|    else:\n",
      "  2364|         0|            0|            0|  0.00%|        raise ValueError(\"mode has to be one of sum, mean or max\")\n",
      "  2365|         0|            0|            0|  0.00%|\n",
      "  2366|         0|            0|            0|  0.00%|    if max_norm is not None:\n",
      "  2367|         0|            0|            0|  0.00%|        # XXX: equivalent to\n",
      "  2368|         0|            0|            0|  0.00%|        # with torch.no_grad():\n",
      "  2369|         0|            0|            0|  0.00%|        #   torch.nembedding_renorm_\n",
      "  2370|         0|            0|            0|  0.00%|        # remove once script supports set_grad_enabled\n",
      "  2371|         0|            0|            0|  0.00%|        _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n",
      "  2372|         0|            0|            0|  0.00%|\n",
      "  2373|         0|            0|            0|  0.00%|    if per_sample_weights is not None and mode != \"sum\":\n",
      "  2374|         0|            0|            0|  0.00%|        raise NotImplementedError(\n",
      "  2375|         0|            0|            0|  0.00%|            \"embedding_bag: per_sample_weights was not None. \"\n",
      "  2376|         0|            0|            0|  0.00%|            \"per_sample_weights is only supported for mode='sum' \"\n",
      "  2377|         0|            0|            0|  0.00%|            \"(got mode='{}'). Please open a feature request on GitHub.\".format(mode)\n",
      "  2378|         0|            0|            0|  0.00%|        )\n",
      "  2379|         0|            0|            0|  0.00%|\n",
      "  2380|         0|            0|            0|  0.00%|    ret, _, _, _ = torch.embedding_bag(\n",
      "  2381|         0|            0|            0|  0.00%|        weight, input, offsets, scale_grad_by_freq, mode_enum, sparse, per_sample_weights, include_last_offset, padding_idx\n",
      "  2382|         0|            0|            0|  0.00%|    )\n",
      "  2383|         0|            0|            0|  0.00%|    return ret\n",
      "  2384|         0|            0|            0|  0.00%|\n",
      "  2385|         0|            0|            0|  0.00%|\n",
      "  2386|         0|            0|            0|  0.00%|if embedding_bag.__doc__:\n",
      "  2387|         0|            0|            0|  0.00%|    embedding_bag.__doc__ = embedding_bag.__doc__.format(**reproducibility_notes)\n",
      "  2388|         0|            0|            0|  0.00%|\n",
      "  2389|         0|            0|            0|  0.00%|\n",
      "  2390|         0|            0|            0|  0.00%|def _verify_batch_size(size: List[int]) -> None:\n",
      "  2391|         0|            0|            0|  0.00%|    # XXX: JIT script does not support the reduce from functools, and mul op is a\n",
      "  2392|         0|            0|            0|  0.00%|    # builtin, which cannot be used as a value to a func yet, so rewrite this size\n",
      "  2393|         0|            0|            0|  0.00%|    # check to a simple equivalent for loop\n",
      "  2394|         0|            0|            0|  0.00%|    #\n",
      "  2395|         0|            0|            0|  0.00%|    # TODO: make use of reduce like below when JIT is ready with the missing features:\n",
      "  2396|         0|            0|            0|  0.00%|    # from operator import mul\n",
      "  2397|         0|            0|            0|  0.00%|    # from functools import reduce\n",
      "  2398|         0|            0|            0|  0.00%|    #\n",
      "  2399|         0|            0|            0|  0.00%|    #   if reduce(mul, size[2:], size[0]) == 1\n",
      "  2400|         0|            0|            0|  0.00%|    size_prods = size[0]\n",
      "  2401|         0|            0|            0|  0.00%|    for i in range(len(size) - 2):\n",
      "  2402|         0|            0|            0|  0.00%|        size_prods *= size[i + 2]\n",
      "  2403|         0|            0|            0|  0.00%|    if size_prods == 1:\n",
      "  2404|         0|            0|            0|  0.00%|        raise ValueError(\"Expected more than 1 value per channel when training, got input size {}\".format(size))\n",
      "  2405|         0|            0|            0|  0.00%|\n",
      "  2406|         0|            0|            0|  0.00%|\n",
      "  2407|         0|            0|            0|  0.00%|def batch_norm(\n",
      "  2408|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  2409|         0|            0|            0|  0.00%|    running_mean: Optional[Tensor],\n",
      "  2410|         0|            0|            0|  0.00%|    running_var: Optional[Tensor],\n",
      "  2411|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,\n",
      "  2412|         0|            0|            0|  0.00%|    bias: Optional[Tensor] = None,\n",
      "  2413|         0|            0|            0|  0.00%|    training: bool = False,\n",
      "  2414|         0|            0|            0|  0.00%|    momentum: float = 0.1,\n",
      "  2415|         0|            0|            0|  0.00%|    eps: float = 1e-5,\n",
      "  2416|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  2417|         0|            0|            0|  0.00%|    r\"\"\"Applies Batch Normalization for each channel across a batch of data.\n",
      "  2418|         0|            0|            0|  0.00%|\n",
      "  2419|         0|            0|            0|  0.00%|    See :class:`~torch.nn.BatchNorm1d`, :class:`~torch.nn.BatchNorm2d`,\n",
      "  2420|         0|            0|            0|  0.00%|    :class:`~torch.nn.BatchNorm3d` for details.\n",
      "  2421|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2422|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, running_mean, running_var, weight, bias):\n",
      "  2423|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  2424|         0|            0|            0|  0.00%|            batch_norm,\n",
      "  2425|         0|            0|            0|  0.00%|            (input, running_mean, running_var, weight, bias),\n",
      "  2426|         0|            0|            0|  0.00%|            input,\n",
      "  2427|         0|            0|            0|  0.00%|            running_mean,\n",
      "  2428|         0|            0|            0|  0.00%|            running_var,\n",
      "  2429|         0|            0|            0|  0.00%|            weight=weight,\n",
      "  2430|         0|            0|            0|  0.00%|            bias=bias,\n",
      "  2431|         0|            0|            0|  0.00%|            training=training,\n",
      "  2432|         0|            0|            0|  0.00%|            momentum=momentum,\n",
      "  2433|         0|            0|            0|  0.00%|            eps=eps,\n",
      "  2434|         0|            0|            0|  0.00%|        )\n",
      "  2435|         0|            0|            0|  0.00%|    if training:\n",
      "  2436|         0|            0|            0|  0.00%|        _verify_batch_size(input.size())\n",
      "  2437|         0|            0|            0|  0.00%|\n",
      "  2438|         0|            0|            0|  0.00%|    return torch.batch_norm(\n",
      "  2439|         0|            0|            0|  0.00%|        input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled\n",
      "  2440|         0|            0|            0|  0.00%|    )\n",
      "  2441|         0|            0|            0|  0.00%|\n",
      "  2442|         0|            0|            0|  0.00%|\n",
      "  2443|         0|            0|            0|  0.00%|def _verify_spatial_size(size: List[int]) -> None:\n",
      "  2444|         0|            0|            0|  0.00%|    # Verify that there is > 1 spatial element for instance norm calculation.\n",
      "  2445|         0|            0|            0|  0.00%|    size_prods = 1\n",
      "  2446|         0|            0|            0|  0.00%|    for i in range(2, len(size)):\n",
      "  2447|         0|            0|            0|  0.00%|        size_prods *= size[i]\n",
      "  2448|         0|            0|            0|  0.00%|    if size_prods == 1:\n",
      "  2449|         0|            0|            0|  0.00%|        raise ValueError(\"Expected more than 1 spatial element when training, got input size {}\".format(size))\n",
      "  2450|         0|            0|            0|  0.00%|\n",
      "  2451|         0|            0|            0|  0.00%|\n",
      "  2452|         0|            0|            0|  0.00%|def instance_norm(\n",
      "  2453|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  2454|         0|            0|            0|  0.00%|    running_mean: Optional[Tensor] = None,\n",
      "  2455|         0|            0|            0|  0.00%|    running_var: Optional[Tensor] = None,\n",
      "  2456|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,\n",
      "  2457|         0|            0|            0|  0.00%|    bias: Optional[Tensor] = None,\n",
      "  2458|         0|            0|            0|  0.00%|    use_input_stats: bool = True,\n",
      "  2459|         0|            0|            0|  0.00%|    momentum: float = 0.1,\n",
      "  2460|         0|            0|            0|  0.00%|    eps: float = 1e-5,\n",
      "  2461|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  2462|         0|            0|            0|  0.00%|    r\"\"\"Applies Instance Normalization for each channel in each data sample in a\n",
      "  2463|         0|            0|            0|  0.00%|    batch.\n",
      "  2464|         0|            0|            0|  0.00%|\n",
      "  2465|         0|            0|            0|  0.00%|    See :class:`~torch.nn.InstanceNorm1d`, :class:`~torch.nn.InstanceNorm2d`,\n",
      "  2466|         0|            0|            0|  0.00%|    :class:`~torch.nn.InstanceNorm3d` for details.\n",
      "  2467|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2468|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, running_mean, running_var, weight, bias):\n",
      "  2469|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  2470|         0|            0|            0|  0.00%|            instance_norm,\n",
      "  2471|         0|            0|            0|  0.00%|            (input, running_mean, running_var, weight, bias),\n",
      "  2472|         0|            0|            0|  0.00%|            input,\n",
      "  2473|         0|            0|            0|  0.00%|            running_mean=running_mean,\n",
      "  2474|         0|            0|            0|  0.00%|            running_var=running_var,\n",
      "  2475|         0|            0|            0|  0.00%|            weight=weight,\n",
      "  2476|         0|            0|            0|  0.00%|            bias=bias,\n",
      "  2477|         0|            0|            0|  0.00%|            use_input_stats=use_input_stats,\n",
      "  2478|         0|            0|            0|  0.00%|            momentum=momentum,\n",
      "  2479|         0|            0|            0|  0.00%|            eps=eps,\n",
      "  2480|         0|            0|            0|  0.00%|        )\n",
      "  2481|         0|            0|            0|  0.00%|    if use_input_stats:\n",
      "  2482|         0|            0|            0|  0.00%|        _verify_spatial_size(input.size())\n",
      "  2483|         0|            0|            0|  0.00%|    return torch.instance_norm(\n",
      "  2484|         0|            0|            0|  0.00%|        input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, torch.backends.cudnn.enabled\n",
      "  2485|         0|            0|            0|  0.00%|    )\n",
      "  2486|         0|            0|            0|  0.00%|\n",
      "  2487|         0|            0|            0|  0.00%|\n",
      "  2488|         0|            0|            0|  0.00%|def layer_norm(\n",
      "  2489|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  2490|         0|            0|            0|  0.00%|    normalized_shape: List[int],\n",
      "  2491|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,\n",
      "  2492|         0|            0|            0|  0.00%|    bias: Optional[Tensor] = None,\n",
      "  2493|         0|            0|            0|  0.00%|    eps: float = 1e-5,\n",
      "  2494|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  2495|         0|            0|            0|  0.00%|    r\"\"\"Applies Layer Normalization for last certain number of dimensions.\n",
      "  2496|         0|            0|            0|  0.00%|\n",
      "  2497|         0|            0|            0|  0.00%|    See :class:`~torch.nn.LayerNorm` for details.\n",
      "  2498|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2499|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, weight, bias):\n",
      "  2500|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  2501|         0|            0|            0|  0.00%|            layer_norm, (input, weight, bias), input, normalized_shape, weight=weight, bias=bias, eps=eps\n",
      "  2502|         0|            0|            0|  0.00%|        )\n",
      "  2503|         0|            0|            0|  0.00%|    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n",
      "  2504|         0|            0|            0|  0.00%|\n",
      "  2505|         0|            0|            0|  0.00%|\n",
      "  2506|         0|            0|            0|  0.00%|def group_norm(\n",
      "  2507|         0|            0|            0|  0.00%|    input: Tensor, num_groups: int, weight: Optional[Tensor] = None, bias: Optional[Tensor] = None, eps: float = 1e-5\n",
      "  2508|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  2509|         0|            0|            0|  0.00%|    r\"\"\"Applies Group Normalization for last certain number of dimensions.\n",
      "  2510|         0|            0|            0|  0.00%|\n",
      "  2511|         0|            0|            0|  0.00%|    See :class:`~torch.nn.GroupNorm` for details.\n",
      "  2512|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2513|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, weight, bias):\n",
      "  2514|         0|            0|            0|  0.00%|        return handle_torch_function(group_norm, (input, weight, bias,), input, num_groups, weight=weight, bias=bias, eps=eps)\n",
      "  2515|         0|            0|            0|  0.00%|    _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))\n",
      "  2516|         0|            0|            0|  0.00%|    return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)\n",
      "  2517|         0|            0|            0|  0.00%|\n",
      "  2518|         0|            0|            0|  0.00%|\n",
      "  2519|         0|            0|            0|  0.00%|def local_response_norm(input: Tensor, size: int, alpha: float = 1e-4, beta: float = 0.75, k: float = 1.0) -> Tensor:\n",
      "  2520|         0|            0|            0|  0.00%|    r\"\"\"Applies local response normalization over an input signal composed of\n",
      "  2521|         0|            0|            0|  0.00%|    several input planes, where channels occupy the second dimension.\n",
      "  2522|         0|            0|            0|  0.00%|    Applies normalization across channels.\n",
      "  2523|         0|            0|            0|  0.00%|\n",
      "  2524|         0|            0|            0|  0.00%|    See :class:`~torch.nn.LocalResponseNorm` for details.\n",
      "  2525|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2526|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  2527|         0|            0|            0|  0.00%|        return handle_torch_function(local_response_norm, (input,), input, size, alpha=alpha, beta=beta, k=k)\n",
      "  2528|         0|            0|            0|  0.00%|    dim = input.dim()\n",
      "  2529|         0|            0|            0|  0.00%|    if dim < 3:\n",
      "  2530|         0|            0|            0|  0.00%|        raise ValueError(\n",
      "  2531|         0|            0|            0|  0.00%|            \"Expected 3D or higher dimensionality \\\n",
      "  2532|         0|            0|            0|  0.00%|                         input (got {} dimensions)\".format(\n",
      "  2533|         0|            0|            0|  0.00%|                dim\n",
      "  2534|         0|            0|            0|  0.00%|            )\n",
      "  2535|         0|            0|            0|  0.00%|        )\n",
      "  2536|         0|            0|            0|  0.00%|\n",
      "  2537|         0|            0|            0|  0.00%|    if input.numel() == 0:\n",
      "  2538|         0|            0|            0|  0.00%|        return input\n",
      "  2539|         0|            0|            0|  0.00%|\n",
      "  2540|         0|            0|            0|  0.00%|    div = input.mul(input).unsqueeze(1)\n",
      "  2541|         0|            0|            0|  0.00%|    if dim == 3:\n",
      "  2542|         0|            0|            0|  0.00%|        div = pad(div, (0, 0, size // 2, (size - 1) // 2))\n",
      "  2543|         0|            0|            0|  0.00%|        div = avg_pool2d(div, (size, 1), stride=1).squeeze(1)\n",
      "  2544|         0|            0|            0|  0.00%|    else:\n",
      "  2545|         0|            0|            0|  0.00%|        sizes = input.size()\n",
      "  2546|         0|            0|            0|  0.00%|        div = div.view(sizes[0], 1, sizes[1], sizes[2], -1)\n",
      "  2547|         0|            0|            0|  0.00%|        div = pad(div, (0, 0, 0, 0, size // 2, (size - 1) // 2))\n",
      "  2548|         0|            0|            0|  0.00%|        div = avg_pool3d(div, (size, 1, 1), stride=1).squeeze(1)\n",
      "  2549|         0|            0|            0|  0.00%|        div = div.view(sizes)\n",
      "  2550|         0|            0|            0|  0.00%|    div = div.mul(alpha).add(k).pow(beta)\n",
      "  2551|         0|            0|            0|  0.00%|    return input / div\n",
      "  2552|         0|            0|            0|  0.00%|\n",
      "  2553|         0|            0|            0|  0.00%|\n",
      "  2554|         0|            0|            0|  0.00%|# loss\n",
      "  2555|         0|            0|            0|  0.00%|\n",
      "  2556|         0|            0|            0|  0.00%|\n",
      "  2557|         0|            0|            0|  0.00%|def ctc_loss(\n",
      "  2558|         0|            0|            0|  0.00%|    log_probs: Tensor,\n",
      "  2559|         0|            0|            0|  0.00%|    targets: Tensor,\n",
      "  2560|         0|            0|            0|  0.00%|    input_lengths: Tensor,\n",
      "  2561|         0|            0|            0|  0.00%|    target_lengths: Tensor,\n",
      "  2562|         0|            0|            0|  0.00%|    blank: int = 0,\n",
      "  2563|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  2564|         0|            0|            0|  0.00%|    zero_infinity: bool = False,\n",
      "  2565|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  2566|         0|            0|            0|  0.00%|    r\"\"\"The Connectionist Temporal Classification loss.\n",
      "  2567|         0|            0|            0|  0.00%|\n",
      "  2568|         0|            0|            0|  0.00%|    See :class:`~torch.nn.CTCLoss` for details.\n",
      "  2569|         0|            0|            0|  0.00%|\n",
      "  2570|         0|            0|            0|  0.00%|    Note:\n",
      "  2571|         0|            0|            0|  0.00%|        {cudnn_reproducibility_note}\n",
      "  2572|         0|            0|            0|  0.00%|\n",
      "  2573|         0|            0|            0|  0.00%|    Note:\n",
      "  2574|         0|            0|            0|  0.00%|        {backward_reproducibility_note}\n",
      "  2575|         0|            0|            0|  0.00%|\n",
      "  2576|         0|            0|            0|  0.00%|    Args:\n",
      "  2577|         0|            0|            0|  0.00%|        log_probs: :math:`(T, N, C)` or :math:`(T, C)` where `C = number of characters in alphabet including blank`,\n",
      "  2578|         0|            0|            0|  0.00%|            `T = input length`, and `N = batch size`.\n",
      "  2579|         0|            0|            0|  0.00%|            The logarithmized probabilities of the outputs\n",
      "  2580|         0|            0|            0|  0.00%|            (e.g. obtained with :func:`torch.nn.functional.log_softmax`).\n",
      "  2581|         0|            0|            0|  0.00%|        targets: :math:`(N, S)` or `(sum(target_lengths))`.\n",
      "  2582|         0|            0|            0|  0.00%|            Targets cannot be blank. In the second form, the targets are assumed to be concatenated.\n",
      "  2583|         0|            0|            0|  0.00%|        input_lengths: :math:`(N)` or :math:`()`.\n",
      "  2584|         0|            0|            0|  0.00%|            Lengths of the inputs (must each be :math:`\\leq T`)\n",
      "  2585|         0|            0|            0|  0.00%|        target_lengths: :math:`(N)` or :math:`()`.\n",
      "  2586|         0|            0|            0|  0.00%|            Lengths of the targets\n",
      "  2587|         0|            0|            0|  0.00%|        blank (int, optional):\n",
      "  2588|         0|            0|            0|  0.00%|            Blank label. Default :math:`0`.\n",
      "  2589|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:\n",
      "  2590|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
      "  2591|         0|            0|            0|  0.00%|            ``'mean'``: the output losses will be divided by the target lengths and\n",
      "  2592|         0|            0|            0|  0.00%|            then the mean over the batch is taken, ``'sum'``: the output will be\n",
      "  2593|         0|            0|            0|  0.00%|            summed. Default: ``'mean'``\n",
      "  2594|         0|            0|            0|  0.00%|        zero_infinity (bool, optional):\n",
      "  2595|         0|            0|            0|  0.00%|            Whether to zero infinite losses and the associated gradients.\n",
      "  2596|         0|            0|            0|  0.00%|            Default: ``False``\n",
      "  2597|         0|            0|            0|  0.00%|            Infinite losses mainly occur when the inputs are too short\n",
      "  2598|         0|            0|            0|  0.00%|            to be aligned to the targets.\n",
      "  2599|         0|            0|            0|  0.00%|\n",
      "  2600|         0|            0|            0|  0.00%|    Example::\n",
      "  2601|         0|            0|            0|  0.00%|\n",
      "  2602|         0|            0|            0|  0.00%|        >>> log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\n",
      "  2603|         0|            0|            0|  0.00%|        >>> targets = torch.randint(1, 20, (16, 30), dtype=torch.long)\n",
      "  2604|         0|            0|            0|  0.00%|        >>> input_lengths = torch.full((16,), 50, dtype=torch.long)\n",
      "  2605|         0|            0|            0|  0.00%|        >>> target_lengths = torch.randint(10,30,(16,), dtype=torch.long)\n",
      "  2606|         0|            0|            0|  0.00%|        >>> loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
      "  2607|         0|            0|            0|  0.00%|        >>> loss.backward()\n",
      "  2608|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2609|         0|            0|            0|  0.00%|    if has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n",
      "  2610|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  2611|         0|            0|            0|  0.00%|            ctc_loss,\n",
      "  2612|         0|            0|            0|  0.00%|            (log_probs, targets, input_lengths, target_lengths),\n",
      "  2613|         0|            0|            0|  0.00%|            log_probs, targets, input_lengths, target_lengths,\n",
      "  2614|         0|            0|            0|  0.00%|            blank=blank, reduction=reduction, zero_infinity=zero_infinity\n",
      "  2615|         0|            0|            0|  0.00%|        )\n",
      "  2616|         0|            0|            0|  0.00%|    return torch.ctc_loss(\n",
      "  2617|         0|            0|            0|  0.00%|        log_probs, targets, input_lengths, target_lengths, blank, _Reduction.get_enum(reduction), zero_infinity\n",
      "  2618|         0|            0|            0|  0.00%|    )\n",
      "  2619|         0|            0|            0|  0.00%|\n",
      "  2620|         0|            0|            0|  0.00%|\n",
      "  2621|         0|            0|            0|  0.00%|if ctc_loss.__doc__:\n",
      "  2622|         0|            0|            0|  0.00%|    ctc_loss.__doc__ = ctc_loss.__doc__.format(**reproducibility_notes)\n",
      "  2623|         0|            0|            0|  0.00%|\n",
      "  2624|         0|            0|            0|  0.00%|\n",
      "  2625|         0|            0|            0|  0.00%|def nll_loss(\n",
      "  2626|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  2627|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  2628|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,\n",
      "  2629|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  2630|         0|            0|            0|  0.00%|    ignore_index: int = -100,\n",
      "  2631|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  2632|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  2633|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  2634|         0|            0|            0|  0.00%|    r\"\"\"The negative log likelihood loss.\n",
      "  2635|         0|            0|            0|  0.00%|\n",
      "  2636|         0|            0|            0|  0.00%|    See :class:`~torch.nn.NLLLoss` for details.\n",
      "  2637|         0|            0|            0|  0.00%|\n",
      "  2638|         0|            0|            0|  0.00%|    Args:\n",
      "  2639|         0|            0|            0|  0.00%|        input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`\n",
      "  2640|         0|            0|            0|  0.00%|            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \\geq 1`\n",
      "  2641|         0|            0|            0|  0.00%|            in the case of K-dimensional loss. `input` is expected to be log-probabilities.\n",
      "  2642|         0|            0|            0|  0.00%|        target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`,\n",
      "  2643|         0|            0|            0|  0.00%|            or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \\geq 1` for\n",
      "  2644|         0|            0|            0|  0.00%|            K-dimensional loss.\n",
      "  2645|         0|            0|            0|  0.00%|        weight (Tensor, optional): a manual rescaling weight given to each\n",
      "  2646|         0|            0|            0|  0.00%|            class. If given, has to be a Tensor of size `C`\n",
      "  2647|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "  2648|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for\n",
      "  2649|         0|            0|            0|  0.00%|            some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
      "  2650|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "  2651|         0|            0|            0|  0.00%|            when reduce is ``False``. Default: ``True``\n",
      "  2652|         0|            0|            0|  0.00%|        ignore_index (int, optional): Specifies a target value that is ignored\n",
      "  2653|         0|            0|            0|  0.00%|            and does not contribute to the input gradient. When :attr:`size_average` is\n",
      "  2654|         0|            0|            0|  0.00%|            ``True``, the loss is averaged over non-ignored targets. Default: -100\n",
      "  2655|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "  2656|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending\n",
      "  2657|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "  2658|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "  2659|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:\n",
      "  2660|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
      "  2661|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of\n",
      "  2662|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
      "  2663|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      "  2664|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
      "  2665|         0|            0|            0|  0.00%|\n",
      "  2666|         0|            0|            0|  0.00%|    Example::\n",
      "  2667|         0|            0|            0|  0.00%|\n",
      "  2668|         0|            0|            0|  0.00%|        >>> # input is of size N x C = 3 x 5\n",
      "  2669|         0|            0|            0|  0.00%|        >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "  2670|         0|            0|            0|  0.00%|        >>> # each element in target has to have 0 <= value < C\n",
      "  2671|         0|            0|            0|  0.00%|        >>> target = torch.tensor([1, 0, 4])\n",
      "  2672|         0|            0|            0|  0.00%|        >>> output = F.nll_loss(F.log_softmax(input), target)\n",
      "  2673|         0|            0|            0|  0.00%|        >>> output.backward()\n",
      "  2674|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2675|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target, weight):\n",
      "  2676|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  2677|         0|            0|            0|  0.00%|            nll_loss,\n",
      "  2678|         0|            0|            0|  0.00%|            (input, target, weight),\n",
      "  2679|         0|            0|            0|  0.00%|            input,\n",
      "  2680|         0|            0|            0|  0.00%|            target,\n",
      "  2681|         0|            0|            0|  0.00%|            weight=weight,\n",
      "  2682|         0|            0|            0|  0.00%|            size_average=size_average,\n",
      "  2683|         0|            0|            0|  0.00%|            ignore_index=ignore_index,\n",
      "  2684|         0|            0|            0|  0.00%|            reduce=reduce,\n",
      "  2685|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  2686|         0|            0|            0|  0.00%|        )\n",
      "  2687|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  2688|         0|            0|            0|  0.00%|        reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
      "  2689|         0|            0|            0|  0.00%|    return torch._C._nn.nll_loss_nd(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
      "  2690|         0|            0|            0|  0.00%|\n",
      "  2691|         0|            0|            0|  0.00%|\n",
      "  2692|         0|            0|            0|  0.00%|def poisson_nll_loss(\n",
      "  2693|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  2694|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  2695|         0|            0|            0|  0.00%|    log_input: bool = True,\n",
      "  2696|         0|            0|            0|  0.00%|    full: bool = False,\n",
      "  2697|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  2698|         0|            0|            0|  0.00%|    eps: float = 1e-8,\n",
      "  2699|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  2700|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  2701|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  2702|         0|            0|            0|  0.00%|    r\"\"\"Poisson negative log likelihood loss.\n",
      "  2703|         0|            0|            0|  0.00%|\n",
      "  2704|         0|            0|            0|  0.00%|    See :class:`~torch.nn.PoissonNLLLoss` for details.\n",
      "  2705|         0|            0|            0|  0.00%|\n",
      "  2706|         0|            0|            0|  0.00%|    Args:\n",
      "  2707|         0|            0|            0|  0.00%|        input: expectation of underlying Poisson distribution.\n",
      "  2708|         0|            0|            0|  0.00%|        target: random sample :math:`target \\sim \\text{Poisson}(input)`.\n",
      "  2709|         0|            0|            0|  0.00%|        log_input: if ``True`` the loss is computed as\n",
      "  2710|         0|            0|            0|  0.00%|            :math:`\\exp(\\text{input}) - \\text{target} * \\text{input}`, if ``False`` then loss is\n",
      "  2711|         0|            0|            0|  0.00%|            :math:`\\text{input} - \\text{target} * \\log(\\text{input}+\\text{eps})`. Default: ``True``\n",
      "  2712|         0|            0|            0|  0.00%|        full: whether to compute full loss, i. e. to add the Stirling\n",
      "  2713|         0|            0|            0|  0.00%|            approximation term. Default: ``False``\n",
      "  2714|         0|            0|            0|  0.00%|            :math:`\\text{target} * \\log(\\text{target}) - \\text{target} + 0.5 * \\log(2 * \\pi * \\text{target})`.\n",
      "  2715|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "  2716|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for\n",
      "  2717|         0|            0|            0|  0.00%|            some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
      "  2718|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "  2719|         0|            0|            0|  0.00%|            when reduce is ``False``. Default: ``True``\n",
      "  2720|         0|            0|            0|  0.00%|        eps (float, optional): Small value to avoid evaluation of :math:`\\log(0)` when\n",
      "  2721|         0|            0|            0|  0.00%|            :attr:`log_input`\\ =\\ ``False``. Default: 1e-8\n",
      "  2722|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "  2723|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending\n",
      "  2724|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "  2725|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "  2726|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:\n",
      "  2727|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
      "  2728|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of\n",
      "  2729|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
      "  2730|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      "  2731|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
      "  2732|         0|            0|            0|  0.00%|\n",
      "  2733|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2734|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):\n",
      "  2735|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  2736|         0|            0|            0|  0.00%|            poisson_nll_loss,\n",
      "  2737|         0|            0|            0|  0.00%|            (input, target),\n",
      "  2738|         0|            0|            0|  0.00%|            input,\n",
      "  2739|         0|            0|            0|  0.00%|            target,\n",
      "  2740|         0|            0|            0|  0.00%|            log_input=log_input,\n",
      "  2741|         0|            0|            0|  0.00%|            full=full,\n",
      "  2742|         0|            0|            0|  0.00%|            size_average=size_average,\n",
      "  2743|         0|            0|            0|  0.00%|            eps=eps,\n",
      "  2744|         0|            0|            0|  0.00%|            reduce=reduce,\n",
      "  2745|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  2746|         0|            0|            0|  0.00%|        )\n",
      "  2747|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  2748|         0|            0|            0|  0.00%|        reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
      "  2749|         0|            0|            0|  0.00%|    if reduction != \"none\" and reduction != \"mean\" and reduction != \"sum\":\n",
      "  2750|         0|            0|            0|  0.00%|        ret = input\n",
      "  2751|         0|            0|            0|  0.00%|        raise ValueError(reduction + \" is not valid\")\n",
      "  2752|         0|            0|            0|  0.00%|\n",
      "  2753|         0|            0|            0|  0.00%|    ret = torch.poisson_nll_loss(input, target, log_input, full, eps, _Reduction.get_enum(reduction))\n",
      "  2754|         0|            0|            0|  0.00%|    return ret\n",
      "  2755|         0|            0|            0|  0.00%|\n",
      "  2756|         0|            0|            0|  0.00%|\n",
      "  2757|         0|            0|            0|  0.00%|def gaussian_nll_loss(\n",
      "  2758|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  2759|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  2760|         0|            0|            0|  0.00%|    var: Tensor,\n",
      "  2761|         0|            0|            0|  0.00%|    full: bool = False,\n",
      "  2762|         0|            0|            0|  0.00%|    eps: float = 1e-6,\n",
      "  2763|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  2764|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  2765|         0|            0|            0|  0.00%|    r\"\"\"Gaussian negative log likelihood loss.\n",
      "  2766|         0|            0|            0|  0.00%|\n",
      "  2767|         0|            0|            0|  0.00%|    See :class:`~torch.nn.GaussianNLLLoss` for details.\n",
      "  2768|         0|            0|            0|  0.00%|\n",
      "  2769|         0|            0|            0|  0.00%|    Args:\n",
      "  2770|         0|            0|            0|  0.00%|        input: expectation of the Gaussian distribution.\n",
      "  2771|         0|            0|            0|  0.00%|        target: sample from the Gaussian distribution.\n",
      "  2772|         0|            0|            0|  0.00%|        var: tensor of positive variance(s), one for each of the expectations\n",
      "  2773|         0|            0|            0|  0.00%|            in the input (heteroscedastic), or a single one (homoscedastic).\n",
      "  2774|         0|            0|            0|  0.00%|        full (bool, optional): include the constant term in the loss calculation. Default: ``False``.\n",
      "  2775|         0|            0|            0|  0.00%|        eps (float, optional): value added to var, for stability. Default: 1e-6.\n",
      "  2776|         0|            0|            0|  0.00%|        reduction (string, optional): specifies the reduction to apply to the output:\n",
      "  2777|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
      "  2778|         0|            0|            0|  0.00%|            ``'mean'``: the output is the average of all batch member losses,\n",
      "  2779|         0|            0|            0|  0.00%|            ``'sum'``: the output is the sum of all batch member losses.\n",
      "  2780|         0|            0|            0|  0.00%|            Default: ``'mean'``.\n",
      "  2781|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2782|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target, var):\n",
      "  2783|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  2784|         0|            0|            0|  0.00%|            gaussian_nll_loss,\n",
      "  2785|         0|            0|            0|  0.00%|            (input, target, var),\n",
      "  2786|         0|            0|            0|  0.00%|            input,\n",
      "  2787|         0|            0|            0|  0.00%|            target,\n",
      "  2788|         0|            0|            0|  0.00%|            var,\n",
      "  2789|         0|            0|            0|  0.00%|            full=full,\n",
      "  2790|         0|            0|            0|  0.00%|            eps=eps,\n",
      "  2791|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  2792|         0|            0|            0|  0.00%|        )\n",
      "  2793|         0|            0|            0|  0.00%|\n",
      "  2794|         0|            0|            0|  0.00%|    # Check var size\n",
      "  2795|         0|            0|            0|  0.00%|    # If var.size == input.size, the case is heteroscedastic and no further checks are needed.\n",
      "  2796|         0|            0|            0|  0.00%|    # Otherwise:\n",
      "  2797|         0|            0|            0|  0.00%|    if var.size() != input.size():\n",
      "  2798|         0|            0|            0|  0.00%|\n",
      "  2799|         0|            0|            0|  0.00%|        # If var is one dimension short of input, but the sizes match otherwise, then this is a homoscedastic case.\n",
      "  2800|         0|            0|            0|  0.00%|        # e.g. input.size = (10, 2, 3), var.size = (10, 2)\n",
      "  2801|         0|            0|            0|  0.00%|        # -> unsqueeze var so that var.shape = (10, 2, 1)\n",
      "  2802|         0|            0|            0|  0.00%|        # this is done so that broadcasting can happen in the loss calculation\n",
      "  2803|         0|            0|            0|  0.00%|        if input.size()[:-1] == var.size():\n",
      "  2804|         0|            0|            0|  0.00%|            var = torch.unsqueeze(var, -1)\n",
      "  2805|         0|            0|            0|  0.00%|\n",
      "  2806|         0|            0|            0|  0.00%|        # This checks if the sizes match up to the final dimension, and the final dimension of var is of size 1.\n",
      "  2807|         0|            0|            0|  0.00%|        # This is also a homoscedastic case.\n",
      "  2808|         0|            0|            0|  0.00%|        # e.g. input.size = (10, 2, 3), var.size = (10, 2, 1)\n",
      "  2809|         0|            0|            0|  0.00%|        elif input.size()[:-1] == var.size()[:-1] and var.size(-1) == 1:  # Heteroscedastic case\n",
      "  2810|         0|            0|            0|  0.00%|            pass\n",
      "  2811|         0|            0|            0|  0.00%|\n",
      "  2812|         0|            0|            0|  0.00%|        # If none of the above pass, then the size of var is incorrect.\n",
      "  2813|         0|            0|            0|  0.00%|        else:\n",
      "  2814|         0|            0|            0|  0.00%|            raise ValueError(\"var is of incorrect size\")\n",
      "  2815|         0|            0|            0|  0.00%|\n",
      "  2816|         0|            0|            0|  0.00%|    # Check validity of reduction mode\n",
      "  2817|         0|            0|            0|  0.00%|    if reduction != 'none' and reduction != 'mean' and reduction != 'sum':\n",
      "  2818|         0|            0|            0|  0.00%|        raise ValueError(reduction + \" is not valid\")\n",
      "  2819|         0|            0|            0|  0.00%|\n",
      "  2820|         0|            0|            0|  0.00%|    # Entries of var must be non-negative\n",
      "  2821|         0|            0|            0|  0.00%|    if torch.any(var < 0):\n",
      "  2822|         0|            0|            0|  0.00%|        raise ValueError(\"var has negative entry/entries\")\n",
      "  2823|         0|            0|            0|  0.00%|\n",
      "  2824|         0|            0|            0|  0.00%|    # Clamp for stability\n",
      "  2825|         0|            0|            0|  0.00%|    var = var.clone()\n",
      "  2826|         0|            0|            0|  0.00%|    with torch.no_grad():\n",
      "  2827|         0|            0|            0|  0.00%|        var.clamp_(min=eps)\n",
      "  2828|         0|            0|            0|  0.00%|\n",
      "  2829|         0|            0|            0|  0.00%|    # Calculate the loss\n",
      "  2830|         0|            0|            0|  0.00%|    loss = 0.5 * (torch.log(var) + (input - target)**2 / var)\n",
      "  2831|         0|            0|            0|  0.00%|    if full:\n",
      "  2832|         0|            0|            0|  0.00%|        loss += 0.5 * math.log(2 * math.pi)\n",
      "  2833|         0|            0|            0|  0.00%|\n",
      "  2834|         0|            0|            0|  0.00%|    if reduction == 'mean':\n",
      "  2835|         0|            0|            0|  0.00%|        return loss.mean()\n",
      "  2836|         0|            0|            0|  0.00%|    elif reduction == 'sum':\n",
      "  2837|         0|            0|            0|  0.00%|        return loss.sum()\n",
      "  2838|         0|            0|            0|  0.00%|    else:\n",
      "  2839|         0|            0|            0|  0.00%|        return loss\n",
      "  2840|         0|            0|            0|  0.00%|\n",
      "  2841|         0|            0|            0|  0.00%|\n",
      "  2842|         0|            0|            0|  0.00%|def kl_div(\n",
      "  2843|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  2844|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  2845|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  2846|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  2847|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  2848|         0|            0|            0|  0.00%|    log_target: bool = False,\n",
      "  2849|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  2850|         0|            0|            0|  0.00%|    r\"\"\"The `Kullback-Leibler divergence Loss\n",
      "  2851|         0|            0|            0|  0.00%|    <https://en.wikipedia.org/wiki/Kullback-Leibler_divergence>`__\n",
      "  2852|         0|            0|            0|  0.00%|\n",
      "  2853|         0|            0|            0|  0.00%|    See :class:`~torch.nn.KLDivLoss` for details.\n",
      "  2854|         0|            0|            0|  0.00%|\n",
      "  2855|         0|            0|            0|  0.00%|    Args:\n",
      "  2856|         0|            0|            0|  0.00%|        input: Tensor of arbitrary shape in log-probabilities.\n",
      "  2857|         0|            0|            0|  0.00%|        target: Tensor of the same shape as input. See :attr:`log_target` for\n",
      "  2858|         0|            0|            0|  0.00%|            the target's interpretation.\n",
      "  2859|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "  2860|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for\n",
      "  2861|         0|            0|            0|  0.00%|            some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
      "  2862|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "  2863|         0|            0|            0|  0.00%|            when reduce is ``False``. Default: ``True``\n",
      "  2864|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "  2865|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending\n",
      "  2866|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "  2867|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "  2868|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:\n",
      "  2869|         0|            0|            0|  0.00%|            ``'none'`` | ``'batchmean'`` | ``'sum'`` | ``'mean'``.\n",
      "  2870|         0|            0|            0|  0.00%|            ``'none'``: no reduction will be applied\n",
      "  2871|         0|            0|            0|  0.00%|            ``'batchmean'``: the sum of the output will be divided by the batchsize\n",
      "  2872|         0|            0|            0|  0.00%|            ``'sum'``: the output will be summed\n",
      "  2873|         0|            0|            0|  0.00%|            ``'mean'``: the output will be divided by the number of elements in the output\n",
      "  2874|         0|            0|            0|  0.00%|            Default: ``'mean'``\n",
      "  2875|         0|            0|            0|  0.00%|        log_target (bool): A flag indicating whether ``target`` is passed in the log space.\n",
      "  2876|         0|            0|            0|  0.00%|            It is recommended to pass certain distributions (like ``softmax``)\n",
      "  2877|         0|            0|            0|  0.00%|            in the log space to avoid numerical issues caused by explicit ``log``.\n",
      "  2878|         0|            0|            0|  0.00%|            Default: ``False``\n",
      "  2879|         0|            0|            0|  0.00%|\n",
      "  2880|         0|            0|            0|  0.00%|    .. note::\n",
      "  2881|         0|            0|            0|  0.00%|        :attr:`size_average` and :attr:`reduce` are in the process of being deprecated,\n",
      "  2882|         0|            0|            0|  0.00%|        and in the meantime, specifying either of those two args will override :attr:`reduction`.\n",
      "  2883|         0|            0|            0|  0.00%|\n",
      "  2884|         0|            0|            0|  0.00%|    .. note::\n",
      "  2885|         0|            0|            0|  0.00%|        :attr:`reduction` = ``'mean'`` doesn't return the true kl divergence value, please use\n",
      "  2886|         0|            0|            0|  0.00%|        :attr:`reduction` = ``'batchmean'`` which aligns with KL math definition.\n",
      "  2887|         0|            0|            0|  0.00%|        In the next major release, ``'mean'`` will be changed to be the same as 'batchmean'.\n",
      "  2888|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2889|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):\n",
      "  2890|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  2891|         0|            0|            0|  0.00%|            kl_div,\n",
      "  2892|         0|            0|            0|  0.00%|            (input, target),\n",
      "  2893|         0|            0|            0|  0.00%|            input,\n",
      "  2894|         0|            0|            0|  0.00%|            target,\n",
      "  2895|         0|            0|            0|  0.00%|            size_average=size_average,\n",
      "  2896|         0|            0|            0|  0.00%|            reduce=reduce,\n",
      "  2897|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  2898|         0|            0|            0|  0.00%|            log_target=log_target,\n",
      "  2899|         0|            0|            0|  0.00%|        )\n",
      "  2900|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  2901|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n",
      "  2902|         0|            0|            0|  0.00%|    else:\n",
      "  2903|         0|            0|            0|  0.00%|        if reduction == \"mean\":\n",
      "  2904|         0|            0|            0|  0.00%|            warnings.warn(\n",
      "  2905|         0|            0|            0|  0.00%|                \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "  2906|         0|            0|            0|  0.00%|                \"'batchmean' divides only by the batch size, and aligns with the KL div math definition.\"\n",
      "  2907|         0|            0|            0|  0.00%|                \"'mean' will be changed to behave the same as 'batchmean' in the next major release.\"\n",
      "  2908|         0|            0|            0|  0.00%|            )\n",
      "  2909|         0|            0|            0|  0.00%|\n",
      "  2910|         0|            0|            0|  0.00%|        # special case for batchmean\n",
      "  2911|         0|            0|            0|  0.00%|        if reduction == \"batchmean\":\n",
      "  2912|         0|            0|            0|  0.00%|            reduction_enum = _Reduction.get_enum(\"sum\")\n",
      "  2913|         0|            0|            0|  0.00%|        else:\n",
      "  2914|         0|            0|            0|  0.00%|            reduction_enum = _Reduction.get_enum(reduction)\n",
      "  2915|         0|            0|            0|  0.00%|\n",
      "  2916|         0|            0|            0|  0.00%|    reduced = torch.kl_div(input, target, reduction_enum, log_target=log_target)\n",
      "  2917|         0|            0|            0|  0.00%|\n",
      "  2918|         0|            0|            0|  0.00%|    if reduction == \"batchmean\" and input.dim() != 0:\n",
      "  2919|         0|            0|            0|  0.00%|        reduced = reduced / input.size()[0]\n",
      "  2920|         0|            0|            0|  0.00%|\n",
      "  2921|         0|            0|            0|  0.00%|    return reduced\n",
      "  2922|         0|            0|            0|  0.00%|\n",
      "  2923|         0|            0|            0|  0.00%|\n",
      "  2924|         0|            0|            0|  0.00%|def cross_entropy(\n",
      "  2925|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  2926|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  2927|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,\n",
      "  2928|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  2929|         0|            0|            0|  0.00%|    ignore_index: int = -100,\n",
      "  2930|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  2931|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  2932|         0|            0|            0|  0.00%|    label_smoothing: float = 0.0,\n",
      "  2933|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  2934|         0|            0|            0|  0.00%|    r\"\"\"This criterion computes the cross entropy loss between input and target.\n",
      "  2935|         0|            0|            0|  0.00%|\n",
      "  2936|         0|            0|            0|  0.00%|    See :class:`~torch.nn.CrossEntropyLoss` for details.\n",
      "  2937|         0|            0|            0|  0.00%|\n",
      "  2938|         0|            0|            0|  0.00%|    Args:\n",
      "  2939|         0|            0|            0|  0.00%|        input (Tensor) : Predicted unnormalized scores (often referred to as logits);\n",
      "  2940|         0|            0|            0|  0.00%|            see Shape section below for supported shapes.\n",
      "  2941|         0|            0|            0|  0.00%|        target (Tensor) : Ground truth class indices or class probabilities;\n",
      "  2942|         0|            0|            0|  0.00%|            see Shape section below for supported shapes.\n",
      "  2943|         0|            0|            0|  0.00%|        weight (Tensor, optional): a manual rescaling weight given to each\n",
      "  2944|         0|            0|            0|  0.00%|            class. If given, has to be a Tensor of size `C`\n",
      "  2945|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "  2946|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for\n",
      "  2947|         0|            0|            0|  0.00%|            some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
      "  2948|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "  2949|         0|            0|            0|  0.00%|            when reduce is ``False``. Default: ``True``\n",
      "  2950|         0|            0|            0|  0.00%|        ignore_index (int, optional): Specifies a target value that is ignored\n",
      "  2951|         0|            0|            0|  0.00%|            and does not contribute to the input gradient. When :attr:`size_average` is\n",
      "  2952|         0|            0|            0|  0.00%|            ``True``, the loss is averaged over non-ignored targets. Note that\n",
      "  2953|         0|            0|            0|  0.00%|            :attr:`ignore_index` is only applicable when the target contains class indices.\n",
      "  2954|         0|            0|            0|  0.00%|            Default: -100\n",
      "  2955|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "  2956|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending\n",
      "  2957|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "  2958|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "  2959|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:\n",
      "  2960|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
      "  2961|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of\n",
      "  2962|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
      "  2963|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      "  2964|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
      "  2965|         0|            0|            0|  0.00%|        label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n",
      "  2966|         0|            0|            0|  0.00%|            of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n",
      "  2967|         0|            0|            0|  0.00%|            become a mixture of the original ground truth and a uniform distribution as described in\n",
      "  2968|         0|            0|            0|  0.00%|            `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n",
      "  2969|         0|            0|            0|  0.00%|\n",
      "  2970|         0|            0|            0|  0.00%|    Shape:\n",
      "  2971|         0|            0|            0|  0.00%|        - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
      "  2972|         0|            0|            0|  0.00%|          in the case of `K`-dimensional loss.\n",
      "  2973|         0|            0|            0|  0.00%|        - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n",
      "  2974|         0|            0|            0|  0.00%|          :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\n",
      "  2975|         0|            0|            0|  0.00%|          If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\n",
      "  2976|         0|            0|            0|  0.00%|\n",
      "  2977|         0|            0|            0|  0.00%|        where:\n",
      "  2978|         0|            0|            0|  0.00%|\n",
      "  2979|         0|            0|            0|  0.00%|        .. math::\n",
      "  2980|         0|            0|            0|  0.00%|            \\begin{aligned}\n",
      "  2981|         0|            0|            0|  0.00%|                C ={} & \\text{number of classes} \\\\\n",
      "  2982|         0|            0|            0|  0.00%|                N ={} & \\text{batch size} \\\\\n",
      "  2983|         0|            0|            0|  0.00%|            \\end{aligned}\n",
      "  2984|         0|            0|            0|  0.00%|\n",
      "  2985|         0|            0|            0|  0.00%|    Examples::\n",
      "  2986|         0|            0|            0|  0.00%|\n",
      "  2987|         0|            0|            0|  0.00%|        >>> # Example of target with class indices\n",
      "  2988|         0|            0|            0|  0.00%|        >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "  2989|         0|            0|            0|  0.00%|        >>> target = torch.randint(5, (3,), dtype=torch.int64)\n",
      "  2990|         0|            0|            0|  0.00%|        >>> loss = F.cross_entropy(input, target)\n",
      "  2991|         0|            0|            0|  0.00%|        >>> loss.backward()\n",
      "  2992|         0|            0|            0|  0.00%|        >>>\n",
      "  2993|         0|            0|            0|  0.00%|        >>> # Example of target with class probabilities\n",
      "  2994|         0|            0|            0|  0.00%|        >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "  2995|         0|            0|            0|  0.00%|        >>> target = torch.randn(3, 5).softmax(dim=1)\n",
      "  2996|         0|            0|            0|  0.00%|        >>> loss = F.cross_entropy(input, target)\n",
      "  2997|         0|            0|            0|  0.00%|        >>> loss.backward()\n",
      "  2998|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  2999|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target, weight):\n",
      "  3000|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3001|         0|            0|            0|  0.00%|            cross_entropy,\n",
      "  3002|         0|            0|            0|  0.00%|            (input, target, weight),\n",
      "  3003|         0|            0|            0|  0.00%|            input,\n",
      "  3004|         0|            0|            0|  0.00%|            target,\n",
      "  3005|         0|            0|            0|  0.00%|            weight=weight,\n",
      "  3006|         0|            0|            0|  0.00%|            size_average=size_average,\n",
      "  3007|         0|            0|            0|  0.00%|            ignore_index=ignore_index,\n",
      "  3008|         0|            0|            0|  0.00%|            reduce=reduce,\n",
      "  3009|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  3010|         0|            0|            0|  0.00%|            label_smoothing=label_smoothing,\n",
      "  3011|         0|            0|            0|  0.00%|        )\n",
      "  3012|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  3013|         0|            0|            0|  0.00%|        reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
      "  3014|         0|            0|            0|  0.00%|    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      "  3015|         0|            0|            0|  0.00%|\n",
      "  3016|         0|            0|            0|  0.00%|\n",
      "  3017|         0|            0|            0|  0.00%|def binary_cross_entropy(\n",
      "  3018|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  3019|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  3020|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,\n",
      "  3021|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  3022|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  3023|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  3024|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  3025|         0|            0|            0|  0.00%|    r\"\"\"Function that measures the Binary Cross Entropy between the target and input\n",
      "  3026|         0|            0|            0|  0.00%|    probabilities.\n",
      "  3027|         0|            0|            0|  0.00%|\n",
      "  3028|         0|            0|            0|  0.00%|    See :class:`~torch.nn.BCELoss` for details.\n",
      "  3029|         0|            0|            0|  0.00%|\n",
      "  3030|         0|            0|            0|  0.00%|    Args:\n",
      "  3031|         0|            0|            0|  0.00%|        input: Tensor of arbitrary shape as probabilities.\n",
      "  3032|         0|            0|            0|  0.00%|        target: Tensor of the same shape as input with values between 0 and 1.\n",
      "  3033|         0|            0|            0|  0.00%|        weight (Tensor, optional): a manual rescaling weight\n",
      "  3034|         0|            0|            0|  0.00%|                if provided it's repeated to match input tensor shape\n",
      "  3035|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "  3036|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for\n",
      "  3037|         0|            0|            0|  0.00%|            some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
      "  3038|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "  3039|         0|            0|            0|  0.00%|            when reduce is ``False``. Default: ``True``\n",
      "  3040|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "  3041|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending\n",
      "  3042|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "  3043|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "  3044|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:\n",
      "  3045|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
      "  3046|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of\n",
      "  3047|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
      "  3048|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      "  3049|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
      "  3050|         0|            0|            0|  0.00%|\n",
      "  3051|         0|            0|            0|  0.00%|    Examples::\n",
      "  3052|         0|            0|            0|  0.00%|\n",
      "  3053|         0|            0|            0|  0.00%|        >>> input = torch.randn(3, 2, requires_grad=True)\n",
      "  3054|         0|            0|            0|  0.00%|        >>> target = torch.rand(3, 2, requires_grad=False)\n",
      "  3055|         0|            0|            0|  0.00%|        >>> loss = F.binary_cross_entropy(torch.sigmoid(input), target)\n",
      "  3056|         0|            0|            0|  0.00%|        >>> loss.backward()\n",
      "  3057|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3058|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target, weight):\n",
      "  3059|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3060|         0|            0|            0|  0.00%|            binary_cross_entropy,\n",
      "  3061|         0|            0|            0|  0.00%|            (input, target, weight),\n",
      "  3062|         0|            0|            0|  0.00%|            input,\n",
      "  3063|         0|            0|            0|  0.00%|            target,\n",
      "  3064|         0|            0|            0|  0.00%|            weight=weight,\n",
      "  3065|         0|            0|            0|  0.00%|            size_average=size_average,\n",
      "  3066|         0|            0|            0|  0.00%|            reduce=reduce,\n",
      "  3067|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  3068|         0|            0|            0|  0.00%|        )\n",
      "  3069|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  3070|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n",
      "  3071|         0|            0|            0|  0.00%|    else:\n",
      "  3072|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)\n",
      "  3073|         0|            0|            0|  0.00%|    if target.size() != input.size():\n",
      "  3074|         0|            0|            0|  0.00%|        raise ValueError(\n",
      "  3075|         0|            0|            0|  0.00%|            \"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\n",
      "  3076|         0|            0|            0|  0.00%|            \"Please ensure they have the same size.\".format(target.size(), input.size())\n",
      "  3077|         0|            0|            0|  0.00%|        )\n",
      "  3078|         0|            0|            0|  0.00%|\n",
      "  3079|         0|            0|            0|  0.00%|    if weight is not None:\n",
      "  3080|         0|            0|            0|  0.00%|        new_size = _infer_size(target.size(), weight.size())\n",
      "  3081|         0|            0|            0|  0.00%|        weight = weight.expand(new_size)\n",
      "  3082|         0|            0|            0|  0.00%|\n",
      "  3083|         0|            0|            0|  0.00%|    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)\n",
      "  3084|         0|            0|            0|  0.00%|\n",
      "  3085|         0|            0|            0|  0.00%|\n",
      "  3086|         0|            0|            0|  0.00%|def binary_cross_entropy_with_logits(\n",
      "  3087|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  3088|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  3089|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,\n",
      "  3090|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  3091|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  3092|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  3093|         0|            0|            0|  0.00%|    pos_weight: Optional[Tensor] = None,\n",
      "  3094|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  3095|         0|            0|            0|  0.00%|    r\"\"\"Function that measures Binary Cross Entropy between target and input\n",
      "  3096|         0|            0|            0|  0.00%|    logits.\n",
      "  3097|         0|            0|            0|  0.00%|\n",
      "  3098|         0|            0|            0|  0.00%|    See :class:`~torch.nn.BCEWithLogitsLoss` for details.\n",
      "  3099|         0|            0|            0|  0.00%|\n",
      "  3100|         0|            0|            0|  0.00%|    Args:\n",
      "  3101|         0|            0|            0|  0.00%|        input: Tensor of arbitrary shape as unnormalized scores (often referred to as logits).\n",
      "  3102|         0|            0|            0|  0.00%|        target: Tensor of the same shape as input with values between 0 and 1\n",
      "  3103|         0|            0|            0|  0.00%|        weight (Tensor, optional): a manual rescaling weight\n",
      "  3104|         0|            0|            0|  0.00%|            if provided it's repeated to match input tensor shape\n",
      "  3105|         0|            0|            0|  0.00%|        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "  3106|         0|            0|            0|  0.00%|            the losses are averaged over each loss element in the batch. Note that for\n",
      "  3107|         0|            0|            0|  0.00%|            some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
      "  3108|         0|            0|            0|  0.00%|            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "  3109|         0|            0|            0|  0.00%|            when reduce is ``False``. Default: ``True``\n",
      "  3110|         0|            0|            0|  0.00%|        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "  3111|         0|            0|            0|  0.00%|            losses are averaged or summed over observations for each minibatch depending\n",
      "  3112|         0|            0|            0|  0.00%|            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "  3113|         0|            0|            0|  0.00%|            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "  3114|         0|            0|            0|  0.00%|        reduction (string, optional): Specifies the reduction to apply to the output:\n",
      "  3115|         0|            0|            0|  0.00%|            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
      "  3116|         0|            0|            0|  0.00%|            ``'mean'``: the sum of the output will be divided by the number of\n",
      "  3117|         0|            0|            0|  0.00%|            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
      "  3118|         0|            0|            0|  0.00%|            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      "  3119|         0|            0|            0|  0.00%|            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
      "  3120|         0|            0|            0|  0.00%|        pos_weight (Tensor, optional): a weight of positive examples.\n",
      "  3121|         0|            0|            0|  0.00%|                Must be a vector with length equal to the number of classes.\n",
      "  3122|         0|            0|            0|  0.00%|\n",
      "  3123|         0|            0|            0|  0.00%|    Examples::\n",
      "  3124|         0|            0|            0|  0.00%|\n",
      "  3125|         0|            0|            0|  0.00%|         >>> input = torch.randn(3, requires_grad=True)\n",
      "  3126|         0|            0|            0|  0.00%|         >>> target = torch.empty(3).random_(2)\n",
      "  3127|         0|            0|            0|  0.00%|         >>> loss = F.binary_cross_entropy_with_logits(input, target)\n",
      "  3128|         0|            0|            0|  0.00%|         >>> loss.backward()\n",
      "  3129|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3130|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target, weight, pos_weight):\n",
      "  3131|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3132|         0|            0|            0|  0.00%|            binary_cross_entropy_with_logits,\n",
      "  3133|         0|            0|            0|  0.00%|            (input, target, weight, pos_weight),\n",
      "  3134|         0|            0|            0|  0.00%|            input,\n",
      "  3135|         0|            0|            0|  0.00%|            target,\n",
      "  3136|         0|            0|            0|  0.00%|            weight=weight,\n",
      "  3137|         0|            0|            0|  0.00%|            size_average=size_average,\n",
      "  3138|         0|            0|            0|  0.00%|            reduce=reduce,\n",
      "  3139|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  3140|         0|            0|            0|  0.00%|            pos_weight=pos_weight,\n",
      "  3141|         0|            0|            0|  0.00%|        )\n",
      "  3142|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  3143|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n",
      "  3144|         0|            0|            0|  0.00%|    else:\n",
      "  3145|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)\n",
      "  3146|         0|            0|            0|  0.00%|\n",
      "  3147|         0|            0|            0|  0.00%|    if not (target.size() == input.size()):\n",
      "  3148|         0|            0|            0|  0.00%|        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
      "  3149|         0|            0|            0|  0.00%|\n",
      "  3150|         0|            0|            0|  0.00%|    return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)\n",
      "  3151|         0|            0|            0|  0.00%|\n",
      "  3152|         0|            0|            0|  0.00%|\n",
      "  3153|         0|            0|            0|  0.00%|def smooth_l1_loss(\n",
      "  3154|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  3155|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  3156|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  3157|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  3158|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  3159|         0|            0|            0|  0.00%|    beta: float = 1.0,\n",
      "  3160|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  3161|         0|            0|            0|  0.00%|    r\"\"\"Function that uses a squared term if the absolute\n",
      "  3162|         0|            0|            0|  0.00%|    element-wise error falls below beta and an L1 term otherwise.\n",
      "  3163|         0|            0|            0|  0.00%|\n",
      "  3164|         0|            0|            0|  0.00%|    See :class:`~torch.nn.SmoothL1Loss` for details.\n",
      "  3165|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3166|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):\n",
      "  3167|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3168|         0|            0|            0|  0.00%|            smooth_l1_loss,\n",
      "  3169|         0|            0|            0|  0.00%|            (input, target),\n",
      "  3170|         0|            0|            0|  0.00%|            input,\n",
      "  3171|         0|            0|            0|  0.00%|            target,\n",
      "  3172|         0|            0|            0|  0.00%|            size_average=size_average,\n",
      "  3173|         0|            0|            0|  0.00%|            reduce=reduce,\n",
      "  3174|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  3175|         0|            0|            0|  0.00%|            beta=beta,\n",
      "  3176|         0|            0|            0|  0.00%|        )\n",
      "  3177|         0|            0|            0|  0.00%|    if not (target.size() == input.size()):\n",
      "  3178|         0|            0|            0|  0.00%|        warnings.warn(\n",
      "  3179|         0|            0|            0|  0.00%|            \"Using a target size ({}) that is different to the input size ({}). \"\n",
      "  3180|         0|            0|            0|  0.00%|            \"This will likely lead to incorrect results due to broadcasting. \"\n",
      "  3181|         0|            0|            0|  0.00%|            \"Please ensure they have the same size.\".format(target.size(), input.size()),\n",
      "  3182|         0|            0|            0|  0.00%|            stacklevel=2,\n",
      "  3183|         0|            0|            0|  0.00%|        )\n",
      "  3184|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  3185|         0|            0|            0|  0.00%|        reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
      "  3186|         0|            0|            0|  0.00%|\n",
      "  3187|         0|            0|            0|  0.00%|    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n",
      "  3188|         0|            0|            0|  0.00%|    return torch._C._nn.smooth_l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction), beta)\n",
      "  3189|         0|            0|            0|  0.00%|\n",
      "  3190|         0|            0|            0|  0.00%|\n",
      "  3191|         0|            0|            0|  0.00%|def huber_loss(\n",
      "  3192|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  3193|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  3194|         0|            0|            0|  0.00%|    reduction: str = 'mean',\n",
      "  3195|         0|            0|            0|  0.00%|    delta: float = 1.0,\n",
      "  3196|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  3197|         0|            0|            0|  0.00%|    r\"\"\"Function that uses a squared term if the absolute\n",
      "  3198|         0|            0|            0|  0.00%|    element-wise error falls below delta and a delta-scaled L1 term otherwise.\n",
      "  3199|         0|            0|            0|  0.00%|\n",
      "  3200|         0|            0|            0|  0.00%|    See :class:`~torch.nn.HuberLoss` for details.\n",
      "  3201|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3202|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):\n",
      "  3203|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3204|         0|            0|            0|  0.00%|            huber_loss,\n",
      "  3205|         0|            0|            0|  0.00%|            (input, target),\n",
      "  3206|         0|            0|            0|  0.00%|            input,\n",
      "  3207|         0|            0|            0|  0.00%|            target,\n",
      "  3208|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  3209|         0|            0|            0|  0.00%|            delta=delta,\n",
      "  3210|         0|            0|            0|  0.00%|        )\n",
      "  3211|         0|            0|            0|  0.00%|    if not (target.size() == input.size()):\n",
      "  3212|         0|            0|            0|  0.00%|        warnings.warn(\"Using a target size ({}) that is different to the input size ({}). \"\n",
      "  3213|         0|            0|            0|  0.00%|                      \"This will likely lead to incorrect results due to broadcasting. \"\n",
      "  3214|         0|            0|            0|  0.00%|                      \"Please ensure they have the same size.\".format(target.size(), input.size()),\n",
      "  3215|         0|            0|            0|  0.00%|                      stacklevel=2)\n",
      "  3216|         0|            0|            0|  0.00%|\n",
      "  3217|         0|            0|            0|  0.00%|    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n",
      "  3218|         0|            0|            0|  0.00%|    return torch._C._nn.huber_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction), delta)\n",
      "  3219|         0|            0|            0|  0.00%|\n",
      "  3220|         0|            0|            0|  0.00%|\n",
      "  3221|         0|            0|            0|  0.00%|def l1_loss(\n",
      "  3222|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  3223|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  3224|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  3225|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  3226|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  3227|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  3228|         0|            0|            0|  0.00%|    r\"\"\"l1_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
      "  3229|         0|            0|            0|  0.00%|\n",
      "  3230|         0|            0|            0|  0.00%|    Function that takes the mean element-wise absolute value difference.\n",
      "  3231|         0|            0|            0|  0.00%|\n",
      "  3232|         0|            0|            0|  0.00%|    See :class:`~torch.nn.L1Loss` for details.\n",
      "  3233|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3234|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):\n",
      "  3235|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3236|         0|            0|            0|  0.00%|            l1_loss, (input, target), input, target, size_average=size_average, reduce=reduce, reduction=reduction\n",
      "  3237|         0|            0|            0|  0.00%|        )\n",
      "  3238|         0|            0|            0|  0.00%|    if not (target.size() == input.size()):\n",
      "  3239|         0|            0|            0|  0.00%|        warnings.warn(\n",
      "  3240|         0|            0|            0|  0.00%|            \"Using a target size ({}) that is different to the input size ({}). \"\n",
      "  3241|         0|            0|            0|  0.00%|            \"This will likely lead to incorrect results due to broadcasting. \"\n",
      "  3242|         0|            0|            0|  0.00%|            \"Please ensure they have the same size.\".format(target.size(), input.size()),\n",
      "  3243|         0|            0|            0|  0.00%|            stacklevel=2,\n",
      "  3244|         0|            0|            0|  0.00%|        )\n",
      "  3245|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  3246|         0|            0|            0|  0.00%|        reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
      "  3247|         0|            0|            0|  0.00%|\n",
      "  3248|         0|            0|            0|  0.00%|    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n",
      "  3249|         0|            0|            0|  0.00%|    return torch._C._nn.l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))\n",
      "  3250|         0|            0|            0|  0.00%|\n",
      "  3251|         0|            0|            0|  0.00%|\n",
      "  3252|         0|            0|            0|  0.00%|def mse_loss(\n",
      "  3253|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  3254|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  3255|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  3256|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  3257|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  3258|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  3259|         0|            0|            0|  0.00%|    r\"\"\"mse_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
      "  3260|         0|            0|            0|  0.00%|\n",
      "  3261|         0|            0|            0|  0.00%|    Measures the element-wise mean squared error.\n",
      "  3262|         0|            0|            0|  0.00%|\n",
      "  3263|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MSELoss` for details.\n",
      "  3264|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3265|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):\n",
      "  3266|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3267|         0|            0|            0|  0.00%|            mse_loss, (input, target), input, target, size_average=size_average, reduce=reduce, reduction=reduction\n",
      "  3268|         0|            0|            0|  0.00%|        )\n",
      "  3269|         0|            0|            0|  0.00%|    if not (target.size() == input.size()):\n",
      "  3270|         0|            0|            0|  0.00%|        warnings.warn(\n",
      "  3271|         0|            0|            0|  0.00%|            \"Using a target size ({}) that is different to the input size ({}). \"\n",
      "  3272|         0|            0|            0|  0.00%|            \"This will likely lead to incorrect results due to broadcasting. \"\n",
      "  3273|         0|            0|            0|  0.00%|            \"Please ensure they have the same size.\".format(target.size(), input.size()),\n",
      "  3274|         0|            0|            0|  0.00%|            stacklevel=2,\n",
      "  3275|         0|            0|            0|  0.00%|        )\n",
      "  3276|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  3277|         0|            0|            0|  0.00%|        reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
      "  3278|         0|            0|            0|  0.00%|\n",
      "  3279|         0|            0|            0|  0.00%|    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n",
      "  3280|         0|            0|            0|  0.00%|    return torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))\n",
      "  3281|         0|            0|            0|  0.00%|\n",
      "  3282|         0|            0|            0|  0.00%|\n",
      "  3283|         0|            0|            0|  0.00%|def margin_ranking_loss(\n",
      "  3284|         0|            0|            0|  0.00%|    input1: Tensor,\n",
      "  3285|         0|            0|            0|  0.00%|    input2: Tensor,\n",
      "  3286|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  3287|         0|            0|            0|  0.00%|    margin: float = 0,\n",
      "  3288|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  3289|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  3290|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  3291|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  3292|         0|            0|            0|  0.00%|    r\"\"\"margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
      "  3293|         0|            0|            0|  0.00%|\n",
      "  3294|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MarginRankingLoss` for details.\n",
      "  3295|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3296|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input1, input2, target):\n",
      "  3297|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3298|         0|            0|            0|  0.00%|            margin_ranking_loss,\n",
      "  3299|         0|            0|            0|  0.00%|            (input1, input2, target),\n",
      "  3300|         0|            0|            0|  0.00%|            input1,\n",
      "  3301|         0|            0|            0|  0.00%|            input2,\n",
      "  3302|         0|            0|            0|  0.00%|            target,\n",
      "  3303|         0|            0|            0|  0.00%|            margin=margin,\n",
      "  3304|         0|            0|            0|  0.00%|            size_average=size_average,\n",
      "  3305|         0|            0|            0|  0.00%|            reduce=reduce,\n",
      "  3306|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  3307|         0|            0|            0|  0.00%|        )\n",
      "  3308|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  3309|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n",
      "  3310|         0|            0|            0|  0.00%|    else:\n",
      "  3311|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)\n",
      "  3312|         0|            0|            0|  0.00%|    if (input1.dim() != input2.dim() or input1.dim() != target.dim()):\n",
      "  3313|         0|            0|            0|  0.00%|        raise RuntimeError(\n",
      "  3314|         0|            0|            0|  0.00%|            (\n",
      "  3315|         0|            0|            0|  0.00%|                \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n",
      "  3316|         0|            0|            0|  0.00%|                \"input1: {}, input2: {}, target: {} \".format(input1.size(), input2.size(), target.size())\n",
      "  3317|         0|            0|            0|  0.00%|            )\n",
      "  3318|         0|            0|            0|  0.00%|        )\n",
      "  3319|         0|            0|            0|  0.00%|    return torch.margin_ranking_loss(input1, input2, target, margin, reduction_enum)\n",
      "  3320|         0|            0|            0|  0.00%|\n",
      "  3321|         0|            0|            0|  0.00%|\n",
      "  3322|         0|            0|            0|  0.00%|def hinge_embedding_loss(\n",
      "  3323|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  3324|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  3325|         0|            0|            0|  0.00%|    margin: float = 1.0,\n",
      "  3326|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  3327|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  3328|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  3329|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  3330|         0|            0|            0|  0.00%|    r\"\"\"hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
      "  3331|         0|            0|            0|  0.00%|\n",
      "  3332|         0|            0|            0|  0.00%|    See :class:`~torch.nn.HingeEmbeddingLoss` for details.\n",
      "  3333|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3334|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):\n",
      "  3335|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3336|         0|            0|            0|  0.00%|            hinge_embedding_loss,\n",
      "  3337|         0|            0|            0|  0.00%|            (input, target),\n",
      "  3338|         0|            0|            0|  0.00%|            input,\n",
      "  3339|         0|            0|            0|  0.00%|            target,\n",
      "  3340|         0|            0|            0|  0.00%|            margin=margin,\n",
      "  3341|         0|            0|            0|  0.00%|            size_average=size_average,\n",
      "  3342|         0|            0|            0|  0.00%|            reduce=reduce,\n",
      "  3343|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  3344|         0|            0|            0|  0.00%|        )\n",
      "  3345|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  3346|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n",
      "  3347|         0|            0|            0|  0.00%|    else:\n",
      "  3348|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)\n",
      "  3349|         0|            0|            0|  0.00%|    return torch.hinge_embedding_loss(input, target, margin, reduction_enum)\n",
      "  3350|         0|            0|            0|  0.00%|\n",
      "  3351|         0|            0|            0|  0.00%|\n",
      "  3352|         0|            0|            0|  0.00%|def multilabel_margin_loss(\n",
      "  3353|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  3354|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  3355|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  3356|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  3357|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  3358|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  3359|         0|            0|            0|  0.00%|    r\"\"\"multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
      "  3360|         0|            0|            0|  0.00%|\n",
      "  3361|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MultiLabelMarginLoss` for details.\n",
      "  3362|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3363|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):\n",
      "  3364|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3365|         0|            0|            0|  0.00%|            multilabel_margin_loss,\n",
      "  3366|         0|            0|            0|  0.00%|            (input, target),\n",
      "  3367|         0|            0|            0|  0.00%|            input,\n",
      "  3368|         0|            0|            0|  0.00%|            target,\n",
      "  3369|         0|            0|            0|  0.00%|            size_average=size_average,\n",
      "  3370|         0|            0|            0|  0.00%|            reduce=reduce,\n",
      "  3371|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  3372|         0|            0|            0|  0.00%|        )\n",
      "  3373|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  3374|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n",
      "  3375|         0|            0|            0|  0.00%|    else:\n",
      "  3376|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)\n",
      "  3377|         0|            0|            0|  0.00%|    return torch._C._nn.multilabel_margin_loss(input, target, reduction_enum)\n",
      "  3378|         0|            0|            0|  0.00%|\n",
      "  3379|         0|            0|            0|  0.00%|\n",
      "  3380|         0|            0|            0|  0.00%|def soft_margin_loss(\n",
      "  3381|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  3382|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  3383|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  3384|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  3385|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  3386|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  3387|         0|            0|            0|  0.00%|    r\"\"\"soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
      "  3388|         0|            0|            0|  0.00%|\n",
      "  3389|         0|            0|            0|  0.00%|    See :class:`~torch.nn.SoftMarginLoss` for details.\n",
      "  3390|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3391|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target):\n",
      "  3392|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3393|         0|            0|            0|  0.00%|            soft_margin_loss, (input, target), input, target, size_average=size_average, reduce=reduce, reduction=reduction\n",
      "  3394|         0|            0|            0|  0.00%|        )\n",
      "  3395|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  3396|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n",
      "  3397|         0|            0|            0|  0.00%|    else:\n",
      "  3398|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)\n",
      "  3399|         0|            0|            0|  0.00%|    return torch._C._nn.soft_margin_loss(input, target, reduction_enum)\n",
      "  3400|         0|            0|            0|  0.00%|\n",
      "  3401|         0|            0|            0|  0.00%|\n",
      "  3402|         0|            0|            0|  0.00%|def multilabel_soft_margin_loss(\n",
      "  3403|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  3404|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  3405|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,\n",
      "  3406|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  3407|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  3408|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  3409|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  3410|         0|            0|            0|  0.00%|    r\"\"\"multilabel_soft_margin_loss(input, target, weight=None, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
      "  3411|         0|            0|            0|  0.00%|\n",
      "  3412|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MultiLabelSoftMarginLoss` for details.\n",
      "  3413|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3414|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target, weight):\n",
      "  3415|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3416|         0|            0|            0|  0.00%|            multilabel_soft_margin_loss,\n",
      "  3417|         0|            0|            0|  0.00%|            (input, target, weight),\n",
      "  3418|         0|            0|            0|  0.00%|            input,\n",
      "  3419|         0|            0|            0|  0.00%|            target,\n",
      "  3420|         0|            0|            0|  0.00%|            weight=weight,\n",
      "  3421|         0|            0|            0|  0.00%|            size_average=size_average,\n",
      "  3422|         0|            0|            0|  0.00%|            reduce=reduce,\n",
      "  3423|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  3424|         0|            0|            0|  0.00%|        )\n",
      "  3425|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  3426|         0|            0|            0|  0.00%|        reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
      "  3427|         0|            0|            0|  0.00%|\n",
      "  3428|         0|            0|            0|  0.00%|    loss = -(target * logsigmoid(input) + (1 - target) * logsigmoid(-input))\n",
      "  3429|         0|            0|            0|  0.00%|\n",
      "  3430|         0|            0|            0|  0.00%|    if weight is not None:\n",
      "  3431|         0|            0|            0|  0.00%|        loss = loss * weight\n",
      "  3432|         0|            0|            0|  0.00%|\n",
      "  3433|         0|            0|            0|  0.00%|    class_dim = input.dim() - 1\n",
      "  3434|         0|            0|            0|  0.00%|    C = input.size(class_dim)\n",
      "  3435|         0|            0|            0|  0.00%|    loss = loss.sum(dim=class_dim) / C  # only return N loss values\n",
      "  3436|         0|            0|            0|  0.00%|\n",
      "  3437|         0|            0|            0|  0.00%|    if reduction == \"none\":\n",
      "  3438|         0|            0|            0|  0.00%|        ret = loss\n",
      "  3439|         0|            0|            0|  0.00%|    elif reduction == \"mean\":\n",
      "  3440|         0|            0|            0|  0.00%|        ret = loss.mean()\n",
      "  3441|         0|            0|            0|  0.00%|    elif reduction == \"sum\":\n",
      "  3442|         0|            0|            0|  0.00%|        ret = loss.sum()\n",
      "  3443|         0|            0|            0|  0.00%|    else:\n",
      "  3444|         0|            0|            0|  0.00%|        ret = input\n",
      "  3445|         0|            0|            0|  0.00%|        raise ValueError(reduction + \" is not valid\")\n",
      "  3446|         0|            0|            0|  0.00%|    return ret\n",
      "  3447|         0|            0|            0|  0.00%|\n",
      "  3448|         0|            0|            0|  0.00%|\n",
      "  3449|         0|            0|            0|  0.00%|def cosine_embedding_loss(\n",
      "  3450|         0|            0|            0|  0.00%|    input1: Tensor,\n",
      "  3451|         0|            0|            0|  0.00%|    input2: Tensor,\n",
      "  3452|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  3453|         0|            0|            0|  0.00%|    margin: float = 0,\n",
      "  3454|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  3455|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  3456|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  3457|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  3458|         0|            0|            0|  0.00%|    r\"\"\"cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
      "  3459|         0|            0|            0|  0.00%|\n",
      "  3460|         0|            0|            0|  0.00%|    See :class:`~torch.nn.CosineEmbeddingLoss` for details.\n",
      "  3461|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3462|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input1, input2, target):\n",
      "  3463|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3464|         0|            0|            0|  0.00%|            cosine_embedding_loss,\n",
      "  3465|         0|            0|            0|  0.00%|            (input1, input2, target),\n",
      "  3466|         0|            0|            0|  0.00%|            input1,\n",
      "  3467|         0|            0|            0|  0.00%|            input2,\n",
      "  3468|         0|            0|            0|  0.00%|            target,\n",
      "  3469|         0|            0|            0|  0.00%|            margin=margin,\n",
      "  3470|         0|            0|            0|  0.00%|            size_average=size_average,\n",
      "  3471|         0|            0|            0|  0.00%|            reduce=reduce,\n",
      "  3472|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  3473|         0|            0|            0|  0.00%|        )\n",
      "  3474|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  3475|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n",
      "  3476|         0|            0|            0|  0.00%|    else:\n",
      "  3477|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)\n",
      "  3478|         0|            0|            0|  0.00%|    return torch.cosine_embedding_loss(input1, input2, target, margin, reduction_enum)\n",
      "  3479|         0|            0|            0|  0.00%|\n",
      "  3480|         0|            0|            0|  0.00%|\n",
      "  3481|         0|            0|            0|  0.00%|def multi_margin_loss(\n",
      "  3482|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  3483|         0|            0|            0|  0.00%|    target: Tensor,\n",
      "  3484|         0|            0|            0|  0.00%|    p: int = 1,\n",
      "  3485|         0|            0|            0|  0.00%|    margin: float = 1.0,\n",
      "  3486|         0|            0|            0|  0.00%|    weight: Optional[Tensor] = None,\n",
      "  3487|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  3488|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  3489|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  3490|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  3491|         0|            0|            0|  0.00%|    r\"\"\"multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
      "  3492|         0|            0|            0|  0.00%|\n",
      "  3493|         0|            0|            0|  0.00%|    See :class:`~torch.nn.MultiMarginLoss` for details.\n",
      "  3494|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3495|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, target, weight):\n",
      "  3496|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3497|         0|            0|            0|  0.00%|            multi_margin_loss,\n",
      "  3498|         0|            0|            0|  0.00%|            (input, target, weight),\n",
      "  3499|         0|            0|            0|  0.00%|            input,\n",
      "  3500|         0|            0|            0|  0.00%|            target,\n",
      "  3501|         0|            0|            0|  0.00%|            p=p,\n",
      "  3502|         0|            0|            0|  0.00%|            margin=margin,\n",
      "  3503|         0|            0|            0|  0.00%|            weight=weight,\n",
      "  3504|         0|            0|            0|  0.00%|            size_average=size_average,\n",
      "  3505|         0|            0|            0|  0.00%|            reduce=reduce,\n",
      "  3506|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  3507|         0|            0|            0|  0.00%|        )\n",
      "  3508|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  3509|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n",
      "  3510|         0|            0|            0|  0.00%|    else:\n",
      "  3511|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)\n",
      "  3512|         0|            0|            0|  0.00%|    if p != 1 and p != 2:\n",
      "  3513|         0|            0|            0|  0.00%|        raise ValueError(\"only p == 1 and p == 2 supported\")\n",
      "  3514|         0|            0|            0|  0.00%|    if weight is not None:\n",
      "  3515|         0|            0|            0|  0.00%|        if weight.dim() != 1:\n",
      "  3516|         0|            0|            0|  0.00%|            raise ValueError(\"weight must be one-dimensional\")\n",
      "  3517|         0|            0|            0|  0.00%|\n",
      "  3518|         0|            0|            0|  0.00%|    return torch._C._nn.multi_margin_loss(input, target, p, margin, weight, reduction_enum)\n",
      "  3519|         0|            0|            0|  0.00%|\n",
      "  3520|         0|            0|            0|  0.00%|\n",
      "  3521|         0|            0|            0|  0.00%|pixel_shuffle = _add_docstr(\n",
      "  3522|         0|            0|            0|  0.00%|    torch.pixel_shuffle,\n",
      "  3523|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  3524|         0|            0|            0|  0.00%|pixel_shuffle(input, upscale_factor) -> Tensor\n",
      "  3525|         0|            0|            0|  0.00%|\n",
      "  3526|         0|            0|            0|  0.00%|Rearranges elements in a tensor of shape :math:`(*, C \\times r^2, H, W)` to a\n",
      "  3527|         0|            0|            0|  0.00%|tensor of shape :math:`(*, C, H \\times r, W \\times r)`, where r is the :attr:`upscale_factor`.\n",
      "  3528|         0|            0|            0|  0.00%|\n",
      "  3529|         0|            0|            0|  0.00%|See :class:`~torch.nn.PixelShuffle` for details.\n",
      "  3530|         0|            0|            0|  0.00%|\n",
      "  3531|         0|            0|            0|  0.00%|Args:\n",
      "  3532|         0|            0|            0|  0.00%|    input (Tensor): the input tensor\n",
      "  3533|         0|            0|            0|  0.00%|    upscale_factor (int): factor to increase spatial resolution by\n",
      "  3534|         0|            0|            0|  0.00%|\n",
      "  3535|         0|            0|            0|  0.00%|Examples::\n",
      "  3536|         0|            0|            0|  0.00%|\n",
      "  3537|         0|            0|            0|  0.00%|    >>> input = torch.randn(1, 9, 4, 4)\n",
      "  3538|         0|            0|            0|  0.00%|    >>> output = torch.nn.functional.pixel_shuffle(input, 3)\n",
      "  3539|         0|            0|            0|  0.00%|    >>> print(output.size())\n",
      "  3540|         0|            0|            0|  0.00%|    torch.Size([1, 1, 12, 12])\n",
      "  3541|         0|            0|            0|  0.00%|\"\"\",\n",
      "  3542|         0|            0|            0|  0.00%|)\n",
      "  3543|         0|            0|            0|  0.00%|\n",
      "  3544|         0|            0|            0|  0.00%|pixel_unshuffle = _add_docstr(\n",
      "  3545|         0|            0|            0|  0.00%|    torch.pixel_unshuffle,\n",
      "  3546|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  3547|         0|            0|            0|  0.00%|pixel_unshuffle(input, downscale_factor) -> Tensor\n",
      "  3548|         0|            0|            0|  0.00%|\n",
      "  3549|         0|            0|            0|  0.00%|Reverses the :class:`~torch.nn.PixelShuffle` operation by rearranging elements in a\n",
      "  3550|         0|            0|            0|  0.00%|tensor of shape :math:`(*, C, H \\times r, W \\times r)` to a tensor of shape\n",
      "  3551|         0|            0|            0|  0.00%|:math:`(*, C \\times r^2, H, W)`, where r is the :attr:`downscale_factor`.\n",
      "  3552|         0|            0|            0|  0.00%|\n",
      "  3553|         0|            0|            0|  0.00%|See :class:`~torch.nn.PixelUnshuffle` for details.\n",
      "  3554|         0|            0|            0|  0.00%|\n",
      "  3555|         0|            0|            0|  0.00%|Args:\n",
      "  3556|         0|            0|            0|  0.00%|    input (Tensor): the input tensor\n",
      "  3557|         0|            0|            0|  0.00%|    downscale_factor (int): factor to increase spatial resolution by\n",
      "  3558|         0|            0|            0|  0.00%|\n",
      "  3559|         0|            0|            0|  0.00%|Examples::\n",
      "  3560|         0|            0|            0|  0.00%|\n",
      "  3561|         0|            0|            0|  0.00%|    >>> input = torch.randn(1, 1, 12, 12)\n",
      "  3562|         0|            0|            0|  0.00%|    >>> output = torch.nn.functional.pixel_unshuffle(input, 3)\n",
      "  3563|         0|            0|            0|  0.00%|    >>> print(output.size())\n",
      "  3564|         0|            0|            0|  0.00%|    torch.Size([1, 9, 4, 4])\n",
      "  3565|         0|            0|            0|  0.00%|\"\"\",\n",
      "  3566|         0|            0|            0|  0.00%|)\n",
      "  3567|         0|            0|            0|  0.00%|\n",
      "  3568|         0|            0|            0|  0.00%|channel_shuffle = _add_docstr(\n",
      "  3569|         0|            0|            0|  0.00%|    torch.channel_shuffle,\n",
      "  3570|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  3571|         0|            0|            0|  0.00%|channel_shuffle(input, groups) -> Tensor\n",
      "  3572|         0|            0|            0|  0.00%|\n",
      "  3573|         0|            0|            0|  0.00%|Divide the channels in a tensor of shape :math:`(*, C , H, W)`\n",
      "  3574|         0|            0|            0|  0.00%|into g groups and rearrange them as :math:`(*, C \\frac g, g, H, W)`,\n",
      "  3575|         0|            0|            0|  0.00%|while keeping the original tensor shape.\n",
      "  3576|         0|            0|            0|  0.00%|\n",
      "  3577|         0|            0|            0|  0.00%|See :class:`~torch.nn.ChannelShuffle` for details.\n",
      "  3578|         0|            0|            0|  0.00%|\n",
      "  3579|         0|            0|            0|  0.00%|Args:\n",
      "  3580|         0|            0|            0|  0.00%|    input (Tensor): the input tensor\n",
      "  3581|         0|            0|            0|  0.00%|    groups (int): number of groups to divide channels in and rearrange.\n",
      "  3582|         0|            0|            0|  0.00%|\n",
      "  3583|         0|            0|            0|  0.00%|Examples::\n",
      "  3584|         0|            0|            0|  0.00%|\n",
      "  3585|         0|            0|            0|  0.00%|    >>> input = torch.randn(1, 4, 2, 2)\n",
      "  3586|         0|            0|            0|  0.00%|    >>> print(input)\n",
      "  3587|         0|            0|            0|  0.00%|    [[[[1, 2],\n",
      "  3588|         0|            0|            0|  0.00%|       [3, 4]],\n",
      "  3589|         0|            0|            0|  0.00%|      [[5, 6],\n",
      "  3590|         0|            0|            0|  0.00%|       [7, 8]],\n",
      "  3591|         0|            0|            0|  0.00%|      [[9, 10],\n",
      "  3592|         0|            0|            0|  0.00%|       [11, 12]],\n",
      "  3593|         0|            0|            0|  0.00%|      [[13, 14],\n",
      "  3594|         0|            0|            0|  0.00%|       [15, 16]],\n",
      "  3595|         0|            0|            0|  0.00%|     ]]\n",
      "  3596|         0|            0|            0|  0.00%|    >>> output = torch.nn.functional.channel_shuffle(input, 2)\n",
      "  3597|         0|            0|            0|  0.00%|    >>> print(output)\n",
      "  3598|         0|            0|            0|  0.00%|    [[[[1, 2],\n",
      "  3599|         0|            0|            0|  0.00%|       [3, 4]],\n",
      "  3600|         0|            0|            0|  0.00%|      [[9, 10],\n",
      "  3601|         0|            0|            0|  0.00%|       [11, 12]],\n",
      "  3602|         0|            0|            0|  0.00%|      [[5, 6],\n",
      "  3603|         0|            0|            0|  0.00%|       [7, 8]],\n",
      "  3604|         0|            0|            0|  0.00%|      [[13, 14],\n",
      "  3605|         0|            0|            0|  0.00%|       [15, 16]],\n",
      "  3606|         0|            0|            0|  0.00%|     ]]\n",
      "  3607|         0|            0|            0|  0.00%|\"\"\",\n",
      "  3608|         0|            0|            0|  0.00%|)\n",
      "  3609|         0|            0|            0|  0.00%|\n",
      "  3610|         0|            0|            0|  0.00%|native_channel_shuffle = _add_docstr(\n",
      "  3611|         0|            0|            0|  0.00%|    torch.native_channel_shuffle,\n",
      "  3612|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  3613|         0|            0|            0|  0.00%|native_channel_shuffle(input, groups) -> Tensor\n",
      "  3614|         0|            0|            0|  0.00%|\n",
      "  3615|         0|            0|            0|  0.00%|Native kernel level implementation of the `channel_shuffle`.\n",
      "  3616|         0|            0|            0|  0.00%|This function might become private in future releases, use with caution.\n",
      "  3617|         0|            0|            0|  0.00%|\n",
      "  3618|         0|            0|            0|  0.00%|Divide the channels in a tensor of shape :math:`(*, C , H, W)`\n",
      "  3619|         0|            0|            0|  0.00%|into g groups and rearrange them as :math:`(*, C \\frac g, g, H, W)`,\n",
      "  3620|         0|            0|            0|  0.00%|while keeping the original tensor shape.\n",
      "  3621|         0|            0|            0|  0.00%|\n",
      "  3622|         0|            0|            0|  0.00%|See :class:`~torch.nn.ChannelShuffle` for details.\n",
      "  3623|         0|            0|            0|  0.00%|\n",
      "  3624|         0|            0|            0|  0.00%|Args:\n",
      "  3625|         0|            0|            0|  0.00%|    input (Tensor): the input tensor\n",
      "  3626|         0|            0|            0|  0.00%|    groups (int): number of groups to divide channels in and rearrange.\n",
      "  3627|         0|            0|            0|  0.00%|\n",
      "  3628|         0|            0|            0|  0.00%|Examples::\n",
      "  3629|         0|            0|            0|  0.00%|\n",
      "  3630|         0|            0|            0|  0.00%|    >>> input = torch.randn(1, 4, 2, 2)\n",
      "  3631|         0|            0|            0|  0.00%|    >>> print(input)\n",
      "  3632|         0|            0|            0|  0.00%|    [[[[1, 2],\n",
      "  3633|         0|            0|            0|  0.00%|       [3, 4]],\n",
      "  3634|         0|            0|            0|  0.00%|      [[5, 6],\n",
      "  3635|         0|            0|            0|  0.00%|       [7, 8]],\n",
      "  3636|         0|            0|            0|  0.00%|      [[9, 10],\n",
      "  3637|         0|            0|            0|  0.00%|       [11, 12]],\n",
      "  3638|         0|            0|            0|  0.00%|      [[13, 14],\n",
      "  3639|         0|            0|            0|  0.00%|       [15, 16]],\n",
      "  3640|         0|            0|            0|  0.00%|     ]]\n",
      "  3641|         0|            0|            0|  0.00%|    >>> output = torch.nn.functional.native_channel_shuffle(input, 2)\n",
      "  3642|         0|            0|            0|  0.00%|    >>> print(output)\n",
      "  3643|         0|            0|            0|  0.00%|    [[[[1, 2],\n",
      "  3644|         0|            0|            0|  0.00%|       [3, 4]],\n",
      "  3645|         0|            0|            0|  0.00%|      [[9, 10],\n",
      "  3646|         0|            0|            0|  0.00%|       [11, 12]],\n",
      "  3647|         0|            0|            0|  0.00%|      [[5, 6],\n",
      "  3648|         0|            0|            0|  0.00%|       [7, 8]],\n",
      "  3649|         0|            0|            0|  0.00%|      [[13, 14],\n",
      "  3650|         0|            0|            0|  0.00%|       [15, 16]],\n",
      "  3651|         0|            0|            0|  0.00%|     ]]\n",
      "  3652|         0|            0|            0|  0.00%|\"\"\",\n",
      "  3653|         0|            0|            0|  0.00%|)\n",
      "  3654|         0|            0|            0|  0.00%|\n",
      "  3655|         0|            0|            0|  0.00%|@_overload  # noqa: F811\n",
      "  3656|         0|            0|            0|  0.00%|def upsample(input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None, mode: str = \"nearest\", align_corners: Optional[bool] = None) -> Tensor:  # noqa: F811\n",
      "  3657|         0|            0|            0|  0.00%|    pass\n",
      "  3658|         0|            0|            0|  0.00%|\n",
      "  3659|         0|            0|            0|  0.00%|\n",
      "  3660|         0|            0|            0|  0.00%|@_overload  # noqa: F811\n",
      "  3661|         0|            0|            0|  0.00%|def upsample(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None, mode: str = \"nearest\", align_corners: Optional[bool] = None) -> Tensor:  # noqa: F811\n",
      "  3662|         0|            0|            0|  0.00%|    pass\n",
      "  3663|         0|            0|            0|  0.00%|\n",
      "  3664|         0|            0|            0|  0.00%|\n",
      "  3665|         0|            0|            0|  0.00%|def upsample(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):  # noqa: F811\n",
      "  3666|         0|            0|            0|  0.00%|    r\"\"\"Upsamples the input to either the given :attr:`size` or the given\n",
      "  3667|         0|            0|            0|  0.00%|    :attr:`scale_factor`\n",
      "  3668|         0|            0|            0|  0.00%|\n",
      "  3669|         0|            0|            0|  0.00%|    .. warning::\n",
      "  3670|         0|            0|            0|  0.00%|        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.\n",
      "  3671|         0|            0|            0|  0.00%|        This is equivalent with ``nn.functional.interpolate(...)``.\n",
      "  3672|         0|            0|            0|  0.00%|\n",
      "  3673|         0|            0|            0|  0.00%|    Note:\n",
      "  3674|         0|            0|            0|  0.00%|        {backward_reproducibility_note}\n",
      "  3675|         0|            0|            0|  0.00%|\n",
      "  3676|         0|            0|            0|  0.00%|    The algorithm used for upsampling is determined by :attr:`mode`.\n",
      "  3677|         0|            0|            0|  0.00%|\n",
      "  3678|         0|            0|            0|  0.00%|    Currently temporal, spatial and volumetric upsampling are supported, i.e.\n",
      "  3679|         0|            0|            0|  0.00%|    expected inputs are 3-D, 4-D or 5-D in shape.\n",
      "  3680|         0|            0|            0|  0.00%|\n",
      "  3681|         0|            0|            0|  0.00%|    The input dimensions are interpreted in the form:\n",
      "  3682|         0|            0|            0|  0.00%|    `mini-batch x channels x [optional depth] x [optional height] x width`.\n",
      "  3683|         0|            0|            0|  0.00%|\n",
      "  3684|         0|            0|            0|  0.00%|    The modes available for upsampling are: `nearest`, `linear` (3D-only),\n",
      "  3685|         0|            0|            0|  0.00%|    `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only)\n",
      "  3686|         0|            0|            0|  0.00%|\n",
      "  3687|         0|            0|            0|  0.00%|    Args:\n",
      "  3688|         0|            0|            0|  0.00%|        input (Tensor): the input tensor\n",
      "  3689|         0|            0|            0|  0.00%|        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):\n",
      "  3690|         0|            0|            0|  0.00%|            output spatial size.\n",
      "  3691|         0|            0|            0|  0.00%|        scale_factor (float or Tuple[float]): multiplier for spatial size. Has to match input size if it is a tuple.\n",
      "  3692|         0|            0|            0|  0.00%|        mode (string): algorithm used for upsampling:\n",
      "  3693|         0|            0|            0|  0.00%|            ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |\n",
      "  3694|         0|            0|            0|  0.00%|            ``'trilinear'``. Default: ``'nearest'``\n",
      "  3695|         0|            0|            0|  0.00%|        align_corners (bool, optional): Geometrically, we consider the pixels of the\n",
      "  3696|         0|            0|            0|  0.00%|            input and output as squares rather than points.\n",
      "  3697|         0|            0|            0|  0.00%|            If set to ``True``, the input and output tensors are aligned by the\n",
      "  3698|         0|            0|            0|  0.00%|            center points of their corner pixels, preserving the values at the corner pixels.\n",
      "  3699|         0|            0|            0|  0.00%|            If set to ``False``, the input and output tensors are aligned by the corner\n",
      "  3700|         0|            0|            0|  0.00%|            points of their corner pixels, and the interpolation uses edge value padding\n",
      "  3701|         0|            0|            0|  0.00%|            for out-of-boundary values, making this operation *independent* of input size\n",
      "  3702|         0|            0|            0|  0.00%|            when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`\n",
      "  3703|         0|            0|            0|  0.00%|            is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``.\n",
      "  3704|         0|            0|            0|  0.00%|            Default: ``False``\n",
      "  3705|         0|            0|            0|  0.00%|\n",
      "  3706|         0|            0|            0|  0.00%|    .. note::\n",
      "  3707|         0|            0|            0|  0.00%|        With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce\n",
      "  3708|         0|            0|            0|  0.00%|        negative values or values greater than 255 for images.\n",
      "  3709|         0|            0|            0|  0.00%|        Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot\n",
      "  3710|         0|            0|            0|  0.00%|        when displaying the image.\n",
      "  3711|         0|            0|            0|  0.00%|\n",
      "  3712|         0|            0|            0|  0.00%|    .. warning::\n",
      "  3713|         0|            0|            0|  0.00%|        With ``align_corners = True``, the linearly interpolating modes\n",
      "  3714|         0|            0|            0|  0.00%|        (`linear`, `bilinear`, and `trilinear`) don't proportionally align the\n",
      "  3715|         0|            0|            0|  0.00%|        output and input pixels, and thus the output values can depend on the\n",
      "  3716|         0|            0|            0|  0.00%|        input size. This was the default behavior for these modes up to version\n",
      "  3717|         0|            0|            0|  0.00%|        0.3.1. Since then, the default behavior is ``align_corners = False``.\n",
      "  3718|         0|            0|            0|  0.00%|        See :class:`~torch.nn.Upsample` for concrete examples on how this\n",
      "  3719|         0|            0|            0|  0.00%|        affects the outputs.\n",
      "  3720|         0|            0|            0|  0.00%|\n",
      "  3721|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3722|         0|            0|            0|  0.00%|    warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "  3723|         0|            0|            0|  0.00%|    return interpolate(input, size, scale_factor, mode, align_corners)\n",
      "  3724|         0|            0|            0|  0.00%|\n",
      "  3725|         0|            0|            0|  0.00%|\n",
      "  3726|         0|            0|            0|  0.00%|if upsample.__doc__:\n",
      "  3727|         0|            0|            0|  0.00%|    upsample.__doc__ = upsample.__doc__.format(**reproducibility_notes)\n",
      "  3728|         0|            0|            0|  0.00%|\n",
      "  3729|         0|            0|            0|  0.00%|\n",
      "  3730|         0|            0|            0|  0.00%|@_overload  # noqa: F811\n",
      "  3731|         0|            0|            0|  0.00%|def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811\n",
      "  3732|         0|            0|            0|  0.00%|    pass\n",
      "  3733|         0|            0|            0|  0.00%|\n",
      "  3734|         0|            0|            0|  0.00%|\n",
      "  3735|         0|            0|            0|  0.00%|@_overload  # noqa: F811\n",
      "  3736|         0|            0|            0|  0.00%|def interpolate(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811\n",
      "  3737|         0|            0|            0|  0.00%|    pass\n",
      "  3738|         0|            0|            0|  0.00%|\n",
      "  3739|         0|            0|            0|  0.00%|\n",
      "  3740|         0|            0|            0|  0.00%|@_overload  # noqa: F811\n",
      "  3741|         0|            0|            0|  0.00%|def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811\n",
      "  3742|         0|            0|            0|  0.00%|    pass\n",
      "  3743|         0|            0|            0|  0.00%|\n",
      "  3744|         0|            0|            0|  0.00%|\n",
      "  3745|         0|            0|            0|  0.00%|@_overload  # noqa: F811\n",
      "  3746|         0|            0|            0|  0.00%|def interpolate(  # noqa: F811\n",
      "  3747|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  3748|         0|            0|            0|  0.00%|    size: Optional[List[int]] = None,\n",
      "  3749|         0|            0|            0|  0.00%|    scale_factor: Optional[float] = None,\n",
      "  3750|         0|            0|            0|  0.00%|    mode: str = \"nearest\",\n",
      "  3751|         0|            0|            0|  0.00%|    align_corners: Optional[bool] = None,\n",
      "  3752|         0|            0|            0|  0.00%|    recompute_scale_factor: Optional[bool] = None,\n",
      "  3753|         0|            0|            0|  0.00%|    antialias: bool = False,\n",
      "  3754|         0|            0|            0|  0.00%|) -> Tensor:  # noqa: F811\n",
      "  3755|         0|            0|            0|  0.00%|    pass\n",
      "  3756|         0|            0|            0|  0.00%|\n",
      "  3757|         0|            0|            0|  0.00%|def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811\n",
      "  3758|         0|            0|            0|  0.00%|    r\"\"\"Down/up samples the input to either the given :attr:`size` or the given\n",
      "  3759|         0|            0|            0|  0.00%|    :attr:`scale_factor`\n",
      "  3760|         0|            0|            0|  0.00%|\n",
      "  3761|         0|            0|            0|  0.00%|    The algorithm used for interpolation is determined by :attr:`mode`.\n",
      "  3762|         0|            0|            0|  0.00%|\n",
      "  3763|         0|            0|            0|  0.00%|    Currently temporal, spatial and volumetric sampling are supported, i.e.\n",
      "  3764|         0|            0|            0|  0.00%|    expected inputs are 3-D, 4-D or 5-D in shape.\n",
      "  3765|         0|            0|            0|  0.00%|\n",
      "  3766|         0|            0|            0|  0.00%|    The input dimensions are interpreted in the form:\n",
      "  3767|         0|            0|            0|  0.00%|    `mini-batch x channels x [optional depth] x [optional height] x width`.\n",
      "  3768|         0|            0|            0|  0.00%|\n",
      "  3769|         0|            0|            0|  0.00%|    The modes available for resizing are: `nearest`, `linear` (3D-only),\n",
      "  3770|         0|            0|            0|  0.00%|    `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`, `nearest-exact`\n",
      "  3771|         0|            0|            0|  0.00%|\n",
      "  3772|         0|            0|            0|  0.00%|    Args:\n",
      "  3773|         0|            0|            0|  0.00%|        input (Tensor): the input tensor\n",
      "  3774|         0|            0|            0|  0.00%|        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):\n",
      "  3775|         0|            0|            0|  0.00%|            output spatial size.\n",
      "  3776|         0|            0|            0|  0.00%|        scale_factor (float or Tuple[float]): multiplier for spatial size. If `scale_factor` is a tuple,\n",
      "  3777|         0|            0|            0|  0.00%|            its length has to match `input.dim()`.\n",
      "  3778|         0|            0|            0|  0.00%|        mode (str): algorithm used for upsampling:\n",
      "  3779|         0|            0|            0|  0.00%|            ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |\n",
      "  3780|         0|            0|            0|  0.00%|            ``'trilinear'`` | ``'area'`` | ``'nearest-exact'``. Default: ``'nearest'``\n",
      "  3781|         0|            0|            0|  0.00%|        align_corners (bool, optional): Geometrically, we consider the pixels of the\n",
      "  3782|         0|            0|            0|  0.00%|            input and output as squares rather than points.\n",
      "  3783|         0|            0|            0|  0.00%|            If set to ``True``, the input and output tensors are aligned by the\n",
      "  3784|         0|            0|            0|  0.00%|            center points of their corner pixels, preserving the values at the corner pixels.\n",
      "  3785|         0|            0|            0|  0.00%|            If set to ``False``, the input and output tensors are aligned by the corner\n",
      "  3786|         0|            0|            0|  0.00%|            points of their corner pixels, and the interpolation uses edge value padding\n",
      "  3787|         0|            0|            0|  0.00%|            for out-of-boundary values, making this operation *independent* of input size\n",
      "  3788|         0|            0|            0|  0.00%|            when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`\n",
      "  3789|         0|            0|            0|  0.00%|            is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``.\n",
      "  3790|         0|            0|            0|  0.00%|            Default: ``False``\n",
      "  3791|         0|            0|            0|  0.00%|        recompute_scale_factor (bool, optional): recompute the scale_factor for use in the\n",
      "  3792|         0|            0|            0|  0.00%|            interpolation calculation. If `recompute_scale_factor` is ``True``, then\n",
      "  3793|         0|            0|            0|  0.00%|            `scale_factor` must be passed in and `scale_factor` is used to compute the\n",
      "  3794|         0|            0|            0|  0.00%|            output `size`. The computed output `size` will be used to infer new scales for\n",
      "  3795|         0|            0|            0|  0.00%|            the interpolation. Note that when `scale_factor` is floating-point, it may differ\n",
      "  3796|         0|            0|            0|  0.00%|            from the recomputed `scale_factor` due to rounding and precision issues.\n",
      "  3797|         0|            0|            0|  0.00%|            If `recompute_scale_factor` is ``False``, then `size` or `scale_factor` will\n",
      "  3798|         0|            0|            0|  0.00%|            be used directly for interpolation. Default: ``None``.\n",
      "  3799|         0|            0|            0|  0.00%|        antialias (bool, optional): flag to apply anti-aliasing. Default: ``False``. Using anti-alias\n",
      "  3800|         0|            0|            0|  0.00%|            option together with ``align_corners=False``, interpolation result would match Pillow\n",
      "  3801|         0|            0|            0|  0.00%|            result for downsampling operation. Supported modes: ``'bilinear'``, ``'bicubic'``.\n",
      "  3802|         0|            0|            0|  0.00%|\n",
      "  3803|         0|            0|            0|  0.00%|    .. note::\n",
      "  3804|         0|            0|            0|  0.00%|        With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce\n",
      "  3805|         0|            0|            0|  0.00%|        negative values or values greater than 255 for images.\n",
      "  3806|         0|            0|            0|  0.00%|        Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot\n",
      "  3807|         0|            0|            0|  0.00%|        when displaying the image.\n",
      "  3808|         0|            0|            0|  0.00%|\n",
      "  3809|         0|            0|            0|  0.00%|    .. note::\n",
      "  3810|         0|            0|            0|  0.00%|        Mode ``mode='nearest-exact'`` matches Scikit-Image and PIL nearest neighbours interpolation\n",
      "  3811|         0|            0|            0|  0.00%|        algorithms and fixes known issues with ``mode='nearest'``. This mode is introduced to keep\n",
      "  3812|         0|            0|            0|  0.00%|        backward compatibility.\n",
      "  3813|         0|            0|            0|  0.00%|        Mode ``mode='nearest'`` matches buggy OpenCV's ``INTER_NEAREST`` interpolation algorithm.\n",
      "  3814|         0|            0|            0|  0.00%|\n",
      "  3815|         0|            0|            0|  0.00%|    Note:\n",
      "  3816|         0|            0|            0|  0.00%|        {backward_reproducibility_note}\n",
      "  3817|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  3818|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  3819|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  3820|         0|            0|            0|  0.00%|            interpolate,\n",
      "  3821|         0|            0|            0|  0.00%|            (input,),\n",
      "  3822|         0|            0|            0|  0.00%|            input,\n",
      "  3823|         0|            0|            0|  0.00%|            size=size,\n",
      "  3824|         0|            0|            0|  0.00%|            scale_factor=scale_factor,\n",
      "  3825|         0|            0|            0|  0.00%|            mode=mode,\n",
      "  3826|         0|            0|            0|  0.00%|            align_corners=align_corners,\n",
      "  3827|         0|            0|            0|  0.00%|            recompute_scale_factor=recompute_scale_factor,\n",
      "  3828|         0|            0|            0|  0.00%|            antialias=antialias\n",
      "  3829|         0|            0|            0|  0.00%|        )\n",
      "  3830|         0|            0|            0|  0.00%|\n",
      "  3831|         0|            0|            0|  0.00%|    if mode in (\"nearest\", \"area\", \"nearest-exact\"):\n",
      "  3832|         0|            0|            0|  0.00%|        if align_corners is not None:\n",
      "  3833|         0|            0|            0|  0.00%|            raise ValueError(\n",
      "  3834|         0|            0|            0|  0.00%|                \"align_corners option can only be set with the \"\n",
      "  3835|         0|            0|            0|  0.00%|                \"interpolating modes: linear | bilinear | bicubic | trilinear\"\n",
      "  3836|         0|            0|            0|  0.00%|            )\n",
      "  3837|         0|            0|            0|  0.00%|    else:\n",
      "  3838|         0|            0|            0|  0.00%|        if align_corners is None:\n",
      "  3839|         0|            0|            0|  0.00%|            align_corners = False\n",
      "  3840|         0|            0|            0|  0.00%|\n",
      "  3841|         0|            0|            0|  0.00%|    dim = input.dim() - 2  # Number of spatial dimensions.\n",
      "  3842|         0|            0|            0|  0.00%|\n",
      "  3843|         0|            0|            0|  0.00%|    # Process size and scale_factor.  Validate that exactly one is set.\n",
      "  3844|         0|            0|            0|  0.00%|    # Validate its length if it is a list, or expand it if it is a scalar.\n",
      "  3845|         0|            0|            0|  0.00%|    # After this block, exactly one of output_size and scale_factors will\n",
      "  3846|         0|            0|            0|  0.00%|    # be non-None, and it will be a list (or tuple).\n",
      "  3847|         0|            0|            0|  0.00%|    if size is not None and scale_factor is not None:\n",
      "  3848|         0|            0|            0|  0.00%|        raise ValueError(\"only one of size or scale_factor should be defined\")\n",
      "  3849|         0|            0|            0|  0.00%|    elif size is not None:\n",
      "  3850|         0|            0|            0|  0.00%|        assert scale_factor is None\n",
      "  3851|         0|            0|            0|  0.00%|        scale_factors = None\n",
      "  3852|         0|            0|            0|  0.00%|        if isinstance(size, (list, tuple)):\n",
      "  3853|         0|            0|            0|  0.00%|            if len(size) != dim:\n",
      "  3854|         0|            0|            0|  0.00%|                raise ValueError(\n",
      "  3855|         0|            0|            0|  0.00%|                    \"Input and output must have the same number of spatial dimensions, but got \"\n",
      "  3856|         0|            0|            0|  0.00%|                    f\"input with with spatial dimensions of {list(input.shape[2:])} and output size of {size}. \"\n",
      "  3857|         0|            0|            0|  0.00%|                    \"Please provide input tensor in (N, C, d1, d2, ...,dK) format and \"\n",
      "  3858|         0|            0|            0|  0.00%|                    \"output size in (o1, o2, ...,oK) format.\"\n",
      "  3859|         0|            0|            0|  0.00%|\n",
      "  3860|         0|            0|            0|  0.00%|                )\n",
      "  3861|         0|            0|            0|  0.00%|            output_size = size\n",
      "  3862|         0|            0|            0|  0.00%|        else:\n",
      "  3863|         0|            0|            0|  0.00%|            output_size = [size for _ in range(dim)]\n",
      "  3864|         0|            0|            0|  0.00%|    elif scale_factor is not None:\n",
      "  3865|         0|            0|            0|  0.00%|        assert size is None\n",
      "  3866|         0|            0|            0|  0.00%|        output_size = None\n",
      "  3867|         0|            0|            0|  0.00%|        if isinstance(scale_factor, (list, tuple)):\n",
      "  3868|         0|            0|            0|  0.00%|            if len(scale_factor) != dim:\n",
      "  3869|         0|            0|            0|  0.00%|                raise ValueError(\n",
      "  3870|         0|            0|            0|  0.00%|                    \"Input and scale_factor must have the same number of spatial dimensions, but \"\n",
      "  3871|         0|            0|            0|  0.00%|                    f\"got input with spatial dimensions of {list(input.shape[2:])} and \"\n",
      "  3872|         0|            0|            0|  0.00%|                    f\"scale_factor of shape {scale_factor}. \"\n",
      "  3873|         0|            0|            0|  0.00%|                    \"Please provide input tensor in (N, C, d1, d2, ...,dK) format and \"\n",
      "  3874|         0|            0|            0|  0.00%|                    \"scale_factor in (s1, s2, ...,sK) format.\"\n",
      "  3875|         0|            0|            0|  0.00%|                )\n",
      "  3876|         0|            0|            0|  0.00%|            scale_factors = scale_factor\n",
      "  3877|         0|            0|            0|  0.00%|        else:\n",
      "  3878|         0|            0|            0|  0.00%|            scale_factors = [scale_factor for _ in range(dim)]\n",
      "  3879|         0|            0|            0|  0.00%|    else:\n",
      "  3880|         0|            0|            0|  0.00%|        raise ValueError(\"either size or scale_factor should be defined\")\n",
      "  3881|         0|            0|            0|  0.00%|\n",
      "  3882|         0|            0|            0|  0.00%|    if recompute_scale_factor is not None and recompute_scale_factor and size is not None:\n",
      "  3883|         0|            0|            0|  0.00%|        raise ValueError(\"recompute_scale_factor is not meaningful with an explicit size.\")\n",
      "  3884|         0|            0|            0|  0.00%|\n",
      "  3885|         0|            0|            0|  0.00%|    # \"area\" mode always requires an explicit size rather than scale factor.\n",
      "  3886|         0|            0|            0|  0.00%|    # Re-use the recompute_scale_factor code path.\n",
      "  3887|         0|            0|            0|  0.00%|    if mode == \"area\" and output_size is None:\n",
      "  3888|         0|            0|            0|  0.00%|        recompute_scale_factor = True\n",
      "  3889|         0|            0|            0|  0.00%|\n",
      "  3890|         0|            0|            0|  0.00%|    if recompute_scale_factor is not None and recompute_scale_factor:\n",
      "  3891|         0|            0|            0|  0.00%|        # We compute output_size here, then un-set scale_factors.\n",
      "  3892|         0|            0|            0|  0.00%|        # The C++ code will recompute it based on the (integer) output size.\n",
      "  3893|         0|            0|            0|  0.00%|        if not torch.jit.is_scripting() and torch._C._get_tracing_state():\n",
      "  3894|         0|            0|            0|  0.00%|            # make scale_factor a tensor in tracing so constant doesn't get baked in\n",
      "  3895|         0|            0|            0|  0.00%|            output_size = [\n",
      "  3896|         0|            0|            0|  0.00%|                (torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))\n",
      "  3897|         0|            0|            0|  0.00%|                for i in range(dim)\n",
      "  3898|         0|            0|            0|  0.00%|            ]\n",
      "  3899|         0|            0|            0|  0.00%|        else:\n",
      "  3900|         0|            0|            0|  0.00%|            assert scale_factors is not None\n",
      "  3901|         0|            0|            0|  0.00%|            output_size = [int(math.floor(float(input.size(i + 2)) * scale_factors[i])) for i in range(dim)]\n",
      "  3902|         0|            0|            0|  0.00%|        scale_factors = None\n",
      "  3903|         0|            0|            0|  0.00%|\n",
      "  3904|         0|            0|            0|  0.00%|    if antialias and not (mode in (\"bilinear\", \"bicubic\") and input.ndim == 4):\n",
      "  3905|         0|            0|            0|  0.00%|        raise ValueError(\"Anti-alias option is only supported for bilinear and bicubic modes\")\n",
      "  3906|         0|            0|            0|  0.00%|\n",
      "  3907|         0|            0|            0|  0.00%|    if input.dim() == 3 and mode == \"nearest\":\n",
      "  3908|         0|            0|            0|  0.00%|        return torch._C._nn.upsample_nearest1d(input, output_size, scale_factors)\n",
      "  3909|         0|            0|            0|  0.00%|    if input.dim() == 4 and mode == \"nearest\":\n",
      "  3910|         0|            0|            0|  0.00%|        return torch._C._nn.upsample_nearest2d(input, output_size, scale_factors)\n",
      "  3911|         0|            0|            0|  0.00%|    if input.dim() == 5 and mode == \"nearest\":\n",
      "  3912|         0|            0|            0|  0.00%|        return torch._C._nn.upsample_nearest3d(input, output_size, scale_factors)\n",
      "  3913|         0|            0|            0|  0.00%|\n",
      "  3914|         0|            0|            0|  0.00%|    if input.dim() == 3 and mode == \"nearest-exact\":\n",
      "  3915|         0|            0|            0|  0.00%|        return torch._C._nn._upsample_nearest_exact1d(input, output_size, scale_factors)\n",
      "  3916|         0|            0|            0|  0.00%|    if input.dim() == 4 and mode == \"nearest-exact\":\n",
      "  3917|         0|            0|            0|  0.00%|        return torch._C._nn._upsample_nearest_exact2d(input, output_size, scale_factors)\n",
      "  3918|         0|            0|            0|  0.00%|    if input.dim() == 5 and mode == \"nearest-exact\":\n",
      "  3919|         0|            0|            0|  0.00%|        return torch._C._nn._upsample_nearest_exact3d(input, output_size, scale_factors)\n",
      "  3920|         0|            0|            0|  0.00%|\n",
      "  3921|         0|            0|            0|  0.00%|    if input.dim() == 3 and mode == \"area\":\n",
      "  3922|         0|            0|            0|  0.00%|        assert output_size is not None\n",
      "  3923|         0|            0|            0|  0.00%|        return adaptive_avg_pool1d(input, output_size)\n",
      "  3924|         0|            0|            0|  0.00%|    if input.dim() == 4 and mode == \"area\":\n",
      "  3925|         0|            0|            0|  0.00%|        assert output_size is not None\n",
      "  3926|         0|            0|            0|  0.00%|        return adaptive_avg_pool2d(input, output_size)\n",
      "  3927|         0|            0|            0|  0.00%|    if input.dim() == 5 and mode == \"area\":\n",
      "  3928|         0|            0|            0|  0.00%|        assert output_size is not None\n",
      "  3929|         0|            0|            0|  0.00%|        return adaptive_avg_pool3d(input, output_size)\n",
      "  3930|         0|            0|            0|  0.00%|\n",
      "  3931|         0|            0|            0|  0.00%|    if input.dim() == 3 and mode == \"linear\":\n",
      "  3932|         0|            0|            0|  0.00%|        assert align_corners is not None\n",
      "  3933|         0|            0|            0|  0.00%|        return torch._C._nn.upsample_linear1d(input, output_size, align_corners, scale_factors)\n",
      "  3934|         0|            0|            0|  0.00%|    if input.dim() == 4 and mode == \"bilinear\":\n",
      "  3935|         0|            0|            0|  0.00%|        assert align_corners is not None\n",
      "  3936|         0|            0|            0|  0.00%|        if antialias:\n",
      "  3937|         0|            0|            0|  0.00%|            return torch._C._nn._upsample_bilinear2d_aa(input, output_size, align_corners, scale_factors)\n",
      "  3938|         0|            0|            0|  0.00%|        return torch._C._nn.upsample_bilinear2d(input, output_size, align_corners, scale_factors)\n",
      "  3939|         0|            0|            0|  0.00%|    if input.dim() == 5 and mode == \"trilinear\":\n",
      "  3940|         0|            0|            0|  0.00%|        assert align_corners is not None\n",
      "  3941|         0|            0|            0|  0.00%|        return torch._C._nn.upsample_trilinear3d(input, output_size, align_corners, scale_factors)\n",
      "  3942|         0|            0|            0|  0.00%|    if input.dim() == 4 and mode == \"bicubic\":\n",
      "  3943|         0|            0|            0|  0.00%|        assert align_corners is not None\n",
      "  3944|         0|            0|            0|  0.00%|        if antialias:\n",
      "  3945|         0|            0|            0|  0.00%|            return torch._C._nn._upsample_bicubic2d_aa(input, output_size, align_corners, scale_factors)\n",
      "  3946|         0|            0|            0|  0.00%|        return torch._C._nn.upsample_bicubic2d(input, output_size, align_corners, scale_factors)\n",
      "  3947|         0|            0|            0|  0.00%|\n",
      "  3948|         0|            0|            0|  0.00%|    if input.dim() == 3 and mode == \"bilinear\":\n",
      "  3949|         0|            0|            0|  0.00%|        raise NotImplementedError(\"Got 3D input, but bilinear mode needs 4D input\")\n",
      "  3950|         0|            0|            0|  0.00%|    if input.dim() == 3 and mode == \"trilinear\":\n",
      "  3951|         0|            0|            0|  0.00%|        raise NotImplementedError(\"Got 3D input, but trilinear mode needs 5D input\")\n",
      "  3952|         0|            0|            0|  0.00%|    if input.dim() == 4 and mode == \"linear\":\n",
      "  3953|         0|            0|            0|  0.00%|        raise NotImplementedError(\"Got 4D input, but linear mode needs 3D input\")\n",
      "  3954|         0|            0|            0|  0.00%|    if input.dim() == 4 and mode == \"trilinear\":\n",
      "  3955|         0|            0|            0|  0.00%|        raise NotImplementedError(\"Got 4D input, but trilinear mode needs 5D input\")\n",
      "  3956|         0|            0|            0|  0.00%|    if input.dim() == 5 and mode == \"linear\":\n",
      "  3957|         0|            0|            0|  0.00%|        raise NotImplementedError(\"Got 5D input, but linear mode needs 3D input\")\n",
      "  3958|         0|            0|            0|  0.00%|    if input.dim() == 5 and mode == \"bilinear\":\n",
      "  3959|         0|            0|            0|  0.00%|        raise NotImplementedError(\"Got 5D input, but bilinear mode needs 4D input\")\n",
      "  3960|         0|            0|            0|  0.00%|\n",
      "  3961|         0|            0|            0|  0.00%|    raise NotImplementedError(\n",
      "  3962|         0|            0|            0|  0.00%|        \"Input Error: Only 3D, 4D and 5D input Tensors supported\"\n",
      "  3963|         0|            0|            0|  0.00%|        \" (got {}D) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact\"\n",
      "  3964|         0|            0|            0|  0.00%|        \" (got {})\".format(input.dim(), mode)\n",
      "  3965|         0|            0|            0|  0.00%|    )\n",
      "  3966|         0|            0|            0|  0.00%|\n",
      "  3967|         0|            0|            0|  0.00%|\n",
      "  3968|         0|            0|            0|  0.00%|if interpolate.__doc__:\n",
      "  3969|         0|            0|            0|  0.00%|    interpolate.__doc__ = interpolate.__doc__.format(**reproducibility_notes)\n",
      "  3970|         0|            0|            0|  0.00%|\n",
      "  3971|         0|            0|            0|  0.00%|\n",
      "  3972|         0|            0|            0|  0.00%|@_overload  # noqa: F811\n",
      "  3973|         0|            0|            0|  0.00%|def upsample_nearest(input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None) -> Tensor:  # noqa: F811\n",
      "  3974|         0|            0|            0|  0.00%|    pass\n",
      "  3975|         0|            0|            0|  0.00%|\n",
      "  3976|         0|            0|            0|  0.00%|\n",
      "  3977|         0|            0|            0|  0.00%|@_overload  # noqa: F811\n",
      "  3978|         0|            0|            0|  0.00%|def upsample_nearest(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None) -> Tensor:  # noqa: F811\n",
      "  3979|         0|            0|            0|  0.00%|    pass\n",
      "  3980|         0|            0|            0|  0.00%|\n",
      "  3981|         0|            0|            0|  0.00%|\n",
      "  3982|         0|            0|            0|  0.00%|def upsample_nearest(input, size=None, scale_factor=None):  # noqa: F811\n",
      "  3983|         0|            0|            0|  0.00%|    r\"\"\"Upsamples the input, using nearest neighbours' pixel values.\n",
      "  3984|         0|            0|            0|  0.00%|\n",
      "  3985|         0|            0|            0|  0.00%|    .. warning::\n",
      "  3986|         0|            0|            0|  0.00%|        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.\n",
      "  3987|         0|            0|            0|  0.00%|        This is equivalent with ``nn.functional.interpolate(..., mode='nearest')``.\n",
      "  3988|         0|            0|            0|  0.00%|\n",
      "  3989|         0|            0|            0|  0.00%|    Currently spatial and volumetric upsampling are supported (i.e. expected\n",
      "  3990|         0|            0|            0|  0.00%|    inputs are 4 or 5 dimensional).\n",
      "  3991|         0|            0|            0|  0.00%|\n",
      "  3992|         0|            0|            0|  0.00%|    Args:\n",
      "  3993|         0|            0|            0|  0.00%|        input (Tensor): input\n",
      "  3994|         0|            0|            0|  0.00%|        size (int or Tuple[int, int] or Tuple[int, int, int]): output spatia\n",
      "  3995|         0|            0|            0|  0.00%|            size.\n",
      "  3996|         0|            0|            0|  0.00%|        scale_factor (int): multiplier for spatial size. Has to be an integer.\n",
      "  3997|         0|            0|            0|  0.00%|\n",
      "  3998|         0|            0|            0|  0.00%|    Note:\n",
      "  3999|         0|            0|            0|  0.00%|        {backward_reproducibility_note}\n",
      "  4000|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  4001|         0|            0|            0|  0.00%|    # DeprecationWarning is ignored by default\n",
      "  4002|         0|            0|            0|  0.00%|    warnings.warn(\"nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.\")\n",
      "  4003|         0|            0|            0|  0.00%|    return interpolate(input, size, scale_factor, mode=\"nearest\")\n",
      "  4004|         0|            0|            0|  0.00%|\n",
      "  4005|         0|            0|            0|  0.00%|\n",
      "  4006|         0|            0|            0|  0.00%|if upsample_nearest.__doc__:\n",
      "  4007|         0|            0|            0|  0.00%|    upsample_nearest.__doc__ = upsample_nearest.__doc__.format(**reproducibility_notes)\n",
      "  4008|         0|            0|            0|  0.00%|\n",
      "  4009|         0|            0|            0|  0.00%|\n",
      "  4010|         0|            0|            0|  0.00%|@_overload  # noqa: F811\n",
      "  4011|         0|            0|            0|  0.00%|def upsample_bilinear(\n",
      "  4012|         0|            0|            0|  0.00%|    input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None\n",
      "  4013|         0|            0|            0|  0.00%|) -> Tensor:  # noqa: F811\n",
      "  4014|         0|            0|            0|  0.00%|    pass\n",
      "  4015|         0|            0|            0|  0.00%|\n",
      "  4016|         0|            0|            0|  0.00%|\n",
      "  4017|         0|            0|            0|  0.00%|@_overload  # noqa: F811\n",
      "  4018|         0|            0|            0|  0.00%|def upsample_bilinear(  # noqa: F811\n",
      "  4019|         0|            0|            0|  0.00%|    input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None\n",
      "  4020|         0|            0|            0|  0.00%|) -> Tensor:  # noqa: F811\n",
      "  4021|         0|            0|            0|  0.00%|    pass\n",
      "  4022|         0|            0|            0|  0.00%|\n",
      "  4023|         0|            0|            0|  0.00%|\n",
      "  4024|         0|            0|            0|  0.00%|@_overload  # noqa: F811\n",
      "  4025|         0|            0|            0|  0.00%|def upsample_bilinear(  # noqa: F811\n",
      "  4026|         0|            0|            0|  0.00%|    input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None\n",
      "  4027|         0|            0|            0|  0.00%|) -> Tensor:  # noqa: F811\n",
      "  4028|         0|            0|            0|  0.00%|    pass\n",
      "  4029|         0|            0|            0|  0.00%|\n",
      "  4030|         0|            0|            0|  0.00%|\n",
      "  4031|         0|            0|            0|  0.00%|@_overload  # noqa: F811\n",
      "  4032|         0|            0|            0|  0.00%|def upsample_bilinear(  # noqa: F811\n",
      "  4033|         0|            0|            0|  0.00%|    input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[List[float]] = None\n",
      "  4034|         0|            0|            0|  0.00%|) -> Tensor:  # noqa: F811\n",
      "  4035|         0|            0|            0|  0.00%|    pass\n",
      "  4036|         0|            0|            0|  0.00%|\n",
      "  4037|         0|            0|            0|  0.00%|\n",
      "  4038|         0|            0|            0|  0.00%|def upsample_bilinear(input, size=None, scale_factor=None):  # noqa: F811\n",
      "  4039|         0|            0|            0|  0.00%|    r\"\"\"Upsamples the input, using bilinear upsampling.\n",
      "  4040|         0|            0|            0|  0.00%|\n",
      "  4041|         0|            0|            0|  0.00%|    .. warning::\n",
      "  4042|         0|            0|            0|  0.00%|        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.\n",
      "  4043|         0|            0|            0|  0.00%|        This is equivalent with\n",
      "  4044|         0|            0|            0|  0.00%|        ``nn.functional.interpolate(..., mode='bilinear', align_corners=True)``.\n",
      "  4045|         0|            0|            0|  0.00%|\n",
      "  4046|         0|            0|            0|  0.00%|    Expected inputs are spatial (4 dimensional). Use `upsample_trilinear` fo\n",
      "  4047|         0|            0|            0|  0.00%|    volumetric (5 dimensional) inputs.\n",
      "  4048|         0|            0|            0|  0.00%|\n",
      "  4049|         0|            0|            0|  0.00%|    Args:\n",
      "  4050|         0|            0|            0|  0.00%|        input (Tensor): input\n",
      "  4051|         0|            0|            0|  0.00%|        size (int or Tuple[int, int]): output spatial size.\n",
      "  4052|         0|            0|            0|  0.00%|        scale_factor (int or Tuple[int, int]): multiplier for spatial size\n",
      "  4053|         0|            0|            0|  0.00%|\n",
      "  4054|         0|            0|            0|  0.00%|    Note:\n",
      "  4055|         0|            0|            0|  0.00%|        {backward_reproducibility_note}\n",
      "  4056|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  4057|         0|            0|            0|  0.00%|    # DeprecationWarning is ignored by default\n",
      "  4058|         0|            0|            0|  0.00%|    warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n",
      "  4059|         0|            0|            0|  0.00%|    return interpolate(input, size, scale_factor, mode=\"bilinear\", align_corners=True)\n",
      "  4060|         0|            0|            0|  0.00%|\n",
      "  4061|         0|            0|            0|  0.00%|\n",
      "  4062|         0|            0|            0|  0.00%|if upsample_bilinear.__doc__:\n",
      "  4063|         0|            0|            0|  0.00%|    upsample_bilinear.__doc__ = upsample_bilinear.__doc__.format(**reproducibility_notes)\n",
      "  4064|         0|            0|            0|  0.00%|\n",
      "  4065|         0|            0|            0|  0.00%|GRID_SAMPLE_INTERPOLATION_MODES = {\n",
      "  4066|         0|            0|            0|  0.00%|    \"bilinear\": 0,\n",
      "  4067|         0|            0|            0|  0.00%|    \"nearest\": 1,\n",
      "  4068|         0|            0|            0|  0.00%|    \"bicubic\": 2,\n",
      "  4069|         0|            0|            0|  0.00%|}\n",
      "  4070|         0|            0|            0|  0.00%|\n",
      "  4071|         0|            0|            0|  0.00%|GRID_SAMPLE_PADDING_MODES = {\n",
      "  4072|         0|            0|            0|  0.00%|    \"zeros\": 0,\n",
      "  4073|         0|            0|            0|  0.00%|    \"border\": 1,\n",
      "  4074|         0|            0|            0|  0.00%|    \"reflection\": 2,\n",
      "  4075|         0|            0|            0|  0.00%|}\n",
      "  4076|         0|            0|            0|  0.00%|\n",
      "  4077|         0|            0|            0|  0.00%|\n",
      "  4078|         0|            0|            0|  0.00%|def grid_sample(\n",
      "  4079|         0|            0|            0|  0.00%|    input: Tensor,\n",
      "  4080|         0|            0|            0|  0.00%|    grid: Tensor,\n",
      "  4081|         0|            0|            0|  0.00%|    mode: str = \"bilinear\",\n",
      "  4082|         0|            0|            0|  0.00%|    padding_mode: str = \"zeros\",\n",
      "  4083|         0|            0|            0|  0.00%|    align_corners: Optional[bool] = None,\n",
      "  4084|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  4085|         0|            0|            0|  0.00%|    r\"\"\"Given an :attr:`input` and a flow-field :attr:`grid`, computes the\n",
      "  4086|         0|            0|            0|  0.00%|    ``output`` using :attr:`input` values and pixel locations from :attr:`grid`.\n",
      "  4087|         0|            0|            0|  0.00%|\n",
      "  4088|         0|            0|            0|  0.00%|    Currently, only spatial (4-D) and volumetric (5-D) :attr:`input` are\n",
      "  4089|         0|            0|            0|  0.00%|    supported.\n",
      "  4090|         0|            0|            0|  0.00%|\n",
      "  4091|         0|            0|            0|  0.00%|    In the spatial (4-D) case, for :attr:`input` with shape\n",
      "  4092|         0|            0|            0|  0.00%|    :math:`(N, C, H_\\text{in}, W_\\text{in})` and :attr:`grid` with shape\n",
      "  4093|         0|            0|            0|  0.00%|    :math:`(N, H_\\text{out}, W_\\text{out}, 2)`, the output will have shape\n",
      "  4094|         0|            0|            0|  0.00%|    :math:`(N, C, H_\\text{out}, W_\\text{out})`.\n",
      "  4095|         0|            0|            0|  0.00%|\n",
      "  4096|         0|            0|            0|  0.00%|    For each output location ``output[n, :, h, w]``, the size-2 vector\n",
      "  4097|         0|            0|            0|  0.00%|    ``grid[n, h, w]`` specifies :attr:`input` pixel locations ``x`` and ``y``,\n",
      "  4098|         0|            0|            0|  0.00%|    which are used to interpolate the output value ``output[n, :, h, w]``.\n",
      "  4099|         0|            0|            0|  0.00%|    In the case of 5D inputs, ``grid[n, d, h, w]`` specifies the\n",
      "  4100|         0|            0|            0|  0.00%|    ``x``, ``y``, ``z`` pixel locations for interpolating\n",
      "  4101|         0|            0|            0|  0.00%|    ``output[n, :, d, h, w]``. :attr:`mode` argument specifies ``nearest`` or\n",
      "  4102|         0|            0|            0|  0.00%|    ``bilinear`` interpolation method to sample the input pixels.\n",
      "  4103|         0|            0|            0|  0.00%|\n",
      "  4104|         0|            0|            0|  0.00%|    :attr:`grid` specifies the sampling pixel locations normalized by the\n",
      "  4105|         0|            0|            0|  0.00%|    :attr:`input` spatial dimensions. Therefore, it should have most values in\n",
      "  4106|         0|            0|            0|  0.00%|    the range of ``[-1, 1]``. For example, values ``x = -1, y = -1`` is the\n",
      "  4107|         0|            0|            0|  0.00%|    left-top pixel of :attr:`input`, and values  ``x = 1, y = 1`` is the\n",
      "  4108|         0|            0|            0|  0.00%|    right-bottom pixel of :attr:`input`.\n",
      "  4109|         0|            0|            0|  0.00%|\n",
      "  4110|         0|            0|            0|  0.00%|    If :attr:`grid` has values outside the range of ``[-1, 1]``, the corresponding\n",
      "  4111|         0|            0|            0|  0.00%|    outputs are handled as defined by :attr:`padding_mode`. Options are\n",
      "  4112|         0|            0|            0|  0.00%|\n",
      "  4113|         0|            0|            0|  0.00%|        * ``padding_mode=\"zeros\"``: use ``0`` for out-of-bound grid locations,\n",
      "  4114|         0|            0|            0|  0.00%|        * ``padding_mode=\"border\"``: use border values for out-of-bound grid locations,\n",
      "  4115|         0|            0|            0|  0.00%|        * ``padding_mode=\"reflection\"``: use values at locations reflected by\n",
      "  4116|         0|            0|            0|  0.00%|          the border for out-of-bound grid locations. For location far away\n",
      "  4117|         0|            0|            0|  0.00%|          from the border, it will keep being reflected until becoming in bound,\n",
      "  4118|         0|            0|            0|  0.00%|          e.g., (normalized) pixel location ``x = -3.5`` reflects by border ``-1``\n",
      "  4119|         0|            0|            0|  0.00%|          and becomes ``x' = 1.5``, then reflects by border ``1`` and becomes\n",
      "  4120|         0|            0|            0|  0.00%|          ``x'' = -0.5``.\n",
      "  4121|         0|            0|            0|  0.00%|\n",
      "  4122|         0|            0|            0|  0.00%|    Note:\n",
      "  4123|         0|            0|            0|  0.00%|        This function is often used in conjunction with :func:`affine_grid`\n",
      "  4124|         0|            0|            0|  0.00%|        to build `Spatial Transformer Networks`_ .\n",
      "  4125|         0|            0|            0|  0.00%|\n",
      "  4126|         0|            0|            0|  0.00%|    Note:\n",
      "  4127|         0|            0|            0|  0.00%|        When using the CUDA backend, this operation may induce nondeterministic\n",
      "  4128|         0|            0|            0|  0.00%|        behaviour in its backward pass that is not easily switched off.\n",
      "  4129|         0|            0|            0|  0.00%|        Please see the notes on :doc:`/notes/randomness` for background.\n",
      "  4130|         0|            0|            0|  0.00%|\n",
      "  4131|         0|            0|            0|  0.00%|    Note:\n",
      "  4132|         0|            0|            0|  0.00%|        NaN values in :attr:`grid` would be interpreted as ``-1``.\n",
      "  4133|         0|            0|            0|  0.00%|\n",
      "  4134|         0|            0|            0|  0.00%|    Args:\n",
      "  4135|         0|            0|            0|  0.00%|        input (Tensor): input of shape :math:`(N, C, H_\\text{in}, W_\\text{in})` (4-D case)\n",
      "  4136|         0|            0|            0|  0.00%|                        or :math:`(N, C, D_\\text{in}, H_\\text{in}, W_\\text{in})` (5-D case)\n",
      "  4137|         0|            0|            0|  0.00%|        grid (Tensor): flow-field of shape :math:`(N, H_\\text{out}, W_\\text{out}, 2)` (4-D case)\n",
      "  4138|         0|            0|            0|  0.00%|                       or :math:`(N, D_\\text{out}, H_\\text{out}, W_\\text{out}, 3)` (5-D case)\n",
      "  4139|         0|            0|            0|  0.00%|        mode (str): interpolation mode to calculate output values\n",
      "  4140|         0|            0|            0|  0.00%|            ``'bilinear'`` | ``'nearest'`` | ``'bicubic'``. Default: ``'bilinear'``\n",
      "  4141|         0|            0|            0|  0.00%|            Note: ``mode='bicubic'`` supports only 4-D input.\n",
      "  4142|         0|            0|            0|  0.00%|            When ``mode='bilinear'`` and the input is 5-D, the interpolation mode\n",
      "  4143|         0|            0|            0|  0.00%|            used internally will actually be trilinear. However, when the input is 4-D,\n",
      "  4144|         0|            0|            0|  0.00%|            the interpolation mode will legitimately be bilinear.\n",
      "  4145|         0|            0|            0|  0.00%|        padding_mode (str): padding mode for outside grid values\n",
      "  4146|         0|            0|            0|  0.00%|            ``'zeros'`` | ``'border'`` | ``'reflection'``. Default: ``'zeros'``\n",
      "  4147|         0|            0|            0|  0.00%|        align_corners (bool, optional): Geometrically, we consider the pixels of the\n",
      "  4148|         0|            0|            0|  0.00%|            input  as squares rather than points.\n",
      "  4149|         0|            0|            0|  0.00%|            If set to ``True``, the extrema (``-1`` and ``1``) are considered as referring\n",
      "  4150|         0|            0|            0|  0.00%|            to the center points of the input's corner pixels. If set to ``False``, they\n",
      "  4151|         0|            0|            0|  0.00%|            are instead considered as referring to the corner points of the input's corner\n",
      "  4152|         0|            0|            0|  0.00%|            pixels, making the sampling more resolution agnostic.\n",
      "  4153|         0|            0|            0|  0.00%|            This option parallels the ``align_corners`` option in\n",
      "  4154|         0|            0|            0|  0.00%|            :func:`interpolate`, and so whichever option is used here\n",
      "  4155|         0|            0|            0|  0.00%|            should also be used there to resize the input image before grid sampling.\n",
      "  4156|         0|            0|            0|  0.00%|            Default: ``False``\n",
      "  4157|         0|            0|            0|  0.00%|\n",
      "  4158|         0|            0|            0|  0.00%|    Returns:\n",
      "  4159|         0|            0|            0|  0.00%|        output (Tensor): output Tensor\n",
      "  4160|         0|            0|            0|  0.00%|\n",
      "  4161|         0|            0|            0|  0.00%|    .. _`Spatial Transformer Networks`:\n",
      "  4162|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1506.02025\n",
      "  4163|         0|            0|            0|  0.00%|\n",
      "  4164|         0|            0|            0|  0.00%|    .. warning::\n",
      "  4165|         0|            0|            0|  0.00%|        When ``align_corners = True``, the grid positions depend on the pixel\n",
      "  4166|         0|            0|            0|  0.00%|        size relative to the input image size, and so the locations sampled by\n",
      "  4167|         0|            0|            0|  0.00%|        :func:`grid_sample` will differ for the same input given at different\n",
      "  4168|         0|            0|            0|  0.00%|        resolutions (that is, after being upsampled or downsampled).\n",
      "  4169|         0|            0|            0|  0.00%|        The default behavior up to version 1.2.0 was ``align_corners = True``.\n",
      "  4170|         0|            0|            0|  0.00%|        Since then, the default behavior has been changed to ``align_corners = False``,\n",
      "  4171|         0|            0|            0|  0.00%|        in order to bring it in line with the default for :func:`interpolate`.\n",
      "  4172|         0|            0|            0|  0.00%|\n",
      "  4173|         0|            0|            0|  0.00%|    .. note::\n",
      "  4174|         0|            0|            0|  0.00%|        ``mode='bicubic'`` is implemented using the `cubic convolution algorithm`_ with :math:`\\alpha=-0.75`.\n",
      "  4175|         0|            0|            0|  0.00%|        The constant :math:`\\alpha` might be different from packages to packages.\n",
      "  4176|         0|            0|            0|  0.00%|        For example, `PIL`_ and `OpenCV`_ use -0.5 and -0.75 respectively.\n",
      "  4177|         0|            0|            0|  0.00%|        This algorithm may \"overshoot\" the range of values it's interpolating.\n",
      "  4178|         0|            0|            0|  0.00%|        For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255].\n",
      "  4179|         0|            0|            0|  0.00%|        Clamp the results with :func: `torch.clamp` to ensure they are within the valid range.\n",
      "  4180|         0|            0|            0|  0.00%|    .. _`cubic convolution algorithm`: https://en.wikipedia.org/wiki/Bicubic_interpolation\n",
      "  4181|         0|            0|            0|  0.00%|    .. _`PIL`: https://github.com/python-pillow/Pillow/blob/4634eafe3c695a014267eefdce830b4a825beed7/src/libImaging/Resample.c#L51\n",
      "  4182|         0|            0|            0|  0.00%|    .. _`OpenCV`: https://github.com/opencv/opencv/blob/f345ed564a06178670750bad59526cfa4033be55/modules/imgproc/src/resize.cpp#L908\n",
      "  4183|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  4184|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, grid):\n",
      "  4185|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  4186|         0|            0|            0|  0.00%|            grid_sample, (input, grid), input, grid, mode=mode, padding_mode=padding_mode, align_corners=align_corners\n",
      "  4187|         0|            0|            0|  0.00%|        )\n",
      "  4188|         0|            0|            0|  0.00%|    if mode != \"bilinear\" and mode != \"nearest\" and mode != \"bicubic\":\n",
      "  4189|         0|            0|            0|  0.00%|        raise ValueError(\n",
      "  4190|         0|            0|            0|  0.00%|            \"nn.functional.grid_sample(): expected mode to be \"\n",
      "  4191|         0|            0|            0|  0.00%|            \"'bilinear', 'nearest' or 'bicubic', but got: '{}'\".format(mode)\n",
      "  4192|         0|            0|            0|  0.00%|        )\n",
      "  4193|         0|            0|            0|  0.00%|    if padding_mode != \"zeros\" and padding_mode != \"border\" and padding_mode != \"reflection\":\n",
      "  4194|         0|            0|            0|  0.00%|        raise ValueError(\n",
      "  4195|         0|            0|            0|  0.00%|            \"nn.functional.grid_sample(): expected padding_mode \"\n",
      "  4196|         0|            0|            0|  0.00%|            \"to be 'zeros', 'border', or 'reflection', \"\n",
      "  4197|         0|            0|            0|  0.00%|            \"but got: '{}'\".format(padding_mode)\n",
      "  4198|         0|            0|            0|  0.00%|        )\n",
      "  4199|         0|            0|            0|  0.00%|\n",
      "  4200|         0|            0|            0|  0.00%|    if mode == \"bilinear\":\n",
      "  4201|         0|            0|            0|  0.00%|        mode_enum = 0\n",
      "  4202|         0|            0|            0|  0.00%|    elif mode == \"nearest\":\n",
      "  4203|         0|            0|            0|  0.00%|        mode_enum = 1\n",
      "  4204|         0|            0|            0|  0.00%|    else:  # mode == 'bicubic'\n",
      "  4205|         0|            0|            0|  0.00%|        mode_enum = 2\n",
      "  4206|         0|            0|            0|  0.00%|\n",
      "  4207|         0|            0|            0|  0.00%|    if padding_mode == \"zeros\":\n",
      "  4208|         0|            0|            0|  0.00%|        padding_mode_enum = 0\n",
      "  4209|         0|            0|            0|  0.00%|    elif padding_mode == \"border\":\n",
      "  4210|         0|            0|            0|  0.00%|        padding_mode_enum = 1\n",
      "  4211|         0|            0|            0|  0.00%|    else:  # padding_mode == 'reflection'\n",
      "  4212|         0|            0|            0|  0.00%|        padding_mode_enum = 2\n",
      "  4213|         0|            0|            0|  0.00%|\n",
      "  4214|         0|            0|            0|  0.00%|    if align_corners is None:\n",
      "  4215|         0|            0|            0|  0.00%|        warnings.warn(\n",
      "  4216|         0|            0|            0|  0.00%|            \"Default grid_sample and affine_grid behavior has changed \"\n",
      "  4217|         0|            0|            0|  0.00%|            \"to align_corners=False since 1.3.0. Please specify \"\n",
      "  4218|         0|            0|            0|  0.00%|            \"align_corners=True if the old behavior is desired. \"\n",
      "  4219|         0|            0|            0|  0.00%|            \"See the documentation of grid_sample for details.\"\n",
      "  4220|         0|            0|            0|  0.00%|        )\n",
      "  4221|         0|            0|            0|  0.00%|        align_corners = False\n",
      "  4222|         0|            0|            0|  0.00%|\n",
      "  4223|         0|            0|            0|  0.00%|    return torch.grid_sampler(input, grid, mode_enum, padding_mode_enum, align_corners)\n",
      "  4224|         0|            0|            0|  0.00%|\n",
      "  4225|         0|            0|            0|  0.00%|\n",
      "  4226|         0|            0|            0|  0.00%|def affine_grid(theta: Tensor, size: List[int], align_corners: Optional[bool] = None) -> Tensor:\n",
      "  4227|         0|            0|            0|  0.00%|    r\"\"\"Generates a 2D or 3D flow field (sampling grid), given a batch of\n",
      "  4228|         0|            0|            0|  0.00%|    affine matrices :attr:`theta`.\n",
      "  4229|         0|            0|            0|  0.00%|\n",
      "  4230|         0|            0|            0|  0.00%|    .. note::\n",
      "  4231|         0|            0|            0|  0.00%|        This function is often used in conjunction with :func:`grid_sample`\n",
      "  4232|         0|            0|            0|  0.00%|        to build `Spatial Transformer Networks`_ .\n",
      "  4233|         0|            0|            0|  0.00%|\n",
      "  4234|         0|            0|            0|  0.00%|    Args:\n",
      "  4235|         0|            0|            0|  0.00%|        theta (Tensor): input batch of affine matrices with shape\n",
      "  4236|         0|            0|            0|  0.00%|            (:math:`N \\times 2 \\times 3`) for 2D or\n",
      "  4237|         0|            0|            0|  0.00%|            (:math:`N \\times 3 \\times 4`) for 3D\n",
      "  4238|         0|            0|            0|  0.00%|        size (torch.Size): the target output image size.\n",
      "  4239|         0|            0|            0|  0.00%|            (:math:`N \\times C \\times H \\times W` for 2D or\n",
      "  4240|         0|            0|            0|  0.00%|            :math:`N \\times C \\times D \\times H \\times W` for 3D)\n",
      "  4241|         0|            0|            0|  0.00%|            Example: torch.Size((32, 3, 24, 24))\n",
      "  4242|         0|            0|            0|  0.00%|        align_corners (bool, optional): if ``True``, consider ``-1`` and ``1``\n",
      "  4243|         0|            0|            0|  0.00%|            to refer to the centers of the corner pixels rather than the image corners.\n",
      "  4244|         0|            0|            0|  0.00%|            Refer to :func:`grid_sample` for a more complete description.\n",
      "  4245|         0|            0|            0|  0.00%|            A grid generated by :func:`affine_grid` should be passed to :func:`grid_sample`\n",
      "  4246|         0|            0|            0|  0.00%|            with the same setting for this option.\n",
      "  4247|         0|            0|            0|  0.00%|            Default: ``False``\n",
      "  4248|         0|            0|            0|  0.00%|\n",
      "  4249|         0|            0|            0|  0.00%|    Returns:\n",
      "  4250|         0|            0|            0|  0.00%|        output (Tensor): output Tensor of size (:math:`N \\times H \\times W \\times 2`)\n",
      "  4251|         0|            0|            0|  0.00%|\n",
      "  4252|         0|            0|            0|  0.00%|    .. _`Spatial Transformer Networks`:\n",
      "  4253|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1506.02025\n",
      "  4254|         0|            0|            0|  0.00%|\n",
      "  4255|         0|            0|            0|  0.00%|    .. warning::\n",
      "  4256|         0|            0|            0|  0.00%|        When ``align_corners = True``, the grid positions depend on the pixel\n",
      "  4257|         0|            0|            0|  0.00%|        size relative to the input image size, and so the locations sampled by\n",
      "  4258|         0|            0|            0|  0.00%|        :func:`grid_sample` will differ for the same input given at different\n",
      "  4259|         0|            0|            0|  0.00%|        resolutions (that is, after being upsampled or downsampled).\n",
      "  4260|         0|            0|            0|  0.00%|        The default behavior up to version 1.2.0 was ``align_corners = True``.\n",
      "  4261|         0|            0|            0|  0.00%|        Since then, the default behavior has been changed to ``align_corners = False``,\n",
      "  4262|         0|            0|            0|  0.00%|        in order to bring it in line with the default for :func:`interpolate`.\n",
      "  4263|         0|            0|            0|  0.00%|    .. warning::\n",
      "  4264|         0|            0|            0|  0.00%|        When ``align_corners = True``, 2D affine transforms on 1D data and\n",
      "  4265|         0|            0|            0|  0.00%|        3D affine transforms on 2D data (that is, when one of the spatial\n",
      "  4266|         0|            0|            0|  0.00%|        dimensions has unit size) are ill-defined, and not an intended use case.\n",
      "  4267|         0|            0|            0|  0.00%|        This is not a problem when ``align_corners = False``.\n",
      "  4268|         0|            0|            0|  0.00%|        Up to version 1.2.0, all grid points along a unit dimension were\n",
      "  4269|         0|            0|            0|  0.00%|        considered arbitrarily to be at ``-1``.\n",
      "  4270|         0|            0|            0|  0.00%|        From version 1.3.0, under ``align_corners = True`` all grid points\n",
      "  4271|         0|            0|            0|  0.00%|        along a unit dimension are considered to be at ``0``\n",
      "  4272|         0|            0|            0|  0.00%|        (the center of the input image).\n",
      "  4273|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  4274|         0|            0|            0|  0.00%|    if has_torch_function_unary(theta):\n",
      "  4275|         0|            0|            0|  0.00%|        return handle_torch_function(affine_grid, (theta,), theta, size, align_corners=align_corners)\n",
      "  4276|         0|            0|            0|  0.00%|    if align_corners is None:\n",
      "  4277|         0|            0|            0|  0.00%|        warnings.warn(\n",
      "  4278|         0|            0|            0|  0.00%|            \"Default grid_sample and affine_grid behavior has changed \"\n",
      "  4279|         0|            0|            0|  0.00%|            \"to align_corners=False since 1.3.0. Please specify \"\n",
      "  4280|         0|            0|            0|  0.00%|            \"align_corners=True if the old behavior is desired. \"\n",
      "  4281|         0|            0|            0|  0.00%|            \"See the documentation of grid_sample for details.\"\n",
      "  4282|         0|            0|            0|  0.00%|        )\n",
      "  4283|         0|            0|            0|  0.00%|        align_corners = False\n",
      "  4284|         0|            0|            0|  0.00%|\n",
      "  4285|         0|            0|            0|  0.00%|    # enforce floating point dtype on theta\n",
      "  4286|         0|            0|            0|  0.00%|    if not theta.is_floating_point():\n",
      "  4287|         0|            0|            0|  0.00%|        raise ValueError(\"Expected theta to have floating point type, but got {}\".format(theta.dtype))\n",
      "  4288|         0|            0|            0|  0.00%|    # check that shapes and sizes match\n",
      "  4289|         0|            0|            0|  0.00%|    if len(size) == 4:\n",
      "  4290|         0|            0|            0|  0.00%|        if theta.dim() != 3 or theta.shape[-2] != 2 or theta.shape[-1] != 3:\n",
      "  4291|         0|            0|            0|  0.00%|            raise ValueError(\n",
      "  4292|         0|            0|            0|  0.00%|                \"Expected a batch of 2D affine matrices of shape Nx2x3 \"\n",
      "  4293|         0|            0|            0|  0.00%|                \"for size {}. Got {}.\".format(size, theta.shape)\n",
      "  4294|         0|            0|            0|  0.00%|            )\n",
      "  4295|         0|            0|            0|  0.00%|        spatial_size = size[-2:]  # spatial dimension sizes\n",
      "  4296|         0|            0|            0|  0.00%|    elif len(size) == 5:\n",
      "  4297|         0|            0|            0|  0.00%|        if theta.dim() != 3 or theta.shape[-2] != 3 or theta.shape[-1] != 4:\n",
      "  4298|         0|            0|            0|  0.00%|            raise ValueError(\n",
      "  4299|         0|            0|            0|  0.00%|                \"Expected a batch of 3D affine matrices of shape Nx3x4 \"\n",
      "  4300|         0|            0|            0|  0.00%|                \"for size {}. Got {}.\".format(size, theta.shape)\n",
      "  4301|         0|            0|            0|  0.00%|            )\n",
      "  4302|         0|            0|            0|  0.00%|        spatial_size = size[-3:]  # spatial dimension sizes\n",
      "  4303|         0|            0|            0|  0.00%|    else:\n",
      "  4304|         0|            0|            0|  0.00%|        raise NotImplementedError(\n",
      "  4305|         0|            0|            0|  0.00%|            \"affine_grid only supports 4D and 5D sizes, \"\n",
      "  4306|         0|            0|            0|  0.00%|            \"for 2D and 3D affine transforms, respectively. \"\n",
      "  4307|         0|            0|            0|  0.00%|            \"Got size {}.\".format(size)\n",
      "  4308|         0|            0|            0|  0.00%|        )\n",
      "  4309|         0|            0|            0|  0.00%|    # check for empty span\n",
      "  4310|         0|            0|            0|  0.00%|    if align_corners and min(spatial_size) == 1:\n",
      "  4311|         0|            0|            0|  0.00%|        warnings.warn(\n",
      "  4312|         0|            0|            0|  0.00%|            \"Since version 1.3.0, affine_grid behavior has changed \"\n",
      "  4313|         0|            0|            0|  0.00%|            \"for unit-size grids when align_corners=True. \"\n",
      "  4314|         0|            0|            0|  0.00%|            \"This is not an intended use case of affine_grid. \"\n",
      "  4315|         0|            0|            0|  0.00%|            \"See the documentation of affine_grid for details.\"\n",
      "  4316|         0|            0|            0|  0.00%|        )\n",
      "  4317|         0|            0|            0|  0.00%|    elif min(size) <= 0:\n",
      "  4318|         0|            0|            0|  0.00%|        raise ValueError(\"Expected non-zero, positive output size. Got {}\".format(size))\n",
      "  4319|         0|            0|            0|  0.00%|\n",
      "  4320|         0|            0|            0|  0.00%|    return torch.affine_grid_generator(theta, size, align_corners)\n",
      "  4321|         0|            0|            0|  0.00%|\n",
      "  4322|         0|            0|            0|  0.00%|\n",
      "  4323|         0|            0|            0|  0.00%|pad = _add_docstr(\n",
      "  4324|         0|            0|            0|  0.00%|    torch._C._nn.pad,\n",
      "  4325|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  4326|         0|            0|            0|  0.00%|pad(input, pad, mode=\"constant\", value=None) -> Tensor\n",
      "  4327|         0|            0|            0|  0.00%|\n",
      "  4328|         0|            0|            0|  0.00%|Pads tensor.\n",
      "  4329|         0|            0|            0|  0.00%|\n",
      "  4330|         0|            0|            0|  0.00%|Padding size:\n",
      "  4331|         0|            0|            0|  0.00%|    The padding size by which to pad some dimensions of :attr:`input`\n",
      "  4332|         0|            0|            0|  0.00%|    are described starting from the last dimension and moving forward.\n",
      "  4333|         0|            0|            0|  0.00%|    :math:`\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor` dimensions\n",
      "  4334|         0|            0|            0|  0.00%|    of ``input`` will be padded.\n",
      "  4335|         0|            0|            0|  0.00%|    For example, to pad only the last dimension of the input tensor, then\n",
      "  4336|         0|            0|            0|  0.00%|    :attr:`pad` has the form\n",
      "  4337|         0|            0|            0|  0.00%|    :math:`(\\text{padding\\_left}, \\text{padding\\_right})`;\n",
      "  4338|         0|            0|            0|  0.00%|    to pad the last 2 dimensions of the input tensor, then use\n",
      "  4339|         0|            0|            0|  0.00%|    :math:`(\\text{padding\\_left}, \\text{padding\\_right},`\n",
      "  4340|         0|            0|            0|  0.00%|    :math:`\\text{padding\\_top}, \\text{padding\\_bottom})`;\n",
      "  4341|         0|            0|            0|  0.00%|    to pad the last 3 dimensions, use\n",
      "  4342|         0|            0|            0|  0.00%|    :math:`(\\text{padding\\_left}, \\text{padding\\_right},`\n",
      "  4343|         0|            0|            0|  0.00%|    :math:`\\text{padding\\_top}, \\text{padding\\_bottom}`\n",
      "  4344|         0|            0|            0|  0.00%|    :math:`\\text{padding\\_front}, \\text{padding\\_back})`.\n",
      "  4345|         0|            0|            0|  0.00%|\n",
      "  4346|         0|            0|            0|  0.00%|Padding mode:\n",
      "  4347|         0|            0|            0|  0.00%|    See :class:`torch.nn.ConstantPad2d`, :class:`torch.nn.ReflectionPad2d`, and\n",
      "  4348|         0|            0|            0|  0.00%|    :class:`torch.nn.ReplicationPad2d` for concrete examples on how each of the\n",
      "  4349|         0|            0|            0|  0.00%|    padding modes works. Constant padding is implemented for arbitrary dimensions.\n",
      "  4350|         0|            0|            0|  0.00%|    Replicate and reflection padding are implemented for padding the last 3\n",
      "  4351|         0|            0|            0|  0.00%|    dimensions of a 4D or 5D input tensor, the last 2 dimensions of a 3D\n",
      "  4352|         0|            0|            0|  0.00%|    or 4D input tensor, or the last dimension of a 2D or 3D input tensor.\n",
      "  4353|         0|            0|            0|  0.00%|\n",
      "  4354|         0|            0|            0|  0.00%|Note:\n",
      "  4355|         0|            0|            0|  0.00%|    When using the CUDA backend, this operation may induce nondeterministic\n",
      "  4356|         0|            0|            0|  0.00%|    behaviour in its backward pass that is not easily switched off.\n",
      "  4357|         0|            0|            0|  0.00%|    Please see the notes on :doc:`/notes/randomness` for background.\n",
      "  4358|         0|            0|            0|  0.00%|\n",
      "  4359|         0|            0|            0|  0.00%|Args:\n",
      "  4360|         0|            0|            0|  0.00%|    input (Tensor): N-dimensional tensor\n",
      "  4361|         0|            0|            0|  0.00%|    pad (tuple): m-elements tuple, where\n",
      "  4362|         0|            0|            0|  0.00%|        :math:`\\frac{m}{2} \\leq` input dimensions and :math:`m` is even.\n",
      "  4363|         0|            0|            0|  0.00%|    mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.\n",
      "  4364|         0|            0|            0|  0.00%|        Default: ``'constant'``\n",
      "  4365|         0|            0|            0|  0.00%|    value: fill value for ``'constant'`` padding. Default: ``0``\n",
      "  4366|         0|            0|            0|  0.00%|\n",
      "  4367|         0|            0|            0|  0.00%|Examples::\n",
      "  4368|         0|            0|            0|  0.00%|\n",
      "  4369|         0|            0|            0|  0.00%|    >>> t4d = torch.empty(3, 3, 4, 2)\n",
      "  4370|         0|            0|            0|  0.00%|    >>> p1d = (1, 1) # pad last dim by 1 on each side\n",
      "  4371|         0|            0|            0|  0.00%|    >>> out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n",
      "  4372|         0|            0|            0|  0.00%|    >>> print(out.size())\n",
      "  4373|         0|            0|            0|  0.00%|    torch.Size([3, 3, 4, 4])\n",
      "  4374|         0|            0|            0|  0.00%|    >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)\n",
      "  4375|         0|            0|            0|  0.00%|    >>> out = F.pad(t4d, p2d, \"constant\", 0)\n",
      "  4376|         0|            0|            0|  0.00%|    >>> print(out.size())\n",
      "  4377|         0|            0|            0|  0.00%|    torch.Size([3, 3, 8, 4])\n",
      "  4378|         0|            0|            0|  0.00%|    >>> t4d = torch.empty(3, 3, 4, 2)\n",
      "  4379|         0|            0|            0|  0.00%|    >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)\n",
      "  4380|         0|            0|            0|  0.00%|    >>> out = F.pad(t4d, p3d, \"constant\", 0)\n",
      "  4381|         0|            0|            0|  0.00%|    >>> print(out.size())\n",
      "  4382|         0|            0|            0|  0.00%|    torch.Size([3, 9, 7, 3])\n",
      "  4383|         0|            0|            0|  0.00%|\n",
      "  4384|         0|            0|            0|  0.00%|\"\"\")\n",
      "  4385|         0|            0|            0|  0.00%|# TODO: Fix via https://github.com/pytorch/pytorch/issues/75798\n",
      "  4386|         0|            0|            0|  0.00%|pad.__module__ = \"torch.nn.functional\"\n",
      "  4387|         0|            0|            0|  0.00%|\n",
      "  4388|         0|            0|            0|  0.00%|# distance\n",
      "  4389|         0|            0|            0|  0.00%|\n",
      "  4390|         0|            0|            0|  0.00%|\n",
      "  4391|         0|            0|            0|  0.00%|pairwise_distance = _add_docstr(\n",
      "  4392|         0|            0|            0|  0.00%|    torch.pairwise_distance,\n",
      "  4393|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  4394|         0|            0|            0|  0.00%|pairwise_distance(x1, x2, p=2.0, eps=1e-6, keepdim=False) -> Tensor\n",
      "  4395|         0|            0|            0|  0.00%|\n",
      "  4396|         0|            0|            0|  0.00%|See :class:`torch.nn.PairwiseDistance` for details\n",
      "  4397|         0|            0|            0|  0.00%|\"\"\")\n",
      "  4398|         0|            0|            0|  0.00%|\n",
      "  4399|         0|            0|            0|  0.00%|\n",
      "  4400|         0|            0|            0|  0.00%|pdist = _add_docstr(\n",
      "  4401|         0|            0|            0|  0.00%|    torch.pdist,\n",
      "  4402|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  4403|         0|            0|            0|  0.00%|pdist(input, p=2) -> Tensor\n",
      "  4404|         0|            0|            0|  0.00%|\n",
      "  4405|         0|            0|            0|  0.00%|Computes the p-norm distance between every pair of row vectors in the input.\n",
      "  4406|         0|            0|            0|  0.00%|This is identical to the upper triangular portion, excluding the diagonal, of\n",
      "  4407|         0|            0|            0|  0.00%|`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster\n",
      "  4408|         0|            0|            0|  0.00%|if the rows are contiguous.\n",
      "  4409|         0|            0|            0|  0.00%|\n",
      "  4410|         0|            0|            0|  0.00%|If input has shape :math:`N \\times M` then the output will have shape\n",
      "  4411|         0|            0|            0|  0.00%|:math:`\\frac{1}{2} N (N - 1)`.\n",
      "  4412|         0|            0|            0|  0.00%|\n",
      "  4413|         0|            0|            0|  0.00%|This function is equivalent to ``scipy.spatial.distance.pdist(input,\n",
      "  4414|         0|            0|            0|  0.00%|'minkowski', p=p)`` if :math:`p \\in (0, \\infty)`. When :math:`p = 0` it is\n",
      "  4415|         0|            0|            0|  0.00%|equivalent to ``scipy.spatial.distance.pdist(input, 'hamming') * M``.\n",
      "  4416|         0|            0|            0|  0.00%|When :math:`p = \\infty`, the closest scipy function is\n",
      "  4417|         0|            0|            0|  0.00%|``scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())``.\n",
      "  4418|         0|            0|            0|  0.00%|\n",
      "  4419|         0|            0|            0|  0.00%|Args:\n",
      "  4420|         0|            0|            0|  0.00%|    input: input tensor of shape :math:`N \\times M`.\n",
      "  4421|         0|            0|            0|  0.00%|    p: p value for the p-norm distance to calculate between each vector pair\n",
      "  4422|         0|            0|            0|  0.00%|        :math:`\\in [0, \\infty]`.\n",
      "  4423|         0|            0|            0|  0.00%|\"\"\",\n",
      "  4424|         0|            0|            0|  0.00%|)\n",
      "  4425|         0|            0|            0|  0.00%|\n",
      "  4426|         0|            0|            0|  0.00%|\n",
      "  4427|         0|            0|            0|  0.00%|cosine_similarity = _add_docstr(\n",
      "  4428|         0|            0|            0|  0.00%|    torch.cosine_similarity,\n",
      "  4429|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  4430|         0|            0|            0|  0.00%|cosine_similarity(x1, x2, dim=1, eps=1e-8) -> Tensor\n",
      "  4431|         0|            0|            0|  0.00%|\n",
      "  4432|         0|            0|            0|  0.00%|Returns cosine similarity between ``x1`` and ``x2``, computed along dim. ``x1`` and ``x2`` must be broadcastable\n",
      "  4433|         0|            0|            0|  0.00%|to a common shape. ``dim`` refers to the dimension in this common shape. Dimension ``dim`` of the output is\n",
      "  4434|         0|            0|            0|  0.00%|squeezed (see :func:`torch.squeeze`), resulting in the\n",
      "  4435|         0|            0|            0|  0.00%|output tensor having 1 fewer dimension.\n",
      "  4436|         0|            0|            0|  0.00%|\n",
      "  4437|         0|            0|            0|  0.00%|.. math ::\n",
      "  4438|         0|            0|            0|  0.00%|    \\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}\n",
      "  4439|         0|            0|            0|  0.00%|\n",
      "  4440|         0|            0|            0|  0.00%|Supports :ref:`type promotion <type-promotion-doc>`.\n",
      "  4441|         0|            0|            0|  0.00%|\n",
      "  4442|         0|            0|            0|  0.00%|Args:\n",
      "  4443|         0|            0|            0|  0.00%|    x1 (Tensor): First input.\n",
      "  4444|         0|            0|            0|  0.00%|    x2 (Tensor): Second input.\n",
      "  4445|         0|            0|            0|  0.00%|    dim (int, optional): Dimension along which cosine similarity is computed. Default: 1\n",
      "  4446|         0|            0|            0|  0.00%|    eps (float, optional): Small value to avoid division by zero.\n",
      "  4447|         0|            0|            0|  0.00%|        Default: 1e-8\n",
      "  4448|         0|            0|            0|  0.00%|\n",
      "  4449|         0|            0|            0|  0.00%|Example::\n",
      "  4450|         0|            0|            0|  0.00%|\n",
      "  4451|         0|            0|            0|  0.00%|    >>> input1 = torch.randn(100, 128)\n",
      "  4452|         0|            0|            0|  0.00%|    >>> input2 = torch.randn(100, 128)\n",
      "  4453|         0|            0|            0|  0.00%|    >>> output = F.cosine_similarity(input1, input2)\n",
      "  4454|         0|            0|            0|  0.00%|    >>> print(output)\n",
      "  4455|         0|            0|            0|  0.00%|\"\"\",\n",
      "  4456|         0|            0|            0|  0.00%|)\n",
      "  4457|         0|            0|            0|  0.00%|\n",
      "  4458|         0|            0|            0|  0.00%|\n",
      "  4459|         0|            0|            0|  0.00%|one_hot = _add_docstr(\n",
      "  4460|         0|            0|            0|  0.00%|    torch._C._nn.one_hot,\n",
      "  4461|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  4462|         0|            0|            0|  0.00%|one_hot(tensor, num_classes=-1) -> LongTensor\n",
      "  4463|         0|            0|            0|  0.00%|\n",
      "  4464|         0|            0|            0|  0.00%|Takes LongTensor with index values of shape ``(*)`` and returns a tensor\n",
      "  4465|         0|            0|            0|  0.00%|of shape ``(*, num_classes)`` that have zeros everywhere except where the\n",
      "  4466|         0|            0|            0|  0.00%|index of last dimension matches the corresponding value of the input tensor,\n",
      "  4467|         0|            0|            0|  0.00%|in which case it will be 1.\n",
      "  4468|         0|            0|            0|  0.00%|\n",
      "  4469|         0|            0|            0|  0.00%|See also `One-hot on Wikipedia`_ .\n",
      "  4470|         0|            0|            0|  0.00%|\n",
      "  4471|         0|            0|            0|  0.00%|.. _One-hot on Wikipedia:\n",
      "  4472|         0|            0|            0|  0.00%|    https://en.wikipedia.org/wiki/One-hot\n",
      "  4473|         0|            0|            0|  0.00%|\n",
      "  4474|         0|            0|            0|  0.00%|Arguments:\n",
      "  4475|         0|            0|            0|  0.00%|    tensor (LongTensor): class values of any shape.\n",
      "  4476|         0|            0|            0|  0.00%|    num_classes (int):  Total number of classes. If set to -1, the number\n",
      "  4477|         0|            0|            0|  0.00%|        of classes will be inferred as one greater than the largest class\n",
      "  4478|         0|            0|            0|  0.00%|        value in the input tensor.\n",
      "  4479|         0|            0|            0|  0.00%|\n",
      "  4480|         0|            0|            0|  0.00%|Returns:\n",
      "  4481|         0|            0|            0|  0.00%|    LongTensor that has one more dimension with 1 values at the\n",
      "  4482|         0|            0|            0|  0.00%|    index of last dimension indicated by the input, and 0 everywhere\n",
      "  4483|         0|            0|            0|  0.00%|    else.\n",
      "  4484|         0|            0|            0|  0.00%|\n",
      "  4485|         0|            0|            0|  0.00%|Examples:\n",
      "  4486|         0|            0|            0|  0.00%|    >>> F.one_hot(torch.arange(0, 5) % 3)\n",
      "  4487|         0|            0|            0|  0.00%|    tensor([[1, 0, 0],\n",
      "  4488|         0|            0|            0|  0.00%|            [0, 1, 0],\n",
      "  4489|         0|            0|            0|  0.00%|            [0, 0, 1],\n",
      "  4490|         0|            0|            0|  0.00%|            [1, 0, 0],\n",
      "  4491|         0|            0|            0|  0.00%|            [0, 1, 0]])\n",
      "  4492|         0|            0|            0|  0.00%|    >>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\n",
      "  4493|         0|            0|            0|  0.00%|    tensor([[1, 0, 0, 0, 0],\n",
      "  4494|         0|            0|            0|  0.00%|            [0, 1, 0, 0, 0],\n",
      "  4495|         0|            0|            0|  0.00%|            [0, 0, 1, 0, 0],\n",
      "  4496|         0|            0|            0|  0.00%|            [1, 0, 0, 0, 0],\n",
      "  4497|         0|            0|            0|  0.00%|            [0, 1, 0, 0, 0]])\n",
      "  4498|         0|            0|            0|  0.00%|    >>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)\n",
      "  4499|         0|            0|            0|  0.00%|    tensor([[[1, 0, 0],\n",
      "  4500|         0|            0|            0|  0.00%|             [0, 1, 0]],\n",
      "  4501|         0|            0|            0|  0.00%|            [[0, 0, 1],\n",
      "  4502|         0|            0|            0|  0.00%|             [1, 0, 0]],\n",
      "  4503|         0|            0|            0|  0.00%|            [[0, 1, 0],\n",
      "  4504|         0|            0|            0|  0.00%|             [0, 0, 1]]])\n",
      "  4505|         0|            0|            0|  0.00%|\"\"\",\n",
      "  4506|         0|            0|            0|  0.00%|)\n",
      "  4507|         0|            0|            0|  0.00%|\n",
      "  4508|         0|            0|            0|  0.00%|\n",
      "  4509|         0|            0|            0|  0.00%|def triplet_margin_loss(\n",
      "  4510|         0|            0|            0|  0.00%|    anchor: Tensor,\n",
      "  4511|         0|            0|            0|  0.00%|    positive: Tensor,\n",
      "  4512|         0|            0|            0|  0.00%|    negative: Tensor,\n",
      "  4513|         0|            0|            0|  0.00%|    margin: float = 1.0,\n",
      "  4514|         0|            0|            0|  0.00%|    p: float = 2,\n",
      "  4515|         0|            0|            0|  0.00%|    eps: float = 1e-6,\n",
      "  4516|         0|            0|            0|  0.00%|    swap: bool = False,\n",
      "  4517|         0|            0|            0|  0.00%|    size_average: Optional[bool] = None,\n",
      "  4518|         0|            0|            0|  0.00%|    reduce: Optional[bool] = None,\n",
      "  4519|         0|            0|            0|  0.00%|    reduction: str = \"mean\",\n",
      "  4520|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  4521|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  4522|         0|            0|            0|  0.00%|    See :class:`~torch.nn.TripletMarginLoss` for details\n",
      "  4523|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  4524|         0|            0|            0|  0.00%|    if has_torch_function_variadic(anchor, positive, negative):\n",
      "  4525|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  4526|         0|            0|            0|  0.00%|            triplet_margin_loss,\n",
      "  4527|         0|            0|            0|  0.00%|            (anchor, positive, negative),\n",
      "  4528|         0|            0|            0|  0.00%|            anchor,\n",
      "  4529|         0|            0|            0|  0.00%|            positive,\n",
      "  4530|         0|            0|            0|  0.00%|            negative,\n",
      "  4531|         0|            0|            0|  0.00%|            margin=margin,\n",
      "  4532|         0|            0|            0|  0.00%|            p=p,\n",
      "  4533|         0|            0|            0|  0.00%|            eps=eps,\n",
      "  4534|         0|            0|            0|  0.00%|            swap=swap,\n",
      "  4535|         0|            0|            0|  0.00%|            size_average=size_average,\n",
      "  4536|         0|            0|            0|  0.00%|            reduce=reduce,\n",
      "  4537|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  4538|         0|            0|            0|  0.00%|        )\n",
      "  4539|         0|            0|            0|  0.00%|    if size_average is not None or reduce is not None:\n",
      "  4540|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n",
      "  4541|         0|            0|            0|  0.00%|    else:\n",
      "  4542|         0|            0|            0|  0.00%|        reduction_enum = _Reduction.get_enum(reduction)\n",
      "  4543|         0|            0|            0|  0.00%|    return torch.triplet_margin_loss(anchor, positive, negative, margin, p, eps, swap, reduction_enum)\n",
      "  4544|         0|            0|            0|  0.00%|\n",
      "  4545|         0|            0|            0|  0.00%|\n",
      "  4546|         0|            0|            0|  0.00%|def triplet_margin_with_distance_loss(\n",
      "  4547|         0|            0|            0|  0.00%|    anchor: Tensor,\n",
      "  4548|         0|            0|            0|  0.00%|    positive: Tensor,\n",
      "  4549|         0|            0|            0|  0.00%|    negative: Tensor,\n",
      "  4550|         0|            0|            0|  0.00%|    *,\n",
      "  4551|         0|            0|            0|  0.00%|    distance_function: Optional[Callable[[Tensor, Tensor], Tensor]] = None,\n",
      "  4552|         0|            0|            0|  0.00%|    margin: float = 1.0,\n",
      "  4553|         0|            0|            0|  0.00%|    swap: bool = False,\n",
      "  4554|         0|            0|            0|  0.00%|    reduction: str = \"mean\"\n",
      "  4555|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  4556|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  4557|         0|            0|            0|  0.00%|    See :class:`~torch.nn.TripletMarginWithDistanceLoss` for details.\n",
      "  4558|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  4559|         0|            0|            0|  0.00%|    if torch.jit.is_scripting():\n",
      "  4560|         0|            0|            0|  0.00%|        raise NotImplementedError(\n",
      "  4561|         0|            0|            0|  0.00%|            \"F.triplet_margin_with_distance_loss does not support JIT scripting: \"\n",
      "  4562|         0|            0|            0|  0.00%|            \"functions requiring Callables cannot be scripted.\"\n",
      "  4563|         0|            0|            0|  0.00%|        )\n",
      "  4564|         0|            0|            0|  0.00%|\n",
      "  4565|         0|            0|            0|  0.00%|    if has_torch_function_variadic(anchor, positive, negative):\n",
      "  4566|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  4567|         0|            0|            0|  0.00%|            triplet_margin_with_distance_loss,\n",
      "  4568|         0|            0|            0|  0.00%|            (anchor, positive, negative),\n",
      "  4569|         0|            0|            0|  0.00%|            anchor,\n",
      "  4570|         0|            0|            0|  0.00%|            positive,\n",
      "  4571|         0|            0|            0|  0.00%|            negative,\n",
      "  4572|         0|            0|            0|  0.00%|            distance_function=distance_function,\n",
      "  4573|         0|            0|            0|  0.00%|            margin=margin,\n",
      "  4574|         0|            0|            0|  0.00%|            swap=swap,\n",
      "  4575|         0|            0|            0|  0.00%|            reduction=reduction,\n",
      "  4576|         0|            0|            0|  0.00%|        )\n",
      "  4577|         0|            0|            0|  0.00%|\n",
      "  4578|         0|            0|            0|  0.00%|    distance_function = distance_function if distance_function is not None else pairwise_distance\n",
      "  4579|         0|            0|            0|  0.00%|\n",
      "  4580|         0|            0|            0|  0.00%|    positive_dist = distance_function(anchor, positive)\n",
      "  4581|         0|            0|            0|  0.00%|    negative_dist = distance_function(anchor, negative)\n",
      "  4582|         0|            0|            0|  0.00%|\n",
      "  4583|         0|            0|            0|  0.00%|    if swap:\n",
      "  4584|         0|            0|            0|  0.00%|        swap_dist = distance_function(positive, negative)\n",
      "  4585|         0|            0|            0|  0.00%|        negative_dist = torch.min(negative_dist, swap_dist)\n",
      "  4586|         0|            0|            0|  0.00%|\n",
      "  4587|         0|            0|            0|  0.00%|    output = torch.clamp(positive_dist - negative_dist + margin, min=0.0)\n",
      "  4588|         0|            0|            0|  0.00%|\n",
      "  4589|         0|            0|            0|  0.00%|    reduction_enum = _Reduction.get_enum(reduction)\n",
      "  4590|         0|            0|            0|  0.00%|    if reduction_enum == 1:\n",
      "  4591|         0|            0|            0|  0.00%|        return output.mean()\n",
      "  4592|         0|            0|            0|  0.00%|    elif reduction_enum == 2:\n",
      "  4593|         0|            0|            0|  0.00%|        return output.sum()\n",
      "  4594|         0|            0|            0|  0.00%|    else:\n",
      "  4595|         0|            0|            0|  0.00%|        return output\n",
      "  4596|         0|            0|            0|  0.00%|\n",
      "  4597|         0|            0|            0|  0.00%|\n",
      "  4598|         0|            0|            0|  0.00%|def normalize(input: Tensor, p: float = 2.0, dim: int = 1, eps: float = 1e-12, out: Optional[Tensor] = None) -> Tensor:\n",
      "  4599|         0|            0|            0|  0.00%|    r\"\"\"Performs :math:`L_p` normalization of inputs over specified dimension.\n",
      "  4600|         0|            0|            0|  0.00%|\n",
      "  4601|         0|            0|            0|  0.00%|    For a tensor :attr:`input` of sizes :math:`(n_0, ..., n_{dim}, ..., n_k)`, each\n",
      "  4602|         0|            0|            0|  0.00%|    :math:`n_{dim}` -element vector :math:`v` along dimension :attr:`dim` is transformed as\n",
      "  4603|         0|            0|            0|  0.00%|\n",
      "  4604|         0|            0|            0|  0.00%|    .. math::\n",
      "  4605|         0|            0|            0|  0.00%|        v = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}.\n",
      "  4606|         0|            0|            0|  0.00%|\n",
      "  4607|         0|            0|            0|  0.00%|    With the default arguments it uses the Euclidean norm over vectors along dimension :math:`1` for normalization.\n",
      "  4608|         0|            0|            0|  0.00%|\n",
      "  4609|         0|            0|            0|  0.00%|    Args:\n",
      "  4610|         0|            0|            0|  0.00%|        input: input tensor of any shape\n",
      "  4611|         0|            0|            0|  0.00%|        p (float): the exponent value in the norm formulation. Default: 2\n",
      "  4612|         0|            0|            0|  0.00%|        dim (int): the dimension to reduce. Default: 1\n",
      "  4613|         0|            0|            0|  0.00%|        eps (float): small value to avoid division by zero. Default: 1e-12\n",
      "  4614|         0|            0|            0|  0.00%|        out (Tensor, optional): the output tensor. If :attr:`out` is used, this\n",
      "  4615|         0|            0|            0|  0.00%|                                operation won't be differentiable.\n",
      "  4616|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  4617|         0|            0|            0|  0.00%|    if has_torch_function_variadic(input, out):\n",
      "  4618|         0|            0|            0|  0.00%|        return handle_torch_function(normalize, (input, out), input, p=p, dim=dim, eps=eps, out=out)\n",
      "  4619|         0|            0|            0|  0.00%|    if out is None:\n",
      "  4620|         0|            0|            0|  0.00%|        denom = input.norm(p, dim, keepdim=True).clamp_min(eps).expand_as(input)\n",
      "  4621|         0|            0|            0|  0.00%|        return input / denom\n",
      "  4622|         0|            0|            0|  0.00%|    else:\n",
      "  4623|         0|            0|            0|  0.00%|        denom = input.norm(p, dim, keepdim=True).clamp_min_(eps).expand_as(input)\n",
      "  4624|         0|            0|            0|  0.00%|        return torch.div(input, denom, out=out)\n",
      "  4625|         0|            0|            0|  0.00%|\n",
      "  4626|         0|            0|            0|  0.00%|\n",
      "  4627|         0|            0|            0|  0.00%|def assert_int_or_pair(arg: List[int], arg_name: str, message: str) -> None:\n",
      "  4628|         0|            0|            0|  0.00%|    assert isinstance(arg, int) or len(arg) == 2, message.format(arg_name)\n",
      "  4629|         0|            0|            0|  0.00%|\n",
      "  4630|         0|            0|            0|  0.00%|\n",
      "  4631|         0|            0|            0|  0.00%|def unfold(\n",
      "  4632|         0|            0|            0|  0.00%|    input: Tensor, kernel_size: BroadcastingList2[int],\n",
      "  4633|         0|            0|            0|  0.00%|    dilation: BroadcastingList2[int] = 1,\n",
      "  4634|         0|            0|            0|  0.00%|    padding: BroadcastingList2[int] = 0,\n",
      "  4635|         0|            0|            0|  0.00%|    stride: BroadcastingList2[int] = 1\n",
      "  4636|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  4637|         0|            0|            0|  0.00%|    r\"\"\"Extracts sliding local blocks from a batched input tensor.\n",
      "  4638|         0|            0|            0|  0.00%|\n",
      "  4639|         0|            0|            0|  0.00%|    .. warning::\n",
      "  4640|         0|            0|            0|  0.00%|        Currently, only 4-D input tensors (batched image-like tensors) are\n",
      "  4641|         0|            0|            0|  0.00%|        supported.\n",
      "  4642|         0|            0|            0|  0.00%|\n",
      "  4643|         0|            0|            0|  0.00%|    .. warning::\n",
      "  4644|         0|            0|            0|  0.00%|\n",
      "  4645|         0|            0|            0|  0.00%|        More than one element of the unfolded tensor may refer to a single\n",
      "  4646|         0|            0|            0|  0.00%|        memory location. As a result, in-place operations (especially ones that\n",
      "  4647|         0|            0|            0|  0.00%|        are vectorized) may result in incorrect behavior. If you need to write\n",
      "  4648|         0|            0|            0|  0.00%|        to the tensor, please clone it first.\n",
      "  4649|         0|            0|            0|  0.00%|\n",
      "  4650|         0|            0|            0|  0.00%|\n",
      "  4651|         0|            0|            0|  0.00%|    See :class:`torch.nn.Unfold` for details\n",
      "  4652|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  4653|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  4654|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  4655|         0|            0|            0|  0.00%|            unfold, (input,), input, kernel_size, dilation=dilation, padding=padding, stride=stride\n",
      "  4656|         0|            0|            0|  0.00%|        )\n",
      "  4657|         0|            0|            0|  0.00%|    if input.dim() == 4:\n",
      "  4658|         0|            0|            0|  0.00%|        msg = \"{} must be int or 2-tuple for 4D input\"\n",
      "  4659|         0|            0|            0|  0.00%|        assert_int_or_pair(kernel_size, \"kernel_size\", msg)\n",
      "  4660|         0|            0|            0|  0.00%|        assert_int_or_pair(dilation, \"dilation\", msg)\n",
      "  4661|         0|            0|            0|  0.00%|        assert_int_or_pair(padding, \"padding\", msg)\n",
      "  4662|         0|            0|            0|  0.00%|        assert_int_or_pair(stride, \"stride\", msg)\n",
      "  4663|         0|            0|            0|  0.00%|\n",
      "  4664|         0|            0|            0|  0.00%|        return torch._C._nn.im2col(input, _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride))\n",
      "  4665|         0|            0|            0|  0.00%|    else:\n",
      "  4666|         0|            0|            0|  0.00%|        raise NotImplementedError(\"Input Error: Only 4D input Tensors are supported (got {}D)\".format(input.dim()))\n",
      "  4667|         0|            0|            0|  0.00%|\n",
      "  4668|         0|            0|            0|  0.00%|\n",
      "  4669|         0|            0|            0|  0.00%|def fold(\n",
      "  4670|         0|            0|            0|  0.00%|    input: Tensor, output_size: BroadcastingList2[int],\n",
      "  4671|         0|            0|            0|  0.00%|    kernel_size: BroadcastingList2[int],\n",
      "  4672|         0|            0|            0|  0.00%|    dilation: BroadcastingList2[int] = 1,\n",
      "  4673|         0|            0|            0|  0.00%|    padding: BroadcastingList2[int] = 0,\n",
      "  4674|         0|            0|            0|  0.00%|    stride: BroadcastingList2[int] = 1\n",
      "  4675|         0|            0|            0|  0.00%|) -> Tensor:\n",
      "  4676|         0|            0|            0|  0.00%|    r\"\"\"Combines an array of sliding local blocks into a large containing\n",
      "  4677|         0|            0|            0|  0.00%|    tensor.\n",
      "  4678|         0|            0|            0|  0.00%|\n",
      "  4679|         0|            0|            0|  0.00%|    .. warning::\n",
      "  4680|         0|            0|            0|  0.00%|        Currently, only unbatched (3D) or batched (4D) image-like output tensors are supported.\n",
      "  4681|         0|            0|            0|  0.00%|\n",
      "  4682|         0|            0|            0|  0.00%|    See :class:`torch.nn.Fold` for details\n",
      "  4683|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  4684|         0|            0|            0|  0.00%|    if has_torch_function_unary(input):\n",
      "  4685|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  4686|         0|            0|            0|  0.00%|            fold, (input,), input, output_size, kernel_size, dilation=dilation, padding=padding, stride=stride\n",
      "  4687|         0|            0|            0|  0.00%|        )\n",
      "  4688|         0|            0|            0|  0.00%|    if input.dim() == 3 or input.dim() == 2:\n",
      "  4689|         0|            0|            0|  0.00%|        msg = \"{} must be int or 2-tuple for 3D input\"\n",
      "  4690|         0|            0|            0|  0.00%|        assert_int_or_pair(output_size, \"output_size\", msg)\n",
      "  4691|         0|            0|            0|  0.00%|        assert_int_or_pair(kernel_size, \"kernel_size\", msg)\n",
      "  4692|         0|            0|            0|  0.00%|        assert_int_or_pair(dilation, \"dilation\", msg)\n",
      "  4693|         0|            0|            0|  0.00%|        assert_int_or_pair(padding, \"padding\", msg)\n",
      "  4694|         0|            0|            0|  0.00%|        assert_int_or_pair(stride, \"stride\", msg)\n",
      "  4695|         0|            0|            0|  0.00%|\n",
      "  4696|         0|            0|            0|  0.00%|        return torch._C._nn.col2im(\n",
      "  4697|         0|            0|            0|  0.00%|            input, _pair(output_size), _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride)\n",
      "  4698|         0|            0|            0|  0.00%|        )\n",
      "  4699|         0|            0|            0|  0.00%|    else:\n",
      "  4700|         0|            0|            0|  0.00%|        raise NotImplementedError(\"Input Error: Only unbatched (2D) or batched (3D) input Tensors\"\n",
      "  4701|         0|            0|            0|  0.00%|                                  f\"are supported (got {input.dim()}D)\")\n",
      "  4702|         0|            0|            0|  0.00%|\n",
      "  4703|         0|            0|            0|  0.00%|#\n",
      "  4704|         0|            0|            0|  0.00%|# multihead attention\n",
      "  4705|         0|            0|            0|  0.00%|#\n",
      "  4706|         0|            0|            0|  0.00%|\n",
      "  4707|         0|            0|            0|  0.00%|def _in_projection_packed(\n",
      "  4708|         0|            0|            0|  0.00%|    q: Tensor,\n",
      "  4709|         0|            0|            0|  0.00%|    k: Tensor,\n",
      "  4710|         0|            0|            0|  0.00%|    v: Tensor,\n",
      "  4711|         0|            0|            0|  0.00%|    w: Tensor,\n",
      "  4712|         0|            0|            0|  0.00%|    b: Optional[Tensor] = None,\n",
      "  4713|         0|            0|            0|  0.00%|) -> List[Tensor]:\n",
      "  4714|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  4715|         0|            0|            0|  0.00%|    Performs the in-projection step of the attention operation, using packed weights.\n",
      "  4716|         0|            0|            0|  0.00%|    Output is a triple containing projection tensors for query, key and value.\n",
      "  4717|         0|            0|            0|  0.00%|\n",
      "  4718|         0|            0|            0|  0.00%|    Args:\n",
      "  4719|         0|            0|            0|  0.00%|        q, k, v: query, key and value tensors to be projected. For self-attention,\n",
      "  4720|         0|            0|            0|  0.00%|            these are typically the same tensor; for encoder-decoder attention,\n",
      "  4721|         0|            0|            0|  0.00%|            k and v are typically the same tensor. (We take advantage of these\n",
      "  4722|         0|            0|            0|  0.00%|            identities for performance if they are present.) Regardless, q, k and v\n",
      "  4723|         0|            0|            0|  0.00%|            must share a common embedding dimension; otherwise their shapes may vary.\n",
      "  4724|         0|            0|            0|  0.00%|        w: projection weights for q, k and v, packed into a single tensor. Weights\n",
      "  4725|         0|            0|            0|  0.00%|            are packed along dimension 0, in q, k, v order.\n",
      "  4726|         0|            0|            0|  0.00%|        b: optional projection biases for q, k and v, packed into a single tensor\n",
      "  4727|         0|            0|            0|  0.00%|            in q, k, v order.\n",
      "  4728|         0|            0|            0|  0.00%|\n",
      "  4729|         0|            0|            0|  0.00%|    Shape:\n",
      "  4730|         0|            0|            0|  0.00%|        Inputs:\n",
      "  4731|         0|            0|            0|  0.00%|        - q: :math:`(..., E)` where E is the embedding dimension\n",
      "  4732|         0|            0|            0|  0.00%|        - k: :math:`(..., E)` where E is the embedding dimension\n",
      "  4733|         0|            0|            0|  0.00%|        - v: :math:`(..., E)` where E is the embedding dimension\n",
      "  4734|         0|            0|            0|  0.00%|        - w: :math:`(E * 3, E)` where E is the embedding dimension\n",
      "  4735|         0|            0|            0|  0.00%|        - b: :math:`E * 3` where E is the embedding dimension\n",
      "  4736|         0|            0|            0|  0.00%|\n",
      "  4737|         0|            0|            0|  0.00%|        Output:\n",
      "  4738|         0|            0|            0|  0.00%|        - in output list :math:`[q', k', v']`, each output tensor will have the\n",
      "  4739|         0|            0|            0|  0.00%|            same shape as the corresponding input tensor.\n",
      "  4740|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  4741|         0|            0|            0|  0.00%|    E = q.size(-1)\n",
      "  4742|         0|            0|            0|  0.00%|    if k is v:\n",
      "  4743|         0|            0|            0|  0.00%|        if q is k:\n",
      "  4744|         0|            0|            0|  0.00%|            # self-attention\n",
      "  4745|         0|            0|            0|  0.00%|            return linear(q, w, b).chunk(3, dim=-1)\n",
      "  4746|         0|            0|            0|  0.00%|        else:\n",
      "  4747|         0|            0|            0|  0.00%|            # encoder-decoder attention\n",
      "  4748|         0|            0|            0|  0.00%|            w_q, w_kv = w.split([E, E * 2])\n",
      "  4749|         0|            0|            0|  0.00%|            if b is None:\n",
      "  4750|         0|            0|            0|  0.00%|                b_q = b_kv = None\n",
      "  4751|         0|            0|            0|  0.00%|            else:\n",
      "  4752|         0|            0|            0|  0.00%|                b_q, b_kv = b.split([E, E * 2])\n",
      "  4753|         0|            0|            0|  0.00%|            return (linear(q, w_q, b_q),) + linear(k, w_kv, b_kv).chunk(2, dim=-1)\n",
      "  4754|         0|            0|            0|  0.00%|    else:\n",
      "  4755|         0|            0|            0|  0.00%|        w_q, w_k, w_v = w.chunk(3)\n",
      "  4756|         0|            0|            0|  0.00%|        if b is None:\n",
      "  4757|         0|            0|            0|  0.00%|            b_q = b_k = b_v = None\n",
      "  4758|         0|            0|            0|  0.00%|        else:\n",
      "  4759|         0|            0|            0|  0.00%|            b_q, b_k, b_v = b.chunk(3)\n",
      "  4760|         0|            0|            0|  0.00%|        return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n",
      "  4761|         0|            0|            0|  0.00%|\n",
      "  4762|         0|            0|            0|  0.00%|\n",
      "  4763|         0|            0|            0|  0.00%|def _in_projection(\n",
      "  4764|         0|            0|            0|  0.00%|    q: Tensor,\n",
      "  4765|         0|            0|            0|  0.00%|    k: Tensor,\n",
      "  4766|         0|            0|            0|  0.00%|    v: Tensor,\n",
      "  4767|         0|            0|            0|  0.00%|    w_q: Tensor,\n",
      "  4768|         0|            0|            0|  0.00%|    w_k: Tensor,\n",
      "  4769|         0|            0|            0|  0.00%|    w_v: Tensor,\n",
      "  4770|         0|            0|            0|  0.00%|    b_q: Optional[Tensor] = None,\n",
      "  4771|         0|            0|            0|  0.00%|    b_k: Optional[Tensor] = None,\n",
      "  4772|         0|            0|            0|  0.00%|    b_v: Optional[Tensor] = None,\n",
      "  4773|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor, Tensor]:\n",
      "  4774|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  4775|         0|            0|            0|  0.00%|    Performs the in-projection step of the attention operation. This is simply\n",
      "  4776|         0|            0|            0|  0.00%|    a triple of linear projections, with shape constraints on the weights which\n",
      "  4777|         0|            0|            0|  0.00%|    ensure embedding dimension uniformity in the projected outputs.\n",
      "  4778|         0|            0|            0|  0.00%|    Output is a triple containing projection tensors for query, key and value.\n",
      "  4779|         0|            0|            0|  0.00%|\n",
      "  4780|         0|            0|            0|  0.00%|    Args:\n",
      "  4781|         0|            0|            0|  0.00%|        q, k, v: query, key and value tensors to be projected.\n",
      "  4782|         0|            0|            0|  0.00%|        w_q, w_k, w_v: weights for q, k and v, respectively.\n",
      "  4783|         0|            0|            0|  0.00%|        b_q, b_k, b_v: optional biases for q, k and v, respectively.\n",
      "  4784|         0|            0|            0|  0.00%|\n",
      "  4785|         0|            0|            0|  0.00%|    Shape:\n",
      "  4786|         0|            0|            0|  0.00%|        Inputs:\n",
      "  4787|         0|            0|            0|  0.00%|        - q: :math:`(Qdims..., Eq)` where Eq is the query embedding dimension and Qdims are any\n",
      "  4788|         0|            0|            0|  0.00%|            number of leading dimensions.\n",
      "  4789|         0|            0|            0|  0.00%|        - k: :math:`(Kdims..., Ek)` where Ek is the key embedding dimension and Kdims are any\n",
      "  4790|         0|            0|            0|  0.00%|            number of leading dimensions.\n",
      "  4791|         0|            0|            0|  0.00%|        - v: :math:`(Vdims..., Ev)` where Ev is the value embedding dimension and Vdims are any\n",
      "  4792|         0|            0|            0|  0.00%|            number of leading dimensions.\n",
      "  4793|         0|            0|            0|  0.00%|        - w_q: :math:`(Eq, Eq)`\n",
      "  4794|         0|            0|            0|  0.00%|        - w_k: :math:`(Eq, Ek)`\n",
      "  4795|         0|            0|            0|  0.00%|        - w_v: :math:`(Eq, Ev)`\n",
      "  4796|         0|            0|            0|  0.00%|        - b_q: :math:`(Eq)`\n",
      "  4797|         0|            0|            0|  0.00%|        - b_k: :math:`(Eq)`\n",
      "  4798|         0|            0|            0|  0.00%|        - b_v: :math:`(Eq)`\n",
      "  4799|         0|            0|            0|  0.00%|\n",
      "  4800|         0|            0|            0|  0.00%|        Output: in output triple :math:`(q', k', v')`,\n",
      "  4801|         0|            0|            0|  0.00%|         - q': :math:`[Qdims..., Eq]`\n",
      "  4802|         0|            0|            0|  0.00%|         - k': :math:`[Kdims..., Eq]`\n",
      "  4803|         0|            0|            0|  0.00%|         - v': :math:`[Vdims..., Eq]`\n",
      "  4804|         0|            0|            0|  0.00%|\n",
      "  4805|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  4806|         0|            0|            0|  0.00%|    Eq, Ek, Ev = q.size(-1), k.size(-1), v.size(-1)\n",
      "  4807|         0|            0|            0|  0.00%|    assert w_q.shape == (Eq, Eq), f\"expecting query weights shape of {(Eq, Eq)}, but got {w_q.shape}\"\n",
      "  4808|         0|            0|            0|  0.00%|    assert w_k.shape == (Eq, Ek), f\"expecting key weights shape of {(Eq, Ek)}, but got {w_k.shape}\"\n",
      "  4809|         0|            0|            0|  0.00%|    assert w_v.shape == (Eq, Ev), f\"expecting value weights shape of {(Eq, Ev)}, but got {w_v.shape}\"\n",
      "  4810|         0|            0|            0|  0.00%|    assert b_q is None or b_q.shape == (Eq,), f\"expecting query bias shape of {(Eq,)}, but got {b_q.shape}\"\n",
      "  4811|         0|            0|            0|  0.00%|    assert b_k is None or b_k.shape == (Eq,), f\"expecting key bias shape of {(Eq,)}, but got {b_k.shape}\"\n",
      "  4812|         0|            0|            0|  0.00%|    assert b_v is None or b_v.shape == (Eq,), f\"expecting value bias shape of {(Eq,)}, but got {b_v.shape}\"\n",
      "  4813|         0|            0|            0|  0.00%|    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n",
      "  4814|         0|            0|            0|  0.00%|\n",
      "  4815|         0|            0|            0|  0.00%|\n",
      "  4816|         0|            0|            0|  0.00%|def _scaled_dot_product_attention(\n",
      "  4817|         0|            0|            0|  0.00%|    q: Tensor,\n",
      "  4818|         0|            0|            0|  0.00%|    k: Tensor,\n",
      "  4819|         0|            0|            0|  0.00%|    v: Tensor,\n",
      "  4820|         0|            0|            0|  0.00%|    attn_mask: Optional[Tensor] = None,\n",
      "  4821|         0|            0|            0|  0.00%|    dropout_p: float = 0.0,\n",
      "  4822|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Tensor]:\n",
      "  4823|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  4824|         0|            0|            0|  0.00%|    Computes scaled dot product attention on query, key and value tensors, using\n",
      "  4825|         0|            0|            0|  0.00%|    an optional attention mask if passed, and applying dropout if a probability\n",
      "  4826|         0|            0|            0|  0.00%|    greater than 0.0 is specified.\n",
      "  4827|         0|            0|            0|  0.00%|    Returns a tensor pair containing attended values and attention weights.\n",
      "  4828|         0|            0|            0|  0.00%|\n",
      "  4829|         0|            0|            0|  0.00%|    Args:\n",
      "  4830|         0|            0|            0|  0.00%|        q, k, v: query, key and value tensors. See Shape section for shape details.\n",
      "  4831|         0|            0|            0|  0.00%|        attn_mask: optional tensor containing mask values to be added to calculated\n",
      "  4832|         0|            0|            0|  0.00%|            attention. May be 2D or 3D; see Shape section for details.\n",
      "  4833|         0|            0|            0|  0.00%|        dropout_p: dropout probability. If greater than 0.0, dropout is applied.\n",
      "  4834|         0|            0|            0|  0.00%|\n",
      "  4835|         0|            0|            0|  0.00%|    Shape:\n",
      "  4836|         0|            0|            0|  0.00%|        - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n",
      "  4837|         0|            0|            0|  0.00%|            and E is embedding dimension.\n",
      "  4838|         0|            0|            0|  0.00%|        - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n",
      "  4839|         0|            0|            0|  0.00%|            and E is embedding dimension.\n",
      "  4840|         0|            0|            0|  0.00%|        - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n",
      "  4841|         0|            0|            0|  0.00%|            and E is embedding dimension.\n",
      "  4842|         0|            0|            0|  0.00%|        - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n",
      "  4843|         0|            0|            0|  0.00%|            shape :math:`(Nt, Ns)`.\n",
      "  4844|         0|            0|            0|  0.00%|\n",
      "  4845|         0|            0|            0|  0.00%|        - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n",
      "  4846|         0|            0|            0|  0.00%|            have shape :math:`(B, Nt, Ns)`\n",
      "  4847|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  4848|         0|            0|            0|  0.00%|    B, Nt, E = q.shape\n",
      "  4849|         0|            0|            0|  0.00%|    q = q / math.sqrt(E)\n",
      "  4850|         0|            0|            0|  0.00%|    # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
      "  4851|         0|            0|            0|  0.00%|    if attn_mask is not None:\n",
      "  4852|         0|            0|            0|  0.00%|        attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n",
      "  4853|         0|            0|            0|  0.00%|    else:\n",
      "  4854|         0|            0|            0|  0.00%|        attn = torch.bmm(q, k.transpose(-2, -1))\n",
      "  4855|         0|            0|            0|  0.00%|\n",
      "  4856|         0|            0|            0|  0.00%|    attn = softmax(attn, dim=-1)\n",
      "  4857|         0|            0|            0|  0.00%|    if dropout_p > 0.0:\n",
      "  4858|         0|            0|            0|  0.00%|        attn = dropout(attn, p=dropout_p)\n",
      "  4859|         0|            0|            0|  0.00%|    # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
      "  4860|         0|            0|            0|  0.00%|    output = torch.bmm(attn, v)\n",
      "  4861|         0|            0|            0|  0.00%|    return output, attn\n",
      "  4862|         0|            0|            0|  0.00%|\n",
      "  4863|         0|            0|            0|  0.00%|\n",
      "  4864|         0|            0|            0|  0.00%|def _mha_shape_check(query: Tensor, key: Tensor, value: Tensor,\n",
      "  4865|         0|            0|            0|  0.00%|                     key_padding_mask: Optional[Tensor], attn_mask: Optional[Tensor], num_heads: int):\n",
      "  4866|         0|            0|            0|  0.00%|    # Verifies the expected shape for `query, `key`, `value`, `key_padding_mask` and `attn_mask`\n",
      "  4867|         0|            0|            0|  0.00%|    # and returns if the input is batched or not.\n",
      "  4868|         0|            0|            0|  0.00%|    # Raises an error if `query` is not 2-D (unbatched) or 3-D (batched) tensor.\n",
      "  4869|         0|            0|            0|  0.00%|\n",
      "  4870|         0|            0|            0|  0.00%|    # Shape check.\n",
      "  4871|         0|            0|            0|  0.00%|    if query.dim() == 3:\n",
      "  4872|         0|            0|            0|  0.00%|        # Batched Inputs\n",
      "  4873|         0|            0|            0|  0.00%|        is_batched = True\n",
      "  4874|         0|            0|            0|  0.00%|        assert key.dim() == 3 and value.dim() == 3, \\\n",
      "  4875|         0|            0|            0|  0.00%|            (\"For batched (3-D) `query`, expected `key` and `value` to be 3-D\"\n",
      "  4876|         0|            0|            0|  0.00%|             f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\")\n",
      "  4877|         0|            0|            0|  0.00%|        if key_padding_mask is not None:\n",
      "  4878|         0|            0|            0|  0.00%|            assert key_padding_mask.dim() == 2, \\\n",
      "  4879|         0|            0|            0|  0.00%|                (\"For batched (3-D) `query`, expected `key_padding_mask` to be `None` or 2-D\"\n",
      "  4880|         0|            0|            0|  0.00%|                 f\" but found {key_padding_mask.dim()}-D tensor instead\")\n",
      "  4881|         0|            0|            0|  0.00%|        if attn_mask is not None:\n",
      "  4882|         0|            0|            0|  0.00%|            assert attn_mask.dim() in (2, 3), \\\n",
      "  4883|         0|            0|            0|  0.00%|                (\"For batched (3-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n",
      "  4884|         0|            0|            0|  0.00%|                 f\" but found {attn_mask.dim()}-D tensor instead\")\n",
      "  4885|         0|            0|            0|  0.00%|    elif query.dim() == 2:\n",
      "  4886|         0|            0|            0|  0.00%|        # Unbatched Inputs\n",
      "  4887|         0|            0|            0|  0.00%|        is_batched = False\n",
      "  4888|         0|            0|            0|  0.00%|        assert key.dim() == 2 and value.dim() == 2, \\\n",
      "  4889|         0|            0|            0|  0.00%|            (\"For unbatched (2-D) `query`, expected `key` and `value` to be 2-D\"\n",
      "  4890|         0|            0|            0|  0.00%|             f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\")\n",
      "  4891|         0|            0|            0|  0.00%|\n",
      "  4892|         0|            0|            0|  0.00%|        if key_padding_mask is not None:\n",
      "  4893|         0|            0|            0|  0.00%|            assert key_padding_mask.dim() == 1, \\\n",
      "  4894|         0|            0|            0|  0.00%|                (\"For unbatched (2-D) `query`, expected `key_padding_mask` to be `None` or 1-D\"\n",
      "  4895|         0|            0|            0|  0.00%|                 f\" but found {key_padding_mask.dim()}-D tensor instead\")\n",
      "  4896|         0|            0|            0|  0.00%|\n",
      "  4897|         0|            0|            0|  0.00%|        if attn_mask is not None:\n",
      "  4898|         0|            0|            0|  0.00%|            assert attn_mask.dim() in (2, 3), \\\n",
      "  4899|         0|            0|            0|  0.00%|                (\"For unbatched (2-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n",
      "  4900|         0|            0|            0|  0.00%|                 f\" but found {attn_mask.dim()}-D tensor instead\")\n",
      "  4901|         0|            0|            0|  0.00%|            if attn_mask.dim() == 3:\n",
      "  4902|         0|            0|            0|  0.00%|                expected_shape = (num_heads, query.shape[0], key.shape[0])\n",
      "  4903|         0|            0|            0|  0.00%|                assert attn_mask.shape == expected_shape, \\\n",
      "  4904|         0|            0|            0|  0.00%|                    (f\"Expected `attn_mask` shape to be {expected_shape} but got {attn_mask.shape}\")\n",
      "  4905|         0|            0|            0|  0.00%|    else:\n",
      "  4906|         0|            0|            0|  0.00%|        raise AssertionError(\n",
      "  4907|         0|            0|            0|  0.00%|            f\"query should be unbatched 2D or batched 3D tensor but received {query.dim()}-D query tensor\")\n",
      "  4908|         0|            0|            0|  0.00%|\n",
      "  4909|         0|            0|            0|  0.00%|    return is_batched\n",
      "  4910|         0|            0|            0|  0.00%|\n",
      "  4911|         0|            0|            0|  0.00%|def multi_head_attention_forward(\n",
      "  4912|         0|            0|            0|  0.00%|    query: Tensor,\n",
      "  4913|         0|            0|            0|  0.00%|    key: Tensor,\n",
      "  4914|         0|            0|            0|  0.00%|    value: Tensor,\n",
      "  4915|         0|            0|            0|  0.00%|    embed_dim_to_check: int,\n",
      "  4916|         0|            0|            0|  0.00%|    num_heads: int,\n",
      "  4917|         0|            0|            0|  0.00%|    in_proj_weight: Optional[Tensor],\n",
      "  4918|         0|            0|            0|  0.00%|    in_proj_bias: Optional[Tensor],\n",
      "  4919|         0|            0|            0|  0.00%|    bias_k: Optional[Tensor],\n",
      "  4920|         0|            0|            0|  0.00%|    bias_v: Optional[Tensor],\n",
      "  4921|         0|            0|            0|  0.00%|    add_zero_attn: bool,\n",
      "  4922|         0|            0|            0|  0.00%|    dropout_p: float,\n",
      "  4923|         0|            0|            0|  0.00%|    out_proj_weight: Tensor,\n",
      "  4924|         0|            0|            0|  0.00%|    out_proj_bias: Optional[Tensor],\n",
      "  4925|         0|            0|            0|  0.00%|    training: bool = True,\n",
      "  4926|         0|            0|            0|  0.00%|    key_padding_mask: Optional[Tensor] = None,\n",
      "  4927|         0|            0|            0|  0.00%|    need_weights: bool = True,\n",
      "  4928|         0|            0|            0|  0.00%|    attn_mask: Optional[Tensor] = None,\n",
      "  4929|         0|            0|            0|  0.00%|    use_separate_proj_weight: bool = False,\n",
      "  4930|         0|            0|            0|  0.00%|    q_proj_weight: Optional[Tensor] = None,\n",
      "  4931|         0|            0|            0|  0.00%|    k_proj_weight: Optional[Tensor] = None,\n",
      "  4932|         0|            0|            0|  0.00%|    v_proj_weight: Optional[Tensor] = None,\n",
      "  4933|         0|            0|            0|  0.00%|    static_k: Optional[Tensor] = None,\n",
      "  4934|         0|            0|            0|  0.00%|    static_v: Optional[Tensor] = None,\n",
      "  4935|         0|            0|            0|  0.00%|    average_attn_weights: bool = True,\n",
      "  4936|         0|            0|            0|  0.00%|) -> Tuple[Tensor, Optional[Tensor]]:\n",
      "  4937|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  4938|         0|            0|            0|  0.00%|    Args:\n",
      "  4939|         0|            0|            0|  0.00%|        query, key, value: map a query and a set of key-value pairs to an output.\n",
      "  4940|         0|            0|            0|  0.00%|            See \"Attention Is All You Need\" for more details.\n",
      "  4941|         0|            0|            0|  0.00%|        embed_dim_to_check: total dimension of the model.\n",
      "  4942|         0|            0|            0|  0.00%|        num_heads: parallel attention heads.\n",
      "  4943|         0|            0|            0|  0.00%|        in_proj_weight, in_proj_bias: input projection weight and bias.\n",
      "  4944|         0|            0|            0|  0.00%|        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
      "  4945|         0|            0|            0|  0.00%|        add_zero_attn: add a new batch of zeros to the key and\n",
      "  4946|         0|            0|            0|  0.00%|                       value sequences at dim=1.\n",
      "  4947|         0|            0|            0|  0.00%|        dropout_p: probability of an element to be zeroed.\n",
      "  4948|         0|            0|            0|  0.00%|        out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
      "  4949|         0|            0|            0|  0.00%|        training: apply dropout if is ``True``.\n",
      "  4950|         0|            0|            0|  0.00%|        key_padding_mask: if provided, specified padding elements in the key will\n",
      "  4951|         0|            0|            0|  0.00%|            be ignored by the attention. This is an binary mask. When the value is True,\n",
      "  4952|         0|            0|            0|  0.00%|            the corresponding value on the attention layer will be filled with -inf.\n",
      "  4953|         0|            0|            0|  0.00%|        need_weights: output attn_output_weights.\n",
      "  4954|         0|            0|            0|  0.00%|        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
      "  4955|         0|            0|            0|  0.00%|            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
      "  4956|         0|            0|            0|  0.00%|        use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
      "  4957|         0|            0|            0|  0.00%|            and value in different forms. If false, in_proj_weight will be used, which is\n",
      "  4958|         0|            0|            0|  0.00%|            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
      "  4959|         0|            0|            0|  0.00%|        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
      "  4960|         0|            0|            0|  0.00%|        static_k, static_v: static key and value used for attention operators.\n",
      "  4961|         0|            0|            0|  0.00%|        average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.\n",
      "  4962|         0|            0|            0|  0.00%|            Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect\n",
      "  4963|         0|            0|            0|  0.00%|            when ``need_weights=True.``. Default: True\n",
      "  4964|         0|            0|            0|  0.00%|\n",
      "  4965|         0|            0|            0|  0.00%|\n",
      "  4966|         0|            0|            0|  0.00%|    Shape:\n",
      "  4967|         0|            0|            0|  0.00%|        Inputs:\n",
      "  4968|         0|            0|            0|  0.00%|        - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
      "  4969|         0|            0|            0|  0.00%|          the embedding dimension.\n",
      "  4970|         0|            0|            0|  0.00%|        - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
      "  4971|         0|            0|            0|  0.00%|          the embedding dimension.\n",
      "  4972|         0|            0|            0|  0.00%|        - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
      "  4973|         0|            0|            0|  0.00%|          the embedding dimension.\n",
      "  4974|         0|            0|            0|  0.00%|        - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
      "  4975|         0|            0|            0|  0.00%|          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions\n",
      "  4976|         0|            0|            0|  0.00%|          will be unchanged. If a BoolTensor is provided, the positions with the\n",
      "  4977|         0|            0|            0|  0.00%|          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
      "  4978|         0|            0|            0|  0.00%|        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
      "  4979|         0|            0|            0|  0.00%|          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
      "  4980|         0|            0|            0|  0.00%|          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n",
      "  4981|         0|            0|            0|  0.00%|          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
      "  4982|         0|            0|            0|  0.00%|          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
      "  4983|         0|            0|            0|  0.00%|          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
      "  4984|         0|            0|            0|  0.00%|          is provided, it will be added to the attention weight.\n",
      "  4985|         0|            0|            0|  0.00%|        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
      "  4986|         0|            0|            0|  0.00%|          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
      "  4987|         0|            0|            0|  0.00%|        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
      "  4988|         0|            0|            0|  0.00%|          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
      "  4989|         0|            0|            0|  0.00%|\n",
      "  4990|         0|            0|            0|  0.00%|        Outputs:\n",
      "  4991|         0|            0|            0|  0.00%|        - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
      "  4992|         0|            0|            0|  0.00%|          E is the embedding dimension.\n",
      "  4993|         0|            0|            0|  0.00%|        - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns\n",
      "  4994|         0|            0|            0|  0.00%|          attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\n",
      "  4995|         0|            0|            0|  0.00%|          :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\n",
      "  4996|         0|            0|            0|  0.00%|          :math:`S` is the source sequence length. If ``average_weights=False``, returns attention weights per\n",
      "  4997|         0|            0|            0|  0.00%|          head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.\n",
      "  4998|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  4999|         0|            0|            0|  0.00%|    tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v, out_proj_weight, out_proj_bias)\n",
      "  5000|         0|            0|            0|  0.00%|    if has_torch_function(tens_ops):\n",
      "  5001|         0|            0|            0|  0.00%|        return handle_torch_function(\n",
      "  5002|         0|            0|            0|  0.00%|            multi_head_attention_forward,\n",
      "  5003|         0|            0|            0|  0.00%|            tens_ops,\n",
      "  5004|         0|            0|            0|  0.00%|            query,\n",
      "  5005|         0|            0|            0|  0.00%|            key,\n",
      "  5006|         0|            0|            0|  0.00%|            value,\n",
      "  5007|         0|            0|            0|  0.00%|            embed_dim_to_check,\n",
      "  5008|         0|            0|            0|  0.00%|            num_heads,\n",
      "  5009|         0|            0|            0|  0.00%|            in_proj_weight,\n",
      "  5010|         0|            0|            0|  0.00%|            in_proj_bias,\n",
      "  5011|         0|            0|            0|  0.00%|            bias_k,\n",
      "  5012|         0|            0|            0|  0.00%|            bias_v,\n",
      "  5013|         0|            0|            0|  0.00%|            add_zero_attn,\n",
      "  5014|         0|            0|            0|  0.00%|            dropout_p,\n",
      "  5015|         0|            0|            0|  0.00%|            out_proj_weight,\n",
      "  5016|         0|            0|            0|  0.00%|            out_proj_bias,\n",
      "  5017|         0|            0|            0|  0.00%|            training=training,\n",
      "  5018|         0|            0|            0|  0.00%|            key_padding_mask=key_padding_mask,\n",
      "  5019|         0|            0|            0|  0.00%|            need_weights=need_weights,\n",
      "  5020|         0|            0|            0|  0.00%|            attn_mask=attn_mask,\n",
      "  5021|         0|            0|            0|  0.00%|            use_separate_proj_weight=use_separate_proj_weight,\n",
      "  5022|         0|            0|            0|  0.00%|            q_proj_weight=q_proj_weight,\n",
      "  5023|         0|            0|            0|  0.00%|            k_proj_weight=k_proj_weight,\n",
      "  5024|         0|            0|            0|  0.00%|            v_proj_weight=v_proj_weight,\n",
      "  5025|         0|            0|            0|  0.00%|            static_k=static_k,\n",
      "  5026|         0|            0|            0|  0.00%|            static_v=static_v,\n",
      "  5027|         0|            0|            0|  0.00%|            average_attn_weights=average_attn_weights,\n",
      "  5028|         0|            0|            0|  0.00%|        )\n",
      "  5029|         0|            0|            0|  0.00%|\n",
      "  5030|         0|            0|            0|  0.00%|    is_batched = _mha_shape_check(query, key, value, key_padding_mask, attn_mask, num_heads)\n",
      "  5031|         0|            0|            0|  0.00%|\n",
      "  5032|         0|            0|            0|  0.00%|    # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\n",
      "  5033|         0|            0|            0|  0.00%|    # is batched, run the computation and before returning squeeze the\n",
      "  5034|         0|            0|            0|  0.00%|    # batch dimension so that the output doesn't carry this temporary batch dimension.\n",
      "  5035|         0|            0|            0|  0.00%|    if not is_batched:\n",
      "  5036|         0|            0|            0|  0.00%|        # unsqueeze if the input is unbatched\n",
      "  5037|         0|            0|            0|  0.00%|        query = query.unsqueeze(1)\n",
      "  5038|         0|            0|            0|  0.00%|        key = key.unsqueeze(1)\n",
      "  5039|         0|            0|            0|  0.00%|        value = value.unsqueeze(1)\n",
      "  5040|         0|            0|            0|  0.00%|        if key_padding_mask is not None:\n",
      "  5041|         0|            0|            0|  0.00%|            key_padding_mask = key_padding_mask.unsqueeze(0)\n",
      "  5042|         0|            0|            0|  0.00%|\n",
      "  5043|         0|            0|            0|  0.00%|    # set up shape vars\n",
      "  5044|         0|            0|            0|  0.00%|    tgt_len, bsz, embed_dim = query.shape\n",
      "  5045|         0|            0|            0|  0.00%|    src_len, _, _ = key.shape\n",
      "  5046|         0|            0|            0|  0.00%|    assert embed_dim == embed_dim_to_check, \\\n",
      "  5047|         0|            0|            0|  0.00%|        f\"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}\"\n",
      "  5048|         0|            0|            0|  0.00%|    if isinstance(embed_dim, torch.Tensor):\n",
      "  5049|         0|            0|            0|  0.00%|        # embed_dim can be a tensor when JIT tracing\n",
      "  5050|         0|            0|            0|  0.00%|        head_dim = embed_dim.div(num_heads, rounding_mode='trunc')\n",
      "  5051|         0|            0|            0|  0.00%|    else:\n",
      "  5052|         0|            0|            0|  0.00%|        head_dim = embed_dim // num_heads\n",
      "  5053|         0|            0|            0|  0.00%|    assert head_dim * num_heads == embed_dim, f\"embed_dim {embed_dim} not divisible by num_heads {num_heads}\"\n",
      "  5054|         0|            0|            0|  0.00%|    if use_separate_proj_weight:\n",
      "  5055|         0|            0|            0|  0.00%|        # allow MHA to have different embedding dimensions when separate projection weights are used\n",
      "  5056|         0|            0|            0|  0.00%|        assert key.shape[:2] == value.shape[:2], \\\n",
      "  5057|         0|            0|            0|  0.00%|            f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\"\n",
      "  5058|         0|            0|            0|  0.00%|    else:\n",
      "  5059|         0|            0|            0|  0.00%|        assert key.shape == value.shape, f\"key shape {key.shape} does not match value shape {value.shape}\"\n",
      "  5060|         0|            0|            0|  0.00%|\n",
      "  5061|         0|            0|            0|  0.00%|    #\n",
      "  5062|         0|            0|            0|  0.00%|    # compute in-projection\n",
      "  5063|         0|            0|            0|  0.00%|    #\n",
      "  5064|         0|            0|            0|  0.00%|    if not use_separate_proj_weight:\n",
      "  5065|         0|            0|            0|  0.00%|        assert in_proj_weight is not None, \"use_separate_proj_weight is False but in_proj_weight is None\"\n",
      "  5066|         0|            0|            0|  0.00%|        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
      "  5067|         0|            0|            0|  0.00%|    else:\n",
      "  5068|         0|            0|            0|  0.00%|        assert q_proj_weight is not None, \"use_separate_proj_weight is True but q_proj_weight is None\"\n",
      "  5069|         0|            0|            0|  0.00%|        assert k_proj_weight is not None, \"use_separate_proj_weight is True but k_proj_weight is None\"\n",
      "  5070|         0|            0|            0|  0.00%|        assert v_proj_weight is not None, \"use_separate_proj_weight is True but v_proj_weight is None\"\n",
      "  5071|         0|            0|            0|  0.00%|        if in_proj_bias is None:\n",
      "  5072|         0|            0|            0|  0.00%|            b_q = b_k = b_v = None\n",
      "  5073|         0|            0|            0|  0.00%|        else:\n",
      "  5074|         0|            0|            0|  0.00%|            b_q, b_k, b_v = in_proj_bias.chunk(3)\n",
      "  5075|         0|            0|            0|  0.00%|        q, k, v = _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)\n",
      "  5076|         0|            0|            0|  0.00%|\n",
      "  5077|         0|            0|            0|  0.00%|    # prep attention mask\n",
      "  5078|         0|            0|            0|  0.00%|    if attn_mask is not None:\n",
      "  5079|         0|            0|            0|  0.00%|        if attn_mask.dtype == torch.uint8:\n",
      "  5080|         0|            0|            0|  0.00%|            warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
      "  5081|         0|            0|            0|  0.00%|            attn_mask = attn_mask.to(torch.bool)\n",
      "  5082|         0|            0|            0|  0.00%|        else:\n",
      "  5083|         0|            0|            0|  0.00%|            assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n",
      "  5084|         0|            0|            0|  0.00%|                f\"Only float, byte, and bool types are supported for attn_mask, not {attn_mask.dtype}\"\n",
      "  5085|         0|            0|            0|  0.00%|        # ensure attn_mask's dim is 3\n",
      "  5086|         0|            0|            0|  0.00%|        if attn_mask.dim() == 2:\n",
      "  5087|         0|            0|            0|  0.00%|            correct_2d_size = (tgt_len, src_len)\n",
      "  5088|         0|            0|            0|  0.00%|            if attn_mask.shape != correct_2d_size:\n",
      "  5089|         0|            0|            0|  0.00%|                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
      "  5090|         0|            0|            0|  0.00%|            attn_mask = attn_mask.unsqueeze(0)\n",
      "  5091|         0|            0|            0|  0.00%|        elif attn_mask.dim() == 3:\n",
      "  5092|         0|            0|            0|  0.00%|            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
      "  5093|         0|            0|            0|  0.00%|            if attn_mask.shape != correct_3d_size:\n",
      "  5094|         0|            0|            0|  0.00%|                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
      "  5095|         0|            0|            0|  0.00%|        else:\n",
      "  5096|         0|            0|            0|  0.00%|            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
      "  5097|         0|            0|            0|  0.00%|\n",
      "  5098|         0|            0|            0|  0.00%|    # prep key padding mask\n",
      "  5099|         0|            0|            0|  0.00%|    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
      "  5100|         0|            0|            0|  0.00%|        warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
      "  5101|         0|            0|            0|  0.00%|        key_padding_mask = key_padding_mask.to(torch.bool)\n",
      "  5102|         0|            0|            0|  0.00%|\n",
      "  5103|         0|            0|            0|  0.00%|    # add bias along batch dimension (currently second)\n",
      "  5104|         0|            0|            0|  0.00%|    if bias_k is not None and bias_v is not None:\n",
      "  5105|         0|            0|            0|  0.00%|        assert static_k is None, \"bias cannot be added to static key.\"\n",
      "  5106|         0|            0|            0|  0.00%|        assert static_v is None, \"bias cannot be added to static value.\"\n",
      "  5107|         0|            0|            0|  0.00%|        k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
      "  5108|         0|            0|            0|  0.00%|        v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
      "  5109|         0|            0|            0|  0.00%|        if attn_mask is not None:\n",
      "  5110|         0|            0|            0|  0.00%|            attn_mask = pad(attn_mask, (0, 1))\n",
      "  5111|         0|            0|            0|  0.00%|        if key_padding_mask is not None:\n",
      "  5112|         0|            0|            0|  0.00%|            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
      "  5113|         0|            0|            0|  0.00%|    else:\n",
      "  5114|         0|            0|            0|  0.00%|        assert bias_k is None\n",
      "  5115|         0|            0|            0|  0.00%|        assert bias_v is None\n",
      "  5116|         0|            0|            0|  0.00%|\n",
      "  5117|         0|            0|            0|  0.00%|    #\n",
      "  5118|         0|            0|            0|  0.00%|    # reshape q, k, v for multihead attention and make em batch first\n",
      "  5119|         0|            0|            0|  0.00%|    #\n",
      "  5120|         0|            0|            0|  0.00%|    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
      "  5121|         0|            0|            0|  0.00%|    if static_k is None:\n",
      "  5122|         0|            0|            0|  0.00%|        k = k.contiguous().view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
      "  5123|         0|            0|            0|  0.00%|    else:\n",
      "  5124|         0|            0|            0|  0.00%|        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
      "  5125|         0|            0|            0|  0.00%|        assert static_k.size(0) == bsz * num_heads, \\\n",
      "  5126|         0|            0|            0|  0.00%|            f\"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}\"\n",
      "  5127|         0|            0|            0|  0.00%|        assert static_k.size(2) == head_dim, \\\n",
      "  5128|         0|            0|            0|  0.00%|            f\"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}\"\n",
      "  5129|         0|            0|            0|  0.00%|        k = static_k\n",
      "  5130|         0|            0|            0|  0.00%|    if static_v is None:\n",
      "  5131|         0|            0|            0|  0.00%|        v = v.contiguous().view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
      "  5132|         0|            0|            0|  0.00%|    else:\n",
      "  5133|         0|            0|            0|  0.00%|        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
      "  5134|         0|            0|            0|  0.00%|        assert static_v.size(0) == bsz * num_heads, \\\n",
      "  5135|         0|            0|            0|  0.00%|            f\"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}\"\n",
      "  5136|         0|            0|            0|  0.00%|        assert static_v.size(2) == head_dim, \\\n",
      "  5137|         0|            0|            0|  0.00%|            f\"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}\"\n",
      "  5138|         0|            0|            0|  0.00%|        v = static_v\n",
      "  5139|         0|            0|            0|  0.00%|\n",
      "  5140|         0|            0|            0|  0.00%|    # add zero attention along batch dimension (now first)\n",
      "  5141|         0|            0|            0|  0.00%|    if add_zero_attn:\n",
      "  5142|         0|            0|            0|  0.00%|        zero_attn_shape = (bsz * num_heads, 1, head_dim)\n",
      "  5143|         0|            0|            0|  0.00%|        k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)\n",
      "  5144|         0|            0|            0|  0.00%|        v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)\n",
      "  5145|         0|            0|            0|  0.00%|        if attn_mask is not None:\n",
      "  5146|         0|            0|            0|  0.00%|            attn_mask = pad(attn_mask, (0, 1))\n",
      "  5147|         0|            0|            0|  0.00%|        if key_padding_mask is not None:\n",
      "  5148|         0|            0|            0|  0.00%|            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
      "  5149|         0|            0|            0|  0.00%|\n",
      "  5150|         0|            0|            0|  0.00%|    # update source sequence length after adjustments\n",
      "  5151|         0|            0|            0|  0.00%|    src_len = k.size(1)\n",
      "  5152|         0|            0|            0|  0.00%|\n",
      "  5153|         0|            0|            0|  0.00%|    # merge key padding and attention masks\n",
      "  5154|         0|            0|            0|  0.00%|    if key_padding_mask is not None:\n",
      "  5155|         0|            0|            0|  0.00%|        assert key_padding_mask.shape == (bsz, src_len), \\\n",
      "  5156|         0|            0|            0|  0.00%|            f\"expecting key_padding_mask shape of {(bsz, src_len)}, but got {key_padding_mask.shape}\"\n",
      "  5157|         0|            0|            0|  0.00%|        key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len).   \\\n",
      "  5158|         0|            0|            0|  0.00%|            expand(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)\n",
      "  5159|         0|            0|            0|  0.00%|        if attn_mask is None:\n",
      "  5160|         0|            0|            0|  0.00%|            attn_mask = key_padding_mask\n",
      "  5161|         0|            0|            0|  0.00%|        elif attn_mask.dtype == torch.bool:\n",
      "  5162|         0|            0|            0|  0.00%|            attn_mask = attn_mask.logical_or(key_padding_mask)\n",
      "  5163|         0|            0|            0|  0.00%|        else:\n",
      "  5164|         0|            0|            0|  0.00%|            attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n",
      "  5165|         0|            0|            0|  0.00%|\n",
      "  5166|         0|            0|            0|  0.00%|    # convert mask to float\n",
      "  5167|         0|            0|            0|  0.00%|    if attn_mask is not None and attn_mask.dtype == torch.bool:\n",
      "  5168|         0|            0|            0|  0.00%|        new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)\n",
      "  5169|         0|            0|            0|  0.00%|        new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n",
      "  5170|         0|            0|            0|  0.00%|        attn_mask = new_attn_mask\n",
      "  5171|         0|            0|            0|  0.00%|\n",
      "  5172|         0|            0|            0|  0.00%|    # adjust dropout probability\n",
      "  5173|         0|            0|            0|  0.00%|    if not training:\n",
      "  5174|         0|            0|            0|  0.00%|        dropout_p = 0.0\n",
      "  5175|         0|            0|            0|  0.00%|\n",
      "  5176|         0|            0|            0|  0.00%|    #\n",
      "  5177|         0|            0|            0|  0.00%|    # (deep breath) calculate attention and out projection\n",
      "  5178|         0|            0|            0|  0.00%|    #\n",
      "  5179|         0|            0|            0|  0.00%|    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)\n",
      "  5180|         0|            0|            0|  0.00%|    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
      "  5181|         0|            0|            0|  0.00%|    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "  5182|         0|            0|            0|  0.00%|    attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
      "  5183|         0|            0|            0|  0.00%|\n",
      "  5184|         0|            0|            0|  0.00%|    if need_weights:\n",
      "  5185|         0|            0|            0|  0.00%|        # optionally average attention weights over heads\n",
      "  5186|         0|            0|            0|  0.00%|        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
      "  5187|         0|            0|            0|  0.00%|        if average_attn_weights:\n",
      "  5188|         0|            0|            0|  0.00%|            attn_output_weights = attn_output_weights.sum(dim=1) / num_heads\n",
      "  5189|         0|            0|            0|  0.00%|\n",
      "  5190|         0|            0|            0|  0.00%|        if not is_batched:\n",
      "  5191|         0|            0|            0|  0.00%|            # squeeze the output if input was unbatched\n",
      "  5192|         0|            0|            0|  0.00%|            attn_output = attn_output.squeeze(1)\n",
      "  5193|         0|            0|            0|  0.00%|            attn_output_weights = attn_output_weights.squeeze(0)\n",
      "  5194|         0|            0|            0|  0.00%|        return attn_output, attn_output_weights\n",
      "  5195|         0|            0|            0|  0.00%|    else:\n",
      "  5196|         0|            0|            0|  0.00%|        if not is_batched:\n",
      "  5197|         0|            0|            0|  0.00%|            # squeeze the output if input was unbatched\n",
      "  5198|         0|            0|            0|  0.00%|            attn_output = attn_output.squeeze(1)\n",
      "  5199|         0|            0|            0|  0.00%|        return attn_output, None\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/numpy/core/multiarray.py\n",
      "File duration: 1.74604s (0.28%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|\"\"\"\n",
      "     2|         0|            0|            0|  0.00%|Create the numpy.core.multiarray namespace for backward compatibility. In v1.16\n",
      "     3|         0|            0|            0|  0.00%|the multiarray and umath c-extension modules were merged into a single\n",
      "     4|         0|            0|            0|  0.00%|_multiarray_umath extension module. So we replicate the old namespace\n",
      "     5|         0|            0|            0|  0.00%|by importing from the extension module.\n",
      "     6|         0|            0|            0|  0.00%|\n",
      "     7|         0|            0|            0|  0.00%|\"\"\"\n",
      "     8|         0|            0|            0|  0.00%|\n",
      "     9|         0|            0|            0|  0.00%|import functools\n",
      "    10|         0|            0|            0|  0.00%|from . import overrides\n",
      "    11|         0|            0|            0|  0.00%|from . import _multiarray_umath\n",
      "    12|         0|            0|            0|  0.00%|from ._multiarray_umath import *  # noqa: F403\n",
      "    13|         0|            0|            0|  0.00%|# These imports are needed for backward compatibility,\n",
      "    14|         0|            0|            0|  0.00%|# do not change them. issue gh-15518\n",
      "    15|         0|            0|            0|  0.00%|# _get_ndarray_c_version is semi-public, on purpose not added to __all__\n",
      "    16|         0|            0|            0|  0.00%|from ._multiarray_umath import (\n",
      "    17|         0|            0|            0|  0.00%|    _fastCopyAndTranspose, _flagdict, _from_dlpack, _insert, _reconstruct,\n",
      "    18|         0|            0|            0|  0.00%|    _vec_string, _ARRAY_API, _monotonicity, _get_ndarray_c_version,\n",
      "    19|         0|            0|            0|  0.00%|    _set_madvise_hugepage,\n",
      "    20|         0|            0|            0|  0.00%|    )\n",
      "    21|         0|            0|            0|  0.00%|\n",
      "    22|         0|            0|            0|  0.00%|__all__ = [\n",
      "    23|         0|            0|            0|  0.00%|    '_ARRAY_API', 'ALLOW_THREADS', 'BUFSIZE', 'CLIP', 'DATETIMEUNITS',\n",
      "    24|         0|            0|            0|  0.00%|    'ITEM_HASOBJECT', 'ITEM_IS_POINTER', 'LIST_PICKLE', 'MAXDIMS',\n",
      "    25|         0|            0|            0|  0.00%|    'MAY_SHARE_BOUNDS', 'MAY_SHARE_EXACT', 'NEEDS_INIT', 'NEEDS_PYAPI',\n",
      "    26|         0|            0|            0|  0.00%|    'RAISE', 'USE_GETITEM', 'USE_SETITEM', 'WRAP', '_fastCopyAndTranspose',\n",
      "    27|         0|            0|            0|  0.00%|    '_flagdict', '_from_dlpack', '_insert', '_reconstruct', '_vec_string',\n",
      "    28|         0|            0|            0|  0.00%|    '_monotonicity', 'add_docstring', 'arange', 'array', 'asarray',\n",
      "    29|         0|            0|            0|  0.00%|    'asanyarray', 'ascontiguousarray', 'asfortranarray', 'bincount',\n",
      "    30|         0|            0|            0|  0.00%|    'broadcast', 'busday_count', 'busday_offset', 'busdaycalendar', 'can_cast',\n",
      "    31|         0|            0|            0|  0.00%|    'compare_chararrays', 'concatenate', 'copyto', 'correlate', 'correlate2',\n",
      "    32|         0|            0|            0|  0.00%|    'count_nonzero', 'c_einsum', 'datetime_as_string', 'datetime_data',\n",
      "    33|         0|            0|            0|  0.00%|    'dot', 'dragon4_positional', 'dragon4_scientific', 'dtype',\n",
      "    34|         0|            0|            0|  0.00%|    'empty', 'empty_like', 'error', 'flagsobj', 'flatiter', 'format_longfloat',\n",
      "    35|         0|            0|            0|  0.00%|    'frombuffer', 'fromfile', 'fromiter', 'fromstring',\n",
      "    36|         0|            0|            0|  0.00%|    'get_handler_name', 'get_handler_version', 'inner', 'interp',\n",
      "    37|         0|            0|            0|  0.00%|    'interp_complex', 'is_busday', 'lexsort', 'matmul', 'may_share_memory',\n",
      "    38|         0|            0|            0|  0.00%|    'min_scalar_type', 'ndarray', 'nditer', 'nested_iters',\n",
      "    39|         0|            0|            0|  0.00%|    'normalize_axis_index', 'packbits', 'promote_types', 'putmask',\n",
      "    40|         0|            0|            0|  0.00%|    'ravel_multi_index', 'result_type', 'scalar', 'set_datetimeparse_function',\n",
      "    41|         0|            0|            0|  0.00%|    'set_legacy_print_mode', 'set_numeric_ops', 'set_string_function',\n",
      "    42|         0|            0|            0|  0.00%|    'set_typeDict', 'shares_memory', 'tracemalloc_domain', 'typeinfo',\n",
      "    43|         0|            0|            0|  0.00%|    'unpackbits', 'unravel_index', 'vdot', 'where', 'zeros']\n",
      "    44|         0|            0|            0|  0.00%|\n",
      "    45|         0|            0|            0|  0.00%|# For backward compatibility, make sure pickle imports these functions from here\n",
      "    46|         0|            0|            0|  0.00%|_reconstruct.__module__ = 'numpy.core.multiarray'\n",
      "    47|         0|            0|            0|  0.00%|scalar.__module__ = 'numpy.core.multiarray'\n",
      "    48|         0|            0|            0|  0.00%|\n",
      "    49|         0|            0|            0|  0.00%|\n",
      "    50|         0|            0|            0|  0.00%|_from_dlpack.__module__ = 'numpy'\n",
      "    51|         0|            0|            0|  0.00%|arange.__module__ = 'numpy'\n",
      "    52|         0|            0|            0|  0.00%|array.__module__ = 'numpy'\n",
      "    53|         0|            0|            0|  0.00%|asarray.__module__ = 'numpy'\n",
      "    54|         0|            0|            0|  0.00%|asanyarray.__module__ = 'numpy'\n",
      "    55|         0|            0|            0|  0.00%|ascontiguousarray.__module__ = 'numpy'\n",
      "    56|         0|            0|            0|  0.00%|asfortranarray.__module__ = 'numpy'\n",
      "    57|         0|            0|            0|  0.00%|datetime_data.__module__ = 'numpy'\n",
      "    58|         0|            0|            0|  0.00%|empty.__module__ = 'numpy'\n",
      "    59|         0|            0|            0|  0.00%|frombuffer.__module__ = 'numpy'\n",
      "    60|         0|            0|            0|  0.00%|fromfile.__module__ = 'numpy'\n",
      "    61|         0|            0|            0|  0.00%|fromiter.__module__ = 'numpy'\n",
      "    62|         0|            0|            0|  0.00%|frompyfunc.__module__ = 'numpy'\n",
      "    63|         0|            0|            0|  0.00%|fromstring.__module__ = 'numpy'\n",
      "    64|         0|            0|            0|  0.00%|geterrobj.__module__ = 'numpy'\n",
      "    65|         0|            0|            0|  0.00%|may_share_memory.__module__ = 'numpy'\n",
      "    66|         0|            0|            0|  0.00%|nested_iters.__module__ = 'numpy'\n",
      "    67|         0|            0|            0|  0.00%|promote_types.__module__ = 'numpy'\n",
      "    68|         0|            0|            0|  0.00%|set_numeric_ops.__module__ = 'numpy'\n",
      "    69|         0|            0|            0|  0.00%|seterrobj.__module__ = 'numpy'\n",
      "    70|         0|            0|            0|  0.00%|zeros.__module__ = 'numpy'\n",
      "    71|         0|            0|            0|  0.00%|\n",
      "    72|         0|            0|            0|  0.00%|\n",
      "    73|         0|            0|            0|  0.00%|# We can't verify dispatcher signatures because NumPy's C functions don't\n",
      "    74|         0|            0|            0|  0.00%|# support introspection.\n",
      "    75|         0|            0|            0|  0.00%|array_function_from_c_func_and_dispatcher = functools.partial(\n",
      "    76|         0|            0|            0|  0.00%|    overrides.array_function_from_dispatcher,\n",
      "    77|         0|            0|            0|  0.00%|    module='numpy', docs_from_dispatcher=True, verify=False)\n",
      "    78|         0|            0|            0|  0.00%|\n",
      "    79|         0|            0|            0|  0.00%|\n",
      "    80|     74874|     0.102047|  1.36292e-06|  0.02%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.empty_like)\n",
      "    81|         0|            0|            0|  0.00%|def empty_like(prototype, dtype=None, order=None, subok=None, shape=None):\n",
      "    82|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    83|         0|            0|            0|  0.00%|    empty_like(prototype, dtype=None, order='K', subok=True, shape=None)\n",
      "    84|         0|            0|            0|  0.00%|\n",
      "    85|         0|            0|            0|  0.00%|    Return a new array with the same shape and type as a given array.\n",
      "    86|         0|            0|            0|  0.00%|\n",
      "    87|         0|            0|            0|  0.00%|    Parameters\n",
      "    88|         0|            0|            0|  0.00%|    ----------\n",
      "    89|         0|            0|            0|  0.00%|    prototype : array_like\n",
      "    90|         0|            0|            0|  0.00%|        The shape and data-type of `prototype` define these same attributes\n",
      "    91|         0|            0|            0|  0.00%|        of the returned array.\n",
      "    92|         0|            0|            0|  0.00%|    dtype : data-type, optional\n",
      "    93|         0|            0|            0|  0.00%|        Overrides the data type of the result.\n",
      "    94|         0|            0|            0|  0.00%|\n",
      "    95|         0|            0|            0|  0.00%|        .. versionadded:: 1.6.0\n",
      "    96|         0|            0|            0|  0.00%|    order : {'C', 'F', 'A', or 'K'}, optional\n",
      "    97|         0|            0|            0|  0.00%|        Overrides the memory layout of the result. 'C' means C-order,\n",
      "    98|         0|            0|            0|  0.00%|        'F' means F-order, 'A' means 'F' if `prototype` is Fortran\n",
      "    99|         0|            0|            0|  0.00%|        contiguous, 'C' otherwise. 'K' means match the layout of `prototype`\n",
      "   100|         0|            0|            0|  0.00%|        as closely as possible.\n",
      "   101|         0|            0|            0|  0.00%|\n",
      "   102|         0|            0|            0|  0.00%|        .. versionadded:: 1.6.0\n",
      "   103|         0|            0|            0|  0.00%|    subok : bool, optional.\n",
      "   104|         0|            0|            0|  0.00%|        If True, then the newly created array will use the sub-class\n",
      "   105|         0|            0|            0|  0.00%|        type of `prototype`, otherwise it will be a base-class array. Defaults\n",
      "   106|         0|            0|            0|  0.00%|        to True.\n",
      "   107|         0|            0|            0|  0.00%|    shape : int or sequence of ints, optional.\n",
      "   108|         0|            0|            0|  0.00%|        Overrides the shape of the result. If order='K' and the number of\n",
      "   109|         0|            0|            0|  0.00%|        dimensions is unchanged, will try to keep order, otherwise,\n",
      "   110|         0|            0|            0|  0.00%|        order='C' is implied.\n",
      "   111|         0|            0|            0|  0.00%|\n",
      "   112|         0|            0|            0|  0.00%|        .. versionadded:: 1.17.0\n",
      "   113|         0|            0|            0|  0.00%|\n",
      "   114|         0|            0|            0|  0.00%|    Returns\n",
      "   115|         0|            0|            0|  0.00%|    -------\n",
      "   116|         0|            0|            0|  0.00%|    out : ndarray\n",
      "   117|         0|            0|            0|  0.00%|        Array of uninitialized (arbitrary) data with the same\n",
      "   118|         0|            0|            0|  0.00%|        shape and type as `prototype`.\n",
      "   119|         0|            0|            0|  0.00%|\n",
      "   120|         0|            0|            0|  0.00%|    See Also\n",
      "   121|         0|            0|            0|  0.00%|    --------\n",
      "   122|         0|            0|            0|  0.00%|    ones_like : Return an array of ones with shape and type of input.\n",
      "   123|         0|            0|            0|  0.00%|    zeros_like : Return an array of zeros with shape and type of input.\n",
      "   124|         0|            0|            0|  0.00%|    full_like : Return a new array with shape of input filled with value.\n",
      "   125|         0|            0|            0|  0.00%|    empty : Return a new uninitialized array.\n",
      "   126|         0|            0|            0|  0.00%|\n",
      "   127|         0|            0|            0|  0.00%|    Notes\n",
      "   128|         0|            0|            0|  0.00%|    -----\n",
      "   129|         0|            0|            0|  0.00%|    This function does *not* initialize the returned array; to do that use\n",
      "   130|         0|            0|            0|  0.00%|    `zeros_like` or `ones_like` instead.  It may be marginally faster than\n",
      "   131|         0|            0|            0|  0.00%|    the functions that do set the array values.\n",
      "   132|         0|            0|            0|  0.00%|\n",
      "   133|         0|            0|            0|  0.00%|    Examples\n",
      "   134|         0|            0|            0|  0.00%|    --------\n",
      "   135|         0|            0|            0|  0.00%|    >>> a = ([1,2,3], [4,5,6])                         # a is array-like\n",
      "   136|         0|            0|            0|  0.00%|    >>> np.empty_like(a)\n",
      "   137|         0|            0|            0|  0.00%|    array([[-1073741821, -1073741821,           3],    # uninitialized\n",
      "   138|         0|            0|            0|  0.00%|           [          0,           0, -1073741821]])\n",
      "   139|         0|            0|            0|  0.00%|    >>> a = np.array([[1., 2., 3.],[4.,5.,6.]])\n",
      "   140|         0|            0|            0|  0.00%|    >>> np.empty_like(a)\n",
      "   141|         0|            0|            0|  0.00%|    array([[ -2.00000715e+000,   1.48219694e-323,  -2.00000572e+000], # uninitialized\n",
      "   142|         0|            0|            0|  0.00%|           [  4.38791518e-305,  -2.00000715e+000,   4.17269252e-309]])\n",
      "   143|         0|            0|            0|  0.00%|\n",
      "   144|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   145|     74874|     0.130403|  1.74163e-06|  0.02%|    return (prototype,)\n",
      "   146|         0|            0|            0|  0.00%|\n",
      "   147|         0|            0|            0|  0.00%|\n",
      "   148|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.concatenate)\n",
      "   149|         0|            0|            0|  0.00%|def concatenate(arrays, axis=None, out=None, *, dtype=None, casting=None):\n",
      "   150|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   151|         0|            0|            0|  0.00%|    concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n",
      "   152|         0|            0|            0|  0.00%|\n",
      "   153|         0|            0|            0|  0.00%|    Join a sequence of arrays along an existing axis.\n",
      "   154|         0|            0|            0|  0.00%|\n",
      "   155|         0|            0|            0|  0.00%|    Parameters\n",
      "   156|         0|            0|            0|  0.00%|    ----------\n",
      "   157|         0|            0|            0|  0.00%|    a1, a2, ... : sequence of array_like\n",
      "   158|         0|            0|            0|  0.00%|        The arrays must have the same shape, except in the dimension\n",
      "   159|         0|            0|            0|  0.00%|        corresponding to `axis` (the first, by default).\n",
      "   160|         0|            0|            0|  0.00%|    axis : int, optional\n",
      "   161|         0|            0|            0|  0.00%|        The axis along which the arrays will be joined.  If axis is None,\n",
      "   162|         0|            0|            0|  0.00%|        arrays are flattened before use.  Default is 0.\n",
      "   163|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "   164|         0|            0|            0|  0.00%|        If provided, the destination to place the result. The shape must be\n",
      "   165|         0|            0|            0|  0.00%|        correct, matching that of what concatenate would have returned if no\n",
      "   166|         0|            0|            0|  0.00%|        out argument were specified.\n",
      "   167|         0|            0|            0|  0.00%|    dtype : str or dtype\n",
      "   168|         0|            0|            0|  0.00%|        If provided, the destination array will have this dtype. Cannot be\n",
      "   169|         0|            0|            0|  0.00%|        provided together with `out`.\n",
      "   170|         0|            0|            0|  0.00%|\n",
      "   171|         0|            0|            0|  0.00%|        .. versionadded:: 1.20.0\n",
      "   172|         0|            0|            0|  0.00%|\n",
      "   173|         0|            0|            0|  0.00%|    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n",
      "   174|         0|            0|            0|  0.00%|        Controls what kind of data casting may occur. Defaults to 'same_kind'.\n",
      "   175|         0|            0|            0|  0.00%|\n",
      "   176|         0|            0|            0|  0.00%|        .. versionadded:: 1.20.0\n",
      "   177|         0|            0|            0|  0.00%|\n",
      "   178|         0|            0|            0|  0.00%|    Returns\n",
      "   179|         0|            0|            0|  0.00%|    -------\n",
      "   180|         0|            0|            0|  0.00%|    res : ndarray\n",
      "   181|         0|            0|            0|  0.00%|        The concatenated array.\n",
      "   182|         0|            0|            0|  0.00%|\n",
      "   183|         0|            0|            0|  0.00%|    See Also\n",
      "   184|         0|            0|            0|  0.00%|    --------\n",
      "   185|         0|            0|            0|  0.00%|    ma.concatenate : Concatenate function that preserves input masks.\n",
      "   186|         0|            0|            0|  0.00%|    array_split : Split an array into multiple sub-arrays of equal or\n",
      "   187|         0|            0|            0|  0.00%|                  near-equal size.\n",
      "   188|         0|            0|            0|  0.00%|    split : Split array into a list of multiple sub-arrays of equal size.\n",
      "   189|         0|            0|            0|  0.00%|    hsplit : Split array into multiple sub-arrays horizontally (column wise).\n",
      "   190|         0|            0|            0|  0.00%|    vsplit : Split array into multiple sub-arrays vertically (row wise).\n",
      "   191|         0|            0|            0|  0.00%|    dsplit : Split array into multiple sub-arrays along the 3rd axis (depth).\n",
      "   192|         0|            0|            0|  0.00%|    stack : Stack a sequence of arrays along a new axis.\n",
      "   193|         0|            0|            0|  0.00%|    block : Assemble arrays from blocks.\n",
      "   194|         0|            0|            0|  0.00%|    hstack : Stack arrays in sequence horizontally (column wise).\n",
      "   195|         0|            0|            0|  0.00%|    vstack : Stack arrays in sequence vertically (row wise).\n",
      "   196|         0|            0|            0|  0.00%|    dstack : Stack arrays in sequence depth wise (along third dimension).\n",
      "   197|         0|            0|            0|  0.00%|    column_stack : Stack 1-D arrays as columns into a 2-D array.\n",
      "   198|         0|            0|            0|  0.00%|\n",
      "   199|         0|            0|            0|  0.00%|    Notes\n",
      "   200|         0|            0|            0|  0.00%|    -----\n",
      "   201|         0|            0|            0|  0.00%|    When one or more of the arrays to be concatenated is a MaskedArray,\n",
      "   202|         0|            0|            0|  0.00%|    this function will return a MaskedArray object instead of an ndarray,\n",
      "   203|         0|            0|            0|  0.00%|    but the input masks are *not* preserved. In cases where a MaskedArray\n",
      "   204|         0|            0|            0|  0.00%|    is expected as input, use the ma.concatenate function from the masked\n",
      "   205|         0|            0|            0|  0.00%|    array module instead.\n",
      "   206|         0|            0|            0|  0.00%|\n",
      "   207|         0|            0|            0|  0.00%|    Examples\n",
      "   208|         0|            0|            0|  0.00%|    --------\n",
      "   209|         0|            0|            0|  0.00%|    >>> a = np.array([[1, 2], [3, 4]])\n",
      "   210|         0|            0|            0|  0.00%|    >>> b = np.array([[5, 6]])\n",
      "   211|         0|            0|            0|  0.00%|    >>> np.concatenate((a, b), axis=0)\n",
      "   212|         0|            0|            0|  0.00%|    array([[1, 2],\n",
      "   213|         0|            0|            0|  0.00%|           [3, 4],\n",
      "   214|         0|            0|            0|  0.00%|           [5, 6]])\n",
      "   215|         0|            0|            0|  0.00%|    >>> np.concatenate((a, b.T), axis=1)\n",
      "   216|         0|            0|            0|  0.00%|    array([[1, 2, 5],\n",
      "   217|         0|            0|            0|  0.00%|           [3, 4, 6]])\n",
      "   218|         0|            0|            0|  0.00%|    >>> np.concatenate((a, b), axis=None)\n",
      "   219|         0|            0|            0|  0.00%|    array([1, 2, 3, 4, 5, 6])\n",
      "   220|         0|            0|            0|  0.00%|\n",
      "   221|         0|            0|            0|  0.00%|    This function will not preserve masking of MaskedArray inputs.\n",
      "   222|         0|            0|            0|  0.00%|\n",
      "   223|         0|            0|            0|  0.00%|    >>> a = np.ma.arange(3)\n",
      "   224|         0|            0|            0|  0.00%|    >>> a[1] = np.ma.masked\n",
      "   225|         0|            0|            0|  0.00%|    >>> b = np.arange(2, 5)\n",
      "   226|         0|            0|            0|  0.00%|    >>> a\n",
      "   227|         0|            0|            0|  0.00%|    masked_array(data=[0, --, 2],\n",
      "   228|         0|            0|            0|  0.00%|                 mask=[False,  True, False],\n",
      "   229|         0|            0|            0|  0.00%|           fill_value=999999)\n",
      "   230|         0|            0|            0|  0.00%|    >>> b\n",
      "   231|         0|            0|            0|  0.00%|    array([2, 3, 4])\n",
      "   232|         0|            0|            0|  0.00%|    >>> np.concatenate([a, b])\n",
      "   233|         0|            0|            0|  0.00%|    masked_array(data=[0, 1, 2, 2, 3, 4],\n",
      "   234|         0|            0|            0|  0.00%|                 mask=False,\n",
      "   235|         0|            0|            0|  0.00%|           fill_value=999999)\n",
      "   236|         0|            0|            0|  0.00%|    >>> np.ma.concatenate([a, b])\n",
      "   237|         0|            0|            0|  0.00%|    masked_array(data=[0, --, 2, 2, 3, 4],\n",
      "   238|         0|            0|            0|  0.00%|                 mask=[False,  True, False, False, False, False],\n",
      "   239|         0|            0|            0|  0.00%|           fill_value=999999)\n",
      "   240|         0|            0|            0|  0.00%|\n",
      "   241|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   242|         0|            0|            0|  0.00%|    if out is not None:\n",
      "   243|         0|            0|            0|  0.00%|        # optimize for the typical case where only arrays is provided\n",
      "   244|         0|            0|            0|  0.00%|        arrays = list(arrays)\n",
      "   245|         0|            0|            0|  0.00%|        arrays.append(out)\n",
      "   246|         0|            0|            0|  0.00%|    return arrays\n",
      "   247|         0|            0|            0|  0.00%|\n",
      "   248|         0|            0|            0|  0.00%|\n",
      "   249|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.inner)\n",
      "   250|         0|            0|            0|  0.00%|def inner(a, b):\n",
      "   251|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   252|         0|            0|            0|  0.00%|    inner(a, b, /)\n",
      "   253|         0|            0|            0|  0.00%|\n",
      "   254|         0|            0|            0|  0.00%|    Inner product of two arrays.\n",
      "   255|         0|            0|            0|  0.00%|\n",
      "   256|         0|            0|            0|  0.00%|    Ordinary inner product of vectors for 1-D arrays (without complex\n",
      "   257|         0|            0|            0|  0.00%|    conjugation), in higher dimensions a sum product over the last axes.\n",
      "   258|         0|            0|            0|  0.00%|\n",
      "   259|         0|            0|            0|  0.00%|    Parameters\n",
      "   260|         0|            0|            0|  0.00%|    ----------\n",
      "   261|         0|            0|            0|  0.00%|    a, b : array_like\n",
      "   262|         0|            0|            0|  0.00%|        If `a` and `b` are nonscalar, their last dimensions must match.\n",
      "   263|         0|            0|            0|  0.00%|\n",
      "   264|         0|            0|            0|  0.00%|    Returns\n",
      "   265|         0|            0|            0|  0.00%|    -------\n",
      "   266|         0|            0|            0|  0.00%|    out : ndarray\n",
      "   267|         0|            0|            0|  0.00%|        If `a` and `b` are both\n",
      "   268|         0|            0|            0|  0.00%|        scalars or both 1-D arrays then a scalar is returned; otherwise\n",
      "   269|         0|            0|            0|  0.00%|        an array is returned.\n",
      "   270|         0|            0|            0|  0.00%|        ``out.shape = (*a.shape[:-1], *b.shape[:-1])``\n",
      "   271|         0|            0|            0|  0.00%|\n",
      "   272|         0|            0|            0|  0.00%|    Raises\n",
      "   273|         0|            0|            0|  0.00%|    ------\n",
      "   274|         0|            0|            0|  0.00%|    ValueError\n",
      "   275|         0|            0|            0|  0.00%|        If both `a` and `b` are nonscalar and their last dimensions have\n",
      "   276|         0|            0|            0|  0.00%|        different sizes.\n",
      "   277|         0|            0|            0|  0.00%|\n",
      "   278|         0|            0|            0|  0.00%|    See Also\n",
      "   279|         0|            0|            0|  0.00%|    --------\n",
      "   280|         0|            0|            0|  0.00%|    tensordot : Sum products over arbitrary axes.\n",
      "   281|         0|            0|            0|  0.00%|    dot : Generalised matrix product, using second last dimension of `b`.\n",
      "   282|         0|            0|            0|  0.00%|    einsum : Einstein summation convention.\n",
      "   283|         0|            0|            0|  0.00%|\n",
      "   284|         0|            0|            0|  0.00%|    Notes\n",
      "   285|         0|            0|            0|  0.00%|    -----\n",
      "   286|         0|            0|            0|  0.00%|    For vectors (1-D arrays) it computes the ordinary inner-product::\n",
      "   287|         0|            0|            0|  0.00%|\n",
      "   288|         0|            0|            0|  0.00%|        np.inner(a, b) = sum(a[:]*b[:])\n",
      "   289|         0|            0|            0|  0.00%|\n",
      "   290|         0|            0|            0|  0.00%|    More generally, if `ndim(a) = r > 0` and `ndim(b) = s > 0`::\n",
      "   291|         0|            0|            0|  0.00%|\n",
      "   292|         0|            0|            0|  0.00%|        np.inner(a, b) = np.tensordot(a, b, axes=(-1,-1))\n",
      "   293|         0|            0|            0|  0.00%|\n",
      "   294|         0|            0|            0|  0.00%|    or explicitly::\n",
      "   295|         0|            0|            0|  0.00%|\n",
      "   296|         0|            0|            0|  0.00%|        np.inner(a, b)[i0,...,ir-2,j0,...,js-2]\n",
      "   297|         0|            0|            0|  0.00%|             = sum(a[i0,...,ir-2,:]*b[j0,...,js-2,:])\n",
      "   298|         0|            0|            0|  0.00%|\n",
      "   299|         0|            0|            0|  0.00%|    In addition `a` or `b` may be scalars, in which case::\n",
      "   300|         0|            0|            0|  0.00%|\n",
      "   301|         0|            0|            0|  0.00%|       np.inner(a,b) = a*b\n",
      "   302|         0|            0|            0|  0.00%|\n",
      "   303|         0|            0|            0|  0.00%|    Examples\n",
      "   304|         0|            0|            0|  0.00%|    --------\n",
      "   305|         0|            0|            0|  0.00%|    Ordinary inner product for vectors:\n",
      "   306|         0|            0|            0|  0.00%|\n",
      "   307|         0|            0|            0|  0.00%|    >>> a = np.array([1,2,3])\n",
      "   308|         0|            0|            0|  0.00%|    >>> b = np.array([0,1,0])\n",
      "   309|         0|            0|            0|  0.00%|    >>> np.inner(a, b)\n",
      "   310|         0|            0|            0|  0.00%|    2\n",
      "   311|         0|            0|            0|  0.00%|\n",
      "   312|         0|            0|            0|  0.00%|    Some multidimensional examples:\n",
      "   313|         0|            0|            0|  0.00%|\n",
      "   314|         0|            0|            0|  0.00%|    >>> a = np.arange(24).reshape((2,3,4))\n",
      "   315|         0|            0|            0|  0.00%|    >>> b = np.arange(4)\n",
      "   316|         0|            0|            0|  0.00%|    >>> c = np.inner(a, b)\n",
      "   317|         0|            0|            0|  0.00%|    >>> c.shape\n",
      "   318|         0|            0|            0|  0.00%|    (2, 3)\n",
      "   319|         0|            0|            0|  0.00%|    >>> c\n",
      "   320|         0|            0|            0|  0.00%|    array([[ 14,  38,  62],\n",
      "   321|         0|            0|            0|  0.00%|           [ 86, 110, 134]])\n",
      "   322|         0|            0|            0|  0.00%|\n",
      "   323|         0|            0|            0|  0.00%|    >>> a = np.arange(2).reshape((1,1,2))\n",
      "   324|         0|            0|            0|  0.00%|    >>> b = np.arange(6).reshape((3,2))\n",
      "   325|         0|            0|            0|  0.00%|    >>> c = np.inner(a, b)\n",
      "   326|         0|            0|            0|  0.00%|    >>> c.shape\n",
      "   327|         0|            0|            0|  0.00%|    (1, 1, 3)\n",
      "   328|         0|            0|            0|  0.00%|    >>> c\n",
      "   329|         0|            0|            0|  0.00%|    array([[[1, 3, 5]]])\n",
      "   330|         0|            0|            0|  0.00%|\n",
      "   331|         0|            0|            0|  0.00%|    An example where `b` is a scalar:\n",
      "   332|         0|            0|            0|  0.00%|\n",
      "   333|         0|            0|            0|  0.00%|    >>> np.inner(np.eye(2), 7)\n",
      "   334|         0|            0|            0|  0.00%|    array([[7., 0.],\n",
      "   335|         0|            0|            0|  0.00%|           [0., 7.]])\n",
      "   336|         0|            0|            0|  0.00%|\n",
      "   337|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   338|         0|            0|            0|  0.00%|    return (a, b)\n",
      "   339|         0|            0|            0|  0.00%|\n",
      "   340|         0|            0|            0|  0.00%|\n",
      "   341|    374390|     0.515151|  1.37597e-06|  0.08%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.where)\n",
      "   342|         0|            0|            0|  0.00%|def where(condition, x=None, y=None):\n",
      "   343|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   344|         0|            0|            0|  0.00%|    where(condition, [x, y], /)\n",
      "   345|         0|            0|            0|  0.00%|\n",
      "   346|         0|            0|            0|  0.00%|    Return elements chosen from `x` or `y` depending on `condition`.\n",
      "   347|         0|            0|            0|  0.00%|\n",
      "   348|         0|            0|            0|  0.00%|    .. note::\n",
      "   349|         0|            0|            0|  0.00%|        When only `condition` is provided, this function is a shorthand for\n",
      "   350|         0|            0|            0|  0.00%|        ``np.asarray(condition).nonzero()``. Using `nonzero` directly should be\n",
      "   351|         0|            0|            0|  0.00%|        preferred, as it behaves correctly for subclasses. The rest of this\n",
      "   352|         0|            0|            0|  0.00%|        documentation covers only the case where all three arguments are\n",
      "   353|         0|            0|            0|  0.00%|        provided.\n",
      "   354|         0|            0|            0|  0.00%|\n",
      "   355|         0|            0|            0|  0.00%|    Parameters\n",
      "   356|         0|            0|            0|  0.00%|    ----------\n",
      "   357|         0|            0|            0|  0.00%|    condition : array_like, bool\n",
      "   358|         0|            0|            0|  0.00%|        Where True, yield `x`, otherwise yield `y`.\n",
      "   359|         0|            0|            0|  0.00%|    x, y : array_like\n",
      "   360|         0|            0|            0|  0.00%|        Values from which to choose. `x`, `y` and `condition` need to be\n",
      "   361|         0|            0|            0|  0.00%|        broadcastable to some shape.\n",
      "   362|         0|            0|            0|  0.00%|\n",
      "   363|         0|            0|            0|  0.00%|    Returns\n",
      "   364|         0|            0|            0|  0.00%|    -------\n",
      "   365|         0|            0|            0|  0.00%|    out : ndarray\n",
      "   366|         0|            0|            0|  0.00%|        An array with elements from `x` where `condition` is True, and elements\n",
      "   367|         0|            0|            0|  0.00%|        from `y` elsewhere.\n",
      "   368|         0|            0|            0|  0.00%|\n",
      "   369|         0|            0|            0|  0.00%|    See Also\n",
      "   370|         0|            0|            0|  0.00%|    --------\n",
      "   371|         0|            0|            0|  0.00%|    choose\n",
      "   372|         0|            0|            0|  0.00%|    nonzero : The function that is called when x and y are omitted\n",
      "   373|         0|            0|            0|  0.00%|\n",
      "   374|         0|            0|            0|  0.00%|    Notes\n",
      "   375|         0|            0|            0|  0.00%|    -----\n",
      "   376|         0|            0|            0|  0.00%|    If all the arrays are 1-D, `where` is equivalent to::\n",
      "   377|         0|            0|            0|  0.00%|\n",
      "   378|         0|            0|            0|  0.00%|        [xv if c else yv\n",
      "   379|         0|            0|            0|  0.00%|         for c, xv, yv in zip(condition, x, y)]\n",
      "   380|         0|            0|            0|  0.00%|\n",
      "   381|         0|            0|            0|  0.00%|    Examples\n",
      "   382|         0|            0|            0|  0.00%|    --------\n",
      "   383|         0|            0|            0|  0.00%|    >>> a = np.arange(10)\n",
      "   384|         0|            0|            0|  0.00%|    >>> a\n",
      "   385|         0|            0|            0|  0.00%|    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "   386|         0|            0|            0|  0.00%|    >>> np.where(a < 5, a, 10*a)\n",
      "   387|         0|            0|            0|  0.00%|    array([ 0,  1,  2,  3,  4, 50, 60, 70, 80, 90])\n",
      "   388|         0|            0|            0|  0.00%|\n",
      "   389|         0|            0|            0|  0.00%|    This can be used on multidimensional arrays too:\n",
      "   390|         0|            0|            0|  0.00%|\n",
      "   391|         0|            0|            0|  0.00%|    >>> np.where([[True, False], [True, True]],\n",
      "   392|         0|            0|            0|  0.00%|    ...          [[1, 2], [3, 4]],\n",
      "   393|         0|            0|            0|  0.00%|    ...          [[9, 8], [7, 6]])\n",
      "   394|         0|            0|            0|  0.00%|    array([[1, 8],\n",
      "   395|         0|            0|            0|  0.00%|           [3, 4]])\n",
      "   396|         0|            0|            0|  0.00%|\n",
      "   397|         0|            0|            0|  0.00%|    The shapes of x, y, and the condition are broadcast together:\n",
      "   398|         0|            0|            0|  0.00%|\n",
      "   399|         0|            0|            0|  0.00%|    >>> x, y = np.ogrid[:3, :4]\n",
      "   400|         0|            0|            0|  0.00%|    >>> np.where(x < y, x, 10 + y)  # both x and 10+y are broadcast\n",
      "   401|         0|            0|            0|  0.00%|    array([[10,  0,  0,  0],\n",
      "   402|         0|            0|            0|  0.00%|           [10, 11,  1,  1],\n",
      "   403|         0|            0|            0|  0.00%|           [10, 11, 12,  2]])\n",
      "   404|         0|            0|            0|  0.00%|\n",
      "   405|         0|            0|            0|  0.00%|    >>> a = np.array([[0, 1, 2],\n",
      "   406|         0|            0|            0|  0.00%|    ...               [0, 2, 4],\n",
      "   407|         0|            0|            0|  0.00%|    ...               [0, 3, 6]])\n",
      "   408|         0|            0|            0|  0.00%|    >>> np.where(a < 4, a, -1)  # -1 is broadcast\n",
      "   409|         0|            0|            0|  0.00%|    array([[ 0,  1,  2],\n",
      "   410|         0|            0|            0|  0.00%|           [ 0,  2, -1],\n",
      "   411|         0|            0|            0|  0.00%|           [ 0,  3, -1]])\n",
      "   412|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   413|    374390|     0.662496|  1.76953e-06|  0.11%|    return (condition, x, y)\n",
      "   414|         0|            0|            0|  0.00%|\n",
      "   415|         0|            0|            0|  0.00%|\n",
      "   416|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.lexsort)\n",
      "   417|         0|            0|            0|  0.00%|def lexsort(keys, axis=None):\n",
      "   418|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   419|         0|            0|            0|  0.00%|    lexsort(keys, axis=-1)\n",
      "   420|         0|            0|            0|  0.00%|\n",
      "   421|         0|            0|            0|  0.00%|    Perform an indirect stable sort using a sequence of keys.\n",
      "   422|         0|            0|            0|  0.00%|\n",
      "   423|         0|            0|            0|  0.00%|    Given multiple sorting keys, which can be interpreted as columns in a\n",
      "   424|         0|            0|            0|  0.00%|    spreadsheet, lexsort returns an array of integer indices that describes\n",
      "   425|         0|            0|            0|  0.00%|    the sort order by multiple columns. The last key in the sequence is used\n",
      "   426|         0|            0|            0|  0.00%|    for the primary sort order, the second-to-last key for the secondary sort\n",
      "   427|         0|            0|            0|  0.00%|    order, and so on. The keys argument must be a sequence of objects that\n",
      "   428|         0|            0|            0|  0.00%|    can be converted to arrays of the same shape. If a 2D array is provided\n",
      "   429|         0|            0|            0|  0.00%|    for the keys argument, its rows are interpreted as the sorting keys and\n",
      "   430|         0|            0|            0|  0.00%|    sorting is according to the last row, second last row etc.\n",
      "   431|         0|            0|            0|  0.00%|\n",
      "   432|         0|            0|            0|  0.00%|    Parameters\n",
      "   433|         0|            0|            0|  0.00%|    ----------\n",
      "   434|         0|            0|            0|  0.00%|    keys : (k, N) array or tuple containing k (N,)-shaped sequences\n",
      "   435|         0|            0|            0|  0.00%|        The `k` different \"columns\" to be sorted.  The last column (or row if\n",
      "   436|         0|            0|            0|  0.00%|        `keys` is a 2D array) is the primary sort key.\n",
      "   437|         0|            0|            0|  0.00%|    axis : int, optional\n",
      "   438|         0|            0|            0|  0.00%|        Axis to be indirectly sorted.  By default, sort over the last axis.\n",
      "   439|         0|            0|            0|  0.00%|\n",
      "   440|         0|            0|            0|  0.00%|    Returns\n",
      "   441|         0|            0|            0|  0.00%|    -------\n",
      "   442|         0|            0|            0|  0.00%|    indices : (N,) ndarray of ints\n",
      "   443|         0|            0|            0|  0.00%|        Array of indices that sort the keys along the specified axis.\n",
      "   444|         0|            0|            0|  0.00%|\n",
      "   445|         0|            0|            0|  0.00%|    See Also\n",
      "   446|         0|            0|            0|  0.00%|    --------\n",
      "   447|         0|            0|            0|  0.00%|    argsort : Indirect sort.\n",
      "   448|         0|            0|            0|  0.00%|    ndarray.sort : In-place sort.\n",
      "   449|         0|            0|            0|  0.00%|    sort : Return a sorted copy of an array.\n",
      "   450|         0|            0|            0|  0.00%|\n",
      "   451|         0|            0|            0|  0.00%|    Examples\n",
      "   452|         0|            0|            0|  0.00%|    --------\n",
      "   453|         0|            0|            0|  0.00%|    Sort names: first by surname, then by name.\n",
      "   454|         0|            0|            0|  0.00%|\n",
      "   455|         0|            0|            0|  0.00%|    >>> surnames =    ('Hertz',    'Galilei', 'Hertz')\n",
      "   456|         0|            0|            0|  0.00%|    >>> first_names = ('Heinrich', 'Galileo', 'Gustav')\n",
      "   457|         0|            0|            0|  0.00%|    >>> ind = np.lexsort((first_names, surnames))\n",
      "   458|         0|            0|            0|  0.00%|    >>> ind\n",
      "   459|         0|            0|            0|  0.00%|    array([1, 2, 0])\n",
      "   460|         0|            0|            0|  0.00%|\n",
      "   461|         0|            0|            0|  0.00%|    >>> [surnames[i] + \", \" + first_names[i] for i in ind]\n",
      "   462|         0|            0|            0|  0.00%|    ['Galilei, Galileo', 'Hertz, Gustav', 'Hertz, Heinrich']\n",
      "   463|         0|            0|            0|  0.00%|\n",
      "   464|         0|            0|            0|  0.00%|    Sort two columns of numbers:\n",
      "   465|         0|            0|            0|  0.00%|\n",
      "   466|         0|            0|            0|  0.00%|    >>> a = [1,5,1,4,3,4,4] # First column\n",
      "   467|         0|            0|            0|  0.00%|    >>> b = [9,4,0,4,0,2,1] # Second column\n",
      "   468|         0|            0|            0|  0.00%|    >>> ind = np.lexsort((b,a)) # Sort by a, then by b\n",
      "   469|         0|            0|            0|  0.00%|    >>> ind\n",
      "   470|         0|            0|            0|  0.00%|    array([2, 0, 4, 6, 5, 3, 1])\n",
      "   471|         0|            0|            0|  0.00%|\n",
      "   472|         0|            0|            0|  0.00%|    >>> [(a[i],b[i]) for i in ind]\n",
      "   473|         0|            0|            0|  0.00%|    [(1, 0), (1, 9), (3, 0), (4, 1), (4, 2), (4, 4), (5, 4)]\n",
      "   474|         0|            0|            0|  0.00%|\n",
      "   475|         0|            0|            0|  0.00%|    Note that sorting is first according to the elements of ``a``.\n",
      "   476|         0|            0|            0|  0.00%|    Secondary sorting is according to the elements of ``b``.\n",
      "   477|         0|            0|            0|  0.00%|\n",
      "   478|         0|            0|            0|  0.00%|    A normal ``argsort`` would have yielded:\n",
      "   479|         0|            0|            0|  0.00%|\n",
      "   480|         0|            0|            0|  0.00%|    >>> [(a[i],b[i]) for i in np.argsort(a)]\n",
      "   481|         0|            0|            0|  0.00%|    [(1, 9), (1, 0), (3, 0), (4, 4), (4, 2), (4, 1), (5, 4)]\n",
      "   482|         0|            0|            0|  0.00%|\n",
      "   483|         0|            0|            0|  0.00%|    Structured arrays are sorted lexically by ``argsort``:\n",
      "   484|         0|            0|            0|  0.00%|\n",
      "   485|         0|            0|            0|  0.00%|    >>> x = np.array([(1,9), (5,4), (1,0), (4,4), (3,0), (4,2), (4,1)],\n",
      "   486|         0|            0|            0|  0.00%|    ...              dtype=np.dtype([('x', int), ('y', int)]))\n",
      "   487|         0|            0|            0|  0.00%|\n",
      "   488|         0|            0|            0|  0.00%|    >>> np.argsort(x) # or np.argsort(x, order=('x', 'y'))\n",
      "   489|         0|            0|            0|  0.00%|    array([2, 0, 4, 6, 5, 3, 1])\n",
      "   490|         0|            0|            0|  0.00%|\n",
      "   491|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   492|         0|            0|            0|  0.00%|    if isinstance(keys, tuple):\n",
      "   493|         0|            0|            0|  0.00%|        return keys\n",
      "   494|         0|            0|            0|  0.00%|    else:\n",
      "   495|         0|            0|            0|  0.00%|        return (keys,)\n",
      "   496|         0|            0|            0|  0.00%|\n",
      "   497|         0|            0|            0|  0.00%|\n",
      "   498|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.can_cast)\n",
      "   499|         0|            0|            0|  0.00%|def can_cast(from_, to, casting=None):\n",
      "   500|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   501|         0|            0|            0|  0.00%|    can_cast(from_, to, casting='safe')\n",
      "   502|         0|            0|            0|  0.00%|\n",
      "   503|         0|            0|            0|  0.00%|    Returns True if cast between data types can occur according to the\n",
      "   504|         0|            0|            0|  0.00%|    casting rule.  If from is a scalar or array scalar, also returns\n",
      "   505|         0|            0|            0|  0.00%|    True if the scalar value can be cast without overflow or truncation\n",
      "   506|         0|            0|            0|  0.00%|    to an integer.\n",
      "   507|         0|            0|            0|  0.00%|\n",
      "   508|         0|            0|            0|  0.00%|    Parameters\n",
      "   509|         0|            0|            0|  0.00%|    ----------\n",
      "   510|         0|            0|            0|  0.00%|    from_ : dtype, dtype specifier, scalar, or array\n",
      "   511|         0|            0|            0|  0.00%|        Data type, scalar, or array to cast from.\n",
      "   512|         0|            0|            0|  0.00%|    to : dtype or dtype specifier\n",
      "   513|         0|            0|            0|  0.00%|        Data type to cast to.\n",
      "   514|         0|            0|            0|  0.00%|    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n",
      "   515|         0|            0|            0|  0.00%|        Controls what kind of data casting may occur.\n",
      "   516|         0|            0|            0|  0.00%|\n",
      "   517|         0|            0|            0|  0.00%|          * 'no' means the data types should not be cast at all.\n",
      "   518|         0|            0|            0|  0.00%|          * 'equiv' means only byte-order changes are allowed.\n",
      "   519|         0|            0|            0|  0.00%|          * 'safe' means only casts which can preserve values are allowed.\n",
      "   520|         0|            0|            0|  0.00%|          * 'same_kind' means only safe casts or casts within a kind,\n",
      "   521|         0|            0|            0|  0.00%|            like float64 to float32, are allowed.\n",
      "   522|         0|            0|            0|  0.00%|          * 'unsafe' means any data conversions may be done.\n",
      "   523|         0|            0|            0|  0.00%|\n",
      "   524|         0|            0|            0|  0.00%|    Returns\n",
      "   525|         0|            0|            0|  0.00%|    -------\n",
      "   526|         0|            0|            0|  0.00%|    out : bool\n",
      "   527|         0|            0|            0|  0.00%|        True if cast can occur according to the casting rule.\n",
      "   528|         0|            0|            0|  0.00%|\n",
      "   529|         0|            0|            0|  0.00%|    Notes\n",
      "   530|         0|            0|            0|  0.00%|    -----\n",
      "   531|         0|            0|            0|  0.00%|    .. versionchanged:: 1.17.0\n",
      "   532|         0|            0|            0|  0.00%|       Casting between a simple data type and a structured one is possible only\n",
      "   533|         0|            0|            0|  0.00%|       for \"unsafe\" casting.  Casting to multiple fields is allowed, but\n",
      "   534|         0|            0|            0|  0.00%|       casting from multiple fields is not.\n",
      "   535|         0|            0|            0|  0.00%|\n",
      "   536|         0|            0|            0|  0.00%|    .. versionchanged:: 1.9.0\n",
      "   537|         0|            0|            0|  0.00%|       Casting from numeric to string types in 'safe' casting mode requires\n",
      "   538|         0|            0|            0|  0.00%|       that the string dtype length is long enough to store the maximum\n",
      "   539|         0|            0|            0|  0.00%|       integer/float value converted.\n",
      "   540|         0|            0|            0|  0.00%|\n",
      "   541|         0|            0|            0|  0.00%|    See also\n",
      "   542|         0|            0|            0|  0.00%|    --------\n",
      "   543|         0|            0|            0|  0.00%|    dtype, result_type\n",
      "   544|         0|            0|            0|  0.00%|\n",
      "   545|         0|            0|            0|  0.00%|    Examples\n",
      "   546|         0|            0|            0|  0.00%|    --------\n",
      "   547|         0|            0|            0|  0.00%|    Basic examples\n",
      "   548|         0|            0|            0|  0.00%|\n",
      "   549|         0|            0|            0|  0.00%|    >>> np.can_cast(np.int32, np.int64)\n",
      "   550|         0|            0|            0|  0.00%|    True\n",
      "   551|         0|            0|            0|  0.00%|    >>> np.can_cast(np.float64, complex)\n",
      "   552|         0|            0|            0|  0.00%|    True\n",
      "   553|         0|            0|            0|  0.00%|    >>> np.can_cast(complex, float)\n",
      "   554|         0|            0|            0|  0.00%|    False\n",
      "   555|         0|            0|            0|  0.00%|\n",
      "   556|         0|            0|            0|  0.00%|    >>> np.can_cast('i8', 'f8')\n",
      "   557|         0|            0|            0|  0.00%|    True\n",
      "   558|         0|            0|            0|  0.00%|    >>> np.can_cast('i8', 'f4')\n",
      "   559|         0|            0|            0|  0.00%|    False\n",
      "   560|         0|            0|            0|  0.00%|    >>> np.can_cast('i4', 'S4')\n",
      "   561|         0|            0|            0|  0.00%|    False\n",
      "   562|         0|            0|            0|  0.00%|\n",
      "   563|         0|            0|            0|  0.00%|    Casting scalars\n",
      "   564|         0|            0|            0|  0.00%|\n",
      "   565|         0|            0|            0|  0.00%|    >>> np.can_cast(100, 'i1')\n",
      "   566|         0|            0|            0|  0.00%|    True\n",
      "   567|         0|            0|            0|  0.00%|    >>> np.can_cast(150, 'i1')\n",
      "   568|         0|            0|            0|  0.00%|    False\n",
      "   569|         0|            0|            0|  0.00%|    >>> np.can_cast(150, 'u1')\n",
      "   570|         0|            0|            0|  0.00%|    True\n",
      "   571|         0|            0|            0|  0.00%|\n",
      "   572|         0|            0|            0|  0.00%|    >>> np.can_cast(3.5e100, np.float32)\n",
      "   573|         0|            0|            0|  0.00%|    False\n",
      "   574|         0|            0|            0|  0.00%|    >>> np.can_cast(1000.0, np.float32)\n",
      "   575|         0|            0|            0|  0.00%|    True\n",
      "   576|         0|            0|            0|  0.00%|\n",
      "   577|         0|            0|            0|  0.00%|    Array scalar checks the value, array does not\n",
      "   578|         0|            0|            0|  0.00%|\n",
      "   579|         0|            0|            0|  0.00%|    >>> np.can_cast(np.array(1000.0), np.float32)\n",
      "   580|         0|            0|            0|  0.00%|    True\n",
      "   581|         0|            0|            0|  0.00%|    >>> np.can_cast(np.array([1000.0]), np.float32)\n",
      "   582|         0|            0|            0|  0.00%|    False\n",
      "   583|         0|            0|            0|  0.00%|\n",
      "   584|         0|            0|            0|  0.00%|    Using the casting rules\n",
      "   585|         0|            0|            0|  0.00%|\n",
      "   586|         0|            0|            0|  0.00%|    >>> np.can_cast('i8', 'i8', 'no')\n",
      "   587|         0|            0|            0|  0.00%|    True\n",
      "   588|         0|            0|            0|  0.00%|    >>> np.can_cast('<i8', '>i8', 'no')\n",
      "   589|         0|            0|            0|  0.00%|    False\n",
      "   590|         0|            0|            0|  0.00%|\n",
      "   591|         0|            0|            0|  0.00%|    >>> np.can_cast('<i8', '>i8', 'equiv')\n",
      "   592|         0|            0|            0|  0.00%|    True\n",
      "   593|         0|            0|            0|  0.00%|    >>> np.can_cast('<i4', '>i8', 'equiv')\n",
      "   594|         0|            0|            0|  0.00%|    False\n",
      "   595|         0|            0|            0|  0.00%|\n",
      "   596|         0|            0|            0|  0.00%|    >>> np.can_cast('<i4', '>i8', 'safe')\n",
      "   597|         0|            0|            0|  0.00%|    True\n",
      "   598|         0|            0|            0|  0.00%|    >>> np.can_cast('<i8', '>i4', 'safe')\n",
      "   599|         0|            0|            0|  0.00%|    False\n",
      "   600|         0|            0|            0|  0.00%|\n",
      "   601|         0|            0|            0|  0.00%|    >>> np.can_cast('<i8', '>i4', 'same_kind')\n",
      "   602|         0|            0|            0|  0.00%|    True\n",
      "   603|         0|            0|            0|  0.00%|    >>> np.can_cast('<i8', '>u4', 'same_kind')\n",
      "   604|         0|            0|            0|  0.00%|    False\n",
      "   605|         0|            0|            0|  0.00%|\n",
      "   606|         0|            0|            0|  0.00%|    >>> np.can_cast('<i8', '>u4', 'unsafe')\n",
      "   607|         0|            0|            0|  0.00%|    True\n",
      "   608|         0|            0|            0|  0.00%|\n",
      "   609|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   610|         0|            0|            0|  0.00%|    return (from_,)\n",
      "   611|         0|            0|            0|  0.00%|\n",
      "   612|         0|            0|            0|  0.00%|\n",
      "   613|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.min_scalar_type)\n",
      "   614|         0|            0|            0|  0.00%|def min_scalar_type(a):\n",
      "   615|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   616|         0|            0|            0|  0.00%|    min_scalar_type(a, /)\n",
      "   617|         0|            0|            0|  0.00%|\n",
      "   618|         0|            0|            0|  0.00%|    For scalar ``a``, returns the data type with the smallest size\n",
      "   619|         0|            0|            0|  0.00%|    and smallest scalar kind which can hold its value.  For non-scalar\n",
      "   620|         0|            0|            0|  0.00%|    array ``a``, returns the vector's dtype unmodified.\n",
      "   621|         0|            0|            0|  0.00%|\n",
      "   622|         0|            0|            0|  0.00%|    Floating point values are not demoted to integers,\n",
      "   623|         0|            0|            0|  0.00%|    and complex values are not demoted to floats.\n",
      "   624|         0|            0|            0|  0.00%|\n",
      "   625|         0|            0|            0|  0.00%|    Parameters\n",
      "   626|         0|            0|            0|  0.00%|    ----------\n",
      "   627|         0|            0|            0|  0.00%|    a : scalar or array_like\n",
      "   628|         0|            0|            0|  0.00%|        The value whose minimal data type is to be found.\n",
      "   629|         0|            0|            0|  0.00%|\n",
      "   630|         0|            0|            0|  0.00%|    Returns\n",
      "   631|         0|            0|            0|  0.00%|    -------\n",
      "   632|         0|            0|            0|  0.00%|    out : dtype\n",
      "   633|         0|            0|            0|  0.00%|        The minimal data type.\n",
      "   634|         0|            0|            0|  0.00%|\n",
      "   635|         0|            0|            0|  0.00%|    Notes\n",
      "   636|         0|            0|            0|  0.00%|    -----\n",
      "   637|         0|            0|            0|  0.00%|    .. versionadded:: 1.6.0\n",
      "   638|         0|            0|            0|  0.00%|\n",
      "   639|         0|            0|            0|  0.00%|    See Also\n",
      "   640|         0|            0|            0|  0.00%|    --------\n",
      "   641|         0|            0|            0|  0.00%|    result_type, promote_types, dtype, can_cast\n",
      "   642|         0|            0|            0|  0.00%|\n",
      "   643|         0|            0|            0|  0.00%|    Examples\n",
      "   644|         0|            0|            0|  0.00%|    --------\n",
      "   645|         0|            0|            0|  0.00%|    >>> np.min_scalar_type(10)\n",
      "   646|         0|            0|            0|  0.00%|    dtype('uint8')\n",
      "   647|         0|            0|            0|  0.00%|\n",
      "   648|         0|            0|            0|  0.00%|    >>> np.min_scalar_type(-260)\n",
      "   649|         0|            0|            0|  0.00%|    dtype('int16')\n",
      "   650|         0|            0|            0|  0.00%|\n",
      "   651|         0|            0|            0|  0.00%|    >>> np.min_scalar_type(3.1)\n",
      "   652|         0|            0|            0|  0.00%|    dtype('float16')\n",
      "   653|         0|            0|            0|  0.00%|\n",
      "   654|         0|            0|            0|  0.00%|    >>> np.min_scalar_type(1e50)\n",
      "   655|         0|            0|            0|  0.00%|    dtype('float64')\n",
      "   656|         0|            0|            0|  0.00%|\n",
      "   657|         0|            0|            0|  0.00%|    >>> np.min_scalar_type(np.arange(4,dtype='f8'))\n",
      "   658|         0|            0|            0|  0.00%|    dtype('float64')\n",
      "   659|         0|            0|            0|  0.00%|\n",
      "   660|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   661|         0|            0|            0|  0.00%|    return (a,)\n",
      "   662|         0|            0|            0|  0.00%|\n",
      "   663|         0|            0|            0|  0.00%|\n",
      "   664|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.result_type)\n",
      "   665|         0|            0|            0|  0.00%|def result_type(*arrays_and_dtypes):\n",
      "   666|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   667|         0|            0|            0|  0.00%|    result_type(*arrays_and_dtypes)\n",
      "   668|         0|            0|            0|  0.00%|\n",
      "   669|         0|            0|            0|  0.00%|    Returns the type that results from applying the NumPy\n",
      "   670|         0|            0|            0|  0.00%|    type promotion rules to the arguments.\n",
      "   671|         0|            0|            0|  0.00%|\n",
      "   672|         0|            0|            0|  0.00%|    Type promotion in NumPy works similarly to the rules in languages\n",
      "   673|         0|            0|            0|  0.00%|    like C++, with some slight differences.  When both scalars and\n",
      "   674|         0|            0|            0|  0.00%|    arrays are used, the array's type takes precedence and the actual value\n",
      "   675|         0|            0|            0|  0.00%|    of the scalar is taken into account.\n",
      "   676|         0|            0|            0|  0.00%|\n",
      "   677|         0|            0|            0|  0.00%|    For example, calculating 3*a, where a is an array of 32-bit floats,\n",
      "   678|         0|            0|            0|  0.00%|    intuitively should result in a 32-bit float output.  If the 3 is a\n",
      "   679|         0|            0|            0|  0.00%|    32-bit integer, the NumPy rules indicate it can't convert losslessly\n",
      "   680|         0|            0|            0|  0.00%|    into a 32-bit float, so a 64-bit float should be the result type.\n",
      "   681|         0|            0|            0|  0.00%|    By examining the value of the constant, '3', we see that it fits in\n",
      "   682|         0|            0|            0|  0.00%|    an 8-bit integer, which can be cast losslessly into the 32-bit float.\n",
      "   683|         0|            0|            0|  0.00%|\n",
      "   684|         0|            0|            0|  0.00%|    Parameters\n",
      "   685|         0|            0|            0|  0.00%|    ----------\n",
      "   686|         0|            0|            0|  0.00%|    arrays_and_dtypes : list of arrays and dtypes\n",
      "   687|         0|            0|            0|  0.00%|        The operands of some operation whose result type is needed.\n",
      "   688|         0|            0|            0|  0.00%|\n",
      "   689|         0|            0|            0|  0.00%|    Returns\n",
      "   690|         0|            0|            0|  0.00%|    -------\n",
      "   691|         0|            0|            0|  0.00%|    out : dtype\n",
      "   692|         0|            0|            0|  0.00%|        The result type.\n",
      "   693|         0|            0|            0|  0.00%|\n",
      "   694|         0|            0|            0|  0.00%|    See also\n",
      "   695|         0|            0|            0|  0.00%|    --------\n",
      "   696|         0|            0|            0|  0.00%|    dtype, promote_types, min_scalar_type, can_cast\n",
      "   697|         0|            0|            0|  0.00%|\n",
      "   698|         0|            0|            0|  0.00%|    Notes\n",
      "   699|         0|            0|            0|  0.00%|    -----\n",
      "   700|         0|            0|            0|  0.00%|    .. versionadded:: 1.6.0\n",
      "   701|         0|            0|            0|  0.00%|\n",
      "   702|         0|            0|            0|  0.00%|    The specific algorithm used is as follows.\n",
      "   703|         0|            0|            0|  0.00%|\n",
      "   704|         0|            0|            0|  0.00%|    Categories are determined by first checking which of boolean,\n",
      "   705|         0|            0|            0|  0.00%|    integer (int/uint), or floating point (float/complex) the maximum\n",
      "   706|         0|            0|            0|  0.00%|    kind of all the arrays and the scalars are.\n",
      "   707|         0|            0|            0|  0.00%|\n",
      "   708|         0|            0|            0|  0.00%|    If there are only scalars or the maximum category of the scalars\n",
      "   709|         0|            0|            0|  0.00%|    is higher than the maximum category of the arrays,\n",
      "   710|         0|            0|            0|  0.00%|    the data types are combined with :func:`promote_types`\n",
      "   711|         0|            0|            0|  0.00%|    to produce the return value.\n",
      "   712|         0|            0|            0|  0.00%|\n",
      "   713|         0|            0|            0|  0.00%|    Otherwise, `min_scalar_type` is called on each array, and\n",
      "   714|         0|            0|            0|  0.00%|    the resulting data types are all combined with :func:`promote_types`\n",
      "   715|         0|            0|            0|  0.00%|    to produce the return value.\n",
      "   716|         0|            0|            0|  0.00%|\n",
      "   717|         0|            0|            0|  0.00%|    The set of int values is not a subset of the uint values for types\n",
      "   718|         0|            0|            0|  0.00%|    with the same number of bits, something not reflected in\n",
      "   719|         0|            0|            0|  0.00%|    :func:`min_scalar_type`, but handled as a special case in `result_type`.\n",
      "   720|         0|            0|            0|  0.00%|\n",
      "   721|         0|            0|            0|  0.00%|    Examples\n",
      "   722|         0|            0|            0|  0.00%|    --------\n",
      "   723|         0|            0|            0|  0.00%|    >>> np.result_type(3, np.arange(7, dtype='i1'))\n",
      "   724|         0|            0|            0|  0.00%|    dtype('int8')\n",
      "   725|         0|            0|            0|  0.00%|\n",
      "   726|         0|            0|            0|  0.00%|    >>> np.result_type('i4', 'c8')\n",
      "   727|         0|            0|            0|  0.00%|    dtype('complex128')\n",
      "   728|         0|            0|            0|  0.00%|\n",
      "   729|         0|            0|            0|  0.00%|    >>> np.result_type(3.0, -2)\n",
      "   730|         0|            0|            0|  0.00%|    dtype('float64')\n",
      "   731|         0|            0|            0|  0.00%|\n",
      "   732|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   733|         0|            0|            0|  0.00%|    return arrays_and_dtypes\n",
      "   734|         0|            0|            0|  0.00%|\n",
      "   735|         0|            0|            0|  0.00%|\n",
      "   736|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.dot)\n",
      "   737|         0|            0|            0|  0.00%|def dot(a, b, out=None):\n",
      "   738|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   739|         0|            0|            0|  0.00%|    dot(a, b, out=None)\n",
      "   740|         0|            0|            0|  0.00%|\n",
      "   741|         0|            0|            0|  0.00%|    Dot product of two arrays. Specifically,\n",
      "   742|         0|            0|            0|  0.00%|\n",
      "   743|         0|            0|            0|  0.00%|    - If both `a` and `b` are 1-D arrays, it is inner product of vectors\n",
      "   744|         0|            0|            0|  0.00%|      (without complex conjugation).\n",
      "   745|         0|            0|            0|  0.00%|\n",
      "   746|         0|            0|            0|  0.00%|    - If both `a` and `b` are 2-D arrays, it is matrix multiplication,\n",
      "   747|         0|            0|            0|  0.00%|      but using :func:`matmul` or ``a @ b`` is preferred.\n",
      "   748|         0|            0|            0|  0.00%|\n",
      "   749|         0|            0|            0|  0.00%|    - If either `a` or `b` is 0-D (scalar), it is equivalent to :func:`multiply`\n",
      "   750|         0|            0|            0|  0.00%|      and using ``numpy.multiply(a, b)`` or ``a * b`` is preferred.\n",
      "   751|         0|            0|            0|  0.00%|\n",
      "   752|         0|            0|            0|  0.00%|    - If `a` is an N-D array and `b` is a 1-D array, it is a sum product over\n",
      "   753|         0|            0|            0|  0.00%|      the last axis of `a` and `b`.\n",
      "   754|         0|            0|            0|  0.00%|\n",
      "   755|         0|            0|            0|  0.00%|    - If `a` is an N-D array and `b` is an M-D array (where ``M>=2``), it is a\n",
      "   756|         0|            0|            0|  0.00%|      sum product over the last axis of `a` and the second-to-last axis of `b`::\n",
      "   757|         0|            0|            0|  0.00%|\n",
      "   758|         0|            0|            0|  0.00%|        dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n",
      "   759|         0|            0|            0|  0.00%|\n",
      "   760|         0|            0|            0|  0.00%|    Parameters\n",
      "   761|         0|            0|            0|  0.00%|    ----------\n",
      "   762|         0|            0|            0|  0.00%|    a : array_like\n",
      "   763|         0|            0|            0|  0.00%|        First argument.\n",
      "   764|         0|            0|            0|  0.00%|    b : array_like\n",
      "   765|         0|            0|            0|  0.00%|        Second argument.\n",
      "   766|         0|            0|            0|  0.00%|    out : ndarray, optional\n",
      "   767|         0|            0|            0|  0.00%|        Output argument. This must have the exact kind that would be returned\n",
      "   768|         0|            0|            0|  0.00%|        if it was not used. In particular, it must have the right type, must be\n",
      "   769|         0|            0|            0|  0.00%|        C-contiguous, and its dtype must be the dtype that would be returned\n",
      "   770|         0|            0|            0|  0.00%|        for `dot(a,b)`. This is a performance feature. Therefore, if these\n",
      "   771|         0|            0|            0|  0.00%|        conditions are not met, an exception is raised, instead of attempting\n",
      "   772|         0|            0|            0|  0.00%|        to be flexible.\n",
      "   773|         0|            0|            0|  0.00%|\n",
      "   774|         0|            0|            0|  0.00%|    Returns\n",
      "   775|         0|            0|            0|  0.00%|    -------\n",
      "   776|         0|            0|            0|  0.00%|    output : ndarray\n",
      "   777|         0|            0|            0|  0.00%|        Returns the dot product of `a` and `b`.  If `a` and `b` are both\n",
      "   778|         0|            0|            0|  0.00%|        scalars or both 1-D arrays then a scalar is returned; otherwise\n",
      "   779|         0|            0|            0|  0.00%|        an array is returned.\n",
      "   780|         0|            0|            0|  0.00%|        If `out` is given, then it is returned.\n",
      "   781|         0|            0|            0|  0.00%|\n",
      "   782|         0|            0|            0|  0.00%|    Raises\n",
      "   783|         0|            0|            0|  0.00%|    ------\n",
      "   784|         0|            0|            0|  0.00%|    ValueError\n",
      "   785|         0|            0|            0|  0.00%|        If the last dimension of `a` is not the same size as\n",
      "   786|         0|            0|            0|  0.00%|        the second-to-last dimension of `b`.\n",
      "   787|         0|            0|            0|  0.00%|\n",
      "   788|         0|            0|            0|  0.00%|    See Also\n",
      "   789|         0|            0|            0|  0.00%|    --------\n",
      "   790|         0|            0|            0|  0.00%|    vdot : Complex-conjugating dot product.\n",
      "   791|         0|            0|            0|  0.00%|    tensordot : Sum products over arbitrary axes.\n",
      "   792|         0|            0|            0|  0.00%|    einsum : Einstein summation convention.\n",
      "   793|         0|            0|            0|  0.00%|    matmul : '@' operator as method with out parameter.\n",
      "   794|         0|            0|            0|  0.00%|    linalg.multi_dot : Chained dot product.\n",
      "   795|         0|            0|            0|  0.00%|\n",
      "   796|         0|            0|            0|  0.00%|    Examples\n",
      "   797|         0|            0|            0|  0.00%|    --------\n",
      "   798|         0|            0|            0|  0.00%|    >>> np.dot(3, 4)\n",
      "   799|         0|            0|            0|  0.00%|    12\n",
      "   800|         0|            0|            0|  0.00%|\n",
      "   801|         0|            0|            0|  0.00%|    Neither argument is complex-conjugated:\n",
      "   802|         0|            0|            0|  0.00%|\n",
      "   803|         0|            0|            0|  0.00%|    >>> np.dot([2j, 3j], [2j, 3j])\n",
      "   804|         0|            0|            0|  0.00%|    (-13+0j)\n",
      "   805|         0|            0|            0|  0.00%|\n",
      "   806|         0|            0|            0|  0.00%|    For 2-D arrays it is the matrix product:\n",
      "   807|         0|            0|            0|  0.00%|\n",
      "   808|         0|            0|            0|  0.00%|    >>> a = [[1, 0], [0, 1]]\n",
      "   809|         0|            0|            0|  0.00%|    >>> b = [[4, 1], [2, 2]]\n",
      "   810|         0|            0|            0|  0.00%|    >>> np.dot(a, b)\n",
      "   811|         0|            0|            0|  0.00%|    array([[4, 1],\n",
      "   812|         0|            0|            0|  0.00%|           [2, 2]])\n",
      "   813|         0|            0|            0|  0.00%|\n",
      "   814|         0|            0|            0|  0.00%|    >>> a = np.arange(3*4*5*6).reshape((3,4,5,6))\n",
      "   815|         0|            0|            0|  0.00%|    >>> b = np.arange(3*4*5*6)[::-1].reshape((5,4,6,3))\n",
      "   816|         0|            0|            0|  0.00%|    >>> np.dot(a, b)[2,3,2,1,2,2]\n",
      "   817|         0|            0|            0|  0.00%|    499128\n",
      "   818|         0|            0|            0|  0.00%|    >>> sum(a[2,3,2,:] * b[1,2,:,2])\n",
      "   819|         0|            0|            0|  0.00%|    499128\n",
      "   820|         0|            0|            0|  0.00%|\n",
      "   821|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   822|         0|            0|            0|  0.00%|    return (a, b, out)\n",
      "   823|         0|            0|            0|  0.00%|\n",
      "   824|         0|            0|            0|  0.00%|\n",
      "   825|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.vdot)\n",
      "   826|         0|            0|            0|  0.00%|def vdot(a, b):\n",
      "   827|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   828|         0|            0|            0|  0.00%|    vdot(a, b, /)\n",
      "   829|         0|            0|            0|  0.00%|\n",
      "   830|         0|            0|            0|  0.00%|    Return the dot product of two vectors.\n",
      "   831|         0|            0|            0|  0.00%|\n",
      "   832|         0|            0|            0|  0.00%|    The vdot(`a`, `b`) function handles complex numbers differently than\n",
      "   833|         0|            0|            0|  0.00%|    dot(`a`, `b`).  If the first argument is complex the complex conjugate\n",
      "   834|         0|            0|            0|  0.00%|    of the first argument is used for the calculation of the dot product.\n",
      "   835|         0|            0|            0|  0.00%|\n",
      "   836|         0|            0|            0|  0.00%|    Note that `vdot` handles multidimensional arrays differently than `dot`:\n",
      "   837|         0|            0|            0|  0.00%|    it does *not* perform a matrix product, but flattens input arguments\n",
      "   838|         0|            0|            0|  0.00%|    to 1-D vectors first. Consequently, it should only be used for vectors.\n",
      "   839|         0|            0|            0|  0.00%|\n",
      "   840|         0|            0|            0|  0.00%|    Parameters\n",
      "   841|         0|            0|            0|  0.00%|    ----------\n",
      "   842|         0|            0|            0|  0.00%|    a : array_like\n",
      "   843|         0|            0|            0|  0.00%|        If `a` is complex the complex conjugate is taken before calculation\n",
      "   844|         0|            0|            0|  0.00%|        of the dot product.\n",
      "   845|         0|            0|            0|  0.00%|    b : array_like\n",
      "   846|         0|            0|            0|  0.00%|        Second argument to the dot product.\n",
      "   847|         0|            0|            0|  0.00%|\n",
      "   848|         0|            0|            0|  0.00%|    Returns\n",
      "   849|         0|            0|            0|  0.00%|    -------\n",
      "   850|         0|            0|            0|  0.00%|    output : ndarray\n",
      "   851|         0|            0|            0|  0.00%|        Dot product of `a` and `b`.  Can be an int, float, or\n",
      "   852|         0|            0|            0|  0.00%|        complex depending on the types of `a` and `b`.\n",
      "   853|         0|            0|            0|  0.00%|\n",
      "   854|         0|            0|            0|  0.00%|    See Also\n",
      "   855|         0|            0|            0|  0.00%|    --------\n",
      "   856|         0|            0|            0|  0.00%|    dot : Return the dot product without using the complex conjugate of the\n",
      "   857|         0|            0|            0|  0.00%|          first argument.\n",
      "   858|         0|            0|            0|  0.00%|\n",
      "   859|         0|            0|            0|  0.00%|    Examples\n",
      "   860|         0|            0|            0|  0.00%|    --------\n",
      "   861|         0|            0|            0|  0.00%|    >>> a = np.array([1+2j,3+4j])\n",
      "   862|         0|            0|            0|  0.00%|    >>> b = np.array([5+6j,7+8j])\n",
      "   863|         0|            0|            0|  0.00%|    >>> np.vdot(a, b)\n",
      "   864|         0|            0|            0|  0.00%|    (70-8j)\n",
      "   865|         0|            0|            0|  0.00%|    >>> np.vdot(b, a)\n",
      "   866|         0|            0|            0|  0.00%|    (70+8j)\n",
      "   867|         0|            0|            0|  0.00%|\n",
      "   868|         0|            0|            0|  0.00%|    Note that higher-dimensional arrays are flattened!\n",
      "   869|         0|            0|            0|  0.00%|\n",
      "   870|         0|            0|            0|  0.00%|    >>> a = np.array([[1, 4], [5, 6]])\n",
      "   871|         0|            0|            0|  0.00%|    >>> b = np.array([[4, 1], [2, 2]])\n",
      "   872|         0|            0|            0|  0.00%|    >>> np.vdot(a, b)\n",
      "   873|         0|            0|            0|  0.00%|    30\n",
      "   874|         0|            0|            0|  0.00%|    >>> np.vdot(b, a)\n",
      "   875|         0|            0|            0|  0.00%|    30\n",
      "   876|         0|            0|            0|  0.00%|    >>> 1*4 + 4*1 + 5*2 + 6*2\n",
      "   877|         0|            0|            0|  0.00%|    30\n",
      "   878|         0|            0|            0|  0.00%|\n",
      "   879|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   880|         0|            0|            0|  0.00%|    return (a, b)\n",
      "   881|         0|            0|            0|  0.00%|\n",
      "   882|         0|            0|            0|  0.00%|\n",
      "   883|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.bincount)\n",
      "   884|         0|            0|            0|  0.00%|def bincount(x, weights=None, minlength=None):\n",
      "   885|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   886|         0|            0|            0|  0.00%|    bincount(x, /, weights=None, minlength=0)\n",
      "   887|         0|            0|            0|  0.00%|\n",
      "   888|         0|            0|            0|  0.00%|    Count number of occurrences of each value in array of non-negative ints.\n",
      "   889|         0|            0|            0|  0.00%|\n",
      "   890|         0|            0|            0|  0.00%|    The number of bins (of size 1) is one larger than the largest value in\n",
      "   891|         0|            0|            0|  0.00%|    `x`. If `minlength` is specified, there will be at least this number\n",
      "   892|         0|            0|            0|  0.00%|    of bins in the output array (though it will be longer if necessary,\n",
      "   893|         0|            0|            0|  0.00%|    depending on the contents of `x`).\n",
      "   894|         0|            0|            0|  0.00%|    Each bin gives the number of occurrences of its index value in `x`.\n",
      "   895|         0|            0|            0|  0.00%|    If `weights` is specified the input array is weighted by it, i.e. if a\n",
      "   896|         0|            0|            0|  0.00%|    value ``n`` is found at position ``i``, ``out[n] += weight[i]`` instead\n",
      "   897|         0|            0|            0|  0.00%|    of ``out[n] += 1``.\n",
      "   898|         0|            0|            0|  0.00%|\n",
      "   899|         0|            0|            0|  0.00%|    Parameters\n",
      "   900|         0|            0|            0|  0.00%|    ----------\n",
      "   901|         0|            0|            0|  0.00%|    x : array_like, 1 dimension, nonnegative ints\n",
      "   902|         0|            0|            0|  0.00%|        Input array.\n",
      "   903|         0|            0|            0|  0.00%|    weights : array_like, optional\n",
      "   904|         0|            0|            0|  0.00%|        Weights, array of the same shape as `x`.\n",
      "   905|         0|            0|            0|  0.00%|    minlength : int, optional\n",
      "   906|         0|            0|            0|  0.00%|        A minimum number of bins for the output array.\n",
      "   907|         0|            0|            0|  0.00%|\n",
      "   908|         0|            0|            0|  0.00%|        .. versionadded:: 1.6.0\n",
      "   909|         0|            0|            0|  0.00%|\n",
      "   910|         0|            0|            0|  0.00%|    Returns\n",
      "   911|         0|            0|            0|  0.00%|    -------\n",
      "   912|         0|            0|            0|  0.00%|    out : ndarray of ints\n",
      "   913|         0|            0|            0|  0.00%|        The result of binning the input array.\n",
      "   914|         0|            0|            0|  0.00%|        The length of `out` is equal to ``np.amax(x)+1``.\n",
      "   915|         0|            0|            0|  0.00%|\n",
      "   916|         0|            0|            0|  0.00%|    Raises\n",
      "   917|         0|            0|            0|  0.00%|    ------\n",
      "   918|         0|            0|            0|  0.00%|    ValueError\n",
      "   919|         0|            0|            0|  0.00%|        If the input is not 1-dimensional, or contains elements with negative\n",
      "   920|         0|            0|            0|  0.00%|        values, or if `minlength` is negative.\n",
      "   921|         0|            0|            0|  0.00%|    TypeError\n",
      "   922|         0|            0|            0|  0.00%|        If the type of the input is float or complex.\n",
      "   923|         0|            0|            0|  0.00%|\n",
      "   924|         0|            0|            0|  0.00%|    See Also\n",
      "   925|         0|            0|            0|  0.00%|    --------\n",
      "   926|         0|            0|            0|  0.00%|    histogram, digitize, unique\n",
      "   927|         0|            0|            0|  0.00%|\n",
      "   928|         0|            0|            0|  0.00%|    Examples\n",
      "   929|         0|            0|            0|  0.00%|    --------\n",
      "   930|         0|            0|            0|  0.00%|    >>> np.bincount(np.arange(5))\n",
      "   931|         0|            0|            0|  0.00%|    array([1, 1, 1, 1, 1])\n",
      "   932|         0|            0|            0|  0.00%|    >>> np.bincount(np.array([0, 1, 1, 3, 2, 1, 7]))\n",
      "   933|         0|            0|            0|  0.00%|    array([1, 3, 1, 1, 0, 0, 0, 1])\n",
      "   934|         0|            0|            0|  0.00%|\n",
      "   935|         0|            0|            0|  0.00%|    >>> x = np.array([0, 1, 1, 3, 2, 1, 7, 23])\n",
      "   936|         0|            0|            0|  0.00%|    >>> np.bincount(x).size == np.amax(x)+1\n",
      "   937|         0|            0|            0|  0.00%|    True\n",
      "   938|         0|            0|            0|  0.00%|\n",
      "   939|         0|            0|            0|  0.00%|    The input array needs to be of integer dtype, otherwise a\n",
      "   940|         0|            0|            0|  0.00%|    TypeError is raised:\n",
      "   941|         0|            0|            0|  0.00%|\n",
      "   942|         0|            0|            0|  0.00%|    >>> np.bincount(np.arange(5, dtype=float))\n",
      "   943|         0|            0|            0|  0.00%|    Traceback (most recent call last):\n",
      "   944|         0|            0|            0|  0.00%|      ...\n",
      "   945|         0|            0|            0|  0.00%|    TypeError: Cannot cast array data from dtype('float64') to dtype('int64')\n",
      "   946|         0|            0|            0|  0.00%|    according to the rule 'safe'\n",
      "   947|         0|            0|            0|  0.00%|\n",
      "   948|         0|            0|            0|  0.00%|    A possible use of ``bincount`` is to perform sums over\n",
      "   949|         0|            0|            0|  0.00%|    variable-size chunks of an array, using the ``weights`` keyword.\n",
      "   950|         0|            0|            0|  0.00%|\n",
      "   951|         0|            0|            0|  0.00%|    >>> w = np.array([0.3, 0.5, 0.2, 0.7, 1., -0.6]) # weights\n",
      "   952|         0|            0|            0|  0.00%|    >>> x = np.array([0, 1, 1, 2, 2, 2])\n",
      "   953|         0|            0|            0|  0.00%|    >>> np.bincount(x,  weights=w)\n",
      "   954|         0|            0|            0|  0.00%|    array([ 0.3,  0.7,  1.1])\n",
      "   955|         0|            0|            0|  0.00%|\n",
      "   956|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   957|         0|            0|            0|  0.00%|    return (x, weights)\n",
      "   958|         0|            0|            0|  0.00%|\n",
      "   959|         0|            0|            0|  0.00%|\n",
      "   960|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.ravel_multi_index)\n",
      "   961|         0|            0|            0|  0.00%|def ravel_multi_index(multi_index, dims, mode=None, order=None):\n",
      "   962|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   963|         0|            0|            0|  0.00%|    ravel_multi_index(multi_index, dims, mode='raise', order='C')\n",
      "   964|         0|            0|            0|  0.00%|\n",
      "   965|         0|            0|            0|  0.00%|    Converts a tuple of index arrays into an array of flat\n",
      "   966|         0|            0|            0|  0.00%|    indices, applying boundary modes to the multi-index.\n",
      "   967|         0|            0|            0|  0.00%|\n",
      "   968|         0|            0|            0|  0.00%|    Parameters\n",
      "   969|         0|            0|            0|  0.00%|    ----------\n",
      "   970|         0|            0|            0|  0.00%|    multi_index : tuple of array_like\n",
      "   971|         0|            0|            0|  0.00%|        A tuple of integer arrays, one array for each dimension.\n",
      "   972|         0|            0|            0|  0.00%|    dims : tuple of ints\n",
      "   973|         0|            0|            0|  0.00%|        The shape of array into which the indices from ``multi_index`` apply.\n",
      "   974|         0|            0|            0|  0.00%|    mode : {'raise', 'wrap', 'clip'}, optional\n",
      "   975|         0|            0|            0|  0.00%|        Specifies how out-of-bounds indices are handled.  Can specify\n",
      "   976|         0|            0|            0|  0.00%|        either one mode or a tuple of modes, one mode per index.\n",
      "   977|         0|            0|            0|  0.00%|\n",
      "   978|         0|            0|            0|  0.00%|        * 'raise' -- raise an error (default)\n",
      "   979|         0|            0|            0|  0.00%|        * 'wrap' -- wrap around\n",
      "   980|         0|            0|            0|  0.00%|        * 'clip' -- clip to the range\n",
      "   981|         0|            0|            0|  0.00%|\n",
      "   982|         0|            0|            0|  0.00%|        In 'clip' mode, a negative index which would normally\n",
      "   983|         0|            0|            0|  0.00%|        wrap will clip to 0 instead.\n",
      "   984|         0|            0|            0|  0.00%|    order : {'C', 'F'}, optional\n",
      "   985|         0|            0|            0|  0.00%|        Determines whether the multi-index should be viewed as\n",
      "   986|         0|            0|            0|  0.00%|        indexing in row-major (C-style) or column-major\n",
      "   987|         0|            0|            0|  0.00%|        (Fortran-style) order.\n",
      "   988|         0|            0|            0|  0.00%|\n",
      "   989|         0|            0|            0|  0.00%|    Returns\n",
      "   990|         0|            0|            0|  0.00%|    -------\n",
      "   991|         0|            0|            0|  0.00%|    raveled_indices : ndarray\n",
      "   992|         0|            0|            0|  0.00%|        An array of indices into the flattened version of an array\n",
      "   993|         0|            0|            0|  0.00%|        of dimensions ``dims``.\n",
      "   994|         0|            0|            0|  0.00%|\n",
      "   995|         0|            0|            0|  0.00%|    See Also\n",
      "   996|         0|            0|            0|  0.00%|    --------\n",
      "   997|         0|            0|            0|  0.00%|    unravel_index\n",
      "   998|         0|            0|            0|  0.00%|\n",
      "   999|         0|            0|            0|  0.00%|    Notes\n",
      "  1000|         0|            0|            0|  0.00%|    -----\n",
      "  1001|         0|            0|            0|  0.00%|    .. versionadded:: 1.6.0\n",
      "  1002|         0|            0|            0|  0.00%|\n",
      "  1003|         0|            0|            0|  0.00%|    Examples\n",
      "  1004|         0|            0|            0|  0.00%|    --------\n",
      "  1005|         0|            0|            0|  0.00%|    >>> arr = np.array([[3,6,6],[4,5,1]])\n",
      "  1006|         0|            0|            0|  0.00%|    >>> np.ravel_multi_index(arr, (7,6))\n",
      "  1007|         0|            0|            0|  0.00%|    array([22, 41, 37])\n",
      "  1008|         0|            0|            0|  0.00%|    >>> np.ravel_multi_index(arr, (7,6), order='F')\n",
      "  1009|         0|            0|            0|  0.00%|    array([31, 41, 13])\n",
      "  1010|         0|            0|            0|  0.00%|    >>> np.ravel_multi_index(arr, (4,6), mode='clip')\n",
      "  1011|         0|            0|            0|  0.00%|    array([22, 23, 19])\n",
      "  1012|         0|            0|            0|  0.00%|    >>> np.ravel_multi_index(arr, (4,4), mode=('clip','wrap'))\n",
      "  1013|         0|            0|            0|  0.00%|    array([12, 13, 13])\n",
      "  1014|         0|            0|            0|  0.00%|\n",
      "  1015|         0|            0|            0|  0.00%|    >>> np.ravel_multi_index((3,1,4,1), (6,7,8,9))\n",
      "  1016|         0|            0|            0|  0.00%|    1621\n",
      "  1017|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1018|         0|            0|            0|  0.00%|    return multi_index\n",
      "  1019|         0|            0|            0|  0.00%|\n",
      "  1020|         0|            0|            0|  0.00%|\n",
      "  1021|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.unravel_index)\n",
      "  1022|         0|            0|            0|  0.00%|def unravel_index(indices, shape=None, order=None):\n",
      "  1023|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1024|         0|            0|            0|  0.00%|    unravel_index(indices, shape, order='C')\n",
      "  1025|         0|            0|            0|  0.00%|\n",
      "  1026|         0|            0|            0|  0.00%|    Converts a flat index or array of flat indices into a tuple\n",
      "  1027|         0|            0|            0|  0.00%|    of coordinate arrays.\n",
      "  1028|         0|            0|            0|  0.00%|\n",
      "  1029|         0|            0|            0|  0.00%|    Parameters\n",
      "  1030|         0|            0|            0|  0.00%|    ----------\n",
      "  1031|         0|            0|            0|  0.00%|    indices : array_like\n",
      "  1032|         0|            0|            0|  0.00%|        An integer array whose elements are indices into the flattened\n",
      "  1033|         0|            0|            0|  0.00%|        version of an array of dimensions ``shape``. Before version 1.6.0,\n",
      "  1034|         0|            0|            0|  0.00%|        this function accepted just one index value.\n",
      "  1035|         0|            0|            0|  0.00%|    shape : tuple of ints\n",
      "  1036|         0|            0|            0|  0.00%|        The shape of the array to use for unraveling ``indices``.\n",
      "  1037|         0|            0|            0|  0.00%|\n",
      "  1038|         0|            0|            0|  0.00%|        .. versionchanged:: 1.16.0\n",
      "  1039|         0|            0|            0|  0.00%|            Renamed from ``dims`` to ``shape``.\n",
      "  1040|         0|            0|            0|  0.00%|\n",
      "  1041|         0|            0|            0|  0.00%|    order : {'C', 'F'}, optional\n",
      "  1042|         0|            0|            0|  0.00%|        Determines whether the indices should be viewed as indexing in\n",
      "  1043|         0|            0|            0|  0.00%|        row-major (C-style) or column-major (Fortran-style) order.\n",
      "  1044|         0|            0|            0|  0.00%|\n",
      "  1045|         0|            0|            0|  0.00%|        .. versionadded:: 1.6.0\n",
      "  1046|         0|            0|            0|  0.00%|\n",
      "  1047|         0|            0|            0|  0.00%|    Returns\n",
      "  1048|         0|            0|            0|  0.00%|    -------\n",
      "  1049|         0|            0|            0|  0.00%|    unraveled_coords : tuple of ndarray\n",
      "  1050|         0|            0|            0|  0.00%|        Each array in the tuple has the same shape as the ``indices``\n",
      "  1051|         0|            0|            0|  0.00%|        array.\n",
      "  1052|         0|            0|            0|  0.00%|\n",
      "  1053|         0|            0|            0|  0.00%|    See Also\n",
      "  1054|         0|            0|            0|  0.00%|    --------\n",
      "  1055|         0|            0|            0|  0.00%|    ravel_multi_index\n",
      "  1056|         0|            0|            0|  0.00%|\n",
      "  1057|         0|            0|            0|  0.00%|    Examples\n",
      "  1058|         0|            0|            0|  0.00%|    --------\n",
      "  1059|         0|            0|            0|  0.00%|    >>> np.unravel_index([22, 41, 37], (7,6))\n",
      "  1060|         0|            0|            0|  0.00%|    (array([3, 6, 6]), array([4, 5, 1]))\n",
      "  1061|         0|            0|            0|  0.00%|    >>> np.unravel_index([31, 41, 13], (7,6), order='F')\n",
      "  1062|         0|            0|            0|  0.00%|    (array([3, 6, 6]), array([4, 5, 1]))\n",
      "  1063|         0|            0|            0|  0.00%|\n",
      "  1064|         0|            0|            0|  0.00%|    >>> np.unravel_index(1621, (6,7,8,9))\n",
      "  1065|         0|            0|            0|  0.00%|    (3, 1, 4, 1)\n",
      "  1066|         0|            0|            0|  0.00%|\n",
      "  1067|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1068|         0|            0|            0|  0.00%|    return (indices,)\n",
      "  1069|         0|            0|            0|  0.00%|\n",
      "  1070|         0|            0|            0|  0.00%|\n",
      "  1071|     99832|     0.146317|  1.46563e-06|  0.02%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.copyto)\n",
      "  1072|         0|            0|            0|  0.00%|def copyto(dst, src, casting=None, where=None):\n",
      "  1073|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1074|         0|            0|            0|  0.00%|    copyto(dst, src, casting='same_kind', where=True)\n",
      "  1075|         0|            0|            0|  0.00%|\n",
      "  1076|         0|            0|            0|  0.00%|    Copies values from one array to another, broadcasting as necessary.\n",
      "  1077|         0|            0|            0|  0.00%|\n",
      "  1078|         0|            0|            0|  0.00%|    Raises a TypeError if the `casting` rule is violated, and if\n",
      "  1079|         0|            0|            0|  0.00%|    `where` is provided, it selects which elements to copy.\n",
      "  1080|         0|            0|            0|  0.00%|\n",
      "  1081|         0|            0|            0|  0.00%|    .. versionadded:: 1.7.0\n",
      "  1082|         0|            0|            0|  0.00%|\n",
      "  1083|         0|            0|            0|  0.00%|    Parameters\n",
      "  1084|         0|            0|            0|  0.00%|    ----------\n",
      "  1085|         0|            0|            0|  0.00%|    dst : ndarray\n",
      "  1086|         0|            0|            0|  0.00%|        The array into which values are copied.\n",
      "  1087|         0|            0|            0|  0.00%|    src : array_like\n",
      "  1088|         0|            0|            0|  0.00%|        The array from which values are copied.\n",
      "  1089|         0|            0|            0|  0.00%|    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n",
      "  1090|         0|            0|            0|  0.00%|        Controls what kind of data casting may occur when copying.\n",
      "  1091|         0|            0|            0|  0.00%|\n",
      "  1092|         0|            0|            0|  0.00%|          * 'no' means the data types should not be cast at all.\n",
      "  1093|         0|            0|            0|  0.00%|          * 'equiv' means only byte-order changes are allowed.\n",
      "  1094|         0|            0|            0|  0.00%|          * 'safe' means only casts which can preserve values are allowed.\n",
      "  1095|         0|            0|            0|  0.00%|          * 'same_kind' means only safe casts or casts within a kind,\n",
      "  1096|         0|            0|            0|  0.00%|            like float64 to float32, are allowed.\n",
      "  1097|         0|            0|            0|  0.00%|          * 'unsafe' means any data conversions may be done.\n",
      "  1098|         0|            0|            0|  0.00%|    where : array_like of bool, optional\n",
      "  1099|         0|            0|            0|  0.00%|        A boolean array which is broadcasted to match the dimensions\n",
      "  1100|         0|            0|            0|  0.00%|        of `dst`, and selects elements to copy from `src` to `dst`\n",
      "  1101|         0|            0|            0|  0.00%|        wherever it contains the value True.\n",
      "  1102|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1103|     99832|     0.189626|  1.89946e-06|  0.03%|    return (dst, src, where)\n",
      "  1104|         0|            0|            0|  0.00%|\n",
      "  1105|         0|            0|            0|  0.00%|\n",
      "  1106|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.putmask)\n",
      "  1107|         0|            0|            0|  0.00%|def putmask(a, mask, values):\n",
      "  1108|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1109|         0|            0|            0|  0.00%|    putmask(a, mask, values)\n",
      "  1110|         0|            0|            0|  0.00%|\n",
      "  1111|         0|            0|            0|  0.00%|    Changes elements of an array based on conditional and input values.\n",
      "  1112|         0|            0|            0|  0.00%|\n",
      "  1113|         0|            0|            0|  0.00%|    Sets ``a.flat[n] = values[n]`` for each n where ``mask.flat[n]==True``.\n",
      "  1114|         0|            0|            0|  0.00%|\n",
      "  1115|         0|            0|            0|  0.00%|    If `values` is not the same size as `a` and `mask` then it will repeat.\n",
      "  1116|         0|            0|            0|  0.00%|    This gives behavior different from ``a[mask] = values``.\n",
      "  1117|         0|            0|            0|  0.00%|\n",
      "  1118|         0|            0|            0|  0.00%|    Parameters\n",
      "  1119|         0|            0|            0|  0.00%|    ----------\n",
      "  1120|         0|            0|            0|  0.00%|    a : ndarray\n",
      "  1121|         0|            0|            0|  0.00%|        Target array.\n",
      "  1122|         0|            0|            0|  0.00%|    mask : array_like\n",
      "  1123|         0|            0|            0|  0.00%|        Boolean mask array. It has to be the same shape as `a`.\n",
      "  1124|         0|            0|            0|  0.00%|    values : array_like\n",
      "  1125|         0|            0|            0|  0.00%|        Values to put into `a` where `mask` is True. If `values` is smaller\n",
      "  1126|         0|            0|            0|  0.00%|        than `a` it will be repeated.\n",
      "  1127|         0|            0|            0|  0.00%|\n",
      "  1128|         0|            0|            0|  0.00%|    See Also\n",
      "  1129|         0|            0|            0|  0.00%|    --------\n",
      "  1130|         0|            0|            0|  0.00%|    place, put, take, copyto\n",
      "  1131|         0|            0|            0|  0.00%|\n",
      "  1132|         0|            0|            0|  0.00%|    Examples\n",
      "  1133|         0|            0|            0|  0.00%|    --------\n",
      "  1134|         0|            0|            0|  0.00%|    >>> x = np.arange(6).reshape(2, 3)\n",
      "  1135|         0|            0|            0|  0.00%|    >>> np.putmask(x, x>2, x**2)\n",
      "  1136|         0|            0|            0|  0.00%|    >>> x\n",
      "  1137|         0|            0|            0|  0.00%|    array([[ 0,  1,  2],\n",
      "  1138|         0|            0|            0|  0.00%|           [ 9, 16, 25]])\n",
      "  1139|         0|            0|            0|  0.00%|\n",
      "  1140|         0|            0|            0|  0.00%|    If `values` is smaller than `a` it is repeated:\n",
      "  1141|         0|            0|            0|  0.00%|\n",
      "  1142|         0|            0|            0|  0.00%|    >>> x = np.arange(5)\n",
      "  1143|         0|            0|            0|  0.00%|    >>> np.putmask(x, x>1, [-33, -44])\n",
      "  1144|         0|            0|            0|  0.00%|    >>> x\n",
      "  1145|         0|            0|            0|  0.00%|    array([  0,   1, -33, -44, -33])\n",
      "  1146|         0|            0|            0|  0.00%|\n",
      "  1147|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1148|         0|            0|            0|  0.00%|    return (a, mask, values)\n",
      "  1149|         0|            0|            0|  0.00%|\n",
      "  1150|         0|            0|            0|  0.00%|\n",
      "  1151|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.packbits)\n",
      "  1152|         0|            0|            0|  0.00%|def packbits(a, axis=None, bitorder='big'):\n",
      "  1153|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1154|         0|            0|            0|  0.00%|    packbits(a, /, axis=None, bitorder='big')\n",
      "  1155|         0|            0|            0|  0.00%|\n",
      "  1156|         0|            0|            0|  0.00%|    Packs the elements of a binary-valued array into bits in a uint8 array.\n",
      "  1157|         0|            0|            0|  0.00%|\n",
      "  1158|         0|            0|            0|  0.00%|    The result is padded to full bytes by inserting zero bits at the end.\n",
      "  1159|         0|            0|            0|  0.00%|\n",
      "  1160|         0|            0|            0|  0.00%|    Parameters\n",
      "  1161|         0|            0|            0|  0.00%|    ----------\n",
      "  1162|         0|            0|            0|  0.00%|    a : array_like\n",
      "  1163|         0|            0|            0|  0.00%|        An array of integers or booleans whose elements should be packed to\n",
      "  1164|         0|            0|            0|  0.00%|        bits.\n",
      "  1165|         0|            0|            0|  0.00%|    axis : int, optional\n",
      "  1166|         0|            0|            0|  0.00%|        The dimension over which bit-packing is done.\n",
      "  1167|         0|            0|            0|  0.00%|        ``None`` implies packing the flattened array.\n",
      "  1168|         0|            0|            0|  0.00%|    bitorder : {'big', 'little'}, optional\n",
      "  1169|         0|            0|            0|  0.00%|        The order of the input bits. 'big' will mimic bin(val),\n",
      "  1170|         0|            0|            0|  0.00%|        ``[0, 0, 0, 0, 0, 0, 1, 1] => 3 = 0b00000011``, 'little' will\n",
      "  1171|         0|            0|            0|  0.00%|        reverse the order so ``[1, 1, 0, 0, 0, 0, 0, 0] => 3``.\n",
      "  1172|         0|            0|            0|  0.00%|        Defaults to 'big'.\n",
      "  1173|         0|            0|            0|  0.00%|\n",
      "  1174|         0|            0|            0|  0.00%|        .. versionadded:: 1.17.0\n",
      "  1175|         0|            0|            0|  0.00%|\n",
      "  1176|         0|            0|            0|  0.00%|    Returns\n",
      "  1177|         0|            0|            0|  0.00%|    -------\n",
      "  1178|         0|            0|            0|  0.00%|    packed : ndarray\n",
      "  1179|         0|            0|            0|  0.00%|        Array of type uint8 whose elements represent bits corresponding to the\n",
      "  1180|         0|            0|            0|  0.00%|        logical (0 or nonzero) value of the input elements. The shape of\n",
      "  1181|         0|            0|            0|  0.00%|        `packed` has the same number of dimensions as the input (unless `axis`\n",
      "  1182|         0|            0|            0|  0.00%|        is None, in which case the output is 1-D).\n",
      "  1183|         0|            0|            0|  0.00%|\n",
      "  1184|         0|            0|            0|  0.00%|    See Also\n",
      "  1185|         0|            0|            0|  0.00%|    --------\n",
      "  1186|         0|            0|            0|  0.00%|    unpackbits: Unpacks elements of a uint8 array into a binary-valued output\n",
      "  1187|         0|            0|            0|  0.00%|                array.\n",
      "  1188|         0|            0|            0|  0.00%|\n",
      "  1189|         0|            0|            0|  0.00%|    Examples\n",
      "  1190|         0|            0|            0|  0.00%|    --------\n",
      "  1191|         0|            0|            0|  0.00%|    >>> a = np.array([[[1,0,1],\n",
      "  1192|         0|            0|            0|  0.00%|    ...                [0,1,0]],\n",
      "  1193|         0|            0|            0|  0.00%|    ...               [[1,1,0],\n",
      "  1194|         0|            0|            0|  0.00%|    ...                [0,0,1]]])\n",
      "  1195|         0|            0|            0|  0.00%|    >>> b = np.packbits(a, axis=-1)\n",
      "  1196|         0|            0|            0|  0.00%|    >>> b\n",
      "  1197|         0|            0|            0|  0.00%|    array([[[160],\n",
      "  1198|         0|            0|            0|  0.00%|            [ 64]],\n",
      "  1199|         0|            0|            0|  0.00%|           [[192],\n",
      "  1200|         0|            0|            0|  0.00%|            [ 32]]], dtype=uint8)\n",
      "  1201|         0|            0|            0|  0.00%|\n",
      "  1202|         0|            0|            0|  0.00%|    Note that in binary 160 = 1010 0000, 64 = 0100 0000, 192 = 1100 0000,\n",
      "  1203|         0|            0|            0|  0.00%|    and 32 = 0010 0000.\n",
      "  1204|         0|            0|            0|  0.00%|\n",
      "  1205|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1206|         0|            0|            0|  0.00%|    return (a,)\n",
      "  1207|         0|            0|            0|  0.00%|\n",
      "  1208|         0|            0|            0|  0.00%|\n",
      "  1209|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.unpackbits)\n",
      "  1210|         0|            0|            0|  0.00%|def unpackbits(a, axis=None, count=None, bitorder='big'):\n",
      "  1211|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1212|         0|            0|            0|  0.00%|    unpackbits(a, /, axis=None, count=None, bitorder='big')\n",
      "  1213|         0|            0|            0|  0.00%|\n",
      "  1214|         0|            0|            0|  0.00%|    Unpacks elements of a uint8 array into a binary-valued output array.\n",
      "  1215|         0|            0|            0|  0.00%|\n",
      "  1216|         0|            0|            0|  0.00%|    Each element of `a` represents a bit-field that should be unpacked\n",
      "  1217|         0|            0|            0|  0.00%|    into a binary-valued output array. The shape of the output array is\n",
      "  1218|         0|            0|            0|  0.00%|    either 1-D (if `axis` is ``None``) or the same shape as the input\n",
      "  1219|         0|            0|            0|  0.00%|    array with unpacking done along the axis specified.\n",
      "  1220|         0|            0|            0|  0.00%|\n",
      "  1221|         0|            0|            0|  0.00%|    Parameters\n",
      "  1222|         0|            0|            0|  0.00%|    ----------\n",
      "  1223|         0|            0|            0|  0.00%|    a : ndarray, uint8 type\n",
      "  1224|         0|            0|            0|  0.00%|       Input array.\n",
      "  1225|         0|            0|            0|  0.00%|    axis : int, optional\n",
      "  1226|         0|            0|            0|  0.00%|        The dimension over which bit-unpacking is done.\n",
      "  1227|         0|            0|            0|  0.00%|        ``None`` implies unpacking the flattened array.\n",
      "  1228|         0|            0|            0|  0.00%|    count : int or None, optional\n",
      "  1229|         0|            0|            0|  0.00%|        The number of elements to unpack along `axis`, provided as a way\n",
      "  1230|         0|            0|            0|  0.00%|        of undoing the effect of packing a size that is not a multiple\n",
      "  1231|         0|            0|            0|  0.00%|        of eight. A non-negative number means to only unpack `count`\n",
      "  1232|         0|            0|            0|  0.00%|        bits. A negative number means to trim off that many bits from\n",
      "  1233|         0|            0|            0|  0.00%|        the end. ``None`` means to unpack the entire array (the\n",
      "  1234|         0|            0|            0|  0.00%|        default). Counts larger than the available number of bits will\n",
      "  1235|         0|            0|            0|  0.00%|        add zero padding to the output. Negative counts must not\n",
      "  1236|         0|            0|            0|  0.00%|        exceed the available number of bits.\n",
      "  1237|         0|            0|            0|  0.00%|\n",
      "  1238|         0|            0|            0|  0.00%|        .. versionadded:: 1.17.0\n",
      "  1239|         0|            0|            0|  0.00%|\n",
      "  1240|         0|            0|            0|  0.00%|    bitorder : {'big', 'little'}, optional\n",
      "  1241|         0|            0|            0|  0.00%|        The order of the returned bits. 'big' will mimic bin(val),\n",
      "  1242|         0|            0|            0|  0.00%|        ``3 = 0b00000011 => [0, 0, 0, 0, 0, 0, 1, 1]``, 'little' will reverse\n",
      "  1243|         0|            0|            0|  0.00%|        the order to ``[1, 1, 0, 0, 0, 0, 0, 0]``.\n",
      "  1244|         0|            0|            0|  0.00%|        Defaults to 'big'.\n",
      "  1245|         0|            0|            0|  0.00%|\n",
      "  1246|         0|            0|            0|  0.00%|        .. versionadded:: 1.17.0\n",
      "  1247|         0|            0|            0|  0.00%|\n",
      "  1248|         0|            0|            0|  0.00%|    Returns\n",
      "  1249|         0|            0|            0|  0.00%|    -------\n",
      "  1250|         0|            0|            0|  0.00%|    unpacked : ndarray, uint8 type\n",
      "  1251|         0|            0|            0|  0.00%|       The elements are binary-valued (0 or 1).\n",
      "  1252|         0|            0|            0|  0.00%|\n",
      "  1253|         0|            0|            0|  0.00%|    See Also\n",
      "  1254|         0|            0|            0|  0.00%|    --------\n",
      "  1255|         0|            0|            0|  0.00%|    packbits : Packs the elements of a binary-valued array into bits in\n",
      "  1256|         0|            0|            0|  0.00%|               a uint8 array.\n",
      "  1257|         0|            0|            0|  0.00%|\n",
      "  1258|         0|            0|            0|  0.00%|    Examples\n",
      "  1259|         0|            0|            0|  0.00%|    --------\n",
      "  1260|         0|            0|            0|  0.00%|    >>> a = np.array([[2], [7], [23]], dtype=np.uint8)\n",
      "  1261|         0|            0|            0|  0.00%|    >>> a\n",
      "  1262|         0|            0|            0|  0.00%|    array([[ 2],\n",
      "  1263|         0|            0|            0|  0.00%|           [ 7],\n",
      "  1264|         0|            0|            0|  0.00%|           [23]], dtype=uint8)\n",
      "  1265|         0|            0|            0|  0.00%|    >>> b = np.unpackbits(a, axis=1)\n",
      "  1266|         0|            0|            0|  0.00%|    >>> b\n",
      "  1267|         0|            0|            0|  0.00%|    array([[0, 0, 0, 0, 0, 0, 1, 0],\n",
      "  1268|         0|            0|            0|  0.00%|           [0, 0, 0, 0, 0, 1, 1, 1],\n",
      "  1269|         0|            0|            0|  0.00%|           [0, 0, 0, 1, 0, 1, 1, 1]], dtype=uint8)\n",
      "  1270|         0|            0|            0|  0.00%|    >>> c = np.unpackbits(a, axis=1, count=-3)\n",
      "  1271|         0|            0|            0|  0.00%|    >>> c\n",
      "  1272|         0|            0|            0|  0.00%|    array([[0, 0, 0, 0, 0],\n",
      "  1273|         0|            0|            0|  0.00%|           [0, 0, 0, 0, 0],\n",
      "  1274|         0|            0|            0|  0.00%|           [0, 0, 0, 1, 0]], dtype=uint8)\n",
      "  1275|         0|            0|            0|  0.00%|\n",
      "  1276|         0|            0|            0|  0.00%|    >>> p = np.packbits(b, axis=0)\n",
      "  1277|         0|            0|            0|  0.00%|    >>> np.unpackbits(p, axis=0)\n",
      "  1278|         0|            0|            0|  0.00%|    array([[0, 0, 0, 0, 0, 0, 1, 0],\n",
      "  1279|         0|            0|            0|  0.00%|           [0, 0, 0, 0, 0, 1, 1, 1],\n",
      "  1280|         0|            0|            0|  0.00%|           [0, 0, 0, 1, 0, 1, 1, 1],\n",
      "  1281|         0|            0|            0|  0.00%|           [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "  1282|         0|            0|            0|  0.00%|           [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "  1283|         0|            0|            0|  0.00%|           [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "  1284|         0|            0|            0|  0.00%|           [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "  1285|         0|            0|            0|  0.00%|           [0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n",
      "  1286|         0|            0|            0|  0.00%|    >>> np.array_equal(b, np.unpackbits(p, axis=0, count=b.shape[0]))\n",
      "  1287|         0|            0|            0|  0.00%|    True\n",
      "  1288|         0|            0|            0|  0.00%|\n",
      "  1289|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1290|         0|            0|            0|  0.00%|    return (a,)\n",
      "  1291|         0|            0|            0|  0.00%|\n",
      "  1292|         0|            0|            0|  0.00%|\n",
      "  1293|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.shares_memory)\n",
      "  1294|         0|            0|            0|  0.00%|def shares_memory(a, b, max_work=None):\n",
      "  1295|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1296|         0|            0|            0|  0.00%|    shares_memory(a, b, /, max_work=None)\n",
      "  1297|         0|            0|            0|  0.00%|\n",
      "  1298|         0|            0|            0|  0.00%|    Determine if two arrays share memory.\n",
      "  1299|         0|            0|            0|  0.00%|\n",
      "  1300|         0|            0|            0|  0.00%|    .. warning::\n",
      "  1301|         0|            0|            0|  0.00%|\n",
      "  1302|         0|            0|            0|  0.00%|       This function can be exponentially slow for some inputs, unless\n",
      "  1303|         0|            0|            0|  0.00%|       `max_work` is set to a finite number or ``MAY_SHARE_BOUNDS``.\n",
      "  1304|         0|            0|            0|  0.00%|       If in doubt, use `numpy.may_share_memory` instead.\n",
      "  1305|         0|            0|            0|  0.00%|\n",
      "  1306|         0|            0|            0|  0.00%|    Parameters\n",
      "  1307|         0|            0|            0|  0.00%|    ----------\n",
      "  1308|         0|            0|            0|  0.00%|    a, b : ndarray\n",
      "  1309|         0|            0|            0|  0.00%|        Input arrays\n",
      "  1310|         0|            0|            0|  0.00%|    max_work : int, optional\n",
      "  1311|         0|            0|            0|  0.00%|        Effort to spend on solving the overlap problem (maximum number\n",
      "  1312|         0|            0|            0|  0.00%|        of candidate solutions to consider). The following special\n",
      "  1313|         0|            0|            0|  0.00%|        values are recognized:\n",
      "  1314|         0|            0|            0|  0.00%|\n",
      "  1315|         0|            0|            0|  0.00%|        max_work=MAY_SHARE_EXACT  (default)\n",
      "  1316|         0|            0|            0|  0.00%|            The problem is solved exactly. In this case, the function returns\n",
      "  1317|         0|            0|            0|  0.00%|            True only if there is an element shared between the arrays. Finding\n",
      "  1318|         0|            0|            0|  0.00%|            the exact solution may take extremely long in some cases.\n",
      "  1319|         0|            0|            0|  0.00%|        max_work=MAY_SHARE_BOUNDS\n",
      "  1320|         0|            0|            0|  0.00%|            Only the memory bounds of a and b are checked.\n",
      "  1321|         0|            0|            0|  0.00%|\n",
      "  1322|         0|            0|            0|  0.00%|    Raises\n",
      "  1323|         0|            0|            0|  0.00%|    ------\n",
      "  1324|         0|            0|            0|  0.00%|    numpy.TooHardError\n",
      "  1325|         0|            0|            0|  0.00%|        Exceeded max_work.\n",
      "  1326|         0|            0|            0|  0.00%|\n",
      "  1327|         0|            0|            0|  0.00%|    Returns\n",
      "  1328|         0|            0|            0|  0.00%|    -------\n",
      "  1329|         0|            0|            0|  0.00%|    out : bool\n",
      "  1330|         0|            0|            0|  0.00%|\n",
      "  1331|         0|            0|            0|  0.00%|    See Also\n",
      "  1332|         0|            0|            0|  0.00%|    --------\n",
      "  1333|         0|            0|            0|  0.00%|    may_share_memory\n",
      "  1334|         0|            0|            0|  0.00%|\n",
      "  1335|         0|            0|            0|  0.00%|    Examples\n",
      "  1336|         0|            0|            0|  0.00%|    --------\n",
      "  1337|         0|            0|            0|  0.00%|    >>> x = np.array([1, 2, 3, 4])\n",
      "  1338|         0|            0|            0|  0.00%|    >>> np.shares_memory(x, np.array([5, 6, 7]))\n",
      "  1339|         0|            0|            0|  0.00%|    False\n",
      "  1340|         0|            0|            0|  0.00%|    >>> np.shares_memory(x[::2], x)\n",
      "  1341|         0|            0|            0|  0.00%|    True\n",
      "  1342|         0|            0|            0|  0.00%|    >>> np.shares_memory(x[::2], x[1::2])\n",
      "  1343|         0|            0|            0|  0.00%|    False\n",
      "  1344|         0|            0|            0|  0.00%|\n",
      "  1345|         0|            0|            0|  0.00%|    Checking whether two arrays share memory is NP-complete, and\n",
      "  1346|         0|            0|            0|  0.00%|    runtime may increase exponentially in the number of\n",
      "  1347|         0|            0|            0|  0.00%|    dimensions. Hence, `max_work` should generally be set to a finite\n",
      "  1348|         0|            0|            0|  0.00%|    number, as it is possible to construct examples that take\n",
      "  1349|         0|            0|            0|  0.00%|    extremely long to run:\n",
      "  1350|         0|            0|            0|  0.00%|\n",
      "  1351|         0|            0|            0|  0.00%|    >>> from numpy.lib.stride_tricks import as_strided\n",
      "  1352|         0|            0|            0|  0.00%|    >>> x = np.zeros([192163377], dtype=np.int8)\n",
      "  1353|         0|            0|            0|  0.00%|    >>> x1 = as_strided(x, strides=(36674, 61119, 85569), shape=(1049, 1049, 1049))\n",
      "  1354|         0|            0|            0|  0.00%|    >>> x2 = as_strided(x[64023025:], strides=(12223, 12224, 1), shape=(1049, 1049, 1))\n",
      "  1355|         0|            0|            0|  0.00%|    >>> np.shares_memory(x1, x2, max_work=1000)\n",
      "  1356|         0|            0|            0|  0.00%|    Traceback (most recent call last):\n",
      "  1357|         0|            0|            0|  0.00%|    ...\n",
      "  1358|         0|            0|            0|  0.00%|    numpy.TooHardError: Exceeded max_work\n",
      "  1359|         0|            0|            0|  0.00%|\n",
      "  1360|         0|            0|            0|  0.00%|    Running ``np.shares_memory(x1, x2)`` without `max_work` set takes\n",
      "  1361|         0|            0|            0|  0.00%|    around 1 minute for this case. It is possible to find problems\n",
      "  1362|         0|            0|            0|  0.00%|    that take still significantly longer.\n",
      "  1363|         0|            0|            0|  0.00%|\n",
      "  1364|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1365|         0|            0|            0|  0.00%|    return (a, b)\n",
      "  1366|         0|            0|            0|  0.00%|\n",
      "  1367|         0|            0|            0|  0.00%|\n",
      "  1368|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.may_share_memory)\n",
      "  1369|         0|            0|            0|  0.00%|def may_share_memory(a, b, max_work=None):\n",
      "  1370|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1371|         0|            0|            0|  0.00%|    may_share_memory(a, b, /, max_work=None)\n",
      "  1372|         0|            0|            0|  0.00%|\n",
      "  1373|         0|            0|            0|  0.00%|    Determine if two arrays might share memory\n",
      "  1374|         0|            0|            0|  0.00%|\n",
      "  1375|         0|            0|            0|  0.00%|    A return of True does not necessarily mean that the two arrays\n",
      "  1376|         0|            0|            0|  0.00%|    share any element.  It just means that they *might*.\n",
      "  1377|         0|            0|            0|  0.00%|\n",
      "  1378|         0|            0|            0|  0.00%|    Only the memory bounds of a and b are checked by default.\n",
      "  1379|         0|            0|            0|  0.00%|\n",
      "  1380|         0|            0|            0|  0.00%|    Parameters\n",
      "  1381|         0|            0|            0|  0.00%|    ----------\n",
      "  1382|         0|            0|            0|  0.00%|    a, b : ndarray\n",
      "  1383|         0|            0|            0|  0.00%|        Input arrays\n",
      "  1384|         0|            0|            0|  0.00%|    max_work : int, optional\n",
      "  1385|         0|            0|            0|  0.00%|        Effort to spend on solving the overlap problem.  See\n",
      "  1386|         0|            0|            0|  0.00%|        `shares_memory` for details.  Default for ``may_share_memory``\n",
      "  1387|         0|            0|            0|  0.00%|        is to do a bounds check.\n",
      "  1388|         0|            0|            0|  0.00%|\n",
      "  1389|         0|            0|            0|  0.00%|    Returns\n",
      "  1390|         0|            0|            0|  0.00%|    -------\n",
      "  1391|         0|            0|            0|  0.00%|    out : bool\n",
      "  1392|         0|            0|            0|  0.00%|\n",
      "  1393|         0|            0|            0|  0.00%|    See Also\n",
      "  1394|         0|            0|            0|  0.00%|    --------\n",
      "  1395|         0|            0|            0|  0.00%|    shares_memory\n",
      "  1396|         0|            0|            0|  0.00%|\n",
      "  1397|         0|            0|            0|  0.00%|    Examples\n",
      "  1398|         0|            0|            0|  0.00%|    --------\n",
      "  1399|         0|            0|            0|  0.00%|    >>> np.may_share_memory(np.array([1,2]), np.array([5,8,9]))\n",
      "  1400|         0|            0|            0|  0.00%|    False\n",
      "  1401|         0|            0|            0|  0.00%|    >>> x = np.zeros([3, 4])\n",
      "  1402|         0|            0|            0|  0.00%|    >>> np.may_share_memory(x[:,0], x[:,1])\n",
      "  1403|         0|            0|            0|  0.00%|    True\n",
      "  1404|         0|            0|            0|  0.00%|\n",
      "  1405|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1406|         0|            0|            0|  0.00%|    return (a, b)\n",
      "  1407|         0|            0|            0|  0.00%|\n",
      "  1408|         0|            0|            0|  0.00%|\n",
      "  1409|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.is_busday)\n",
      "  1410|         0|            0|            0|  0.00%|def is_busday(dates, weekmask=None, holidays=None, busdaycal=None, out=None):\n",
      "  1411|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1412|         0|            0|            0|  0.00%|    is_busday(dates, weekmask='1111100', holidays=None, busdaycal=None, out=None)\n",
      "  1413|         0|            0|            0|  0.00%|\n",
      "  1414|         0|            0|            0|  0.00%|    Calculates which of the given dates are valid days, and which are not.\n",
      "  1415|         0|            0|            0|  0.00%|\n",
      "  1416|         0|            0|            0|  0.00%|    .. versionadded:: 1.7.0\n",
      "  1417|         0|            0|            0|  0.00%|\n",
      "  1418|         0|            0|            0|  0.00%|    Parameters\n",
      "  1419|         0|            0|            0|  0.00%|    ----------\n",
      "  1420|         0|            0|            0|  0.00%|    dates : array_like of datetime64[D]\n",
      "  1421|         0|            0|            0|  0.00%|        The array of dates to process.\n",
      "  1422|         0|            0|            0|  0.00%|    weekmask : str or array_like of bool, optional\n",
      "  1423|         0|            0|            0|  0.00%|        A seven-element array indicating which of Monday through Sunday are\n",
      "  1424|         0|            0|            0|  0.00%|        valid days. May be specified as a length-seven list or array, like\n",
      "  1425|         0|            0|            0|  0.00%|        [1,1,1,1,1,0,0]; a length-seven string, like '1111100'; or a string\n",
      "  1426|         0|            0|            0|  0.00%|        like \"Mon Tue Wed Thu Fri\", made up of 3-character abbreviations for\n",
      "  1427|         0|            0|            0|  0.00%|        weekdays, optionally separated by white space. Valid abbreviations\n",
      "  1428|         0|            0|            0|  0.00%|        are: Mon Tue Wed Thu Fri Sat Sun\n",
      "  1429|         0|            0|            0|  0.00%|    holidays : array_like of datetime64[D], optional\n",
      "  1430|         0|            0|            0|  0.00%|        An array of dates to consider as invalid dates.  They may be\n",
      "  1431|         0|            0|            0|  0.00%|        specified in any order, and NaT (not-a-time) dates are ignored.\n",
      "  1432|         0|            0|            0|  0.00%|        This list is saved in a normalized form that is suited for\n",
      "  1433|         0|            0|            0|  0.00%|        fast calculations of valid days.\n",
      "  1434|         0|            0|            0|  0.00%|    busdaycal : busdaycalendar, optional\n",
      "  1435|         0|            0|            0|  0.00%|        A `busdaycalendar` object which specifies the valid days. If this\n",
      "  1436|         0|            0|            0|  0.00%|        parameter is provided, neither weekmask nor holidays may be\n",
      "  1437|         0|            0|            0|  0.00%|        provided.\n",
      "  1438|         0|            0|            0|  0.00%|    out : array of bool, optional\n",
      "  1439|         0|            0|            0|  0.00%|        If provided, this array is filled with the result.\n",
      "  1440|         0|            0|            0|  0.00%|\n",
      "  1441|         0|            0|            0|  0.00%|    Returns\n",
      "  1442|         0|            0|            0|  0.00%|    -------\n",
      "  1443|         0|            0|            0|  0.00%|    out : array of bool\n",
      "  1444|         0|            0|            0|  0.00%|        An array with the same shape as ``dates``, containing True for\n",
      "  1445|         0|            0|            0|  0.00%|        each valid day, and False for each invalid day.\n",
      "  1446|         0|            0|            0|  0.00%|\n",
      "  1447|         0|            0|            0|  0.00%|    See Also\n",
      "  1448|         0|            0|            0|  0.00%|    --------\n",
      "  1449|         0|            0|            0|  0.00%|    busdaycalendar : An object that specifies a custom set of valid days.\n",
      "  1450|         0|            0|            0|  0.00%|    busday_offset : Applies an offset counted in valid days.\n",
      "  1451|         0|            0|            0|  0.00%|    busday_count : Counts how many valid days are in a half-open date range.\n",
      "  1452|         0|            0|            0|  0.00%|\n",
      "  1453|         0|            0|            0|  0.00%|    Examples\n",
      "  1454|         0|            0|            0|  0.00%|    --------\n",
      "  1455|         0|            0|            0|  0.00%|    >>> # The weekdays are Friday, Saturday, and Monday\n",
      "  1456|         0|            0|            0|  0.00%|    ... np.is_busday(['2011-07-01', '2011-07-02', '2011-07-18'],\n",
      "  1457|         0|            0|            0|  0.00%|    ...                 holidays=['2011-07-01', '2011-07-04', '2011-07-17'])\n",
      "  1458|         0|            0|            0|  0.00%|    array([False, False,  True])\n",
      "  1459|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1460|         0|            0|            0|  0.00%|    return (dates, weekmask, holidays, out)\n",
      "  1461|         0|            0|            0|  0.00%|\n",
      "  1462|         0|            0|            0|  0.00%|\n",
      "  1463|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.busday_offset)\n",
      "  1464|         0|            0|            0|  0.00%|def busday_offset(dates, offsets, roll=None, weekmask=None, holidays=None,\n",
      "  1465|         0|            0|            0|  0.00%|                  busdaycal=None, out=None):\n",
      "  1466|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1467|         0|            0|            0|  0.00%|    busday_offset(dates, offsets, roll='raise', weekmask='1111100', holidays=None, busdaycal=None, out=None)\n",
      "  1468|         0|            0|            0|  0.00%|\n",
      "  1469|         0|            0|            0|  0.00%|    First adjusts the date to fall on a valid day according to\n",
      "  1470|         0|            0|            0|  0.00%|    the ``roll`` rule, then applies offsets to the given dates\n",
      "  1471|         0|            0|            0|  0.00%|    counted in valid days.\n",
      "  1472|         0|            0|            0|  0.00%|\n",
      "  1473|         0|            0|            0|  0.00%|    .. versionadded:: 1.7.0\n",
      "  1474|         0|            0|            0|  0.00%|\n",
      "  1475|         0|            0|            0|  0.00%|    Parameters\n",
      "  1476|         0|            0|            0|  0.00%|    ----------\n",
      "  1477|         0|            0|            0|  0.00%|    dates : array_like of datetime64[D]\n",
      "  1478|         0|            0|            0|  0.00%|        The array of dates to process.\n",
      "  1479|         0|            0|            0|  0.00%|    offsets : array_like of int\n",
      "  1480|         0|            0|            0|  0.00%|        The array of offsets, which is broadcast with ``dates``.\n",
      "  1481|         0|            0|            0|  0.00%|    roll : {'raise', 'nat', 'forward', 'following', 'backward', 'preceding', 'modifiedfollowing', 'modifiedpreceding'}, optional\n",
      "  1482|         0|            0|            0|  0.00%|        How to treat dates that do not fall on a valid day. The default\n",
      "  1483|         0|            0|            0|  0.00%|        is 'raise'.\n",
      "  1484|         0|            0|            0|  0.00%|\n",
      "  1485|         0|            0|            0|  0.00%|          * 'raise' means to raise an exception for an invalid day.\n",
      "  1486|         0|            0|            0|  0.00%|          * 'nat' means to return a NaT (not-a-time) for an invalid day.\n",
      "  1487|         0|            0|            0|  0.00%|          * 'forward' and 'following' mean to take the first valid day\n",
      "  1488|         0|            0|            0|  0.00%|            later in time.\n",
      "  1489|         0|            0|            0|  0.00%|          * 'backward' and 'preceding' mean to take the first valid day\n",
      "  1490|         0|            0|            0|  0.00%|            earlier in time.\n",
      "  1491|         0|            0|            0|  0.00%|          * 'modifiedfollowing' means to take the first valid day\n",
      "  1492|         0|            0|            0|  0.00%|            later in time unless it is across a Month boundary, in which\n",
      "  1493|         0|            0|            0|  0.00%|            case to take the first valid day earlier in time.\n",
      "  1494|         0|            0|            0|  0.00%|          * 'modifiedpreceding' means to take the first valid day\n",
      "  1495|         0|            0|            0|  0.00%|            earlier in time unless it is across a Month boundary, in which\n",
      "  1496|         0|            0|            0|  0.00%|            case to take the first valid day later in time.\n",
      "  1497|         0|            0|            0|  0.00%|    weekmask : str or array_like of bool, optional\n",
      "  1498|         0|            0|            0|  0.00%|        A seven-element array indicating which of Monday through Sunday are\n",
      "  1499|         0|            0|            0|  0.00%|        valid days. May be specified as a length-seven list or array, like\n",
      "  1500|         0|            0|            0|  0.00%|        [1,1,1,1,1,0,0]; a length-seven string, like '1111100'; or a string\n",
      "  1501|         0|            0|            0|  0.00%|        like \"Mon Tue Wed Thu Fri\", made up of 3-character abbreviations for\n",
      "  1502|         0|            0|            0|  0.00%|        weekdays, optionally separated by white space. Valid abbreviations\n",
      "  1503|         0|            0|            0|  0.00%|        are: Mon Tue Wed Thu Fri Sat Sun\n",
      "  1504|         0|            0|            0|  0.00%|    holidays : array_like of datetime64[D], optional\n",
      "  1505|         0|            0|            0|  0.00%|        An array of dates to consider as invalid dates.  They may be\n",
      "  1506|         0|            0|            0|  0.00%|        specified in any order, and NaT (not-a-time) dates are ignored.\n",
      "  1507|         0|            0|            0|  0.00%|        This list is saved in a normalized form that is suited for\n",
      "  1508|         0|            0|            0|  0.00%|        fast calculations of valid days.\n",
      "  1509|         0|            0|            0|  0.00%|    busdaycal : busdaycalendar, optional\n",
      "  1510|         0|            0|            0|  0.00%|        A `busdaycalendar` object which specifies the valid days. If this\n",
      "  1511|         0|            0|            0|  0.00%|        parameter is provided, neither weekmask nor holidays may be\n",
      "  1512|         0|            0|            0|  0.00%|        provided.\n",
      "  1513|         0|            0|            0|  0.00%|    out : array of datetime64[D], optional\n",
      "  1514|         0|            0|            0|  0.00%|        If provided, this array is filled with the result.\n",
      "  1515|         0|            0|            0|  0.00%|\n",
      "  1516|         0|            0|            0|  0.00%|    Returns\n",
      "  1517|         0|            0|            0|  0.00%|    -------\n",
      "  1518|         0|            0|            0|  0.00%|    out : array of datetime64[D]\n",
      "  1519|         0|            0|            0|  0.00%|        An array with a shape from broadcasting ``dates`` and ``offsets``\n",
      "  1520|         0|            0|            0|  0.00%|        together, containing the dates with offsets applied.\n",
      "  1521|         0|            0|            0|  0.00%|\n",
      "  1522|         0|            0|            0|  0.00%|    See Also\n",
      "  1523|         0|            0|            0|  0.00%|    --------\n",
      "  1524|         0|            0|            0|  0.00%|    busdaycalendar : An object that specifies a custom set of valid days.\n",
      "  1525|         0|            0|            0|  0.00%|    is_busday : Returns a boolean array indicating valid days.\n",
      "  1526|         0|            0|            0|  0.00%|    busday_count : Counts how many valid days are in a half-open date range.\n",
      "  1527|         0|            0|            0|  0.00%|\n",
      "  1528|         0|            0|            0|  0.00%|    Examples\n",
      "  1529|         0|            0|            0|  0.00%|    --------\n",
      "  1530|         0|            0|            0|  0.00%|    >>> # First business day in October 2011 (not accounting for holidays)\n",
      "  1531|         0|            0|            0|  0.00%|    ... np.busday_offset('2011-10', 0, roll='forward')\n",
      "  1532|         0|            0|            0|  0.00%|    numpy.datetime64('2011-10-03')\n",
      "  1533|         0|            0|            0|  0.00%|    >>> # Last business day in February 2012 (not accounting for holidays)\n",
      "  1534|         0|            0|            0|  0.00%|    ... np.busday_offset('2012-03', -1, roll='forward')\n",
      "  1535|         0|            0|            0|  0.00%|    numpy.datetime64('2012-02-29')\n",
      "  1536|         0|            0|            0|  0.00%|    >>> # Third Wednesday in January 2011\n",
      "  1537|         0|            0|            0|  0.00%|    ... np.busday_offset('2011-01', 2, roll='forward', weekmask='Wed')\n",
      "  1538|         0|            0|            0|  0.00%|    numpy.datetime64('2011-01-19')\n",
      "  1539|         0|            0|            0|  0.00%|    >>> # 2012 Mother's Day in Canada and the U.S.\n",
      "  1540|         0|            0|            0|  0.00%|    ... np.busday_offset('2012-05', 1, roll='forward', weekmask='Sun')\n",
      "  1541|         0|            0|            0|  0.00%|    numpy.datetime64('2012-05-13')\n",
      "  1542|         0|            0|            0|  0.00%|\n",
      "  1543|         0|            0|            0|  0.00%|    >>> # First business day on or after a date\n",
      "  1544|         0|            0|            0|  0.00%|    ... np.busday_offset('2011-03-20', 0, roll='forward')\n",
      "  1545|         0|            0|            0|  0.00%|    numpy.datetime64('2011-03-21')\n",
      "  1546|         0|            0|            0|  0.00%|    >>> np.busday_offset('2011-03-22', 0, roll='forward')\n",
      "  1547|         0|            0|            0|  0.00%|    numpy.datetime64('2011-03-22')\n",
      "  1548|         0|            0|            0|  0.00%|    >>> # First business day after a date\n",
      "  1549|         0|            0|            0|  0.00%|    ... np.busday_offset('2011-03-20', 1, roll='backward')\n",
      "  1550|         0|            0|            0|  0.00%|    numpy.datetime64('2011-03-21')\n",
      "  1551|         0|            0|            0|  0.00%|    >>> np.busday_offset('2011-03-22', 1, roll='backward')\n",
      "  1552|         0|            0|            0|  0.00%|    numpy.datetime64('2011-03-23')\n",
      "  1553|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1554|         0|            0|            0|  0.00%|    return (dates, offsets, weekmask, holidays, out)\n",
      "  1555|         0|            0|            0|  0.00%|\n",
      "  1556|         0|            0|            0|  0.00%|\n",
      "  1557|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(_multiarray_umath.busday_count)\n",
      "  1558|         0|            0|            0|  0.00%|def busday_count(begindates, enddates, weekmask=None, holidays=None,\n",
      "  1559|         0|            0|            0|  0.00%|                 busdaycal=None, out=None):\n",
      "  1560|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1561|         0|            0|            0|  0.00%|    busday_count(begindates, enddates, weekmask='1111100', holidays=[], busdaycal=None, out=None)\n",
      "  1562|         0|            0|            0|  0.00%|\n",
      "  1563|         0|            0|            0|  0.00%|    Counts the number of valid days between `begindates` and\n",
      "  1564|         0|            0|            0|  0.00%|    `enddates`, not including the day of `enddates`.\n",
      "  1565|         0|            0|            0|  0.00%|\n",
      "  1566|         0|            0|            0|  0.00%|    If ``enddates`` specifies a date value that is earlier than the\n",
      "  1567|         0|            0|            0|  0.00%|    corresponding ``begindates`` date value, the count will be negative.\n",
      "  1568|         0|            0|            0|  0.00%|\n",
      "  1569|         0|            0|            0|  0.00%|    .. versionadded:: 1.7.0\n",
      "  1570|         0|            0|            0|  0.00%|\n",
      "  1571|         0|            0|            0|  0.00%|    Parameters\n",
      "  1572|         0|            0|            0|  0.00%|    ----------\n",
      "  1573|         0|            0|            0|  0.00%|    begindates : array_like of datetime64[D]\n",
      "  1574|         0|            0|            0|  0.00%|        The array of the first dates for counting.\n",
      "  1575|         0|            0|            0|  0.00%|    enddates : array_like of datetime64[D]\n",
      "  1576|         0|            0|            0|  0.00%|        The array of the end dates for counting, which are excluded\n",
      "  1577|         0|            0|            0|  0.00%|        from the count themselves.\n",
      "  1578|         0|            0|            0|  0.00%|    weekmask : str or array_like of bool, optional\n",
      "  1579|         0|            0|            0|  0.00%|        A seven-element array indicating which of Monday through Sunday are\n",
      "  1580|         0|            0|            0|  0.00%|        valid days. May be specified as a length-seven list or array, like\n",
      "  1581|         0|            0|            0|  0.00%|        [1,1,1,1,1,0,0]; a length-seven string, like '1111100'; or a string\n",
      "  1582|         0|            0|            0|  0.00%|        like \"Mon Tue Wed Thu Fri\", made up of 3-character abbreviations for\n",
      "  1583|         0|            0|            0|  0.00%|        weekdays, optionally separated by white space. Valid abbreviations\n",
      "  1584|         0|            0|            0|  0.00%|        are: Mon Tue Wed Thu Fri Sat Sun\n",
      "  1585|         0|            0|            0|  0.00%|    holidays : array_like of datetime64[D], optional\n",
      "  1586|         0|            0|            0|  0.00%|        An array of dates to consider as invalid dates.  They may be\n",
      "  1587|         0|            0|            0|  0.00%|        specified in any order, and NaT (not-a-time) dates are ignored.\n",
      "  1588|         0|            0|            0|  0.00%|        This list is saved in a normalized form that is suited for\n",
      "  1589|         0|            0|            0|  0.00%|        fast calculations of valid days.\n",
      "  1590|         0|            0|            0|  0.00%|    busdaycal : busdaycalendar, optional\n",
      "  1591|         0|            0|            0|  0.00%|        A `busdaycalendar` object which specifies the valid days. If this\n",
      "  1592|         0|            0|            0|  0.00%|        parameter is provided, neither weekmask nor holidays may be\n",
      "  1593|         0|            0|            0|  0.00%|        provided.\n",
      "  1594|         0|            0|            0|  0.00%|    out : array of int, optional\n",
      "  1595|         0|            0|            0|  0.00%|        If provided, this array is filled with the result.\n",
      "  1596|         0|            0|            0|  0.00%|\n",
      "  1597|         0|            0|            0|  0.00%|    Returns\n",
      "  1598|         0|            0|            0|  0.00%|    -------\n",
      "  1599|         0|            0|            0|  0.00%|    out : array of int\n",
      "  1600|         0|            0|            0|  0.00%|        An array with a shape from broadcasting ``begindates`` and ``enddates``\n",
      "  1601|         0|            0|            0|  0.00%|        together, containing the number of valid days between\n",
      "  1602|         0|            0|            0|  0.00%|        the begin and end dates.\n",
      "  1603|         0|            0|            0|  0.00%|\n",
      "  1604|         0|            0|            0|  0.00%|    See Also\n",
      "  1605|         0|            0|            0|  0.00%|    --------\n",
      "  1606|         0|            0|            0|  0.00%|    busdaycalendar : An object that specifies a custom set of valid days.\n",
      "  1607|         0|            0|            0|  0.00%|    is_busday : Returns a boolean array indicating valid days.\n",
      "  1608|         0|            0|            0|  0.00%|    busday_offset : Applies an offset counted in valid days.\n",
      "  1609|         0|            0|            0|  0.00%|\n",
      "  1610|         0|            0|            0|  0.00%|    Examples\n",
      "  1611|         0|            0|            0|  0.00%|    --------\n",
      "  1612|         0|            0|            0|  0.00%|    >>> # Number of weekdays in January 2011\n",
      "  1613|         0|            0|            0|  0.00%|    ... np.busday_count('2011-01', '2011-02')\n",
      "  1614|         0|            0|            0|  0.00%|    21\n",
      "  1615|         0|            0|            0|  0.00%|    >>> # Number of weekdays in 2011\n",
      "  1616|         0|            0|            0|  0.00%|    >>> np.busday_count('2011', '2012')\n",
      "  1617|         0|            0|            0|  0.00%|    260\n",
      "  1618|         0|            0|            0|  0.00%|    >>> # Number of Saturdays in 2011\n",
      "  1619|         0|            0|            0|  0.00%|    ... np.busday_count('2011', '2012', weekmask='Sat')\n",
      "  1620|         0|            0|            0|  0.00%|    53\n",
      "  1621|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1622|         0|            0|            0|  0.00%|    return (begindates, enddates, weekmask, holidays, out)\n",
      "  1623|         0|            0|            0|  0.00%|\n",
      "  1624|         0|            0|            0|  0.00%|\n",
      "  1625|         0|            0|            0|  0.00%|@array_function_from_c_func_and_dispatcher(\n",
      "  1626|         0|            0|            0|  0.00%|    _multiarray_umath.datetime_as_string)\n",
      "  1627|         0|            0|            0|  0.00%|def datetime_as_string(arr, unit=None, timezone=None, casting=None):\n",
      "  1628|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1629|         0|            0|            0|  0.00%|    datetime_as_string(arr, unit=None, timezone='naive', casting='same_kind')\n",
      "  1630|         0|            0|            0|  0.00%|\n",
      "  1631|         0|            0|            0|  0.00%|    Convert an array of datetimes into an array of strings.\n",
      "  1632|         0|            0|            0|  0.00%|\n",
      "  1633|         0|            0|            0|  0.00%|    Parameters\n",
      "  1634|         0|            0|            0|  0.00%|    ----------\n",
      "  1635|         0|            0|            0|  0.00%|    arr : array_like of datetime64\n",
      "  1636|         0|            0|            0|  0.00%|        The array of UTC timestamps to format.\n",
      "  1637|         0|            0|            0|  0.00%|    unit : str\n",
      "  1638|         0|            0|            0|  0.00%|        One of None, 'auto', or a :ref:`datetime unit <arrays.dtypes.dateunits>`.\n",
      "  1639|         0|            0|            0|  0.00%|    timezone : {'naive', 'UTC', 'local'} or tzinfo\n",
      "  1640|         0|            0|            0|  0.00%|        Timezone information to use when displaying the datetime. If 'UTC', end\n",
      "  1641|         0|            0|            0|  0.00%|        with a Z to indicate UTC time. If 'local', convert to the local timezone\n",
      "  1642|         0|            0|            0|  0.00%|        first, and suffix with a +-#### timezone offset. If a tzinfo object,\n",
      "  1643|         0|            0|            0|  0.00%|        then do as with 'local', but use the specified timezone.\n",
      "  1644|         0|            0|            0|  0.00%|    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}\n",
      "  1645|         0|            0|            0|  0.00%|        Casting to allow when changing between datetime units.\n",
      "  1646|         0|            0|            0|  0.00%|\n",
      "  1647|         0|            0|            0|  0.00%|    Returns\n",
      "  1648|         0|            0|            0|  0.00%|    -------\n",
      "  1649|         0|            0|            0|  0.00%|    str_arr : ndarray\n",
      "  1650|         0|            0|            0|  0.00%|        An array of strings the same shape as `arr`.\n",
      "  1651|         0|            0|            0|  0.00%|\n",
      "  1652|         0|            0|            0|  0.00%|    Examples\n",
      "  1653|         0|            0|            0|  0.00%|    --------\n",
      "  1654|         0|            0|            0|  0.00%|    >>> import pytz\n",
      "  1655|         0|            0|            0|  0.00%|    >>> d = np.arange('2002-10-27T04:30', 4*60, 60, dtype='M8[m]')\n",
      "  1656|         0|            0|            0|  0.00%|    >>> d\n",
      "  1657|         0|            0|            0|  0.00%|    array(['2002-10-27T04:30', '2002-10-27T05:30', '2002-10-27T06:30',\n",
      "  1658|         0|            0|            0|  0.00%|           '2002-10-27T07:30'], dtype='datetime64[m]')\n",
      "  1659|         0|            0|            0|  0.00%|\n",
      "  1660|         0|            0|            0|  0.00%|    Setting the timezone to UTC shows the same information, but with a Z suffix\n",
      "  1661|         0|            0|            0|  0.00%|\n",
      "  1662|         0|            0|            0|  0.00%|    >>> np.datetime_as_string(d, timezone='UTC')\n",
      "  1663|         0|            0|            0|  0.00%|    array(['2002-10-27T04:30Z', '2002-10-27T05:30Z', '2002-10-27T06:30Z',\n",
      "  1664|         0|            0|            0|  0.00%|           '2002-10-27T07:30Z'], dtype='<U35')\n",
      "  1665|         0|            0|            0|  0.00%|\n",
      "  1666|         0|            0|            0|  0.00%|    Note that we picked datetimes that cross a DST boundary. Passing in a\n",
      "  1667|         0|            0|            0|  0.00%|    ``pytz`` timezone object will print the appropriate offset\n",
      "  1668|         0|            0|            0|  0.00%|\n",
      "  1669|         0|            0|            0|  0.00%|    >>> np.datetime_as_string(d, timezone=pytz.timezone('US/Eastern'))\n",
      "  1670|         0|            0|            0|  0.00%|    array(['2002-10-27T00:30-0400', '2002-10-27T01:30-0400',\n",
      "  1671|         0|            0|            0|  0.00%|           '2002-10-27T01:30-0500', '2002-10-27T02:30-0500'], dtype='<U39')\n",
      "  1672|         0|            0|            0|  0.00%|\n",
      "  1673|         0|            0|            0|  0.00%|    Passing in a unit will change the precision\n",
      "  1674|         0|            0|            0|  0.00%|\n",
      "  1675|         0|            0|            0|  0.00%|    >>> np.datetime_as_string(d, unit='h')\n",
      "  1676|         0|            0|            0|  0.00%|    array(['2002-10-27T04', '2002-10-27T05', '2002-10-27T06', '2002-10-27T07'],\n",
      "  1677|         0|            0|            0|  0.00%|          dtype='<U32')\n",
      "  1678|         0|            0|            0|  0.00%|    >>> np.datetime_as_string(d, unit='s')\n",
      "  1679|         0|            0|            0|  0.00%|    array(['2002-10-27T04:30:00', '2002-10-27T05:30:00', '2002-10-27T06:30:00',\n",
      "  1680|         0|            0|            0|  0.00%|           '2002-10-27T07:30:00'], dtype='<U38')\n",
      "  1681|         0|            0|            0|  0.00%|\n",
      "  1682|         0|            0|            0|  0.00%|    'casting' can be used to specify whether precision can be changed\n",
      "  1683|         0|            0|            0|  0.00%|\n",
      "  1684|         0|            0|            0|  0.00%|    >>> np.datetime_as_string(d, unit='h', casting='safe')\n",
      "  1685|         0|            0|            0|  0.00%|    Traceback (most recent call last):\n",
      "  1686|         0|            0|            0|  0.00%|        ...\n",
      "  1687|         0|            0|            0|  0.00%|    TypeError: Cannot create a datetime string as units 'h' from a NumPy\n",
      "  1688|         0|            0|            0|  0.00%|    datetime with units 'm' according to the rule 'safe'\n",
      "  1689|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1690|         0|            0|            0|  0.00%|    return (arr,)\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/utils.py\n",
      "File duration: 1.67123s (0.27%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|from functools import update_wrapper\n",
      "     2|         0|            0|            0|  0.00%|from numbers import Number\n",
      "     3|         0|            0|            0|  0.00%|import torch\n",
      "     4|         0|            0|            0|  0.00%|import torch.nn.functional as F\n",
      "     5|         0|            0|            0|  0.00%|from typing import Dict, Any\n",
      "     6|         0|            0|            0|  0.00%|from torch.overrides import is_tensor_like\n",
      "     7|         0|            0|            0|  0.00%|\n",
      "     8|         0|            0|            0|  0.00%|euler_constant = 0.57721566490153286060  # Euler Mascheroni Constant\n",
      "     9|         0|            0|            0|  0.00%|\n",
      "    10|         0|            0|            0|  0.00%|\n",
      "    11|         0|            0|            0|  0.00%|def broadcast_all(*values):\n",
      "    12|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "    13|         0|            0|            0|  0.00%|    Given a list of values (possibly containing numbers), returns a list where each\n",
      "    14|         0|            0|            0|  0.00%|    value is broadcasted based on the following rules:\n",
      "    15|         0|            0|            0|  0.00%|      - `torch.*Tensor` instances are broadcasted as per :ref:`_broadcasting-semantics`.\n",
      "    16|         0|            0|            0|  0.00%|      - numbers.Number instances (scalars) are upcast to tensors having\n",
      "    17|         0|            0|            0|  0.00%|        the same size and type as the first tensor passed to `values`.  If all the\n",
      "    18|         0|            0|            0|  0.00%|        values are scalars, then they are upcasted to scalar Tensors.\n",
      "    19|         0|            0|            0|  0.00%|\n",
      "    20|         0|            0|            0|  0.00%|    Args:\n",
      "    21|         0|            0|            0|  0.00%|        values (list of `numbers.Number`, `torch.*Tensor` or objects implementing __torch_function__)\n",
      "    22|         0|            0|            0|  0.00%|\n",
      "    23|         0|            0|            0|  0.00%|    Raises:\n",
      "    24|         0|            0|            0|  0.00%|        ValueError: if any of the values is not a `numbers.Number` instance,\n",
      "    25|         0|            0|            0|  0.00%|            a `torch.*Tensor` instance, or an instance implementing __torch_function__\n",
      "    26|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    27|         0|            0|            0|  0.00%|    if not all(is_tensor_like(v) or isinstance(v, Number)\n",
      "    28|         0|            0|            0|  0.00%|               for v in values):\n",
      "    29|         0|            0|            0|  0.00%|        raise ValueError('Input arguments must all be instances of numbers.Number, '\n",
      "    30|         0|            0|            0|  0.00%|                         'torch.Tensor or objects implementing __torch_function__.')\n",
      "    31|         0|            0|            0|  0.00%|    if not all(is_tensor_like(v) for v in values):\n",
      "    32|         0|            0|            0|  0.00%|        options: Dict[str, Any] = dict(dtype=torch.get_default_dtype())\n",
      "    33|         0|            0|            0|  0.00%|        for value in values:\n",
      "    34|         0|            0|            0|  0.00%|            if isinstance(value, torch.Tensor):\n",
      "    35|         0|            0|            0|  0.00%|                options = dict(dtype=value.dtype, device=value.device)\n",
      "    36|         0|            0|            0|  0.00%|                break\n",
      "    37|         0|            0|            0|  0.00%|        new_values = [v if is_tensor_like(v) else torch.tensor(v, **options)\n",
      "    38|         0|            0|            0|  0.00%|                      for v in values]\n",
      "    39|         0|            0|            0|  0.00%|        return torch.broadcast_tensors(*new_values)\n",
      "    40|         0|            0|            0|  0.00%|    return torch.broadcast_tensors(*values)\n",
      "    41|         0|            0|            0|  0.00%|\n",
      "    42|         0|            0|            0|  0.00%|\n",
      "    43|         0|            0|            0|  0.00%|def _standard_normal(shape, dtype, device):\n",
      "    44|         0|            0|            0|  0.00%|    if torch._C._get_tracing_state():\n",
      "    45|         0|            0|            0|  0.00%|        # [JIT WORKAROUND] lack of support for .normal_()\n",
      "    46|         0|            0|            0|  0.00%|        return torch.normal(torch.zeros(shape, dtype=dtype, device=device),\n",
      "    47|         0|            0|            0|  0.00%|                            torch.ones(shape, dtype=dtype, device=device))\n",
      "    48|         0|            0|            0|  0.00%|    return torch.empty(shape, dtype=dtype, device=device).normal_()\n",
      "    49|         0|            0|            0|  0.00%|\n",
      "    50|         0|            0|            0|  0.00%|\n",
      "    51|         0|            0|            0|  0.00%|def _sum_rightmost(value, dim):\n",
      "    52|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "    53|         0|            0|            0|  0.00%|    Sum out ``dim`` many rightmost dimensions of a given tensor.\n",
      "    54|         0|            0|            0|  0.00%|\n",
      "    55|         0|            0|            0|  0.00%|    Args:\n",
      "    56|         0|            0|            0|  0.00%|        value (Tensor): A tensor of ``.dim()`` at least ``dim``.\n",
      "    57|         0|            0|            0|  0.00%|        dim (int): The number of rightmost dims to sum out.\n",
      "    58|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    59|         0|            0|            0|  0.00%|    if dim == 0:\n",
      "    60|         0|            0|            0|  0.00%|        return value\n",
      "    61|         0|            0|            0|  0.00%|    required_shape = value.shape[:-dim] + (-1,)\n",
      "    62|         0|            0|            0|  0.00%|    return value.reshape(required_shape).sum(-1)\n",
      "    63|         0|            0|            0|  0.00%|\n",
      "    64|         0|            0|            0|  0.00%|\n",
      "    65|     31590|    0.0541508|  1.71418e-06|  0.01%|def logits_to_probs(logits, is_binary=False):\n",
      "    66|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "    67|         0|            0|            0|  0.00%|    Converts a tensor of logits into probabilities. Note that for the\n",
      "    68|         0|            0|            0|  0.00%|    binary case, each value denotes log odds, whereas for the\n",
      "    69|         0|            0|            0|  0.00%|    multi-dimensional case, the values along the last dimension denote\n",
      "    70|         0|            0|            0|  0.00%|    the log probabilities (possibly unnormalized) of the events.\n",
      "    71|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    72|     31590|    0.0582359|  1.84349e-06|  0.01%|    if is_binary:\n",
      "    73|         0|            0|            0|  0.00%|        return torch.sigmoid(logits)\n",
      "    74|     31590|     0.220409|  6.97719e-06|  0.04%|    return F.softmax(logits, dim=-1)\n",
      "(call)|     31590|     0.782285|  2.47637e-05|  0.13%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/functional.py:1804 softmax\n",
      "    75|         0|            0|            0|  0.00%|\n",
      "    76|         0|            0|            0|  0.00%|\n",
      "    77|         0|            0|            0|  0.00%|def clamp_probs(probs):\n",
      "    78|         0|            0|            0|  0.00%|    eps = torch.finfo(probs.dtype).eps\n",
      "    79|         0|            0|            0|  0.00%|    return probs.clamp(min=eps, max=1 - eps)\n",
      "    80|         0|            0|            0|  0.00%|\n",
      "    81|         0|            0|            0|  0.00%|\n",
      "    82|         0|            0|            0|  0.00%|def probs_to_logits(probs, is_binary=False):\n",
      "    83|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "    84|         0|            0|            0|  0.00%|    Converts a tensor of probabilities into logits. For the binary case,\n",
      "    85|         0|            0|            0|  0.00%|    this denotes the probability of occurrence of the event indexed by `1`.\n",
      "    86|         0|            0|            0|  0.00%|    For the multi-dimensional case, the values along the last dimension\n",
      "    87|         0|            0|            0|  0.00%|    denote the probabilities of occurrence of each of the events.\n",
      "    88|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    89|         0|            0|            0|  0.00%|    ps_clamped = clamp_probs(probs)\n",
      "    90|         0|            0|            0|  0.00%|    if is_binary:\n",
      "    91|         0|            0|            0|  0.00%|        return torch.log(ps_clamped) - torch.log1p(-ps_clamped)\n",
      "    92|         0|            0|            0|  0.00%|    return torch.log(ps_clamped)\n",
      "    93|         0|            0|            0|  0.00%|\n",
      "    94|         0|            0|            0|  0.00%|\n",
      "    95|         0|            0|            0|  0.00%|class lazy_property:\n",
      "    96|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "    97|         0|            0|            0|  0.00%|    Used as a decorator for lazy loading of class attributes. This uses a\n",
      "    98|         0|            0|            0|  0.00%|    non-data descriptor that calls the wrapped method to compute the property on\n",
      "    99|         0|            0|            0|  0.00%|    first call; thereafter replacing the wrapped method into an instance\n",
      "   100|         0|            0|            0|  0.00%|    attribute.\n",
      "   101|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   102|         0|            0|            0|  0.00%|    def __init__(self, wrapped):\n",
      "   103|         0|            0|            0|  0.00%|        self.wrapped = wrapped\n",
      "   104|         0|            0|            0|  0.00%|        update_wrapper(self, wrapped)\n",
      "   105|         0|            0|            0|  0.00%|\n",
      "   106|     63180|     0.121733|  1.92676e-06|  0.02%|    def __get__(self, instance, obj_type=None):\n",
      "   107|     63180|     0.135466|  2.14413e-06|  0.02%|        if instance is None:\n",
      "   108|     31590|     0.215312|  6.81582e-06|  0.03%|            return _lazy_property_and_property(self.wrapped)\n",
      "(call)|     31590|      0.15091|  4.77716e-06|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/utils.py:121 __init__\n",
      "   109|     31590|     0.225478|  7.13765e-06|  0.04%|        with torch.enable_grad():\n",
      "(call)|     31590|       0.2145|  6.79014e-06|  0.03%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:177 __enter__\n",
      "   110|     31590|     0.342231|  1.08335e-05|  0.06%|            value = self.wrapped(instance)\n",
      "(call)|     31590|      1.35538|  4.29054e-05|  0.22%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/distributions/categorical.py:92 probs\n",
      "(call)|     31590|     0.132449|  4.19276e-06|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:181 __exit__\n",
      "   111|     31590|    0.0846498|  2.67964e-06|  0.01%|        setattr(instance, self.wrapped.__name__, value)\n",
      "   112|     31590|    0.0626583|  1.98349e-06|  0.01%|        return value\n",
      "   113|         0|            0|            0|  0.00%|\n",
      "   114|         0|            0|            0|  0.00%|\n",
      "   115|         0|            0|            0|  0.00%|class _lazy_property_and_property(lazy_property, property):\n",
      "   116|         0|            0|            0|  0.00%|    \"\"\"We want lazy properties to look like multiple things.\n",
      "   117|         0|            0|            0|  0.00%|\n",
      "   118|         0|            0|            0|  0.00%|    * property when Sphinx autodoc looks\n",
      "   119|         0|            0|            0|  0.00%|    * lazy_property when Distribution validate_args looks\n",
      "   120|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   121|     31590|    0.0469632|  1.48665e-06|  0.01%|    def __init__(self, wrapped):\n",
      "   122|     31590|     0.103947|  3.29051e-06|  0.02%|        return property.__init__(self, wrapped)\n",
      "   123|         0|            0|            0|  0.00%|\n",
      "   124|         0|            0|            0|  0.00%|\n",
      "   125|         0|            0|            0|  0.00%|def tril_matrix_to_vec(mat, diag=0):\n",
      "   126|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   127|         0|            0|            0|  0.00%|    Convert a `D x D` matrix or a batch of matrices into a (batched) vector\n",
      "   128|         0|            0|            0|  0.00%|    which comprises of lower triangular elements from the matrix in row order.\n",
      "   129|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   130|         0|            0|            0|  0.00%|    n = mat.shape[-1]\n",
      "   131|         0|            0|            0|  0.00%|    if not torch._C._get_tracing_state() and (diag < -n or diag >= n):\n",
      "   132|         0|            0|            0|  0.00%|        raise ValueError(f'diag ({diag}) provided is outside [{-n}, {n-1}].')\n",
      "   133|         0|            0|            0|  0.00%|    arange = torch.arange(n, device=mat.device)\n",
      "   134|         0|            0|            0|  0.00%|    tril_mask = arange < arange.view(-1, 1) + (diag + 1)\n",
      "   135|         0|            0|            0|  0.00%|    vec = mat[..., tril_mask]\n",
      "   136|         0|            0|            0|  0.00%|    return vec\n",
      "   137|         0|            0|            0|  0.00%|\n",
      "   138|         0|            0|            0|  0.00%|\n",
      "   139|         0|            0|            0|  0.00%|def vec_to_tril_matrix(vec, diag=0):\n",
      "   140|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   141|         0|            0|            0|  0.00%|    Convert a vector or a batch of vectors into a batched `D x D`\n",
      "   142|         0|            0|            0|  0.00%|    lower triangular matrix containing elements from the vector in row order.\n",
      "   143|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   144|         0|            0|            0|  0.00%|    # +ve root of D**2 + (1+2*diag)*D - |diag| * (diag+1) - 2*vec.shape[-1] = 0\n",
      "   145|         0|            0|            0|  0.00%|    n = (-(1 + 2 * diag) + ((1 + 2 * diag)**2 + 8 * vec.shape[-1] + 4 * abs(diag) * (diag + 1))**0.5) / 2\n",
      "   146|         0|            0|            0|  0.00%|    eps = torch.finfo(vec.dtype).eps\n",
      "   147|         0|            0|            0|  0.00%|    if not torch._C._get_tracing_state() and (round(n) - n > eps):\n",
      "   148|         0|            0|            0|  0.00%|        raise ValueError(f'The size of last dimension is {vec.shape[-1]} which cannot be expressed as ' +\n",
      "   149|         0|            0|            0|  0.00%|                         'the lower triangular part of a square D x D matrix.')\n",
      "   150|         0|            0|            0|  0.00%|    n = torch.round(n).long() if isinstance(n, torch.Tensor) else round(n)\n",
      "   151|         0|            0|            0|  0.00%|    mat = vec.new_zeros(vec.shape[:-1] + torch.Size((n, n)))\n",
      "   152|         0|            0|            0|  0.00%|    arange = torch.arange(n, device=vec.device)\n",
      "   153|         0|            0|            0|  0.00%|    tril_mask = arange < arange.view(-1, 1) + (diag + 1)\n",
      "   154|         0|            0|            0|  0.00%|    mat[..., tril_mask] = vec\n",
      "   155|         0|            0|            0|  0.00%|    return mat\n",
      "File: <string>\n",
      "File duration: 1.47007s (0.24%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|    648964|      1.47007|  2.26526e-06|  0.24%|\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py\n",
      "File duration: 1.24649s (0.20%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|import warnings\n",
      "     2|         0|            0|            0|  0.00%|from typing import Optional, Tuple\n",
      "     3|         0|            0|            0|  0.00%|\n",
      "     4|         0|            0|            0|  0.00%|import torch\n",
      "     5|         0|            0|            0|  0.00%|from torch import Tensor\n",
      "     6|         0|            0|            0|  0.00%|from .linear import NonDynamicallyQuantizableLinear\n",
      "     7|         0|            0|            0|  0.00%|from torch.nn.init import constant_, xavier_normal_, xavier_uniform_\n",
      "     8|         0|            0|            0|  0.00%|from torch.nn.parameter import Parameter\n",
      "     9|         0|            0|            0|  0.00%|from .module import Module\n",
      "    10|         0|            0|            0|  0.00%|from .. import functional as F\n",
      "    11|         0|            0|            0|  0.00%|\n",
      "    12|         0|            0|            0|  0.00%|\n",
      "    13|         0|            0|            0|  0.00%|class Threshold(Module):\n",
      "    14|         0|            0|            0|  0.00%|    r\"\"\"Thresholds each element of the input Tensor.\n",
      "    15|         0|            0|            0|  0.00%|\n",
      "    16|         0|            0|            0|  0.00%|    Threshold is defined as:\n",
      "    17|         0|            0|            0|  0.00%|\n",
      "    18|         0|            0|            0|  0.00%|    .. math::\n",
      "    19|         0|            0|            0|  0.00%|        y =\n",
      "    20|         0|            0|            0|  0.00%|        \\begin{cases}\n",
      "    21|         0|            0|            0|  0.00%|        x, &\\text{ if } x > \\text{threshold} \\\\\n",
      "    22|         0|            0|            0|  0.00%|        \\text{value}, &\\text{ otherwise }\n",
      "    23|         0|            0|            0|  0.00%|        \\end{cases}\n",
      "    24|         0|            0|            0|  0.00%|\n",
      "    25|         0|            0|            0|  0.00%|    Args:\n",
      "    26|         0|            0|            0|  0.00%|        threshold: The value to threshold at\n",
      "    27|         0|            0|            0|  0.00%|        value: The value to replace with\n",
      "    28|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``\n",
      "    29|         0|            0|            0|  0.00%|\n",
      "    30|         0|            0|            0|  0.00%|    Shape:\n",
      "    31|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "    32|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "    33|         0|            0|            0|  0.00%|\n",
      "    34|         0|            0|            0|  0.00%|    Examples::\n",
      "    35|         0|            0|            0|  0.00%|\n",
      "    36|         0|            0|            0|  0.00%|        >>> m = nn.Threshold(0.1, 20)\n",
      "    37|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "    38|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "    39|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    40|         0|            0|            0|  0.00%|    __constants__ = ['threshold', 'value', 'inplace']\n",
      "    41|         0|            0|            0|  0.00%|\n",
      "    42|         0|            0|            0|  0.00%|    threshold: float\n",
      "    43|         0|            0|            0|  0.00%|    value: float\n",
      "    44|         0|            0|            0|  0.00%|    inplace: bool\n",
      "    45|         0|            0|            0|  0.00%|\n",
      "    46|         0|            0|            0|  0.00%|    def __init__(self, threshold: float, value: float, inplace: bool = False) -> None:\n",
      "    47|         0|            0|            0|  0.00%|        super(Threshold, self).__init__()\n",
      "    48|         0|            0|            0|  0.00%|        self.threshold = threshold\n",
      "    49|         0|            0|            0|  0.00%|        self.value = value\n",
      "    50|         0|            0|            0|  0.00%|        self.inplace = inplace\n",
      "    51|         0|            0|            0|  0.00%|        # TODO: check in THNN (if inplace == True, then assert value <= threshold)\n",
      "    52|         0|            0|            0|  0.00%|\n",
      "    53|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "    54|         0|            0|            0|  0.00%|        return F.threshold(input, self.threshold, self.value, self.inplace)\n",
      "    55|         0|            0|            0|  0.00%|\n",
      "    56|         0|            0|            0|  0.00%|    def extra_repr(self):\n",
      "    57|         0|            0|            0|  0.00%|        inplace_str = ', inplace=True' if self.inplace else ''\n",
      "    58|         0|            0|            0|  0.00%|        return 'threshold={}, value={}{}'.format(\n",
      "    59|         0|            0|            0|  0.00%|            self.threshold, self.value, inplace_str\n",
      "    60|         0|            0|            0|  0.00%|        )\n",
      "    61|         0|            0|            0|  0.00%|\n",
      "    62|         0|            0|            0|  0.00%|\n",
      "    63|         0|            0|            0|  0.00%|class ReLU(Module):\n",
      "    64|         0|            0|            0|  0.00%|    r\"\"\"Applies the rectified linear unit function element-wise:\n",
      "    65|         0|            0|            0|  0.00%|\n",
      "    66|         0|            0|            0|  0.00%|    :math:`\\text{ReLU}(x) = (x)^+ = \\max(0, x)`\n",
      "    67|         0|            0|            0|  0.00%|\n",
      "    68|         0|            0|            0|  0.00%|    Args:\n",
      "    69|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``\n",
      "    70|         0|            0|            0|  0.00%|\n",
      "    71|         0|            0|            0|  0.00%|    Shape:\n",
      "    72|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "    73|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "    74|         0|            0|            0|  0.00%|\n",
      "    75|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/ReLU.png\n",
      "    76|         0|            0|            0|  0.00%|\n",
      "    77|         0|            0|            0|  0.00%|    Examples::\n",
      "    78|         0|            0|            0|  0.00%|\n",
      "    79|         0|            0|            0|  0.00%|        >>> m = nn.ReLU()\n",
      "    80|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "    81|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "    82|         0|            0|            0|  0.00%|\n",
      "    83|         0|            0|            0|  0.00%|\n",
      "    84|         0|            0|            0|  0.00%|      An implementation of CReLU - https://arxiv.org/abs/1603.05201\n",
      "    85|         0|            0|            0|  0.00%|\n",
      "    86|         0|            0|            0|  0.00%|        >>> m = nn.ReLU()\n",
      "    87|         0|            0|            0|  0.00%|        >>> input = torch.randn(2).unsqueeze(0)\n",
      "    88|         0|            0|            0|  0.00%|        >>> output = torch.cat((m(input),m(-input)))\n",
      "    89|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    90|         0|            0|            0|  0.00%|    __constants__ = ['inplace']\n",
      "    91|         0|            0|            0|  0.00%|    inplace: bool\n",
      "    92|         0|            0|            0|  0.00%|\n",
      "    93|         0|            0|            0|  0.00%|    def __init__(self, inplace: bool = False):\n",
      "    94|         0|            0|            0|  0.00%|        super(ReLU, self).__init__()\n",
      "    95|         0|            0|            0|  0.00%|        self.inplace = inplace\n",
      "    96|         0|            0|            0|  0.00%|\n",
      "    97|     63180|     0.113626|  1.79845e-06|  0.02%|    def forward(self, input: Tensor) -> Tensor:\n",
      "    98|     63180|     0.422064|  6.68034e-06|  0.07%|        return F.relu(input, inplace=self.inplace)\n",
      "(call)|     63180|      1.10859|  1.75465e-05|  0.18%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/functional.py:1446 relu\n",
      "    99|         0|            0|            0|  0.00%|\n",
      "   100|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   101|         0|            0|            0|  0.00%|        inplace_str = 'inplace=True' if self.inplace else ''\n",
      "   102|         0|            0|            0|  0.00%|        return inplace_str\n",
      "   103|         0|            0|            0|  0.00%|\n",
      "   104|         0|            0|            0|  0.00%|\n",
      "   105|         0|            0|            0|  0.00%|class RReLU(Module):\n",
      "   106|         0|            0|            0|  0.00%|    r\"\"\"Applies the randomized leaky rectified liner unit function, element-wise,\n",
      "   107|         0|            0|            0|  0.00%|    as described in the paper:\n",
      "   108|         0|            0|            0|  0.00%|\n",
      "   109|         0|            0|            0|  0.00%|    `Empirical Evaluation of Rectified Activations in Convolutional Network`_.\n",
      "   110|         0|            0|            0|  0.00%|\n",
      "   111|         0|            0|            0|  0.00%|    The function is defined as:\n",
      "   112|         0|            0|            0|  0.00%|\n",
      "   113|         0|            0|            0|  0.00%|    .. math::\n",
      "   114|         0|            0|            0|  0.00%|        \\text{RReLU}(x) =\n",
      "   115|         0|            0|            0|  0.00%|        \\begin{cases}\n",
      "   116|         0|            0|            0|  0.00%|            x & \\text{if } x \\geq 0 \\\\\n",
      "   117|         0|            0|            0|  0.00%|            ax & \\text{ otherwise }\n",
      "   118|         0|            0|            0|  0.00%|        \\end{cases}\n",
      "   119|         0|            0|            0|  0.00%|\n",
      "   120|         0|            0|            0|  0.00%|    where :math:`a` is randomly sampled from uniform distribution\n",
      "   121|         0|            0|            0|  0.00%|    :math:`\\mathcal{U}(\\text{lower}, \\text{upper})`.\n",
      "   122|         0|            0|            0|  0.00%|\n",
      "   123|         0|            0|            0|  0.00%|     See: https://arxiv.org/pdf/1505.00853.pdf\n",
      "   124|         0|            0|            0|  0.00%|\n",
      "   125|         0|            0|            0|  0.00%|    Args:\n",
      "   126|         0|            0|            0|  0.00%|        lower: lower bound of the uniform distribution. Default: :math:`\\frac{1}{8}`\n",
      "   127|         0|            0|            0|  0.00%|        upper: upper bound of the uniform distribution. Default: :math:`\\frac{1}{3}`\n",
      "   128|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``\n",
      "   129|         0|            0|            0|  0.00%|\n",
      "   130|         0|            0|            0|  0.00%|    Shape:\n",
      "   131|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   132|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   133|         0|            0|            0|  0.00%|\n",
      "   134|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/RReLU.png\n",
      "   135|         0|            0|            0|  0.00%|\n",
      "   136|         0|            0|            0|  0.00%|    Examples::\n",
      "   137|         0|            0|            0|  0.00%|\n",
      "   138|         0|            0|            0|  0.00%|        >>> m = nn.RReLU(0.1, 0.3)\n",
      "   139|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   140|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   141|         0|            0|            0|  0.00%|\n",
      "   142|         0|            0|            0|  0.00%|    .. _`Empirical Evaluation of Rectified Activations in Convolutional Network`:\n",
      "   143|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1505.00853\n",
      "   144|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   145|         0|            0|            0|  0.00%|    __constants__ = ['lower', 'upper', 'inplace']\n",
      "   146|         0|            0|            0|  0.00%|\n",
      "   147|         0|            0|            0|  0.00%|    lower: float\n",
      "   148|         0|            0|            0|  0.00%|    upper: float\n",
      "   149|         0|            0|            0|  0.00%|    inplace: bool\n",
      "   150|         0|            0|            0|  0.00%|\n",
      "   151|         0|            0|            0|  0.00%|    def __init__(\n",
      "   152|         0|            0|            0|  0.00%|        self,\n",
      "   153|         0|            0|            0|  0.00%|        lower: float = 1. / 8,\n",
      "   154|         0|            0|            0|  0.00%|        upper: float = 1. / 3,\n",
      "   155|         0|            0|            0|  0.00%|        inplace: bool = False\n",
      "   156|         0|            0|            0|  0.00%|    ):\n",
      "   157|         0|            0|            0|  0.00%|        super(RReLU, self).__init__()\n",
      "   158|         0|            0|            0|  0.00%|        self.lower = lower\n",
      "   159|         0|            0|            0|  0.00%|        self.upper = upper\n",
      "   160|         0|            0|            0|  0.00%|        self.inplace = inplace\n",
      "   161|         0|            0|            0|  0.00%|\n",
      "   162|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   163|         0|            0|            0|  0.00%|        return F.rrelu(input, self.lower, self.upper, self.training, self.inplace)\n",
      "   164|         0|            0|            0|  0.00%|\n",
      "   165|         0|            0|            0|  0.00%|    def extra_repr(self):\n",
      "   166|         0|            0|            0|  0.00%|        inplace_str = ', inplace=True' if self.inplace else ''\n",
      "   167|         0|            0|            0|  0.00%|        return 'lower={}, upper={}{}'.format(self.lower, self.upper, inplace_str)\n",
      "   168|         0|            0|            0|  0.00%|\n",
      "   169|         0|            0|            0|  0.00%|\n",
      "   170|         0|            0|            0|  0.00%|class Hardtanh(Module):\n",
      "   171|         0|            0|            0|  0.00%|    r\"\"\"Applies the HardTanh function element-wise.\n",
      "   172|         0|            0|            0|  0.00%|\n",
      "   173|         0|            0|            0|  0.00%|    HardTanh is defined as:\n",
      "   174|         0|            0|            0|  0.00%|\n",
      "   175|         0|            0|            0|  0.00%|    .. math::\n",
      "   176|         0|            0|            0|  0.00%|        \\text{HardTanh}(x) = \\begin{cases}\n",
      "   177|         0|            0|            0|  0.00%|            \\text{max\\_val} & \\text{ if } x > \\text{ max\\_val } \\\\\n",
      "   178|         0|            0|            0|  0.00%|            \\text{min\\_val} & \\text{ if } x < \\text{ min\\_val } \\\\\n",
      "   179|         0|            0|            0|  0.00%|            x & \\text{ otherwise } \\\\\n",
      "   180|         0|            0|            0|  0.00%|        \\end{cases}\n",
      "   181|         0|            0|            0|  0.00%|\n",
      "   182|         0|            0|            0|  0.00%|    Args:\n",
      "   183|         0|            0|            0|  0.00%|        min_val: minimum value of the linear region range. Default: -1\n",
      "   184|         0|            0|            0|  0.00%|        max_val: maximum value of the linear region range. Default: 1\n",
      "   185|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``\n",
      "   186|         0|            0|            0|  0.00%|\n",
      "   187|         0|            0|            0|  0.00%|    Keyword arguments :attr:`min_value` and :attr:`max_value`\n",
      "   188|         0|            0|            0|  0.00%|    have been deprecated in favor of :attr:`min_val` and :attr:`max_val`.\n",
      "   189|         0|            0|            0|  0.00%|\n",
      "   190|         0|            0|            0|  0.00%|    Shape:\n",
      "   191|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   192|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   193|         0|            0|            0|  0.00%|\n",
      "   194|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Hardtanh.png\n",
      "   195|         0|            0|            0|  0.00%|\n",
      "   196|         0|            0|            0|  0.00%|    Examples::\n",
      "   197|         0|            0|            0|  0.00%|\n",
      "   198|         0|            0|            0|  0.00%|        >>> m = nn.Hardtanh(-2, 2)\n",
      "   199|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   200|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   201|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   202|         0|            0|            0|  0.00%|    __constants__ = ['min_val', 'max_val', 'inplace']\n",
      "   203|         0|            0|            0|  0.00%|\n",
      "   204|         0|            0|            0|  0.00%|    min_val: float\n",
      "   205|         0|            0|            0|  0.00%|    max_val: float\n",
      "   206|         0|            0|            0|  0.00%|    inplace: bool\n",
      "   207|         0|            0|            0|  0.00%|\n",
      "   208|         0|            0|            0|  0.00%|    def __init__(\n",
      "   209|         0|            0|            0|  0.00%|        self,\n",
      "   210|         0|            0|            0|  0.00%|        min_val: float = -1.,\n",
      "   211|         0|            0|            0|  0.00%|        max_val: float = 1.,\n",
      "   212|         0|            0|            0|  0.00%|        inplace: bool = False,\n",
      "   213|         0|            0|            0|  0.00%|        min_value: Optional[float] = None,\n",
      "   214|         0|            0|            0|  0.00%|        max_value: Optional[float] = None\n",
      "   215|         0|            0|            0|  0.00%|    ) -> None:\n",
      "   216|         0|            0|            0|  0.00%|        super(Hardtanh, self).__init__()\n",
      "   217|         0|            0|            0|  0.00%|        if min_value is not None:\n",
      "   218|         0|            0|            0|  0.00%|            warnings.warn(\"keyword argument min_value is deprecated and rename to min_val\")\n",
      "   219|         0|            0|            0|  0.00%|            min_val = min_value\n",
      "   220|         0|            0|            0|  0.00%|        if max_value is not None:\n",
      "   221|         0|            0|            0|  0.00%|            warnings.warn(\"keyword argument max_value is deprecated and rename to max_val\")\n",
      "   222|         0|            0|            0|  0.00%|            max_val = max_value\n",
      "   223|         0|            0|            0|  0.00%|\n",
      "   224|         0|            0|            0|  0.00%|        self.min_val = min_val\n",
      "   225|         0|            0|            0|  0.00%|        self.max_val = max_val\n",
      "   226|         0|            0|            0|  0.00%|        self.inplace = inplace\n",
      "   227|         0|            0|            0|  0.00%|        assert self.max_val > self.min_val\n",
      "   228|         0|            0|            0|  0.00%|\n",
      "   229|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   230|         0|            0|            0|  0.00%|        return F.hardtanh(input, self.min_val, self.max_val, self.inplace)\n",
      "   231|         0|            0|            0|  0.00%|\n",
      "   232|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   233|         0|            0|            0|  0.00%|        inplace_str = ', inplace=True' if self.inplace else ''\n",
      "   234|         0|            0|            0|  0.00%|        return 'min_val={}, max_val={}{}'.format(\n",
      "   235|         0|            0|            0|  0.00%|            self.min_val, self.max_val, inplace_str\n",
      "   236|         0|            0|            0|  0.00%|        )\n",
      "   237|         0|            0|            0|  0.00%|\n",
      "   238|         0|            0|            0|  0.00%|\n",
      "   239|         0|            0|            0|  0.00%|class ReLU6(Hardtanh):\n",
      "   240|         0|            0|            0|  0.00%|    r\"\"\"Applies the element-wise function:\n",
      "   241|         0|            0|            0|  0.00%|\n",
      "   242|         0|            0|            0|  0.00%|    .. math::\n",
      "   243|         0|            0|            0|  0.00%|        \\text{ReLU6}(x) = \\min(\\max(0,x), 6)\n",
      "   244|         0|            0|            0|  0.00%|\n",
      "   245|         0|            0|            0|  0.00%|    Args:\n",
      "   246|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``\n",
      "   247|         0|            0|            0|  0.00%|\n",
      "   248|         0|            0|            0|  0.00%|    Shape:\n",
      "   249|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   250|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   251|         0|            0|            0|  0.00%|\n",
      "   252|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/ReLU6.png\n",
      "   253|         0|            0|            0|  0.00%|\n",
      "   254|         0|            0|            0|  0.00%|    Examples::\n",
      "   255|         0|            0|            0|  0.00%|\n",
      "   256|         0|            0|            0|  0.00%|        >>> m = nn.ReLU6()\n",
      "   257|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   258|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   259|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   260|         0|            0|            0|  0.00%|\n",
      "   261|         0|            0|            0|  0.00%|    def __init__(self, inplace: bool = False):\n",
      "   262|         0|            0|            0|  0.00%|        super(ReLU6, self).__init__(0., 6., inplace)\n",
      "   263|         0|            0|            0|  0.00%|\n",
      "   264|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   265|         0|            0|            0|  0.00%|        inplace_str = 'inplace=True' if self.inplace else ''\n",
      "   266|         0|            0|            0|  0.00%|        return inplace_str\n",
      "   267|         0|            0|            0|  0.00%|\n",
      "   268|         0|            0|            0|  0.00%|\n",
      "   269|         0|            0|            0|  0.00%|class Sigmoid(Module):\n",
      "   270|         0|            0|            0|  0.00%|    r\"\"\"Applies the element-wise function:\n",
      "   271|         0|            0|            0|  0.00%|\n",
      "   272|         0|            0|            0|  0.00%|    .. math::\n",
      "   273|         0|            0|            0|  0.00%|        \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n",
      "   274|         0|            0|            0|  0.00%|\n",
      "   275|         0|            0|            0|  0.00%|\n",
      "   276|         0|            0|            0|  0.00%|    Shape:\n",
      "   277|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   278|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   279|         0|            0|            0|  0.00%|\n",
      "   280|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Sigmoid.png\n",
      "   281|         0|            0|            0|  0.00%|\n",
      "   282|         0|            0|            0|  0.00%|    Examples::\n",
      "   283|         0|            0|            0|  0.00%|\n",
      "   284|         0|            0|            0|  0.00%|        >>> m = nn.Sigmoid()\n",
      "   285|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   286|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   287|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   288|         0|            0|            0|  0.00%|\n",
      "   289|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   290|         0|            0|            0|  0.00%|        return torch.sigmoid(input)\n",
      "   291|         0|            0|            0|  0.00%|\n",
      "   292|         0|            0|            0|  0.00%|\n",
      "   293|         0|            0|            0|  0.00%|class Hardsigmoid(Module):\n",
      "   294|         0|            0|            0|  0.00%|    r\"\"\"Applies the Hardsigmoid function element-wise.\n",
      "   295|         0|            0|            0|  0.00%|\n",
      "   296|         0|            0|            0|  0.00%|    Hardsigmoid is defined as:\n",
      "   297|         0|            0|            0|  0.00%|\n",
      "   298|         0|            0|            0|  0.00%|    .. math::\n",
      "   299|         0|            0|            0|  0.00%|        \\text{Hardsigmoid}(x) = \\begin{cases}\n",
      "   300|         0|            0|            0|  0.00%|            0 & \\text{if~} x \\le -3, \\\\\n",
      "   301|         0|            0|            0|  0.00%|            1 & \\text{if~} x \\ge +3, \\\\\n",
      "   302|         0|            0|            0|  0.00%|            x / 6 + 1 / 2 & \\text{otherwise}\n",
      "   303|         0|            0|            0|  0.00%|        \\end{cases}\n",
      "   304|         0|            0|            0|  0.00%|\n",
      "   305|         0|            0|            0|  0.00%|    Args:\n",
      "   306|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``\n",
      "   307|         0|            0|            0|  0.00%|\n",
      "   308|         0|            0|            0|  0.00%|    Shape:\n",
      "   309|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   310|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   311|         0|            0|            0|  0.00%|\n",
      "   312|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Hardsigmoid.png\n",
      "   313|         0|            0|            0|  0.00%|\n",
      "   314|         0|            0|            0|  0.00%|    Examples::\n",
      "   315|         0|            0|            0|  0.00%|\n",
      "   316|         0|            0|            0|  0.00%|        >>> m = nn.Hardsigmoid()\n",
      "   317|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   318|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   319|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   320|         0|            0|            0|  0.00%|    __constants__ = ['inplace']\n",
      "   321|         0|            0|            0|  0.00%|\n",
      "   322|         0|            0|            0|  0.00%|    inplace: bool\n",
      "   323|         0|            0|            0|  0.00%|\n",
      "   324|         0|            0|            0|  0.00%|    def __init__(self, inplace : bool = False) -> None:\n",
      "   325|         0|            0|            0|  0.00%|        super(Hardsigmoid, self).__init__()\n",
      "   326|         0|            0|            0|  0.00%|        self.inplace = inplace\n",
      "   327|         0|            0|            0|  0.00%|\n",
      "   328|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   329|         0|            0|            0|  0.00%|        return F.hardsigmoid(input, self.inplace)\n",
      "   330|         0|            0|            0|  0.00%|\n",
      "   331|         0|            0|            0|  0.00%|\n",
      "   332|         0|            0|            0|  0.00%|class Tanh(Module):\n",
      "   333|         0|            0|            0|  0.00%|    r\"\"\"Applies the Hyperbolic Tangent (Tanh) function element-wise.\n",
      "   334|         0|            0|            0|  0.00%|\n",
      "   335|         0|            0|            0|  0.00%|    Tanh is defined as:\n",
      "   336|         0|            0|            0|  0.00%|\n",
      "   337|         0|            0|            0|  0.00%|    .. math::\n",
      "   338|         0|            0|            0|  0.00%|        \\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)} {\\exp(x) + \\exp(-x)}\n",
      "   339|         0|            0|            0|  0.00%|\n",
      "   340|         0|            0|            0|  0.00%|    Shape:\n",
      "   341|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   342|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   343|         0|            0|            0|  0.00%|\n",
      "   344|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Tanh.png\n",
      "   345|         0|            0|            0|  0.00%|\n",
      "   346|         0|            0|            0|  0.00%|    Examples::\n",
      "   347|         0|            0|            0|  0.00%|\n",
      "   348|         0|            0|            0|  0.00%|        >>> m = nn.Tanh()\n",
      "   349|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   350|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   351|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   352|         0|            0|            0|  0.00%|\n",
      "   353|     63960|     0.113818|  1.77952e-06|  0.02%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   354|     63960|     0.596984|   9.3337e-06|  0.10%|        return torch.tanh(input)\n",
      "   355|         0|            0|            0|  0.00%|\n",
      "   356|         0|            0|            0|  0.00%|class SiLU(Module):\n",
      "   357|         0|            0|            0|  0.00%|    r\"\"\"Applies the Sigmoid Linear Unit (SiLU) function, element-wise.\n",
      "   358|         0|            0|            0|  0.00%|    The SiLU function is also known as the swish function.\n",
      "   359|         0|            0|            0|  0.00%|\n",
      "   360|         0|            0|            0|  0.00%|    .. math::\n",
      "   361|         0|            0|            0|  0.00%|        \\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.}\n",
      "   362|         0|            0|            0|  0.00%|\n",
      "   363|         0|            0|            0|  0.00%|    .. note::\n",
      "   364|         0|            0|            0|  0.00%|        See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_\n",
      "   365|         0|            0|            0|  0.00%|        where the SiLU (Sigmoid Linear Unit) was originally coined, and see\n",
      "   366|         0|            0|            0|  0.00%|        `Sigmoid-Weighted Linear Units for Neural Network Function Approximation\n",
      "   367|         0|            0|            0|  0.00%|        in Reinforcement Learning <https://arxiv.org/abs/1702.03118>`_ and `Swish:\n",
      "   368|         0|            0|            0|  0.00%|        a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941v1>`_\n",
      "   369|         0|            0|            0|  0.00%|        where the SiLU was experimented with later.\n",
      "   370|         0|            0|            0|  0.00%|\n",
      "   371|         0|            0|            0|  0.00%|    Shape:\n",
      "   372|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   373|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   374|         0|            0|            0|  0.00%|\n",
      "   375|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/SiLU.png\n",
      "   376|         0|            0|            0|  0.00%|\n",
      "   377|         0|            0|            0|  0.00%|    Examples::\n",
      "   378|         0|            0|            0|  0.00%|\n",
      "   379|         0|            0|            0|  0.00%|        >>> m = nn.SiLU()\n",
      "   380|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   381|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   382|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   383|         0|            0|            0|  0.00%|    __constants__ = ['inplace']\n",
      "   384|         0|            0|            0|  0.00%|    inplace: bool\n",
      "   385|         0|            0|            0|  0.00%|\n",
      "   386|         0|            0|            0|  0.00%|    def __init__(self, inplace: bool = False):\n",
      "   387|         0|            0|            0|  0.00%|        super(SiLU, self).__init__()\n",
      "   388|         0|            0|            0|  0.00%|        self.inplace = inplace\n",
      "   389|         0|            0|            0|  0.00%|\n",
      "   390|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   391|         0|            0|            0|  0.00%|        return F.silu(input, inplace=self.inplace)\n",
      "   392|         0|            0|            0|  0.00%|\n",
      "   393|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   394|         0|            0|            0|  0.00%|        inplace_str = 'inplace=True' if self.inplace else ''\n",
      "   395|         0|            0|            0|  0.00%|        return inplace_str\n",
      "   396|         0|            0|            0|  0.00%|\n",
      "   397|         0|            0|            0|  0.00%|class Mish(Module):\n",
      "   398|         0|            0|            0|  0.00%|    r\"\"\"Applies the Mish function, element-wise.\n",
      "   399|         0|            0|            0|  0.00%|    Mish: A Self Regularized Non-Monotonic Neural Activation Function.\n",
      "   400|         0|            0|            0|  0.00%|\n",
      "   401|         0|            0|            0|  0.00%|    .. math::\n",
      "   402|         0|            0|            0|  0.00%|        \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))\n",
      "   403|         0|            0|            0|  0.00%|\n",
      "   404|         0|            0|            0|  0.00%|    .. note::\n",
      "   405|         0|            0|            0|  0.00%|        See `Mish: A Self Regularized Non-Monotonic Neural Activation Function <https://arxiv.org/abs/1908.08681>`_\n",
      "   406|         0|            0|            0|  0.00%|\n",
      "   407|         0|            0|            0|  0.00%|    Shape:\n",
      "   408|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   409|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   410|         0|            0|            0|  0.00%|\n",
      "   411|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Mish.png\n",
      "   412|         0|            0|            0|  0.00%|\n",
      "   413|         0|            0|            0|  0.00%|    Examples::\n",
      "   414|         0|            0|            0|  0.00%|\n",
      "   415|         0|            0|            0|  0.00%|        >>> m = nn.Mish()\n",
      "   416|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   417|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   418|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   419|         0|            0|            0|  0.00%|    __constants__ = ['inplace']\n",
      "   420|         0|            0|            0|  0.00%|    inplace: bool\n",
      "   421|         0|            0|            0|  0.00%|\n",
      "   422|         0|            0|            0|  0.00%|    def __init__(self, inplace: bool = False):\n",
      "   423|         0|            0|            0|  0.00%|        super(Mish, self).__init__()\n",
      "   424|         0|            0|            0|  0.00%|        self.inplace = inplace\n",
      "   425|         0|            0|            0|  0.00%|\n",
      "   426|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   427|         0|            0|            0|  0.00%|        return F.mish(input, inplace=self.inplace)\n",
      "   428|         0|            0|            0|  0.00%|\n",
      "   429|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   430|         0|            0|            0|  0.00%|        inplace_str = 'inplace=True' if self.inplace else ''\n",
      "   431|         0|            0|            0|  0.00%|        return inplace_str\n",
      "   432|         0|            0|            0|  0.00%|\n",
      "   433|         0|            0|            0|  0.00%|class Hardswish(Module):\n",
      "   434|         0|            0|            0|  0.00%|    r\"\"\"Applies the hardswish function, element-wise, as described in the paper:\n",
      "   435|         0|            0|            0|  0.00%|\n",
      "   436|         0|            0|            0|  0.00%|    `Searching for MobileNetV3`_.\n",
      "   437|         0|            0|            0|  0.00%|\n",
      "   438|         0|            0|            0|  0.00%|    .. math::\n",
      "   439|         0|            0|            0|  0.00%|        \\text{Hardswish}(x) = \\begin{cases}\n",
      "   440|         0|            0|            0|  0.00%|            0 & \\text{if~} x \\le -3, \\\\\n",
      "   441|         0|            0|            0|  0.00%|            x & \\text{if~} x \\ge +3, \\\\\n",
      "   442|         0|            0|            0|  0.00%|            x \\cdot (x + 3) /6 & \\text{otherwise}\n",
      "   443|         0|            0|            0|  0.00%|        \\end{cases}\n",
      "   444|         0|            0|            0|  0.00%|\n",
      "   445|         0|            0|            0|  0.00%|    Args:\n",
      "   446|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``\n",
      "   447|         0|            0|            0|  0.00%|\n",
      "   448|         0|            0|            0|  0.00%|    Shape:\n",
      "   449|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   450|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   451|         0|            0|            0|  0.00%|\n",
      "   452|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Hardswish.png\n",
      "   453|         0|            0|            0|  0.00%|\n",
      "   454|         0|            0|            0|  0.00%|    Examples::\n",
      "   455|         0|            0|            0|  0.00%|\n",
      "   456|         0|            0|            0|  0.00%|        >>> m = nn.Hardswish()\n",
      "   457|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   458|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   459|         0|            0|            0|  0.00%|\n",
      "   460|         0|            0|            0|  0.00%|    .. _`Searching for MobileNetV3`:\n",
      "   461|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1905.02244\n",
      "   462|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   463|         0|            0|            0|  0.00%|    __constants__ = ['inplace']\n",
      "   464|         0|            0|            0|  0.00%|\n",
      "   465|         0|            0|            0|  0.00%|    inplace: bool\n",
      "   466|         0|            0|            0|  0.00%|\n",
      "   467|         0|            0|            0|  0.00%|    def __init__(self, inplace : bool = False) -> None:\n",
      "   468|         0|            0|            0|  0.00%|        super(Hardswish, self).__init__()\n",
      "   469|         0|            0|            0|  0.00%|        self.inplace = inplace\n",
      "   470|         0|            0|            0|  0.00%|\n",
      "   471|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   472|         0|            0|            0|  0.00%|        return F.hardswish(input, self.inplace)\n",
      "   473|         0|            0|            0|  0.00%|\n",
      "   474|         0|            0|            0|  0.00%|\n",
      "   475|         0|            0|            0|  0.00%|class ELU(Module):\n",
      "   476|         0|            0|            0|  0.00%|    r\"\"\"Applies the Exponential Linear Unit (ELU) function, element-wise, as described\n",
      "   477|         0|            0|            0|  0.00%|    in the paper: `Fast and Accurate Deep Network Learning by Exponential Linear\n",
      "   478|         0|            0|            0|  0.00%|    Units (ELUs) <https://arxiv.org/abs/1511.07289>`__.\n",
      "   479|         0|            0|            0|  0.00%|\n",
      "   480|         0|            0|            0|  0.00%|    ELU is defined as:\n",
      "   481|         0|            0|            0|  0.00%|\n",
      "   482|         0|            0|            0|  0.00%|    .. math::\n",
      "   483|         0|            0|            0|  0.00%|        \\text{ELU}(x) = \\begin{cases}\n",
      "   484|         0|            0|            0|  0.00%|        x, & \\text{ if } x > 0\\\\\n",
      "   485|         0|            0|            0|  0.00%|        \\alpha * (\\exp(x) - 1), & \\text{ if } x \\leq 0\n",
      "   486|         0|            0|            0|  0.00%|        \\end{cases}\n",
      "   487|         0|            0|            0|  0.00%|\n",
      "   488|         0|            0|            0|  0.00%|    Args:\n",
      "   489|         0|            0|            0|  0.00%|        alpha: the :math:`\\alpha` value for the ELU formulation. Default: 1.0\n",
      "   490|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``\n",
      "   491|         0|            0|            0|  0.00%|\n",
      "   492|         0|            0|            0|  0.00%|    Shape:\n",
      "   493|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   494|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   495|         0|            0|            0|  0.00%|\n",
      "   496|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/ELU.png\n",
      "   497|         0|            0|            0|  0.00%|\n",
      "   498|         0|            0|            0|  0.00%|    Examples::\n",
      "   499|         0|            0|            0|  0.00%|\n",
      "   500|         0|            0|            0|  0.00%|        >>> m = nn.ELU()\n",
      "   501|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   502|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   503|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   504|         0|            0|            0|  0.00%|    __constants__ = ['alpha', 'inplace']\n",
      "   505|         0|            0|            0|  0.00%|    alpha: float\n",
      "   506|         0|            0|            0|  0.00%|    inplace: bool\n",
      "   507|         0|            0|            0|  0.00%|\n",
      "   508|         0|            0|            0|  0.00%|    def __init__(self, alpha: float = 1., inplace: bool = False) -> None:\n",
      "   509|         0|            0|            0|  0.00%|        super(ELU, self).__init__()\n",
      "   510|         0|            0|            0|  0.00%|        self.alpha = alpha\n",
      "   511|         0|            0|            0|  0.00%|        self.inplace = inplace\n",
      "   512|         0|            0|            0|  0.00%|\n",
      "   513|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   514|         0|            0|            0|  0.00%|        return F.elu(input, self.alpha, self.inplace)\n",
      "   515|         0|            0|            0|  0.00%|\n",
      "   516|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   517|         0|            0|            0|  0.00%|        inplace_str = ', inplace=True' if self.inplace else ''\n",
      "   518|         0|            0|            0|  0.00%|        return 'alpha={}{}'.format(self.alpha, inplace_str)\n",
      "   519|         0|            0|            0|  0.00%|\n",
      "   520|         0|            0|            0|  0.00%|\n",
      "   521|         0|            0|            0|  0.00%|class CELU(Module):\n",
      "   522|         0|            0|            0|  0.00%|    r\"\"\"Applies the element-wise function:\n",
      "   523|         0|            0|            0|  0.00%|\n",
      "   524|         0|            0|            0|  0.00%|    .. math::\n",
      "   525|         0|            0|            0|  0.00%|        \\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))\n",
      "   526|         0|            0|            0|  0.00%|\n",
      "   527|         0|            0|            0|  0.00%|    More details can be found in the paper `Continuously Differentiable Exponential Linear Units`_ .\n",
      "   528|         0|            0|            0|  0.00%|\n",
      "   529|         0|            0|            0|  0.00%|    Args:\n",
      "   530|         0|            0|            0|  0.00%|        alpha: the :math:`\\alpha` value for the CELU formulation. Default: 1.0\n",
      "   531|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``\n",
      "   532|         0|            0|            0|  0.00%|\n",
      "   533|         0|            0|            0|  0.00%|    Shape:\n",
      "   534|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   535|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   536|         0|            0|            0|  0.00%|\n",
      "   537|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/CELU.png\n",
      "   538|         0|            0|            0|  0.00%|\n",
      "   539|         0|            0|            0|  0.00%|    Examples::\n",
      "   540|         0|            0|            0|  0.00%|\n",
      "   541|         0|            0|            0|  0.00%|        >>> m = nn.CELU()\n",
      "   542|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   543|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   544|         0|            0|            0|  0.00%|\n",
      "   545|         0|            0|            0|  0.00%|    .. _`Continuously Differentiable Exponential Linear Units`:\n",
      "   546|         0|            0|            0|  0.00%|        https://arxiv.org/abs/1704.07483\n",
      "   547|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   548|         0|            0|            0|  0.00%|    __constants__ = ['alpha', 'inplace']\n",
      "   549|         0|            0|            0|  0.00%|    alpha: float\n",
      "   550|         0|            0|            0|  0.00%|    inplace: bool\n",
      "   551|         0|            0|            0|  0.00%|\n",
      "   552|         0|            0|            0|  0.00%|    def __init__(self, alpha: float = 1., inplace: bool = False) -> None:\n",
      "   553|         0|            0|            0|  0.00%|        super(CELU, self).__init__()\n",
      "   554|         0|            0|            0|  0.00%|        self.alpha = alpha\n",
      "   555|         0|            0|            0|  0.00%|        self.inplace = inplace\n",
      "   556|         0|            0|            0|  0.00%|\n",
      "   557|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   558|         0|            0|            0|  0.00%|        return F.celu(input, self.alpha, self.inplace)\n",
      "   559|         0|            0|            0|  0.00%|\n",
      "   560|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   561|         0|            0|            0|  0.00%|        inplace_str = ', inplace=True' if self.inplace else ''\n",
      "   562|         0|            0|            0|  0.00%|        return 'alpha={}{}'.format(self.alpha, inplace_str)\n",
      "   563|         0|            0|            0|  0.00%|\n",
      "   564|         0|            0|            0|  0.00%|\n",
      "   565|         0|            0|            0|  0.00%|class SELU(Module):\n",
      "   566|         0|            0|            0|  0.00%|    r\"\"\"Applied element-wise, as:\n",
      "   567|         0|            0|            0|  0.00%|\n",
      "   568|         0|            0|            0|  0.00%|    .. math::\n",
      "   569|         0|            0|            0|  0.00%|        \\text{SELU}(x) = \\text{scale} * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))\n",
      "   570|         0|            0|            0|  0.00%|\n",
      "   571|         0|            0|            0|  0.00%|    with :math:`\\alpha = 1.6732632423543772848170429916717` and\n",
      "   572|         0|            0|            0|  0.00%|    :math:`\\text{scale} = 1.0507009873554804934193349852946`.\n",
      "   573|         0|            0|            0|  0.00%|\n",
      "   574|         0|            0|            0|  0.00%|    .. warning::\n",
      "   575|         0|            0|            0|  0.00%|        When using ``kaiming_normal`` or ``kaiming_normal_`` for initialisation,\n",
      "   576|         0|            0|            0|  0.00%|        ``nonlinearity='linear'`` should be used instead of ``nonlinearity='selu'``\n",
      "   577|         0|            0|            0|  0.00%|        in order to get `Self-Normalizing Neural Networks`_.\n",
      "   578|         0|            0|            0|  0.00%|        See :func:`torch.nn.init.calculate_gain` for more information.\n",
      "   579|         0|            0|            0|  0.00%|\n",
      "   580|         0|            0|            0|  0.00%|    More details can be found in the paper `Self-Normalizing Neural Networks`_ .\n",
      "   581|         0|            0|            0|  0.00%|\n",
      "   582|         0|            0|            0|  0.00%|    Args:\n",
      "   583|         0|            0|            0|  0.00%|        inplace (bool, optional): can optionally do the operation in-place. Default: ``False``\n",
      "   584|         0|            0|            0|  0.00%|\n",
      "   585|         0|            0|            0|  0.00%|    Shape:\n",
      "   586|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   587|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   588|         0|            0|            0|  0.00%|\n",
      "   589|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/SELU.png\n",
      "   590|         0|            0|            0|  0.00%|\n",
      "   591|         0|            0|            0|  0.00%|    Examples::\n",
      "   592|         0|            0|            0|  0.00%|\n",
      "   593|         0|            0|            0|  0.00%|        >>> m = nn.SELU()\n",
      "   594|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   595|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   596|         0|            0|            0|  0.00%|\n",
      "   597|         0|            0|            0|  0.00%|    .. _Self-Normalizing Neural Networks: https://arxiv.org/abs/1706.02515\n",
      "   598|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   599|         0|            0|            0|  0.00%|    __constants__ = ['inplace']\n",
      "   600|         0|            0|            0|  0.00%|    inplace: bool\n",
      "   601|         0|            0|            0|  0.00%|\n",
      "   602|         0|            0|            0|  0.00%|    def __init__(self, inplace: bool = False) -> None:\n",
      "   603|         0|            0|            0|  0.00%|        super(SELU, self).__init__()\n",
      "   604|         0|            0|            0|  0.00%|        self.inplace = inplace\n",
      "   605|         0|            0|            0|  0.00%|\n",
      "   606|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   607|         0|            0|            0|  0.00%|        return F.selu(input, self.inplace)\n",
      "   608|         0|            0|            0|  0.00%|\n",
      "   609|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   610|         0|            0|            0|  0.00%|        inplace_str = 'inplace=True' if self.inplace else ''\n",
      "   611|         0|            0|            0|  0.00%|        return inplace_str\n",
      "   612|         0|            0|            0|  0.00%|\n",
      "   613|         0|            0|            0|  0.00%|\n",
      "   614|         0|            0|            0|  0.00%|class GLU(Module):\n",
      "   615|         0|            0|            0|  0.00%|    r\"\"\"Applies the gated linear unit function\n",
      "   616|         0|            0|            0|  0.00%|    :math:`{GLU}(a, b)= a \\otimes \\sigma(b)` where :math:`a` is the first half\n",
      "   617|         0|            0|            0|  0.00%|    of the input matrices and :math:`b` is the second half.\n",
      "   618|         0|            0|            0|  0.00%|\n",
      "   619|         0|            0|            0|  0.00%|    Args:\n",
      "   620|         0|            0|            0|  0.00%|        dim (int): the dimension on which to split the input. Default: -1\n",
      "   621|         0|            0|            0|  0.00%|\n",
      "   622|         0|            0|            0|  0.00%|    Shape:\n",
      "   623|         0|            0|            0|  0.00%|        - Input: :math:`(\\ast_1, N, \\ast_2)` where `*` means, any number of additional\n",
      "   624|         0|            0|            0|  0.00%|          dimensions\n",
      "   625|         0|            0|            0|  0.00%|        - Output: :math:`(\\ast_1, M, \\ast_2)` where :math:`M=N/2`\n",
      "   626|         0|            0|            0|  0.00%|\n",
      "   627|         0|            0|            0|  0.00%|    Examples::\n",
      "   628|         0|            0|            0|  0.00%|\n",
      "   629|         0|            0|            0|  0.00%|        >>> m = nn.GLU()\n",
      "   630|         0|            0|            0|  0.00%|        >>> input = torch.randn(4, 2)\n",
      "   631|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   632|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   633|         0|            0|            0|  0.00%|    __constants__ = ['dim']\n",
      "   634|         0|            0|            0|  0.00%|    dim: int\n",
      "   635|         0|            0|            0|  0.00%|\n",
      "   636|         0|            0|            0|  0.00%|    def __init__(self, dim: int = -1) -> None:\n",
      "   637|         0|            0|            0|  0.00%|        super(GLU, self).__init__()\n",
      "   638|         0|            0|            0|  0.00%|        self.dim = dim\n",
      "   639|         0|            0|            0|  0.00%|\n",
      "   640|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   641|         0|            0|            0|  0.00%|        return F.glu(input, self.dim)\n",
      "   642|         0|            0|            0|  0.00%|\n",
      "   643|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   644|         0|            0|            0|  0.00%|        return 'dim={}'.format(self.dim)\n",
      "   645|         0|            0|            0|  0.00%|\n",
      "   646|         0|            0|            0|  0.00%|\n",
      "   647|         0|            0|            0|  0.00%|class GELU(Module):\n",
      "   648|         0|            0|            0|  0.00%|    r\"\"\"Applies the Gaussian Error Linear Units function:\n",
      "   649|         0|            0|            0|  0.00%|\n",
      "   650|         0|            0|            0|  0.00%|    .. math:: \\text{GELU}(x) = x * \\Phi(x)\n",
      "   651|         0|            0|            0|  0.00%|\n",
      "   652|         0|            0|            0|  0.00%|    where :math:`\\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.\n",
      "   653|         0|            0|            0|  0.00%|\n",
      "   654|         0|            0|            0|  0.00%|    When the approximate argument is 'tanh', Gelu is estimated with:\n",
      "   655|         0|            0|            0|  0.00%|        :math:: \\text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt(2 / \\pi) * (x + 0.044715 * x^3)))\n",
      "   656|         0|            0|            0|  0.00%|\n",
      "   657|         0|            0|            0|  0.00%|    Args:\n",
      "   658|         0|            0|            0|  0.00%|        approximate (string, optional): the gelu approximation algorithm to use:\n",
      "   659|         0|            0|            0|  0.00%|            ``'none'`` | ``'tanh'``. Default: ``'none'``\n",
      "   660|         0|            0|            0|  0.00%|\n",
      "   661|         0|            0|            0|  0.00%|    Shape:\n",
      "   662|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   663|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   664|         0|            0|            0|  0.00%|\n",
      "   665|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/GELU.png\n",
      "   666|         0|            0|            0|  0.00%|\n",
      "   667|         0|            0|            0|  0.00%|    Examples::\n",
      "   668|         0|            0|            0|  0.00%|\n",
      "   669|         0|            0|            0|  0.00%|        >>> m = nn.GELU()\n",
      "   670|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   671|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   672|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   673|         0|            0|            0|  0.00%|    __constants__ = ['approximate']\n",
      "   674|         0|            0|            0|  0.00%|    approximate: str\n",
      "   675|         0|            0|            0|  0.00%|\n",
      "   676|         0|            0|            0|  0.00%|    def __init__(self, approximate: str = 'none') -> None:\n",
      "   677|         0|            0|            0|  0.00%|        super(GELU, self).__init__()\n",
      "   678|         0|            0|            0|  0.00%|        self.approximate = approximate\n",
      "   679|         0|            0|            0|  0.00%|\n",
      "   680|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   681|         0|            0|            0|  0.00%|        return F.gelu(input, approximate=self.approximate)\n",
      "   682|         0|            0|            0|  0.00%|\n",
      "   683|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   684|         0|            0|            0|  0.00%|        return 'approximate={}'.format(self.approximate)\n",
      "   685|         0|            0|            0|  0.00%|\n",
      "   686|         0|            0|            0|  0.00%|\n",
      "   687|         0|            0|            0|  0.00%|class Hardshrink(Module):\n",
      "   688|         0|            0|            0|  0.00%|    r\"\"\"Applies the Hard Shrinkage (Hardshrink) function element-wise.\n",
      "   689|         0|            0|            0|  0.00%|\n",
      "   690|         0|            0|            0|  0.00%|    Hardshrink is defined as:\n",
      "   691|         0|            0|            0|  0.00%|\n",
      "   692|         0|            0|            0|  0.00%|    .. math::\n",
      "   693|         0|            0|            0|  0.00%|        \\text{HardShrink}(x) =\n",
      "   694|         0|            0|            0|  0.00%|        \\begin{cases}\n",
      "   695|         0|            0|            0|  0.00%|        x, & \\text{ if } x > \\lambda \\\\\n",
      "   696|         0|            0|            0|  0.00%|        x, & \\text{ if } x < -\\lambda \\\\\n",
      "   697|         0|            0|            0|  0.00%|        0, & \\text{ otherwise }\n",
      "   698|         0|            0|            0|  0.00%|        \\end{cases}\n",
      "   699|         0|            0|            0|  0.00%|\n",
      "   700|         0|            0|            0|  0.00%|    Args:\n",
      "   701|         0|            0|            0|  0.00%|        lambd: the :math:`\\lambda` value for the Hardshrink formulation. Default: 0.5\n",
      "   702|         0|            0|            0|  0.00%|\n",
      "   703|         0|            0|            0|  0.00%|    Shape:\n",
      "   704|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   705|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   706|         0|            0|            0|  0.00%|\n",
      "   707|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Hardshrink.png\n",
      "   708|         0|            0|            0|  0.00%|\n",
      "   709|         0|            0|            0|  0.00%|    Examples::\n",
      "   710|         0|            0|            0|  0.00%|\n",
      "   711|         0|            0|            0|  0.00%|        >>> m = nn.Hardshrink()\n",
      "   712|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   713|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   714|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   715|         0|            0|            0|  0.00%|    __constants__ = ['lambd']\n",
      "   716|         0|            0|            0|  0.00%|    lambd: float\n",
      "   717|         0|            0|            0|  0.00%|\n",
      "   718|         0|            0|            0|  0.00%|    def __init__(self, lambd: float = 0.5) -> None:\n",
      "   719|         0|            0|            0|  0.00%|        super(Hardshrink, self).__init__()\n",
      "   720|         0|            0|            0|  0.00%|        self.lambd = lambd\n",
      "   721|         0|            0|            0|  0.00%|\n",
      "   722|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   723|         0|            0|            0|  0.00%|        return F.hardshrink(input, self.lambd)\n",
      "   724|         0|            0|            0|  0.00%|\n",
      "   725|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   726|         0|            0|            0|  0.00%|        return '{}'.format(self.lambd)\n",
      "   727|         0|            0|            0|  0.00%|\n",
      "   728|         0|            0|            0|  0.00%|\n",
      "   729|         0|            0|            0|  0.00%|class LeakyReLU(Module):\n",
      "   730|         0|            0|            0|  0.00%|    r\"\"\"Applies the element-wise function:\n",
      "   731|         0|            0|            0|  0.00%|\n",
      "   732|         0|            0|            0|  0.00%|    .. math::\n",
      "   733|         0|            0|            0|  0.00%|        \\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)\n",
      "   734|         0|            0|            0|  0.00%|\n",
      "   735|         0|            0|            0|  0.00%|\n",
      "   736|         0|            0|            0|  0.00%|    or\n",
      "   737|         0|            0|            0|  0.00%|\n",
      "   738|         0|            0|            0|  0.00%|    .. math::\n",
      "   739|         0|            0|            0|  0.00%|        \\text{LeakyRELU}(x) =\n",
      "   740|         0|            0|            0|  0.00%|        \\begin{cases}\n",
      "   741|         0|            0|            0|  0.00%|        x, & \\text{ if } x \\geq 0 \\\\\n",
      "   742|         0|            0|            0|  0.00%|        \\text{negative\\_slope} \\times x, & \\text{ otherwise }\n",
      "   743|         0|            0|            0|  0.00%|        \\end{cases}\n",
      "   744|         0|            0|            0|  0.00%|\n",
      "   745|         0|            0|            0|  0.00%|    Args:\n",
      "   746|         0|            0|            0|  0.00%|        negative_slope: Controls the angle of the negative slope. Default: 1e-2\n",
      "   747|         0|            0|            0|  0.00%|        inplace: can optionally do the operation in-place. Default: ``False``\n",
      "   748|         0|            0|            0|  0.00%|\n",
      "   749|         0|            0|            0|  0.00%|    Shape:\n",
      "   750|         0|            0|            0|  0.00%|        - Input: :math:`(*)` where `*` means, any number of additional\n",
      "   751|         0|            0|            0|  0.00%|          dimensions\n",
      "   752|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input\n",
      "   753|         0|            0|            0|  0.00%|\n",
      "   754|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/LeakyReLU.png\n",
      "   755|         0|            0|            0|  0.00%|\n",
      "   756|         0|            0|            0|  0.00%|    Examples::\n",
      "   757|         0|            0|            0|  0.00%|\n",
      "   758|         0|            0|            0|  0.00%|        >>> m = nn.LeakyReLU(0.1)\n",
      "   759|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   760|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   761|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   762|         0|            0|            0|  0.00%|    __constants__ = ['inplace', 'negative_slope']\n",
      "   763|         0|            0|            0|  0.00%|    inplace: bool\n",
      "   764|         0|            0|            0|  0.00%|    negative_slope: float\n",
      "   765|         0|            0|            0|  0.00%|\n",
      "   766|         0|            0|            0|  0.00%|    def __init__(self, negative_slope: float = 1e-2, inplace: bool = False) -> None:\n",
      "   767|         0|            0|            0|  0.00%|        super(LeakyReLU, self).__init__()\n",
      "   768|         0|            0|            0|  0.00%|        self.negative_slope = negative_slope\n",
      "   769|         0|            0|            0|  0.00%|        self.inplace = inplace\n",
      "   770|         0|            0|            0|  0.00%|\n",
      "   771|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   772|         0|            0|            0|  0.00%|        return F.leaky_relu(input, self.negative_slope, self.inplace)\n",
      "   773|         0|            0|            0|  0.00%|\n",
      "   774|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   775|         0|            0|            0|  0.00%|        inplace_str = ', inplace=True' if self.inplace else ''\n",
      "   776|         0|            0|            0|  0.00%|        return 'negative_slope={}{}'.format(self.negative_slope, inplace_str)\n",
      "   777|         0|            0|            0|  0.00%|\n",
      "   778|         0|            0|            0|  0.00%|\n",
      "   779|         0|            0|            0|  0.00%|class LogSigmoid(Module):\n",
      "   780|         0|            0|            0|  0.00%|    r\"\"\"Applies the element-wise function:\n",
      "   781|         0|            0|            0|  0.00%|\n",
      "   782|         0|            0|            0|  0.00%|    .. math::\n",
      "   783|         0|            0|            0|  0.00%|        \\text{LogSigmoid}(x) = \\log\\left(\\frac{ 1 }{ 1 + \\exp(-x)}\\right)\n",
      "   784|         0|            0|            0|  0.00%|\n",
      "   785|         0|            0|            0|  0.00%|    Shape:\n",
      "   786|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   787|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   788|         0|            0|            0|  0.00%|\n",
      "   789|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/LogSigmoid.png\n",
      "   790|         0|            0|            0|  0.00%|\n",
      "   791|         0|            0|            0|  0.00%|    Examples::\n",
      "   792|         0|            0|            0|  0.00%|\n",
      "   793|         0|            0|            0|  0.00%|        >>> m = nn.LogSigmoid()\n",
      "   794|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   795|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   796|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   797|         0|            0|            0|  0.00%|\n",
      "   798|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   799|         0|            0|            0|  0.00%|        return F.logsigmoid(input)\n",
      "   800|         0|            0|            0|  0.00%|\n",
      "   801|         0|            0|            0|  0.00%|\n",
      "   802|         0|            0|            0|  0.00%|class Softplus(Module):\n",
      "   803|         0|            0|            0|  0.00%|    r\"\"\"Applies the Softplus function :math:`\\text{Softplus}(x) = \\frac{1}{\\beta} *\n",
      "   804|         0|            0|            0|  0.00%|    \\log(1 + \\exp(\\beta * x))` element-wise.\n",
      "   805|         0|            0|            0|  0.00%|\n",
      "   806|         0|            0|            0|  0.00%|    SoftPlus is a smooth approximation to the ReLU function and can be used\n",
      "   807|         0|            0|            0|  0.00%|    to constrain the output of a machine to always be positive.\n",
      "   808|         0|            0|            0|  0.00%|\n",
      "   809|         0|            0|            0|  0.00%|    For numerical stability the implementation reverts to the linear function\n",
      "   810|         0|            0|            0|  0.00%|    when :math:`input \\times \\beta > threshold`.\n",
      "   811|         0|            0|            0|  0.00%|\n",
      "   812|         0|            0|            0|  0.00%|    Args:\n",
      "   813|         0|            0|            0|  0.00%|        beta: the :math:`\\beta` value for the Softplus formulation. Default: 1\n",
      "   814|         0|            0|            0|  0.00%|        threshold: values above this revert to a linear function. Default: 20\n",
      "   815|         0|            0|            0|  0.00%|\n",
      "   816|         0|            0|            0|  0.00%|    Shape:\n",
      "   817|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   818|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   819|         0|            0|            0|  0.00%|\n",
      "   820|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Softplus.png\n",
      "   821|         0|            0|            0|  0.00%|\n",
      "   822|         0|            0|            0|  0.00%|    Examples::\n",
      "   823|         0|            0|            0|  0.00%|\n",
      "   824|         0|            0|            0|  0.00%|        >>> m = nn.Softplus()\n",
      "   825|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   826|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   827|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   828|         0|            0|            0|  0.00%|    __constants__ = ['beta', 'threshold']\n",
      "   829|         0|            0|            0|  0.00%|    beta: int\n",
      "   830|         0|            0|            0|  0.00%|    threshold: int\n",
      "   831|         0|            0|            0|  0.00%|\n",
      "   832|         0|            0|            0|  0.00%|    def __init__(self, beta: int = 1, threshold: int = 20) -> None:\n",
      "   833|         0|            0|            0|  0.00%|        super(Softplus, self).__init__()\n",
      "   834|         0|            0|            0|  0.00%|        self.beta = beta\n",
      "   835|         0|            0|            0|  0.00%|        self.threshold = threshold\n",
      "   836|         0|            0|            0|  0.00%|\n",
      "   837|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   838|         0|            0|            0|  0.00%|        return F.softplus(input, self.beta, self.threshold)\n",
      "   839|         0|            0|            0|  0.00%|\n",
      "   840|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   841|         0|            0|            0|  0.00%|        return 'beta={}, threshold={}'.format(self.beta, self.threshold)\n",
      "   842|         0|            0|            0|  0.00%|\n",
      "   843|         0|            0|            0|  0.00%|\n",
      "   844|         0|            0|            0|  0.00%|class Softshrink(Module):\n",
      "   845|         0|            0|            0|  0.00%|    r\"\"\"Applies the soft shrinkage function elementwise:\n",
      "   846|         0|            0|            0|  0.00%|\n",
      "   847|         0|            0|            0|  0.00%|    .. math::\n",
      "   848|         0|            0|            0|  0.00%|        \\text{SoftShrinkage}(x) =\n",
      "   849|         0|            0|            0|  0.00%|        \\begin{cases}\n",
      "   850|         0|            0|            0|  0.00%|        x - \\lambda, & \\text{ if } x > \\lambda \\\\\n",
      "   851|         0|            0|            0|  0.00%|        x + \\lambda, & \\text{ if } x < -\\lambda \\\\\n",
      "   852|         0|            0|            0|  0.00%|        0, & \\text{ otherwise }\n",
      "   853|         0|            0|            0|  0.00%|        \\end{cases}\n",
      "   854|         0|            0|            0|  0.00%|\n",
      "   855|         0|            0|            0|  0.00%|    Args:\n",
      "   856|         0|            0|            0|  0.00%|        lambd: the :math:`\\lambda` (must be no less than zero) value for the Softshrink formulation. Default: 0.5\n",
      "   857|         0|            0|            0|  0.00%|\n",
      "   858|         0|            0|            0|  0.00%|    Shape:\n",
      "   859|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "   860|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "   861|         0|            0|            0|  0.00%|\n",
      "   862|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Softshrink.png\n",
      "   863|         0|            0|            0|  0.00%|\n",
      "   864|         0|            0|            0|  0.00%|    Examples::\n",
      "   865|         0|            0|            0|  0.00%|\n",
      "   866|         0|            0|            0|  0.00%|        >>> m = nn.Softshrink()\n",
      "   867|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "   868|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "   869|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   870|         0|            0|            0|  0.00%|    __constants__ = ['lambd']\n",
      "   871|         0|            0|            0|  0.00%|    lambd: float\n",
      "   872|         0|            0|            0|  0.00%|\n",
      "   873|         0|            0|            0|  0.00%|    def __init__(self, lambd: float = 0.5) -> None:\n",
      "   874|         0|            0|            0|  0.00%|        super(Softshrink, self).__init__()\n",
      "   875|         0|            0|            0|  0.00%|        self.lambd = lambd\n",
      "   876|         0|            0|            0|  0.00%|\n",
      "   877|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "   878|         0|            0|            0|  0.00%|        return F.softshrink(input, self.lambd)\n",
      "   879|         0|            0|            0|  0.00%|\n",
      "   880|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "   881|         0|            0|            0|  0.00%|        return str(self.lambd)\n",
      "   882|         0|            0|            0|  0.00%|\n",
      "   883|         0|            0|            0|  0.00%|\n",
      "   884|         0|            0|            0|  0.00%|class MultiheadAttention(Module):\n",
      "   885|         0|            0|            0|  0.00%|    r\"\"\"Allows the model to jointly attend to information\n",
      "   886|         0|            0|            0|  0.00%|    from different representation subspaces as described in the paper:\n",
      "   887|         0|            0|            0|  0.00%|    `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\n",
      "   888|         0|            0|            0|  0.00%|\n",
      "   889|         0|            0|            0|  0.00%|    Multi-Head Attention is defined as:\n",
      "   890|         0|            0|            0|  0.00%|\n",
      "   891|         0|            0|            0|  0.00%|    .. math::\n",
      "   892|         0|            0|            0|  0.00%|        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
      "   893|         0|            0|            0|  0.00%|\n",
      "   894|         0|            0|            0|  0.00%|    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n",
      "   895|         0|            0|            0|  0.00%|\n",
      "   896|         0|            0|            0|  0.00%|    ``forward()`` will use a special optimized implementation if all of the following\n",
      "   897|         0|            0|            0|  0.00%|    conditions are met:\n",
      "   898|         0|            0|            0|  0.00%|\n",
      "   899|         0|            0|            0|  0.00%|    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor. This\n",
      "   900|         0|            0|            0|  0.00%|      restriction will be loosened in the future.)\n",
      "   901|         0|            0|            0|  0.00%|    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n",
      "   902|         0|            0|            0|  0.00%|    - training is disabled (using ``.eval()``)\n",
      "   903|         0|            0|            0|  0.00%|    - dropout is 0\n",
      "   904|         0|            0|            0|  0.00%|    - ``add_bias_kv`` is ``False``\n",
      "   905|         0|            0|            0|  0.00%|    - ``add_zero_attn`` is ``False``\n",
      "   906|         0|            0|            0|  0.00%|    - ``batch_first`` is ``True`` and the input is batched\n",
      "   907|         0|            0|            0|  0.00%|    - ``kdim`` and ``vdim`` are equal to ``embed_dim``\n",
      "   908|         0|            0|            0|  0.00%|    - at most one of ``key_padding_mask`` or ``attn_mask`` is passed\n",
      "   909|         0|            0|            0|  0.00%|    - if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n",
      "   910|         0|            0|            0|  0.00%|      nor ``attn_mask`` is passed\n",
      "   911|         0|            0|            0|  0.00%|\n",
      "   912|         0|            0|            0|  0.00%|    If the optimized implementation is in use, a\n",
      "   913|         0|            0|            0|  0.00%|    `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n",
      "   914|         0|            0|            0|  0.00%|    ``query``/``key``/``value`` to represent padding more efficiently than using a\n",
      "   915|         0|            0|            0|  0.00%|    padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\n",
      "   916|         0|            0|            0|  0.00%|    will be returned, and an additional speedup proportional to the fraction of the input\n",
      "   917|         0|            0|            0|  0.00%|    that is padding can be expected.\n",
      "   918|         0|            0|            0|  0.00%|\n",
      "   919|         0|            0|            0|  0.00%|    Args:\n",
      "   920|         0|            0|            0|  0.00%|        embed_dim: Total dimension of the model.\n",
      "   921|         0|            0|            0|  0.00%|        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n",
      "   922|         0|            0|            0|  0.00%|            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n",
      "   923|         0|            0|            0|  0.00%|        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n",
      "   924|         0|            0|            0|  0.00%|        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n",
      "   925|         0|            0|            0|  0.00%|        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n",
      "   926|         0|            0|            0|  0.00%|        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n",
      "   927|         0|            0|            0|  0.00%|            Default: ``False``.\n",
      "   928|         0|            0|            0|  0.00%|        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n",
      "   929|         0|            0|            0|  0.00%|        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n",
      "   930|         0|            0|            0|  0.00%|        batch_first: If ``True``, then the input and output tensors are provided\n",
      "   931|         0|            0|            0|  0.00%|            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
      "   932|         0|            0|            0|  0.00%|\n",
      "   933|         0|            0|            0|  0.00%|    Examples::\n",
      "   934|         0|            0|            0|  0.00%|\n",
      "   935|         0|            0|            0|  0.00%|        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "   936|         0|            0|            0|  0.00%|        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
      "   937|         0|            0|            0|  0.00%|\n",
      "   938|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   939|         0|            0|            0|  0.00%|    __constants__ = ['batch_first']\n",
      "   940|         0|            0|            0|  0.00%|    bias_k: Optional[torch.Tensor]\n",
      "   941|         0|            0|            0|  0.00%|    bias_v: Optional[torch.Tensor]\n",
      "   942|         0|            0|            0|  0.00%|\n",
      "   943|         0|            0|            0|  0.00%|    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\n",
      "   944|         0|            0|            0|  0.00%|                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:\n",
      "   945|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "   946|         0|            0|            0|  0.00%|        super(MultiheadAttention, self).__init__()\n",
      "   947|         0|            0|            0|  0.00%|        self.embed_dim = embed_dim\n",
      "   948|         0|            0|            0|  0.00%|        self.kdim = kdim if kdim is not None else embed_dim\n",
      "   949|         0|            0|            0|  0.00%|        self.vdim = vdim if vdim is not None else embed_dim\n",
      "   950|         0|            0|            0|  0.00%|        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
      "   951|         0|            0|            0|  0.00%|\n",
      "   952|         0|            0|            0|  0.00%|        self.num_heads = num_heads\n",
      "   953|         0|            0|            0|  0.00%|        self.dropout = dropout\n",
      "   954|         0|            0|            0|  0.00%|        self.batch_first = batch_first\n",
      "   955|         0|            0|            0|  0.00%|        self.head_dim = embed_dim // num_heads\n",
      "   956|         0|            0|            0|  0.00%|        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
      "   957|         0|            0|            0|  0.00%|\n",
      "   958|         0|            0|            0|  0.00%|        if self._qkv_same_embed_dim is False:\n",
      "   959|         0|            0|            0|  0.00%|            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n",
      "   960|         0|            0|            0|  0.00%|            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n",
      "   961|         0|            0|            0|  0.00%|            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n",
      "   962|         0|            0|            0|  0.00%|            self.register_parameter('in_proj_weight', None)\n",
      "   963|         0|            0|            0|  0.00%|        else:\n",
      "   964|         0|            0|            0|  0.00%|            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\n",
      "   965|         0|            0|            0|  0.00%|            self.register_parameter('q_proj_weight', None)\n",
      "   966|         0|            0|            0|  0.00%|            self.register_parameter('k_proj_weight', None)\n",
      "   967|         0|            0|            0|  0.00%|            self.register_parameter('v_proj_weight', None)\n",
      "   968|         0|            0|            0|  0.00%|\n",
      "   969|         0|            0|            0|  0.00%|        if bias:\n",
      "   970|         0|            0|            0|  0.00%|            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n",
      "   971|         0|            0|            0|  0.00%|        else:\n",
      "   972|         0|            0|            0|  0.00%|            self.register_parameter('in_proj_bias', None)\n",
      "   973|         0|            0|            0|  0.00%|        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n",
      "   974|         0|            0|            0|  0.00%|\n",
      "   975|         0|            0|            0|  0.00%|        if add_bias_kv:\n",
      "   976|         0|            0|            0|  0.00%|            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
      "   977|         0|            0|            0|  0.00%|            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
      "   978|         0|            0|            0|  0.00%|        else:\n",
      "   979|         0|            0|            0|  0.00%|            self.bias_k = self.bias_v = None\n",
      "   980|         0|            0|            0|  0.00%|\n",
      "   981|         0|            0|            0|  0.00%|        self.add_zero_attn = add_zero_attn\n",
      "   982|         0|            0|            0|  0.00%|\n",
      "   983|         0|            0|            0|  0.00%|        self._reset_parameters()\n",
      "   984|         0|            0|            0|  0.00%|\n",
      "   985|         0|            0|            0|  0.00%|    def _reset_parameters(self):\n",
      "   986|         0|            0|            0|  0.00%|        if self._qkv_same_embed_dim:\n",
      "   987|         0|            0|            0|  0.00%|            xavier_uniform_(self.in_proj_weight)\n",
      "   988|         0|            0|            0|  0.00%|        else:\n",
      "   989|         0|            0|            0|  0.00%|            xavier_uniform_(self.q_proj_weight)\n",
      "   990|         0|            0|            0|  0.00%|            xavier_uniform_(self.k_proj_weight)\n",
      "   991|         0|            0|            0|  0.00%|            xavier_uniform_(self.v_proj_weight)\n",
      "   992|         0|            0|            0|  0.00%|\n",
      "   993|         0|            0|            0|  0.00%|        if self.in_proj_bias is not None:\n",
      "   994|         0|            0|            0|  0.00%|            constant_(self.in_proj_bias, 0.)\n",
      "   995|         0|            0|            0|  0.00%|            constant_(self.out_proj.bias, 0.)\n",
      "   996|         0|            0|            0|  0.00%|        if self.bias_k is not None:\n",
      "   997|         0|            0|            0|  0.00%|            xavier_normal_(self.bias_k)\n",
      "   998|         0|            0|            0|  0.00%|        if self.bias_v is not None:\n",
      "   999|         0|            0|            0|  0.00%|            xavier_normal_(self.bias_v)\n",
      "  1000|         0|            0|            0|  0.00%|\n",
      "  1001|         0|            0|            0|  0.00%|    def __setstate__(self, state):\n",
      "  1002|         0|            0|            0|  0.00%|        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
      "  1003|         0|            0|            0|  0.00%|        if '_qkv_same_embed_dim' not in state:\n",
      "  1004|         0|            0|            0|  0.00%|            state['_qkv_same_embed_dim'] = True\n",
      "  1005|         0|            0|            0|  0.00%|\n",
      "  1006|         0|            0|            0|  0.00%|        super(MultiheadAttention, self).__setstate__(state)\n",
      "  1007|         0|            0|            0|  0.00%|\n",
      "  1008|         0|            0|            0|  0.00%|    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,\n",
      "  1009|         0|            0|            0|  0.00%|                need_weights: bool = True, attn_mask: Optional[Tensor] = None,\n",
      "  1010|         0|            0|            0|  0.00%|                average_attn_weights: bool = True) -> Tuple[Tensor, Optional[Tensor]]:\n",
      "  1011|         0|            0|            0|  0.00%|        r\"\"\"\n",
      "  1012|         0|            0|            0|  0.00%|    Args:\n",
      "  1013|         0|            0|            0|  0.00%|        query: Query embeddings of shape :math:`(L, E_q)` for unbatched input, :math:`(L, N, E_q)` when ``batch_first=False``\n",
      "  1014|         0|            0|            0|  0.00%|            or :math:`(N, L, E_q)` when ``batch_first=True``, where :math:`L` is the target sequence length,\n",
      "  1015|         0|            0|            0|  0.00%|            :math:`N` is the batch size, and :math:`E_q` is the query embedding dimension ``embed_dim``.\n",
      "  1016|         0|            0|            0|  0.00%|            Queries are compared against key-value pairs to produce the output.\n",
      "  1017|         0|            0|            0|  0.00%|            See \"Attention Is All You Need\" for more details.\n",
      "  1018|         0|            0|            0|  0.00%|        key: Key embeddings of shape :math:`(S, E_k)` for unbatched input, :math:`(S, N, E_k)` when ``batch_first=False``\n",
      "  1019|         0|            0|            0|  0.00%|            or :math:`(N, S, E_k)` when ``batch_first=True``, where :math:`S` is the source sequence length,\n",
      "  1020|         0|            0|            0|  0.00%|            :math:`N` is the batch size, and :math:`E_k` is the key embedding dimension ``kdim``.\n",
      "  1021|         0|            0|            0|  0.00%|            See \"Attention Is All You Need\" for more details.\n",
      "  1022|         0|            0|            0|  0.00%|        value: Value embeddings of shape :math:`(S, E_v)` for unbatched input, :math:`(S, N, E_v)` when\n",
      "  1023|         0|            0|            0|  0.00%|            ``batch_first=False`` or :math:`(N, S, E_v)` when ``batch_first=True``, where :math:`S` is the source\n",
      "  1024|         0|            0|            0|  0.00%|            sequence length, :math:`N` is the batch size, and :math:`E_v` is the value embedding dimension ``vdim``.\n",
      "  1025|         0|            0|            0|  0.00%|            See \"Attention Is All You Need\" for more details.\n",
      "  1026|         0|            0|            0|  0.00%|        key_padding_mask: If specified, a mask of shape :math:`(N, S)` indicating which elements within ``key``\n",
      "  1027|         0|            0|            0|  0.00%|            to ignore for the purpose of attention (i.e. treat as \"padding\"). For unbatched `query`, shape should be :math:`(S)`.\n",
      "  1028|         0|            0|            0|  0.00%|            Binary and byte masks are supported.\n",
      "  1029|         0|            0|            0|  0.00%|            For a binary mask, a ``True`` value indicates that the corresponding ``key`` value will be ignored for\n",
      "  1030|         0|            0|            0|  0.00%|            the purpose of attention. For a byte mask, a non-zero value indicates that the corresponding ``key``\n",
      "  1031|         0|            0|            0|  0.00%|            value will be ignored.\n",
      "  1032|         0|            0|            0|  0.00%|        need_weights: If specified, returns ``attn_output_weights`` in addition to ``attn_outputs``.\n",
      "  1033|         0|            0|            0|  0.00%|            Default: ``True``.\n",
      "  1034|         0|            0|            0|  0.00%|        attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape\n",
      "  1035|         0|            0|            0|  0.00%|            :math:`(L, S)` or :math:`(N\\cdot\\text{num\\_heads}, L, S)`, where :math:`N` is the batch size,\n",
      "  1036|         0|            0|            0|  0.00%|            :math:`L` is the target sequence length, and :math:`S` is the source sequence length. A 2D mask will be\n",
      "  1037|         0|            0|            0|  0.00%|            broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.\n",
      "  1038|         0|            0|            0|  0.00%|            Binary, byte, and float masks are supported. For a binary mask, a ``True`` value indicates that the\n",
      "  1039|         0|            0|            0|  0.00%|            corresponding position is not allowed to attend. For a byte mask, a non-zero value indicates that the\n",
      "  1040|         0|            0|            0|  0.00%|            corresponding position is not allowed to attend. For a float mask, the mask values will be added to\n",
      "  1041|         0|            0|            0|  0.00%|            the attention weight.\n",
      "  1042|         0|            0|            0|  0.00%|        average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n",
      "  1043|         0|            0|            0|  0.00%|            heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n",
      "  1044|         0|            0|            0|  0.00%|            effect when ``need_weights=True``. Default: ``True`` (i.e. average weights across heads)\n",
      "  1045|         0|            0|            0|  0.00%|\n",
      "  1046|         0|            0|            0|  0.00%|    Outputs:\n",
      "  1047|         0|            0|            0|  0.00%|        - **attn_output** - Attention outputs of shape :math:`(L, E)` when input is unbatched,\n",
      "  1048|         0|            0|            0|  0.00%|          :math:`(L, N, E)` when ``batch_first=False`` or :math:`(N, L, E)` when ``batch_first=True``,\n",
      "  1049|         0|            0|            0|  0.00%|          where :math:`L` is the target sequence length, :math:`N` is the batch size, and :math:`E` is the\n",
      "  1050|         0|            0|            0|  0.00%|          embedding dimension ``embed_dim``.\n",
      "  1051|         0|            0|            0|  0.00%|        - **attn_output_weights** - Only returned when ``need_weights=True``. If ``average_attn_weights=True``,\n",
      "  1052|         0|            0|            0|  0.00%|          returns attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\n",
      "  1053|         0|            0|            0|  0.00%|          :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\n",
      "  1054|         0|            0|            0|  0.00%|          :math:`S` is the source sequence length. If ``average_weights=False``, returns attention weights per\n",
      "  1055|         0|            0|            0|  0.00%|          head of shape :math:`(\\text{num\\_heads}, L, S)` when input is unbatched or :math:`(N, \\text{num\\_heads}, L, S)`.\n",
      "  1056|         0|            0|            0|  0.00%|\n",
      "  1057|         0|            0|            0|  0.00%|        .. note::\n",
      "  1058|         0|            0|            0|  0.00%|            `batch_first` argument is ignored for unbatched inputs.\n",
      "  1059|         0|            0|            0|  0.00%|        \"\"\"\n",
      "  1060|         0|            0|            0|  0.00%|        is_batched = query.dim() == 3\n",
      "  1061|         0|            0|            0|  0.00%|        why_not_fast_path = ''\n",
      "  1062|         0|            0|            0|  0.00%|        if not is_batched:\n",
      "  1063|         0|            0|            0|  0.00%|            why_not_fast_path = f\"input not batched; expected query.dim() of 3 but got {query.dim()}\"\n",
      "  1064|         0|            0|            0|  0.00%|        elif query is not key or key is not value:\n",
      "  1065|         0|            0|            0|  0.00%|            # When lifting this restriction, don't forget to either\n",
      "  1066|         0|            0|            0|  0.00%|            # enforce that the dtypes all match or test cases where\n",
      "  1067|         0|            0|            0|  0.00%|            # they don't!\n",
      "  1068|         0|            0|            0|  0.00%|            why_not_fast_path = \"non-self attention was used (query, key, and value are not the same Tensor)\"\n",
      "  1069|         0|            0|            0|  0.00%|        elif self.in_proj_bias is not None and query.dtype != self.in_proj_bias.dtype:\n",
      "  1070|         0|            0|            0|  0.00%|            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_bias ({self.in_proj_bias.dtype}) don't match\"\n",
      "  1071|         0|            0|            0|  0.00%|        elif self.in_proj_weight is not None and query.dtype != self.in_proj_weight.dtype:\n",
      "  1072|         0|            0|            0|  0.00%|            # this case will fail anyway, but at least they'll get a useful error message.\n",
      "  1073|         0|            0|            0|  0.00%|            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_weight ({self.in_proj_weight.dtype}) don't match\"\n",
      "  1074|         0|            0|            0|  0.00%|        elif self.training:\n",
      "  1075|         0|            0|            0|  0.00%|            why_not_fast_path = \"training is enabled\"\n",
      "  1076|         0|            0|            0|  0.00%|        elif not self.batch_first:\n",
      "  1077|         0|            0|            0|  0.00%|            why_not_fast_path = \"batch_first was not True\"\n",
      "  1078|         0|            0|            0|  0.00%|        elif self.bias_k is not None:\n",
      "  1079|         0|            0|            0|  0.00%|            why_not_fast_path = \"self.bias_k was not None\"\n",
      "  1080|         0|            0|            0|  0.00%|        elif self.bias_v is not None:\n",
      "  1081|         0|            0|            0|  0.00%|            why_not_fast_path = \"self.bias_v was not None\"\n",
      "  1082|         0|            0|            0|  0.00%|        elif self.dropout:\n",
      "  1083|         0|            0|            0|  0.00%|            why_not_fast_path = f\"dropout was {self.dropout}, required zero\"\n",
      "  1084|         0|            0|            0|  0.00%|        elif self.add_zero_attn:\n",
      "  1085|         0|            0|            0|  0.00%|            why_not_fast_path = \"add_zero_attn was enabled\"\n",
      "  1086|         0|            0|            0|  0.00%|        elif not self._qkv_same_embed_dim:\n",
      "  1087|         0|            0|            0|  0.00%|            why_not_fast_path = \"_qkv_same_embed_dim was not True\"\n",
      "  1088|         0|            0|            0|  0.00%|        elif attn_mask is not None:\n",
      "  1089|         0|            0|            0|  0.00%|            why_not_fast_path = \"attn_mask was not None\"\n",
      "  1090|         0|            0|            0|  0.00%|        elif query.is_nested and key_padding_mask is not None:\n",
      "  1091|         0|            0|            0|  0.00%|            why_not_fast_path = \"key_padding_mask is not supported with NestedTensor input\"\n",
      "  1092|         0|            0|            0|  0.00%|\n",
      "  1093|         0|            0|            0|  0.00%|        if not why_not_fast_path:\n",
      "  1094|         0|            0|            0|  0.00%|            tensor_args = (\n",
      "  1095|         0|            0|            0|  0.00%|                query,\n",
      "  1096|         0|            0|            0|  0.00%|                key,\n",
      "  1097|         0|            0|            0|  0.00%|                value,\n",
      "  1098|         0|            0|            0|  0.00%|                self.in_proj_weight,\n",
      "  1099|         0|            0|            0|  0.00%|                self.in_proj_bias,\n",
      "  1100|         0|            0|            0|  0.00%|                self.out_proj.weight,\n",
      "  1101|         0|            0|            0|  0.00%|                self.out_proj.bias,\n",
      "  1102|         0|            0|            0|  0.00%|            )\n",
      "  1103|         0|            0|            0|  0.00%|            # We have to use list comprehensions below because TorchScript does not support\n",
      "  1104|         0|            0|            0|  0.00%|            # generator expressions.\n",
      "  1105|         0|            0|            0|  0.00%|            if torch.overrides.has_torch_function(tensor_args):\n",
      "  1106|         0|            0|            0|  0.00%|                why_not_fast_path = \"some Tensor argument has_torch_function\"\n",
      "  1107|         0|            0|            0|  0.00%|            elif not all([(x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]):\n",
      "  1108|         0|            0|            0|  0.00%|                why_not_fast_path = \"some Tensor argument is neither CUDA nor CPU\"\n",
      "  1109|         0|            0|            0|  0.00%|            elif torch.is_grad_enabled() and any([x.requires_grad for x in tensor_args]):\n",
      "  1110|         0|            0|            0|  0.00%|                why_not_fast_path = (\"grad is enabled and at least one of query or the \"\n",
      "  1111|         0|            0|            0|  0.00%|                                     \"input/output projection weights or biases requires_grad\")\n",
      "  1112|         0|            0|            0|  0.00%|            if not why_not_fast_path:\n",
      "  1113|         0|            0|            0|  0.00%|                return torch._native_multi_head_attention(\n",
      "  1114|         0|            0|            0|  0.00%|                    query,\n",
      "  1115|         0|            0|            0|  0.00%|                    key,\n",
      "  1116|         0|            0|            0|  0.00%|                    value,\n",
      "  1117|         0|            0|            0|  0.00%|                    self.embed_dim,\n",
      "  1118|         0|            0|            0|  0.00%|                    self.num_heads,\n",
      "  1119|         0|            0|            0|  0.00%|                    self.in_proj_weight,\n",
      "  1120|         0|            0|            0|  0.00%|                    self.in_proj_bias,\n",
      "  1121|         0|            0|            0|  0.00%|                    self.out_proj.weight,\n",
      "  1122|         0|            0|            0|  0.00%|                    self.out_proj.bias,\n",
      "  1123|         0|            0|            0|  0.00%|                    key_padding_mask if key_padding_mask is not None else attn_mask,\n",
      "  1124|         0|            0|            0|  0.00%|                    need_weights,\n",
      "  1125|         0|            0|            0|  0.00%|                    average_attn_weights)\n",
      "  1126|         0|            0|            0|  0.00%|        any_nested = query.is_nested or key.is_nested or value.is_nested\n",
      "  1127|         0|            0|            0|  0.00%|        assert not any_nested, (\"MultiheadAttention does not support NestedTensor outside of its fast path. \" +\n",
      "  1128|         0|            0|            0|  0.00%|                                f\"The fast path was not hit because {why_not_fast_path}\")\n",
      "  1129|         0|            0|            0|  0.00%|\n",
      "  1130|         0|            0|            0|  0.00%|        if self.batch_first and is_batched:\n",
      "  1131|         0|            0|            0|  0.00%|            # make sure that the transpose op does not affect the \"is\" property\n",
      "  1132|         0|            0|            0|  0.00%|            if key is value:\n",
      "  1133|         0|            0|            0|  0.00%|                if query is key:\n",
      "  1134|         0|            0|            0|  0.00%|                    query = key = value = query.transpose(1, 0)\n",
      "  1135|         0|            0|            0|  0.00%|                else:\n",
      "  1136|         0|            0|            0|  0.00%|                    query, key = [x.transpose(1, 0) for x in (query, key)]\n",
      "  1137|         0|            0|            0|  0.00%|                    value = key\n",
      "  1138|         0|            0|            0|  0.00%|            else:\n",
      "  1139|         0|            0|            0|  0.00%|                query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n",
      "  1140|         0|            0|            0|  0.00%|\n",
      "  1141|         0|            0|            0|  0.00%|        if not self._qkv_same_embed_dim:\n",
      "  1142|         0|            0|            0|  0.00%|            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "  1143|         0|            0|            0|  0.00%|                query, key, value, self.embed_dim, self.num_heads,\n",
      "  1144|         0|            0|            0|  0.00%|                self.in_proj_weight, self.in_proj_bias,\n",
      "  1145|         0|            0|            0|  0.00%|                self.bias_k, self.bias_v, self.add_zero_attn,\n",
      "  1146|         0|            0|            0|  0.00%|                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
      "  1147|         0|            0|            0|  0.00%|                training=self.training,\n",
      "  1148|         0|            0|            0|  0.00%|                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
      "  1149|         0|            0|            0|  0.00%|                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
      "  1150|         0|            0|            0|  0.00%|                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
      "  1151|         0|            0|            0|  0.00%|                v_proj_weight=self.v_proj_weight, average_attn_weights=average_attn_weights)\n",
      "  1152|         0|            0|            0|  0.00%|        else:\n",
      "  1153|         0|            0|            0|  0.00%|            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "  1154|         0|            0|            0|  0.00%|                query, key, value, self.embed_dim, self.num_heads,\n",
      "  1155|         0|            0|            0|  0.00%|                self.in_proj_weight, self.in_proj_bias,\n",
      "  1156|         0|            0|            0|  0.00%|                self.bias_k, self.bias_v, self.add_zero_attn,\n",
      "  1157|         0|            0|            0|  0.00%|                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
      "  1158|         0|            0|            0|  0.00%|                training=self.training,\n",
      "  1159|         0|            0|            0|  0.00%|                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
      "  1160|         0|            0|            0|  0.00%|                attn_mask=attn_mask, average_attn_weights=average_attn_weights)\n",
      "  1161|         0|            0|            0|  0.00%|        if self.batch_first and is_batched:\n",
      "  1162|         0|            0|            0|  0.00%|            return attn_output.transpose(1, 0), attn_output_weights\n",
      "  1163|         0|            0|            0|  0.00%|        else:\n",
      "  1164|         0|            0|            0|  0.00%|            return attn_output, attn_output_weights\n",
      "  1165|         0|            0|            0|  0.00%|\n",
      "  1166|         0|            0|            0|  0.00%|class PReLU(Module):\n",
      "  1167|         0|            0|            0|  0.00%|    r\"\"\"Applies the element-wise function:\n",
      "  1168|         0|            0|            0|  0.00%|\n",
      "  1169|         0|            0|            0|  0.00%|    .. math::\n",
      "  1170|         0|            0|            0|  0.00%|        \\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)\n",
      "  1171|         0|            0|            0|  0.00%|\n",
      "  1172|         0|            0|            0|  0.00%|    or\n",
      "  1173|         0|            0|            0|  0.00%|\n",
      "  1174|         0|            0|            0|  0.00%|    .. math::\n",
      "  1175|         0|            0|            0|  0.00%|        \\text{PReLU}(x) =\n",
      "  1176|         0|            0|            0|  0.00%|        \\begin{cases}\n",
      "  1177|         0|            0|            0|  0.00%|        x, & \\text{ if } x \\geq 0 \\\\\n",
      "  1178|         0|            0|            0|  0.00%|        ax, & \\text{ otherwise }\n",
      "  1179|         0|            0|            0|  0.00%|        \\end{cases}\n",
      "  1180|         0|            0|            0|  0.00%|\n",
      "  1181|         0|            0|            0|  0.00%|    Here :math:`a` is a learnable parameter. When called without arguments, `nn.PReLU()` uses a single\n",
      "  1182|         0|            0|            0|  0.00%|    parameter :math:`a` across all input channels. If called with `nn.PReLU(nChannels)`,\n",
      "  1183|         0|            0|            0|  0.00%|    a separate :math:`a` is used for each input channel.\n",
      "  1184|         0|            0|            0|  0.00%|\n",
      "  1185|         0|            0|            0|  0.00%|\n",
      "  1186|         0|            0|            0|  0.00%|    .. note::\n",
      "  1187|         0|            0|            0|  0.00%|        weight decay should not be used when learning :math:`a` for good performance.\n",
      "  1188|         0|            0|            0|  0.00%|\n",
      "  1189|         0|            0|            0|  0.00%|    .. note::\n",
      "  1190|         0|            0|            0|  0.00%|        Channel dim is the 2nd dim of input. When input has dims < 2, then there is\n",
      "  1191|         0|            0|            0|  0.00%|        no channel dim and the number of channels = 1.\n",
      "  1192|         0|            0|            0|  0.00%|\n",
      "  1193|         0|            0|            0|  0.00%|    Args:\n",
      "  1194|         0|            0|            0|  0.00%|        num_parameters (int): number of :math:`a` to learn.\n",
      "  1195|         0|            0|            0|  0.00%|            Although it takes an int as input, there is only two values are legitimate:\n",
      "  1196|         0|            0|            0|  0.00%|            1, or the number of channels at input. Default: 1\n",
      "  1197|         0|            0|            0|  0.00%|        init (float): the initial value of :math:`a`. Default: 0.25\n",
      "  1198|         0|            0|            0|  0.00%|\n",
      "  1199|         0|            0|            0|  0.00%|    Shape:\n",
      "  1200|         0|            0|            0|  0.00%|        - Input: :math:`( *)` where `*` means, any number of additional\n",
      "  1201|         0|            0|            0|  0.00%|          dimensions.\n",
      "  1202|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "  1203|         0|            0|            0|  0.00%|\n",
      "  1204|         0|            0|            0|  0.00%|    Attributes:\n",
      "  1205|         0|            0|            0|  0.00%|        weight (Tensor): the learnable weights of shape (:attr:`num_parameters`).\n",
      "  1206|         0|            0|            0|  0.00%|\n",
      "  1207|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/PReLU.png\n",
      "  1208|         0|            0|            0|  0.00%|\n",
      "  1209|         0|            0|            0|  0.00%|    Examples::\n",
      "  1210|         0|            0|            0|  0.00%|\n",
      "  1211|         0|            0|            0|  0.00%|        >>> m = nn.PReLU()\n",
      "  1212|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "  1213|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "  1214|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1215|         0|            0|            0|  0.00%|    __constants__ = ['num_parameters']\n",
      "  1216|         0|            0|            0|  0.00%|    num_parameters: int\n",
      "  1217|         0|            0|            0|  0.00%|\n",
      "  1218|         0|            0|            0|  0.00%|    def __init__(self, num_parameters: int = 1, init: float = 0.25,\n",
      "  1219|         0|            0|            0|  0.00%|                 device=None, dtype=None) -> None:\n",
      "  1220|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "  1221|         0|            0|            0|  0.00%|        self.num_parameters = num_parameters\n",
      "  1222|         0|            0|            0|  0.00%|        super(PReLU, self).__init__()\n",
      "  1223|         0|            0|            0|  0.00%|        self.weight = Parameter(torch.empty(num_parameters, **factory_kwargs).fill_(init))\n",
      "  1224|         0|            0|            0|  0.00%|\n",
      "  1225|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "  1226|         0|            0|            0|  0.00%|        return F.prelu(input, self.weight)\n",
      "  1227|         0|            0|            0|  0.00%|\n",
      "  1228|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "  1229|         0|            0|            0|  0.00%|        return 'num_parameters={}'.format(self.num_parameters)\n",
      "  1230|         0|            0|            0|  0.00%|\n",
      "  1231|         0|            0|            0|  0.00%|\n",
      "  1232|         0|            0|            0|  0.00%|class Softsign(Module):\n",
      "  1233|         0|            0|            0|  0.00%|    r\"\"\"Applies the element-wise function:\n",
      "  1234|         0|            0|            0|  0.00%|\n",
      "  1235|         0|            0|            0|  0.00%|    .. math::\n",
      "  1236|         0|            0|            0|  0.00%|        \\text{SoftSign}(x) = \\frac{x}{ 1 + |x|}\n",
      "  1237|         0|            0|            0|  0.00%|\n",
      "  1238|         0|            0|            0|  0.00%|    Shape:\n",
      "  1239|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "  1240|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "  1241|         0|            0|            0|  0.00%|\n",
      "  1242|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Softsign.png\n",
      "  1243|         0|            0|            0|  0.00%|\n",
      "  1244|         0|            0|            0|  0.00%|    Examples::\n",
      "  1245|         0|            0|            0|  0.00%|\n",
      "  1246|         0|            0|            0|  0.00%|        >>> m = nn.Softsign()\n",
      "  1247|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "  1248|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "  1249|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1250|         0|            0|            0|  0.00%|\n",
      "  1251|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "  1252|         0|            0|            0|  0.00%|        return F.softsign(input)\n",
      "  1253|         0|            0|            0|  0.00%|\n",
      "  1254|         0|            0|            0|  0.00%|\n",
      "  1255|         0|            0|            0|  0.00%|class Tanhshrink(Module):\n",
      "  1256|         0|            0|            0|  0.00%|    r\"\"\"Applies the element-wise function:\n",
      "  1257|         0|            0|            0|  0.00%|\n",
      "  1258|         0|            0|            0|  0.00%|    .. math::\n",
      "  1259|         0|            0|            0|  0.00%|        \\text{Tanhshrink}(x) = x - \\tanh(x)\n",
      "  1260|         0|            0|            0|  0.00%|\n",
      "  1261|         0|            0|            0|  0.00%|    Shape:\n",
      "  1262|         0|            0|            0|  0.00%|        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      "  1263|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input.\n",
      "  1264|         0|            0|            0|  0.00%|\n",
      "  1265|         0|            0|            0|  0.00%|    .. image:: ../scripts/activation_images/Tanhshrink.png\n",
      "  1266|         0|            0|            0|  0.00%|\n",
      "  1267|         0|            0|            0|  0.00%|    Examples::\n",
      "  1268|         0|            0|            0|  0.00%|\n",
      "  1269|         0|            0|            0|  0.00%|        >>> m = nn.Tanhshrink()\n",
      "  1270|         0|            0|            0|  0.00%|        >>> input = torch.randn(2)\n",
      "  1271|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "  1272|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1273|         0|            0|            0|  0.00%|\n",
      "  1274|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "  1275|         0|            0|            0|  0.00%|        return F.tanhshrink(input)\n",
      "  1276|         0|            0|            0|  0.00%|\n",
      "  1277|         0|            0|            0|  0.00%|\n",
      "  1278|         0|            0|            0|  0.00%|class Softmin(Module):\n",
      "  1279|         0|            0|            0|  0.00%|    r\"\"\"Applies the Softmin function to an n-dimensional input Tensor\n",
      "  1280|         0|            0|            0|  0.00%|    rescaling them so that the elements of the n-dimensional output Tensor\n",
      "  1281|         0|            0|            0|  0.00%|    lie in the range `[0, 1]` and sum to 1.\n",
      "  1282|         0|            0|            0|  0.00%|\n",
      "  1283|         0|            0|            0|  0.00%|    Softmin is defined as:\n",
      "  1284|         0|            0|            0|  0.00%|\n",
      "  1285|         0|            0|            0|  0.00%|    .. math::\n",
      "  1286|         0|            0|            0|  0.00%|        \\text{Softmin}(x_{i}) = \\frac{\\exp(-x_i)}{\\sum_j \\exp(-x_j)}\n",
      "  1287|         0|            0|            0|  0.00%|\n",
      "  1288|         0|            0|            0|  0.00%|    Shape:\n",
      "  1289|         0|            0|            0|  0.00%|        - Input: :math:`(*)` where `*` means, any number of additional\n",
      "  1290|         0|            0|            0|  0.00%|          dimensions\n",
      "  1291|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input\n",
      "  1292|         0|            0|            0|  0.00%|\n",
      "  1293|         0|            0|            0|  0.00%|    Args:\n",
      "  1294|         0|            0|            0|  0.00%|        dim (int): A dimension along which Softmin will be computed (so every slice\n",
      "  1295|         0|            0|            0|  0.00%|            along dim will sum to 1).\n",
      "  1296|         0|            0|            0|  0.00%|\n",
      "  1297|         0|            0|            0|  0.00%|    Returns:\n",
      "  1298|         0|            0|            0|  0.00%|        a Tensor of the same dimension and shape as the input, with\n",
      "  1299|         0|            0|            0|  0.00%|        values in the range [0, 1]\n",
      "  1300|         0|            0|            0|  0.00%|\n",
      "  1301|         0|            0|            0|  0.00%|    Examples::\n",
      "  1302|         0|            0|            0|  0.00%|\n",
      "  1303|         0|            0|            0|  0.00%|        >>> m = nn.Softmin()\n",
      "  1304|         0|            0|            0|  0.00%|        >>> input = torch.randn(2, 3)\n",
      "  1305|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "  1306|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1307|         0|            0|            0|  0.00%|    __constants__ = ['dim']\n",
      "  1308|         0|            0|            0|  0.00%|    dim: Optional[int]\n",
      "  1309|         0|            0|            0|  0.00%|\n",
      "  1310|         0|            0|            0|  0.00%|    def __init__(self, dim: Optional[int] = None) -> None:\n",
      "  1311|         0|            0|            0|  0.00%|        super(Softmin, self).__init__()\n",
      "  1312|         0|            0|            0|  0.00%|        self.dim = dim\n",
      "  1313|         0|            0|            0|  0.00%|\n",
      "  1314|         0|            0|            0|  0.00%|    def __setstate__(self, state):\n",
      "  1315|         0|            0|            0|  0.00%|        super().__setstate__(state)\n",
      "  1316|         0|            0|            0|  0.00%|        if not hasattr(self, 'dim'):\n",
      "  1317|         0|            0|            0|  0.00%|            self.dim = None\n",
      "  1318|         0|            0|            0|  0.00%|\n",
      "  1319|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "  1320|         0|            0|            0|  0.00%|        return F.softmin(input, self.dim, _stacklevel=5)\n",
      "  1321|         0|            0|            0|  0.00%|\n",
      "  1322|         0|            0|            0|  0.00%|    def extra_repr(self):\n",
      "  1323|         0|            0|            0|  0.00%|        return 'dim={dim}'.format(dim=self.dim)\n",
      "  1324|         0|            0|            0|  0.00%|\n",
      "  1325|         0|            0|            0|  0.00%|class Softmax(Module):\n",
      "  1326|         0|            0|            0|  0.00%|    r\"\"\"Applies the Softmax function to an n-dimensional input Tensor\n",
      "  1327|         0|            0|            0|  0.00%|    rescaling them so that the elements of the n-dimensional output Tensor\n",
      "  1328|         0|            0|            0|  0.00%|    lie in the range [0,1] and sum to 1.\n",
      "  1329|         0|            0|            0|  0.00%|\n",
      "  1330|         0|            0|            0|  0.00%|    Softmax is defined as:\n",
      "  1331|         0|            0|            0|  0.00%|\n",
      "  1332|         0|            0|            0|  0.00%|    .. math::\n",
      "  1333|         0|            0|            0|  0.00%|        \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
      "  1334|         0|            0|            0|  0.00%|\n",
      "  1335|         0|            0|            0|  0.00%|    When the input Tensor is a sparse tensor then the unspecifed\n",
      "  1336|         0|            0|            0|  0.00%|    values are treated as ``-inf``.\n",
      "  1337|         0|            0|            0|  0.00%|\n",
      "  1338|         0|            0|            0|  0.00%|    Shape:\n",
      "  1339|         0|            0|            0|  0.00%|        - Input: :math:`(*)` where `*` means, any number of additional\n",
      "  1340|         0|            0|            0|  0.00%|          dimensions\n",
      "  1341|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input\n",
      "  1342|         0|            0|            0|  0.00%|\n",
      "  1343|         0|            0|            0|  0.00%|    Returns:\n",
      "  1344|         0|            0|            0|  0.00%|        a Tensor of the same dimension and shape as the input with\n",
      "  1345|         0|            0|            0|  0.00%|        values in the range [0, 1]\n",
      "  1346|         0|            0|            0|  0.00%|\n",
      "  1347|         0|            0|            0|  0.00%|    Args:\n",
      "  1348|         0|            0|            0|  0.00%|        dim (int): A dimension along which Softmax will be computed (so every slice\n",
      "  1349|         0|            0|            0|  0.00%|            along dim will sum to 1).\n",
      "  1350|         0|            0|            0|  0.00%|\n",
      "  1351|         0|            0|            0|  0.00%|    .. note::\n",
      "  1352|         0|            0|            0|  0.00%|        This module doesn't work directly with NLLLoss,\n",
      "  1353|         0|            0|            0|  0.00%|        which expects the Log to be computed between the Softmax and itself.\n",
      "  1354|         0|            0|            0|  0.00%|        Use `LogSoftmax` instead (it's faster and has better numerical properties).\n",
      "  1355|         0|            0|            0|  0.00%|\n",
      "  1356|         0|            0|            0|  0.00%|    Examples::\n",
      "  1357|         0|            0|            0|  0.00%|\n",
      "  1358|         0|            0|            0|  0.00%|        >>> m = nn.Softmax(dim=1)\n",
      "  1359|         0|            0|            0|  0.00%|        >>> input = torch.randn(2, 3)\n",
      "  1360|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "  1361|         0|            0|            0|  0.00%|\n",
      "  1362|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1363|         0|            0|            0|  0.00%|    __constants__ = ['dim']\n",
      "  1364|         0|            0|            0|  0.00%|    dim: Optional[int]\n",
      "  1365|         0|            0|            0|  0.00%|\n",
      "  1366|         0|            0|            0|  0.00%|    def __init__(self, dim: Optional[int] = None) -> None:\n",
      "  1367|         0|            0|            0|  0.00%|        super(Softmax, self).__init__()\n",
      "  1368|         0|            0|            0|  0.00%|        self.dim = dim\n",
      "  1369|         0|            0|            0|  0.00%|\n",
      "  1370|         0|            0|            0|  0.00%|    def __setstate__(self, state):\n",
      "  1371|         0|            0|            0|  0.00%|        super().__setstate__(state)\n",
      "  1372|         0|            0|            0|  0.00%|        if not hasattr(self, 'dim'):\n",
      "  1373|         0|            0|            0|  0.00%|            self.dim = None\n",
      "  1374|         0|            0|            0|  0.00%|\n",
      "  1375|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "  1376|         0|            0|            0|  0.00%|        return F.softmax(input, self.dim, _stacklevel=5)\n",
      "  1377|         0|            0|            0|  0.00%|\n",
      "  1378|         0|            0|            0|  0.00%|    def extra_repr(self) -> str:\n",
      "  1379|         0|            0|            0|  0.00%|        return 'dim={dim}'.format(dim=self.dim)\n",
      "  1380|         0|            0|            0|  0.00%|\n",
      "  1381|         0|            0|            0|  0.00%|\n",
      "  1382|         0|            0|            0|  0.00%|class Softmax2d(Module):\n",
      "  1383|         0|            0|            0|  0.00%|    r\"\"\"Applies SoftMax over features to each spatial location.\n",
      "  1384|         0|            0|            0|  0.00%|\n",
      "  1385|         0|            0|            0|  0.00%|    When given an image of ``Channels x Height x Width``, it will\n",
      "  1386|         0|            0|            0|  0.00%|    apply `Softmax` to each location :math:`(Channels, h_i, w_j)`\n",
      "  1387|         0|            0|            0|  0.00%|\n",
      "  1388|         0|            0|            0|  0.00%|    Shape:\n",
      "  1389|         0|            0|            0|  0.00%|        - Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`.\n",
      "  1390|         0|            0|            0|  0.00%|        - Output: :math:`(N, C, H, W)` or :math:`(C, H, W)` (same shape as input)\n",
      "  1391|         0|            0|            0|  0.00%|\n",
      "  1392|         0|            0|            0|  0.00%|    Returns:\n",
      "  1393|         0|            0|            0|  0.00%|        a Tensor of the same dimension and shape as the input with\n",
      "  1394|         0|            0|            0|  0.00%|        values in the range [0, 1]\n",
      "  1395|         0|            0|            0|  0.00%|\n",
      "  1396|         0|            0|            0|  0.00%|    Examples::\n",
      "  1397|         0|            0|            0|  0.00%|\n",
      "  1398|         0|            0|            0|  0.00%|        >>> m = nn.Softmax2d()\n",
      "  1399|         0|            0|            0|  0.00%|        >>> # you softmax over the 2nd dimension\n",
      "  1400|         0|            0|            0|  0.00%|        >>> input = torch.randn(2, 3, 12, 13)\n",
      "  1401|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "  1402|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1403|         0|            0|            0|  0.00%|\n",
      "  1404|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "  1405|         0|            0|            0|  0.00%|        assert input.dim() == 4 or input.dim() == 3, 'Softmax2d requires a 3D or 4D tensor as input'\n",
      "  1406|         0|            0|            0|  0.00%|        return F.softmax(input, -3, _stacklevel=5)\n",
      "  1407|         0|            0|            0|  0.00%|\n",
      "  1408|         0|            0|            0|  0.00%|\n",
      "  1409|         0|            0|            0|  0.00%|class LogSoftmax(Module):\n",
      "  1410|         0|            0|            0|  0.00%|    r\"\"\"Applies the :math:`\\log(\\text{Softmax}(x))` function to an n-dimensional\n",
      "  1411|         0|            0|            0|  0.00%|    input Tensor. The LogSoftmax formulation can be simplified as:\n",
      "  1412|         0|            0|            0|  0.00%|\n",
      "  1413|         0|            0|            0|  0.00%|    .. math::\n",
      "  1414|         0|            0|            0|  0.00%|        \\text{LogSoftmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} \\right)\n",
      "  1415|         0|            0|            0|  0.00%|\n",
      "  1416|         0|            0|            0|  0.00%|    Shape:\n",
      "  1417|         0|            0|            0|  0.00%|        - Input: :math:`(*)` where `*` means, any number of additional\n",
      "  1418|         0|            0|            0|  0.00%|          dimensions\n",
      "  1419|         0|            0|            0|  0.00%|        - Output: :math:`(*)`, same shape as the input\n",
      "  1420|         0|            0|            0|  0.00%|\n",
      "  1421|         0|            0|            0|  0.00%|    Args:\n",
      "  1422|         0|            0|            0|  0.00%|        dim (int): A dimension along which LogSoftmax will be computed.\n",
      "  1423|         0|            0|            0|  0.00%|\n",
      "  1424|         0|            0|            0|  0.00%|    Returns:\n",
      "  1425|         0|            0|            0|  0.00%|        a Tensor of the same dimension and shape as the input with\n",
      "  1426|         0|            0|            0|  0.00%|        values in the range [-inf, 0)\n",
      "  1427|         0|            0|            0|  0.00%|\n",
      "  1428|         0|            0|            0|  0.00%|    Examples::\n",
      "  1429|         0|            0|            0|  0.00%|\n",
      "  1430|         0|            0|            0|  0.00%|        >>> m = nn.LogSoftmax()\n",
      "  1431|         0|            0|            0|  0.00%|        >>> input = torch.randn(2, 3)\n",
      "  1432|         0|            0|            0|  0.00%|        >>> output = m(input)\n",
      "  1433|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1434|         0|            0|            0|  0.00%|    __constants__ = ['dim']\n",
      "  1435|         0|            0|            0|  0.00%|    dim: Optional[int]\n",
      "  1436|         0|            0|            0|  0.00%|\n",
      "  1437|         0|            0|            0|  0.00%|    def __init__(self, dim: Optional[int] = None) -> None:\n",
      "  1438|         0|            0|            0|  0.00%|        super(LogSoftmax, self).__init__()\n",
      "  1439|         0|            0|            0|  0.00%|        self.dim = dim\n",
      "  1440|         0|            0|            0|  0.00%|\n",
      "  1441|         0|            0|            0|  0.00%|    def __setstate__(self, state):\n",
      "  1442|         0|            0|            0|  0.00%|        super().__setstate__(state)\n",
      "  1443|         0|            0|            0|  0.00%|        if not hasattr(self, 'dim'):\n",
      "  1444|         0|            0|            0|  0.00%|            self.dim = None\n",
      "  1445|         0|            0|            0|  0.00%|\n",
      "  1446|         0|            0|            0|  0.00%|    def forward(self, input: Tensor) -> Tensor:\n",
      "  1447|         0|            0|            0|  0.00%|        return F.log_softmax(input, self.dim, _stacklevel=5)\n",
      "  1448|         0|            0|            0|  0.00%|\n",
      "  1449|         0|            0|            0|  0.00%|    def extra_repr(self):\n",
      "  1450|         0|            0|            0|  0.00%|        return 'dim={dim}'.format(dim=self.dim)\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/autograd/profiler.py\n",
      "File duration: 0.498396s (0.08%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|from torch.autograd.profiler_util import (\n",
      "     2|         0|            0|            0|  0.00%|    EventList, FunctionEvent, MemRecordsAcc, MEMORY_EVENT_NAME,\n",
      "     3|         0|            0|            0|  0.00%|    _filter_name, _filter_stack_entry, _rewrite_name\n",
      "     4|         0|            0|            0|  0.00%|)\n",
      "     5|         0|            0|            0|  0.00%|\n",
      "     6|         0|            0|            0|  0.00%|from torch.autograd import (\n",
      "     7|         0|            0|            0|  0.00%|    DeviceType, ProfilerActivity, ProfilerConfig, ProfilerState,\n",
      "     8|         0|            0|            0|  0.00%|    kineto_available, _ProfilerResult, _disable_profiler, _enable_profiler,\n",
      "     9|         0|            0|            0|  0.00%|    _prepare_profiler, _supported_activities, _kineto_step,\n",
      "    10|         0|            0|            0|  0.00%|)\n",
      "    11|         0|            0|            0|  0.00%|from torch._C._autograd import _ExperimentalConfig\n",
      "    12|         0|            0|            0|  0.00%|import torch\n",
      "    13|         0|            0|            0|  0.00%|import torch.cuda\n",
      "    14|         0|            0|            0|  0.00%|from torch.futures import Future\n",
      "    15|         0|            0|            0|  0.00%|from typing import Any, Dict, List, Optional\n",
      "    16|         0|            0|            0|  0.00%|from warnings import warn\n",
      "    17|         0|            0|            0|  0.00%|\n",
      "    18|         0|            0|            0|  0.00%|\n",
      "    19|         0|            0|            0|  0.00%|try:\n",
      "    20|         0|            0|            0|  0.00%|    # Available in Python >= 3.2\n",
      "    21|         0|            0|            0|  0.00%|    from contextlib import ContextDecorator\n",
      "    22|         0|            0|            0|  0.00%|except ImportError:\n",
      "    23|         0|            0|            0|  0.00%|    import functools\n",
      "    24|         0|            0|            0|  0.00%|\n",
      "    25|         0|            0|            0|  0.00%|    class ContextDecorator(object):  # type: ignore[no-redef]\n",
      "    26|         0|            0|            0|  0.00%|\n",
      "    27|         0|            0|            0|  0.00%|        def __enter__(self):\n",
      "    28|         0|            0|            0|  0.00%|            raise NotImplementedError\n",
      "    29|         0|            0|            0|  0.00%|\n",
      "    30|         0|            0|            0|  0.00%|        def __exit__(self, exc_type, exc_val, exc_tb):\n",
      "    31|         0|            0|            0|  0.00%|            raise NotImplementedError\n",
      "    32|         0|            0|            0|  0.00%|\n",
      "    33|         0|            0|            0|  0.00%|        def __call__(self, func):\n",
      "    34|         0|            0|            0|  0.00%|            @functools.wraps(func)\n",
      "    35|         0|            0|            0|  0.00%|            def wrapped(*args, **kwargs):\n",
      "    36|         0|            0|            0|  0.00%|                with self:\n",
      "    37|         0|            0|            0|  0.00%|                    return func(*args, **kwargs)\n",
      "    38|         0|            0|            0|  0.00%|\n",
      "    39|         0|            0|            0|  0.00%|            return wrapped\n",
      "    40|         0|            0|            0|  0.00%|\n",
      "    41|         0|            0|            0|  0.00%|\n",
      "    42|         0|            0|            0|  0.00%|class profile(object):\n",
      "    43|         0|            0|            0|  0.00%|    \"\"\"Context manager that manages autograd profiler state and holds a summary of results.\n",
      "    44|         0|            0|            0|  0.00%|    Under the hood it just records events of functions being executed in C++ and\n",
      "    45|         0|            0|            0|  0.00%|    exposes those events to Python. You can wrap any code into it and it will\n",
      "    46|         0|            0|            0|  0.00%|    only report runtime of PyTorch functions.\n",
      "    47|         0|            0|            0|  0.00%|    Note: profiler is thread local and is automatically propagated into the async tasks\n",
      "    48|         0|            0|            0|  0.00%|\n",
      "    49|         0|            0|            0|  0.00%|    Args:\n",
      "    50|         0|            0|            0|  0.00%|        enabled (bool, optional): Setting this to False makes this context manager a no-op.\n",
      "    51|         0|            0|            0|  0.00%|\n",
      "    52|         0|            0|            0|  0.00%|        use_cuda (bool, optional): Enables timing of CUDA events as well using the cudaEvent API.\n",
      "    53|         0|            0|            0|  0.00%|            Adds approximately 4us of overhead to each tensor operation.\n",
      "    54|         0|            0|            0|  0.00%|\n",
      "    55|         0|            0|            0|  0.00%|        record_shapes (bool, optional): If shapes recording is set, information\n",
      "    56|         0|            0|            0|  0.00%|            about input dimensions will be collected. This allows one to see which\n",
      "    57|         0|            0|            0|  0.00%|            dimensions have been used under the hood and further group by them\n",
      "    58|         0|            0|            0|  0.00%|            using prof.key_averages(group_by_input_shape=True). Please note that\n",
      "    59|         0|            0|            0|  0.00%|            shape recording might skew your profiling data. It is recommended to\n",
      "    60|         0|            0|            0|  0.00%|            use separate runs with and without shape recording to validate the timing.\n",
      "    61|         0|            0|            0|  0.00%|            Most likely the skew will be negligible for bottom most events (in a case\n",
      "    62|         0|            0|            0|  0.00%|            of nested function calls). But for higher level functions the total\n",
      "    63|         0|            0|            0|  0.00%|            self cpu time might be artificially increased because of the shape\n",
      "    64|         0|            0|            0|  0.00%|            collection.\n",
      "    65|         0|            0|            0|  0.00%|\n",
      "    66|         0|            0|            0|  0.00%|        with_flops (bool, optional): If with_flops is set, the profiler will estimate\n",
      "    67|         0|            0|            0|  0.00%|            the FLOPs (floating point operations) value using the operator's input shape.\n",
      "    68|         0|            0|            0|  0.00%|            This allows one to estimate the hardware performance. Currently,\n",
      "    69|         0|            0|            0|  0.00%|            this option only works for the matrix multiplication and 2D convolution operators.\n",
      "    70|         0|            0|            0|  0.00%|\n",
      "    71|         0|            0|            0|  0.00%|        profile_memory (bool, optional): track tensor memory allocation/deallocation.\n",
      "    72|         0|            0|            0|  0.00%|\n",
      "    73|         0|            0|            0|  0.00%|        with_stack (bool, optional): record source information (file and line number) for the ops.\n",
      "    74|         0|            0|            0|  0.00%|\n",
      "    75|         0|            0|            0|  0.00%|        with_modules (bool): record module hierarchy (including function names)\n",
      "    76|         0|            0|            0|  0.00%|            corresponding to the callstack of the op. e.g. If module A's forward call's\n",
      "    77|         0|            0|            0|  0.00%|            module B's forward which contains an aten::add op,\n",
      "    78|         0|            0|            0|  0.00%|            then aten::add's module hierarchy is A.B\n",
      "    79|         0|            0|            0|  0.00%|            Note that this support exist, at the moment, only for TorchScript models\n",
      "    80|         0|            0|            0|  0.00%|            and not eager mode models.\n",
      "    81|         0|            0|            0|  0.00%|\n",
      "    82|         0|            0|            0|  0.00%|        use_kineto (bool, optional): experimental, enable profiling with Kineto profiler.\n",
      "    83|         0|            0|            0|  0.00%|\n",
      "    84|         0|            0|            0|  0.00%|        use_cpu (bool, optional): profile CPU events; setting to ``False`` requires\n",
      "    85|         0|            0|            0|  0.00%|            ``use_kineto=True`` and can be used to lower the overhead for GPU-only profiling.\n",
      "    86|         0|            0|            0|  0.00%|\n",
      "    87|         0|            0|            0|  0.00%|        experimental_config (_ExperimentalConfig) : A set of experimental options\n",
      "    88|         0|            0|            0|  0.00%|            used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.\n",
      "    89|         0|            0|            0|  0.00%|\n",
      "    90|         0|            0|            0|  0.00%|\n",
      "    91|         0|            0|            0|  0.00%|    .. warning:\n",
      "    92|         0|            0|            0|  0.00%|        Enabling memory profiling or source attribution incurs additional profiler\n",
      "    93|         0|            0|            0|  0.00%|        overhead\n",
      "    94|         0|            0|            0|  0.00%|\n",
      "    95|         0|            0|            0|  0.00%|    .. warning:\n",
      "    96|         0|            0|            0|  0.00%|        This context managers should not be called recursively, i.e. no nested\n",
      "    97|         0|            0|            0|  0.00%|        instances are allowed\n",
      "    98|         0|            0|            0|  0.00%|\n",
      "    99|         0|            0|            0|  0.00%|    .. warning:\n",
      "   100|         0|            0|            0|  0.00%|        Due to some CUDA multiprocessing limitations (multiprocessing-cuda-note_),\n",
      "   101|         0|            0|            0|  0.00%|        one cannot use the profiler with ``use_cuda = True`` to benchmark\n",
      "   102|         0|            0|            0|  0.00%|        DataLoaders with ``num_workers > 0``. If you wish to benchmark data loading,\n",
      "   103|         0|            0|            0|  0.00%|        please use ``use_cuda = False`` or ``num_workers = 0``.\n",
      "   104|         0|            0|            0|  0.00%|\n",
      "   105|         0|            0|            0|  0.00%|    Example:\n",
      "   106|         0|            0|            0|  0.00%|        >>> x = torch.randn((1, 1), requires_grad=True)\n",
      "   107|         0|            0|            0|  0.00%|        >>> with torch.autograd.profiler.profile() as prof:\n",
      "   108|         0|            0|            0|  0.00%|        >>>     for _ in range(100):  # any normal python code, really!\n",
      "   109|         0|            0|            0|  0.00%|        >>>         y = x ** 2\n",
      "   110|         0|            0|            0|  0.00%|        >>          y.backward()\n",
      "   111|         0|            0|            0|  0.00%|        >>> # NOTE: some columns were removed for brevity\n",
      "   112|         0|            0|            0|  0.00%|        >>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n",
      "   113|         0|            0|            0|  0.00%|        -----------------------------------  ---------------  ---------------  ---------------\n",
      "   114|         0|            0|            0|  0.00%|        Name                                 Self CPU total   CPU time avg     Number of Calls\n",
      "   115|         0|            0|            0|  0.00%|        -----------------------------------  ---------------  ---------------  ---------------\n",
      "   116|         0|            0|            0|  0.00%|        mul                                  32.048ms         32.048ms         200\n",
      "   117|         0|            0|            0|  0.00%|        pow                                  27.041ms         27.041ms         200\n",
      "   118|         0|            0|            0|  0.00%|        PowBackward0                         9.727ms          55.483ms         100\n",
      "   119|         0|            0|            0|  0.00%|        torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n",
      "   120|         0|            0|            0|  0.00%|        torch::autograd::GraphRoot           691.816us        691.816us        100\n",
      "   121|         0|            0|            0|  0.00%|        -----------------------------------  ---------------  ---------------  ---------------\n",
      "   122|         0|            0|            0|  0.00%|\n",
      "   123|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   124|         0|            0|            0|  0.00%|    def __init__(\n",
      "   125|         0|            0|            0|  0.00%|            self,\n",
      "   126|         0|            0|            0|  0.00%|            enabled=True,\n",
      "   127|         0|            0|            0|  0.00%|            *,\n",
      "   128|         0|            0|            0|  0.00%|            use_cuda=False,\n",
      "   129|         0|            0|            0|  0.00%|            record_shapes=False,\n",
      "   130|         0|            0|            0|  0.00%|            with_flops=False,\n",
      "   131|         0|            0|            0|  0.00%|            profile_memory=False,\n",
      "   132|         0|            0|            0|  0.00%|            with_stack=False,\n",
      "   133|         0|            0|            0|  0.00%|            with_modules=False,\n",
      "   134|         0|            0|            0|  0.00%|            use_kineto=False,\n",
      "   135|         0|            0|            0|  0.00%|            use_cpu=True,\n",
      "   136|         0|            0|            0|  0.00%|            experimental_config=None):\n",
      "   137|         0|            0|            0|  0.00%|        self.enabled: bool = enabled\n",
      "   138|         0|            0|            0|  0.00%|        if not self.enabled:\n",
      "   139|         0|            0|            0|  0.00%|            return\n",
      "   140|         0|            0|            0|  0.00%|        self.use_cuda = use_cuda\n",
      "   141|         0|            0|            0|  0.00%|        self.function_events: Optional[EventList] = None\n",
      "   142|         0|            0|            0|  0.00%|        self.entered = False\n",
      "   143|         0|            0|            0|  0.00%|        self.record_shapes = record_shapes\n",
      "   144|         0|            0|            0|  0.00%|        self.with_flops = with_flops\n",
      "   145|         0|            0|            0|  0.00%|        self.record_shapes |= self.with_flops\n",
      "   146|         0|            0|            0|  0.00%|        self.profile_memory = profile_memory\n",
      "   147|         0|            0|            0|  0.00%|        self.with_stack = with_stack\n",
      "   148|         0|            0|            0|  0.00%|        self.with_modules = with_modules\n",
      "   149|         0|            0|            0|  0.00%|        self.use_cpu = use_cpu\n",
      "   150|         0|            0|            0|  0.00%|        if experimental_config is None:\n",
      "   151|         0|            0|            0|  0.00%|            experimental_config = _ExperimentalConfig()\n",
      "   152|         0|            0|            0|  0.00%|        self.experimental_config = experimental_config\n",
      "   153|         0|            0|            0|  0.00%|        self.kineto_results: Optional[_ProfilerResult] = None\n",
      "   154|         0|            0|            0|  0.00%|\n",
      "   155|         0|            0|            0|  0.00%|        if not self.use_cpu:\n",
      "   156|         0|            0|            0|  0.00%|            assert use_kineto, \\\n",
      "   157|         0|            0|            0|  0.00%|                \"Device-only events supported only with Kineto (use_kineto=True)\"\n",
      "   158|         0|            0|            0|  0.00%|\n",
      "   159|         0|            0|            0|  0.00%|        if self.use_cuda and not torch.cuda.is_available():\n",
      "   160|         0|            0|            0|  0.00%|            warn(\"CUDA is not available, disabling CUDA profiling\")\n",
      "   161|         0|            0|            0|  0.00%|            self.use_cuda = False\n",
      "   162|         0|            0|            0|  0.00%|\n",
      "   163|         0|            0|            0|  0.00%|        self.kineto_activities = set()\n",
      "   164|         0|            0|            0|  0.00%|        if self.use_cpu:\n",
      "   165|         0|            0|            0|  0.00%|            self.kineto_activities.add(ProfilerActivity.CPU)\n",
      "   166|         0|            0|            0|  0.00%|\n",
      "   167|         0|            0|            0|  0.00%|        self.profiler_kind = ProfilerState.KINETO\n",
      "   168|         0|            0|            0|  0.00%|        if self.use_cuda:\n",
      "   169|         0|            0|            0|  0.00%|            if (not use_kineto or ProfilerActivity.CUDA not in\n",
      "   170|         0|            0|            0|  0.00%|                    _supported_activities()):\n",
      "   171|         0|            0|            0|  0.00%|                assert self.use_cpu, \"Legacy CUDA profiling requires use_cpu=True\"\n",
      "   172|         0|            0|            0|  0.00%|                self.profiler_kind = ProfilerState.KINETO_GPU_FALLBACK\n",
      "   173|         0|            0|            0|  0.00%|            else:\n",
      "   174|         0|            0|            0|  0.00%|                self.kineto_activities.add(ProfilerActivity.CUDA)\n",
      "   175|         0|            0|            0|  0.00%|\n",
      "   176|         0|            0|            0|  0.00%|        assert len(self.kineto_activities) > 0, \\\n",
      "   177|         0|            0|            0|  0.00%|            \"No activities specified for the profiler\"\n",
      "   178|         0|            0|            0|  0.00%|\n",
      "   179|         0|            0|            0|  0.00%|\n",
      "   180|         0|            0|            0|  0.00%|    def config(self):\n",
      "   181|         0|            0|            0|  0.00%|        return ProfilerConfig(\n",
      "   182|         0|            0|            0|  0.00%|            self.profiler_kind,\n",
      "   183|         0|            0|            0|  0.00%|            self.record_shapes,\n",
      "   184|         0|            0|            0|  0.00%|            self.profile_memory,\n",
      "   185|         0|            0|            0|  0.00%|            self.with_stack,\n",
      "   186|         0|            0|            0|  0.00%|            self.with_flops,\n",
      "   187|         0|            0|            0|  0.00%|            self.with_modules,\n",
      "   188|         0|            0|            0|  0.00%|            self.experimental_config)\n",
      "   189|         0|            0|            0|  0.00%|\n",
      "   190|         0|            0|            0|  0.00%|    def __enter__(self):\n",
      "   191|         0|            0|            0|  0.00%|        if not self.enabled:\n",
      "   192|         0|            0|            0|  0.00%|            return\n",
      "   193|         0|            0|            0|  0.00%|        if self.entered:\n",
      "   194|         0|            0|            0|  0.00%|            raise RuntimeError(\"Profiler context manager is not reentrant\")\n",
      "   195|         0|            0|            0|  0.00%|        self._prepare_trace()\n",
      "   196|         0|            0|            0|  0.00%|        self._start_trace()\n",
      "   197|         0|            0|            0|  0.00%|        return self\n",
      "   198|         0|            0|            0|  0.00%|\n",
      "   199|         0|            0|            0|  0.00%|    def _prepare_trace(self):\n",
      "   200|         0|            0|            0|  0.00%|        self.entered = True\n",
      "   201|         0|            0|            0|  0.00%|        _prepare_profiler(self.config(), self.kineto_activities)\n",
      "   202|         0|            0|            0|  0.00%|\n",
      "   203|         0|            0|            0|  0.00%|    def _start_trace(self):\n",
      "   204|         0|            0|            0|  0.00%|        self.entered = True\n",
      "   205|         0|            0|            0|  0.00%|        _enable_profiler(self.config(), self.kineto_activities)\n",
      "   206|         0|            0|            0|  0.00%|\n",
      "   207|         0|            0|            0|  0.00%|    def __exit__(self, exc_type, exc_val, exc_tb):\n",
      "   208|         0|            0|            0|  0.00%|        if not self.enabled:\n",
      "   209|         0|            0|            0|  0.00%|            return\n",
      "   210|         0|            0|            0|  0.00%|        if self.use_cuda:\n",
      "   211|         0|            0|            0|  0.00%|            torch.cuda.synchronize()\n",
      "   212|         0|            0|            0|  0.00%|        self.kineto_results = _disable_profiler()\n",
      "   213|         0|            0|            0|  0.00%|        parsed_results = self._parse_kineto_results(self.kineto_results)\n",
      "   214|         0|            0|            0|  0.00%|        self.function_events = EventList(\n",
      "   215|         0|            0|            0|  0.00%|            parsed_results,\n",
      "   216|         0|            0|            0|  0.00%|            use_cuda=self.use_cuda,\n",
      "   217|         0|            0|            0|  0.00%|            profile_memory=self.profile_memory,\n",
      "   218|         0|            0|            0|  0.00%|            with_flops=self.with_flops)\n",
      "   219|         0|            0|            0|  0.00%|        self.function_events._build_tree()\n",
      "   220|         0|            0|            0|  0.00%|        return False\n",
      "   221|         0|            0|            0|  0.00%|\n",
      "   222|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "   223|         0|            0|            0|  0.00%|        if self.function_events is None:\n",
      "   224|         0|            0|            0|  0.00%|            return '<unfinished torch.autograd.profile>'\n",
      "   225|         0|            0|            0|  0.00%|        return repr(self.function_events)\n",
      "   226|         0|            0|            0|  0.00%|\n",
      "   227|         0|            0|            0|  0.00%|    def __str__(self):\n",
      "   228|         0|            0|            0|  0.00%|        if self.function_events is None:\n",
      "   229|         0|            0|            0|  0.00%|            return '<unfinished torch.autograd.profile>'\n",
      "   230|         0|            0|            0|  0.00%|        return str(self.function_events)\n",
      "   231|         0|            0|            0|  0.00%|\n",
      "   232|         0|            0|            0|  0.00%|    def _check_finish(self):\n",
      "   233|         0|            0|            0|  0.00%|        if self.function_events is None:\n",
      "   234|         0|            0|            0|  0.00%|            raise RuntimeError(\"Profiler didn't finish running\")\n",
      "   235|         0|            0|            0|  0.00%|\n",
      "   236|         0|            0|            0|  0.00%|    def table(self, sort_by=None, row_limit=100, max_src_column_width=75, header=None, top_level_events_only=False):\n",
      "   237|         0|            0|            0|  0.00%|        self._check_finish()\n",
      "   238|         0|            0|            0|  0.00%|        assert self.function_events is not None\n",
      "   239|         0|            0|            0|  0.00%|        return self.function_events.table(\n",
      "   240|         0|            0|            0|  0.00%|            sort_by=sort_by, row_limit=row_limit, max_src_column_width=max_src_column_width, header=header,\n",
      "   241|         0|            0|            0|  0.00%|            top_level_events_only=top_level_events_only\n",
      "   242|         0|            0|            0|  0.00%|        )\n",
      "   243|         0|            0|            0|  0.00%|    table.__doc__ = EventList.table.__doc__\n",
      "   244|         0|            0|            0|  0.00%|\n",
      "   245|         0|            0|            0|  0.00%|    def export_chrome_trace(self, path):\n",
      "   246|         0|            0|            0|  0.00%|        self._check_finish()\n",
      "   247|         0|            0|            0|  0.00%|        if kineto_available():\n",
      "   248|         0|            0|            0|  0.00%|            self.kineto_results.save(path)  # type: ignore[union-attr]\n",
      "   249|         0|            0|            0|  0.00%|        else:\n",
      "   250|         0|            0|            0|  0.00%|            return self.function_events.export_chrome_trace(path)  # type: ignore[union-attr]\n",
      "   251|         0|            0|            0|  0.00%|    export_chrome_trace.__doc__ = EventList.export_chrome_trace.__doc__\n",
      "   252|         0|            0|            0|  0.00%|\n",
      "   253|         0|            0|            0|  0.00%|    def export_stacks(self, path: str, metric: str = \"self_cpu_time_total\"):\n",
      "   254|         0|            0|            0|  0.00%|        self._check_finish()\n",
      "   255|         0|            0|            0|  0.00%|        assert self.function_events is not None, \"Expected profiling results\"\n",
      "   256|         0|            0|            0|  0.00%|        assert self.with_stack, \"export_stacks() requires with_stack=True\"\n",
      "   257|         0|            0|            0|  0.00%|        return self.function_events.export_stacks(path, metric)\n",
      "   258|         0|            0|            0|  0.00%|\n",
      "   259|         0|            0|            0|  0.00%|    def key_averages(self, group_by_input_shape=False, group_by_stack_n=0):\n",
      "   260|         0|            0|            0|  0.00%|        self._check_finish()\n",
      "   261|         0|            0|            0|  0.00%|        assert self.function_events is not None, \"Expected profiling results\"\n",
      "   262|         0|            0|            0|  0.00%|        return self.function_events.key_averages(group_by_input_shape, group_by_stack_n)\n",
      "   263|         0|            0|            0|  0.00%|    key_averages.__doc__ = EventList.key_averages.__doc__\n",
      "   264|         0|            0|            0|  0.00%|\n",
      "   265|         0|            0|            0|  0.00%|    def total_average(self):\n",
      "   266|         0|            0|            0|  0.00%|        self._check_finish()\n",
      "   267|         0|            0|            0|  0.00%|        assert self.function_events is not None, \"Expected profiling results\"\n",
      "   268|         0|            0|            0|  0.00%|        return self.function_events.total_average()\n",
      "   269|         0|            0|            0|  0.00%|    total_average.__doc__ = EventList.total_average.__doc__\n",
      "   270|         0|            0|            0|  0.00%|\n",
      "   271|         0|            0|            0|  0.00%|    @property\n",
      "   272|         0|            0|            0|  0.00%|    def self_cpu_time_total(self):\n",
      "   273|         0|            0|            0|  0.00%|        \"\"\" Returns total time spent on CPU obtained as a sum of\n",
      "   274|         0|            0|            0|  0.00%|        all self times across all the events.\n",
      "   275|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   276|         0|            0|            0|  0.00%|        self._check_finish()\n",
      "   277|         0|            0|            0|  0.00%|        assert self.function_events is not None\n",
      "   278|         0|            0|            0|  0.00%|        return self.function_events.self_cpu_time_total\n",
      "   279|         0|            0|            0|  0.00%|\n",
      "   280|         0|            0|            0|  0.00%|    def _parse_kineto_results(self, result):\n",
      "   281|         0|            0|            0|  0.00%|        # result.events() has most of the events - PyTorch op-level and device-level events\n",
      "   282|         0|            0|            0|  0.00%|\n",
      "   283|         0|            0|            0|  0.00%|        trace_start_us = result.trace_start_us()\n",
      "   284|         0|            0|            0|  0.00%|        mem_records = [[evt, False] for evt in result.events() if evt.name() == MEMORY_EVENT_NAME]\n",
      "   285|         0|            0|            0|  0.00%|        mem_records_acc = MemRecordsAcc(mem_records)\n",
      "   286|         0|            0|            0|  0.00%|\n",
      "   287|         0|            0|            0|  0.00%|        def _cpu_memory_usage(mem_record):\n",
      "   288|         0|            0|            0|  0.00%|            return mem_record.nbytes() if \\\n",
      "   289|         0|            0|            0|  0.00%|                mem_record.device_type() in [DeviceType.CPU, DeviceType.MKLDNN, DeviceType.IDEEP] \\\n",
      "   290|         0|            0|            0|  0.00%|                else 0\n",
      "   291|         0|            0|            0|  0.00%|\n",
      "   292|         0|            0|            0|  0.00%|        def _cuda_memory_usage(mem_record):\n",
      "   293|         0|            0|            0|  0.00%|            return mem_record.nbytes() if \\\n",
      "   294|         0|            0|            0|  0.00%|                mem_record.device_type() in [DeviceType.CUDA, DeviceType.HIP] \\\n",
      "   295|         0|            0|            0|  0.00%|                else 0\n",
      "   296|         0|            0|            0|  0.00%|\n",
      "   297|         0|            0|            0|  0.00%|        # Create and return FunctionEvent list\n",
      "   298|         0|            0|            0|  0.00%|        function_events = []\n",
      "   299|         0|            0|            0|  0.00%|        cuda_corr_map: Dict[int, List[FunctionEvent]] = {}\n",
      "   300|         0|            0|            0|  0.00%|        max_evt_id = 0\n",
      "   301|         0|            0|            0|  0.00%|        for kineto_event in result.events():\n",
      "   302|         0|            0|            0|  0.00%|            if _filter_name(kineto_event.name()):\n",
      "   303|         0|            0|            0|  0.00%|                continue\n",
      "   304|         0|            0|            0|  0.00%|            rel_start_us = kineto_event.start_us() - trace_start_us\n",
      "   305|         0|            0|            0|  0.00%|            rel_end_us = rel_start_us + kineto_event.duration_us()\n",
      "   306|         0|            0|            0|  0.00%|            abs_end_us = kineto_event.start_us() + kineto_event.duration_us()\n",
      "   307|         0|            0|            0|  0.00%|\n",
      "   308|         0|            0|            0|  0.00%|            cpu_memory_usage = 0\n",
      "   309|         0|            0|            0|  0.00%|            cuda_memory_usage = 0\n",
      "   310|         0|            0|            0|  0.00%|            if kineto_event.device_type() == DeviceType.CPU:\n",
      "   311|         0|            0|            0|  0.00%|                # find the corresponding memory allocation events\n",
      "   312|         0|            0|            0|  0.00%|                for mem_record in mem_records_acc.in_interval(kineto_event.start_us(), abs_end_us):\n",
      "   313|         0|            0|            0|  0.00%|                    cpu_memory_usage += _cpu_memory_usage(mem_record[0])\n",
      "   314|         0|            0|            0|  0.00%|                    cuda_memory_usage += _cuda_memory_usage(mem_record[0])\n",
      "   315|         0|            0|            0|  0.00%|                    mem_record[1] = True\n",
      "   316|         0|            0|            0|  0.00%|\n",
      "   317|         0|            0|            0|  0.00%|            is_async = kineto_event.is_async() or (\n",
      "   318|         0|            0|            0|  0.00%|                kineto_event.start_thread_id() != kineto_event.end_thread_id()\n",
      "   319|         0|            0|            0|  0.00%|            )\n",
      "   320|         0|            0|            0|  0.00%|\n",
      "   321|         0|            0|            0|  0.00%|            fe = FunctionEvent(\n",
      "   322|         0|            0|            0|  0.00%|                id=kineto_event.correlation_id(),\n",
      "   323|         0|            0|            0|  0.00%|                name=_rewrite_name(name=kineto_event.name(), with_wildcard=True),\n",
      "   324|         0|            0|            0|  0.00%|                trace_name=_rewrite_name(name=kineto_event.name(), with_wildcard=False),\n",
      "   325|         0|            0|            0|  0.00%|                thread=kineto_event.start_thread_id(),\n",
      "   326|         0|            0|            0|  0.00%|                start_us=rel_start_us,\n",
      "   327|         0|            0|            0|  0.00%|                end_us=rel_end_us,\n",
      "   328|         0|            0|            0|  0.00%|                fwd_thread=kineto_event.fwd_thread_id(),\n",
      "   329|         0|            0|            0|  0.00%|                input_shapes=kineto_event.shapes(),\n",
      "   330|         0|            0|            0|  0.00%|                stack=[entry for entry in kineto_event.stack() if _filter_stack_entry(entry)],\n",
      "   331|         0|            0|            0|  0.00%|                scope=kineto_event.scope(),\n",
      "   332|         0|            0|            0|  0.00%|                cpu_memory_usage=cpu_memory_usage,\n",
      "   333|         0|            0|            0|  0.00%|                cuda_memory_usage=cuda_memory_usage,\n",
      "   334|         0|            0|            0|  0.00%|                is_async=is_async,\n",
      "   335|         0|            0|            0|  0.00%|                sequence_nr=kineto_event.sequence_nr(),\n",
      "   336|         0|            0|            0|  0.00%|                device_type=kineto_event.device_type(),\n",
      "   337|         0|            0|            0|  0.00%|                device_index=kineto_event.device_index(),\n",
      "   338|         0|            0|            0|  0.00%|                flops=kineto_event.flops(),\n",
      "   339|         0|            0|            0|  0.00%|            )\n",
      "   340|         0|            0|            0|  0.00%|            max_evt_id = fe.id if fe.id > max_evt_id else max_evt_id\n",
      "   341|         0|            0|            0|  0.00%|            if fe.device_type == DeviceType.CPU and not fe.is_async:\n",
      "   342|         0|            0|            0|  0.00%|                # Check if we have CUDA time as a fallback\n",
      "   343|         0|            0|            0|  0.00%|                cuda_time = kineto_event.cuda_elapsed_us()\n",
      "   344|         0|            0|            0|  0.00%|                if cuda_time > 0:\n",
      "   345|         0|            0|            0|  0.00%|                    fe.append_kernel(\n",
      "   346|         0|            0|            0|  0.00%|                        fe.name,\n",
      "   347|         0|            0|            0|  0.00%|                        fe.device_index,\n",
      "   348|         0|            0|            0|  0.00%|                        cuda_time)\n",
      "   349|         0|            0|            0|  0.00%|                    fe.is_legacy = True\n",
      "   350|         0|            0|            0|  0.00%|            function_events.append(fe)\n",
      "   351|         0|            0|            0|  0.00%|            corr_id = kineto_event.linked_correlation_id()\n",
      "   352|         0|            0|            0|  0.00%|            if corr_id > 0:\n",
      "   353|         0|            0|            0|  0.00%|                if corr_id not in cuda_corr_map:\n",
      "   354|         0|            0|            0|  0.00%|                    cuda_corr_map[corr_id] = []\n",
      "   355|         0|            0|            0|  0.00%|                cuda_corr_map[corr_id].append(fe)\n",
      "   356|         0|            0|            0|  0.00%|\n",
      "   357|         0|            0|            0|  0.00%|        # associate CUDA kernels and CUDA runtime (CPU) with CPU events\n",
      "   358|         0|            0|            0|  0.00%|        for fe in function_events:\n",
      "   359|         0|            0|            0|  0.00%|            if (fe.device_type == DeviceType.CPU and not fe.is_async and\n",
      "   360|         0|            0|            0|  0.00%|                    fe.id in cuda_corr_map):\n",
      "   361|         0|            0|            0|  0.00%|                for f_evt in cuda_corr_map[fe.id]:\n",
      "   362|         0|            0|            0|  0.00%|                    if f_evt.device_type == DeviceType.CUDA:\n",
      "   363|         0|            0|            0|  0.00%|                        fe.append_kernel(\n",
      "   364|         0|            0|            0|  0.00%|                            f_evt.name,\n",
      "   365|         0|            0|            0|  0.00%|                            f_evt.device_index,\n",
      "   366|         0|            0|            0|  0.00%|                            f_evt.time_range.end - f_evt.time_range.start)\n",
      "   367|         0|            0|            0|  0.00%|                    elif f_evt.device_type == DeviceType.CPU:\n",
      "   368|         0|            0|            0|  0.00%|                        # make sure that 'thread' of a CPU Kineto (e.g. CUDA Runtime) event is associated\n",
      "   369|         0|            0|            0|  0.00%|                        # with the 'thread' of the corresponding linked PyTorch event to properly track\n",
      "   370|         0|            0|            0|  0.00%|                        # parents and children\n",
      "   371|         0|            0|            0|  0.00%|                        f_evt.thread = fe.thread\n",
      "   372|         0|            0|            0|  0.00%|\n",
      "   373|         0|            0|            0|  0.00%|        # output top-level memory events\n",
      "   374|         0|            0|            0|  0.00%|        for mem_record in mem_records:\n",
      "   375|         0|            0|            0|  0.00%|            if not mem_record[1]:\n",
      "   376|         0|            0|            0|  0.00%|                rel_start_us = mem_record[0].start_us() - trace_start_us\n",
      "   377|         0|            0|            0|  0.00%|                max_evt_id += 1\n",
      "   378|         0|            0|            0|  0.00%|                fe = FunctionEvent(\n",
      "   379|         0|            0|            0|  0.00%|                    id=max_evt_id,\n",
      "   380|         0|            0|            0|  0.00%|                    name=MEMORY_EVENT_NAME,\n",
      "   381|         0|            0|            0|  0.00%|                    trace_name=None,  # not outputting in the trace\n",
      "   382|         0|            0|            0|  0.00%|                    thread=mem_record[0].start_thread_id(),\n",
      "   383|         0|            0|            0|  0.00%|                    start_us=rel_start_us,\n",
      "   384|         0|            0|            0|  0.00%|                    end_us=rel_start_us,  # no duration\n",
      "   385|         0|            0|            0|  0.00%|                    fwd_thread=mem_record[0].start_thread_id(),\n",
      "   386|         0|            0|            0|  0.00%|                    input_shapes=[],\n",
      "   387|         0|            0|            0|  0.00%|                    stack=[],\n",
      "   388|         0|            0|            0|  0.00%|                    scope=0,  # RecordScope::FUNCTION\n",
      "   389|         0|            0|            0|  0.00%|                    cpu_memory_usage=_cpu_memory_usage(mem_record[0]),\n",
      "   390|         0|            0|            0|  0.00%|                    cuda_memory_usage=_cuda_memory_usage(mem_record[0]),\n",
      "   391|         0|            0|            0|  0.00%|                    is_async=False,\n",
      "   392|         0|            0|            0|  0.00%|                    sequence_nr=-1,\n",
      "   393|         0|            0|            0|  0.00%|                    device_type=DeviceType.CPU,\n",
      "   394|         0|            0|            0|  0.00%|                    device_index=0,\n",
      "   395|         0|            0|            0|  0.00%|                )\n",
      "   396|         0|            0|            0|  0.00%|                function_events.append(fe)\n",
      "   397|         0|            0|            0|  0.00%|\n",
      "   398|         0|            0|            0|  0.00%|        function_events.sort(key=lambda evt: [evt.time_range.start, -evt.time_range.end])\n",
      "   399|         0|            0|            0|  0.00%|        return function_events\n",
      "   400|         0|            0|            0|  0.00%|\n",
      "   401|         0|            0|            0|  0.00%|\n",
      "   402|         0|            0|            0|  0.00%|class record_function(ContextDecorator):\n",
      "   403|         0|            0|            0|  0.00%|    \"\"\"Context manager/function decorator that adds a label to a block of\n",
      "   404|         0|            0|            0|  0.00%|    Python code (or function) when running autograd profiler. It is\n",
      "   405|         0|            0|            0|  0.00%|    useful when tracing the code profile.\n",
      "   406|         0|            0|            0|  0.00%|\n",
      "   407|         0|            0|            0|  0.00%|    Args:\n",
      "   408|         0|            0|            0|  0.00%|        name (str): Label assigned to the block of code.\n",
      "   409|         0|            0|            0|  0.00%|        node_id (int): ID of node, for distributed profiling. Unset in\n",
      "   410|         0|            0|            0|  0.00%|        non-distributed cases.\n",
      "   411|         0|            0|            0|  0.00%|\n",
      "   412|         0|            0|            0|  0.00%|    Example:\n",
      "   413|         0|            0|            0|  0.00%|        >>> x = torch.randn((1, 1), requires_grad=True)\n",
      "   414|         0|            0|            0|  0.00%|        >>> with torch.autograd.profiler.profile() as prof:\n",
      "   415|         0|            0|            0|  0.00%|        ...     y = x ** 2\n",
      "   416|         0|            0|            0|  0.00%|        ...     with torch.autograd.profiler.record_function(\"label-z\"): # label the block\n",
      "   417|         0|            0|            0|  0.00%|        ...         z = y ** 3\n",
      "   418|         0|            0|            0|  0.00%|        ...     y.backward()\n",
      "   419|         0|            0|            0|  0.00%|        ...\n",
      "   420|         0|            0|            0|  0.00%|        >>> # NOTE: some columns were removed for brevity\n",
      "   421|         0|            0|            0|  0.00%|        >>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n",
      "   422|         0|            0|            0|  0.00%|        -----------------------------------  ---------------  ---------------  ---------------\n",
      "   423|         0|            0|            0|  0.00%|        Name                                 Self CPU total %  CPU time avg     Number of Calls\n",
      "   424|         0|            0|            0|  0.00%|        -----------------------------------  ---------------  ---------------  ---------------\n",
      "   425|         0|            0|            0|  0.00%|        pow                                  60.77%           47.470us         3\n",
      "   426|         0|            0|            0|  0.00%|        mul                                  21.73%           25.465us         2\n",
      "   427|         0|            0|            0|  0.00%|        PowBackward0                         12.03%           121.891us        1\n",
      "   428|         0|            0|            0|  0.00%|        torch::autograd::AccumulateGrad      2.70%            6.324us          1\n",
      "   429|         0|            0|            0|  0.00%|        label-z                              2.13%            12.421us         1\n",
      "   430|         0|            0|            0|  0.00%|        torch::autograd::GraphRoot           0.64%            1.503us          1\n",
      "   431|         0|            0|            0|  0.00%|        -----------------------------------  ---------------  ---------------  ---------------\n",
      "   432|         0|            0|            0|  0.00%|        Self CPU time total: 234.344us\n",
      "   433|         0|            0|            0|  0.00%|        CUDA time total: 0.000us\n",
      "   434|         0|            0|            0|  0.00%|\n",
      "   435|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   436|     12480|    0.0230086|  1.84364e-06|  0.00%|    def __init__(self, name: str, args: Optional[str] = None):\n",
      "   437|     12480|      0.03421|  2.74118e-06|  0.01%|        self.name: str = name\n",
      "   438|     12480|    0.0210767|  1.68884e-06|  0.00%|        self.args: Optional[str] = args\n",
      "   439|         0|            0|            0|  0.00%|        # Whether or not we should run record function's end callbacks when exiting.\n",
      "   440|     12480|    0.0206993|  1.65859e-06|  0.00%|        self.run_callbacks_on_exit: bool = True\n",
      "   441|         0|            0|            0|  0.00%|        # Stores underlying RecordFunction as a tensor. TODO: move to custom\n",
      "   442|         0|            0|            0|  0.00%|        # class (https://github.com/pytorch/pytorch/issues/35026).\n",
      "   443|     12480|     0.108955|  8.73036e-06|  0.02%|        self.handle: torch.Tensor = torch.zeros(1)\n",
      "   444|         0|            0|            0|  0.00%|\n",
      "   445|     12480|    0.0200019|  1.60272e-06|  0.00%|    def __enter__(self):\n",
      "   446|     12480|     0.110376|   8.8442e-06|  0.02%|        self.handle = torch.ops.profiler._record_function_enter(self.name, self.args)\n",
      "(call)|     12480|     0.173942|  1.39377e-05|  0.03%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_ops.py:138 __call__\n",
      "   447|     12480|    0.0213647|  1.71191e-06|  0.00%|        return self\n",
      "   448|         0|            0|            0|  0.00%|\n",
      "   449|     12480|    0.0213277|  1.70895e-06|  0.00%|    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any):\n",
      "   450|     12480|    0.0255797|  2.04965e-06|  0.00%|        if self.run_callbacks_on_exit:\n",
      "   451|     12480|    0.0917969|  7.35552e-06|  0.01%|            torch.ops.profiler._record_function_exit(self.handle)\n",
      "(call)|     12480|     0.105013|  8.41449e-06|  0.02%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_ops.py:138 __call__\n",
      "   452|         0|            0|            0|  0.00%|\n",
      "   453|         0|            0|            0|  0.00%|    def _call_end_callbacks_on_future(self, fut: Future[Any]) -> Future[Any]:\n",
      "   454|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   455|         0|            0|            0|  0.00%|        _call_end_callbacks_on_future is meant to be used for profiling async\n",
      "   456|         0|            0|            0|  0.00%|        calls that return a future. Calling this function will extend recording\n",
      "   457|         0|            0|            0|  0.00%|        beyond this scope, until the future is satisfied. It is useful for profiling\n",
      "   458|         0|            0|            0|  0.00%|        the end to end time of asynchronous calls. This function should only be called\n",
      "   459|         0|            0|            0|  0.00%|        once to attach the callback onto the future, and will throw if called multiple\n",
      "   460|         0|            0|            0|  0.00%|        times.\n",
      "   461|         0|            0|            0|  0.00%|\n",
      "   462|         0|            0|            0|  0.00%|        Args:\n",
      "   463|         0|            0|            0|  0.00%|            fut: (torch._C.Future): future for which to schedule\n",
      "   464|         0|            0|            0|  0.00%|            callback for.\n",
      "   465|         0|            0|            0|  0.00%|\n",
      "   466|         0|            0|            0|  0.00%|        Returns:\n",
      "   467|         0|            0|            0|  0.00%|            A future that completes with the value of the passed in future when\n",
      "   468|         0|            0|            0|  0.00%|            the profiling callbacks have ran.\n",
      "   469|         0|            0|            0|  0.00%|\n",
      "   470|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   471|         0|            0|            0|  0.00%|        # Throw if we have already attached a callback onto the future.\n",
      "   472|         0|            0|            0|  0.00%|        if not self.run_callbacks_on_exit:\n",
      "   473|         0|            0|            0|  0.00%|            raise RuntimeError(\"_call_end_callbacks_on_future can only be called once.\")\n",
      "   474|         0|            0|            0|  0.00%|\n",
      "   475|         0|            0|            0|  0.00%|        # We are scheduling to run this RecordFunction's end callbacks when the\n",
      "   476|         0|            0|            0|  0.00%|        # passed in future completes, so don't run end callbacks on exit.\n",
      "   477|         0|            0|            0|  0.00%|        self.run_callbacks_on_exit = False\n",
      "   478|         0|            0|            0|  0.00%|        profiled_future = torch.ops.profiler._call_end_callbacks_on_jit_fut(self.handle, fut)\n",
      "   479|         0|            0|            0|  0.00%|        return profiled_future\n",
      "   480|         0|            0|            0|  0.00%|\n",
      "   481|         0|            0|            0|  0.00%|\n",
      "   482|         0|            0|            0|  0.00%|class emit_nvtx(object):\n",
      "   483|         0|            0|            0|  0.00%|    \"\"\"Context manager that makes every autograd operation emit an NVTX range.\n",
      "   484|         0|            0|            0|  0.00%|\n",
      "   485|         0|            0|            0|  0.00%|    It is useful when running the program under nvprof::\n",
      "   486|         0|            0|            0|  0.00%|\n",
      "   487|         0|            0|            0|  0.00%|        nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n",
      "   488|         0|            0|            0|  0.00%|\n",
      "   489|         0|            0|            0|  0.00%|    Unfortunately, there's no way to force nvprof to flush the data it collected\n",
      "   490|         0|            0|            0|  0.00%|    to disk, so for CUDA profiling one has to use this context manager to annotate\n",
      "   491|         0|            0|            0|  0.00%|    nvprof traces and wait for the process to exit before inspecting them.\n",
      "   492|         0|            0|            0|  0.00%|    Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\n",
      "   493|         0|            0|            0|  0.00%|    :func:`torch.autograd.profiler.load_nvprof` can load the results for inspection\n",
      "   494|         0|            0|            0|  0.00%|    e.g. in Python REPL.\n",
      "   495|         0|            0|            0|  0.00%|\n",
      "   496|         0|            0|            0|  0.00%|    .. warning:\n",
      "   497|         0|            0|            0|  0.00%|        This context manager should not be called recursively, i.e. at most one\n",
      "   498|         0|            0|            0|  0.00%|        instance should be enabled at any given time.\n",
      "   499|         0|            0|            0|  0.00%|\n",
      "   500|         0|            0|            0|  0.00%|    Args:\n",
      "   501|         0|            0|            0|  0.00%|        enabled (bool, optional, default=True): Setting ``enabled=False`` makes this context manager a no-op.\n",
      "   502|         0|            0|            0|  0.00%|            Default: ``True``.\n",
      "   503|         0|            0|            0|  0.00%|        record_shapes (bool, optional, default=False): If ``record_shapes=True``, the nvtx range wrapping\n",
      "   504|         0|            0|            0|  0.00%|            each autograd op will append information about the sizes of Tensor arguments received\n",
      "   505|         0|            0|            0|  0.00%|            by that op, in the following format:\n",
      "   506|         0|            0|            0|  0.00%|            ``[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]``\n",
      "   507|         0|            0|            0|  0.00%|            Non-tensor arguments will be represented by ``[]``.\n",
      "   508|         0|            0|            0|  0.00%|            Arguments will be listed in the order they are received by the backend op.\n",
      "   509|         0|            0|            0|  0.00%|            Please note that this order may not match the order in which those arguments were passed\n",
      "   510|         0|            0|            0|  0.00%|            on the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.\n",
      "   511|         0|            0|            0|  0.00%|\n",
      "   512|         0|            0|            0|  0.00%|    Example:\n",
      "   513|         0|            0|            0|  0.00%|        >>> with torch.cuda.profiler.profile():\n",
      "   514|         0|            0|            0|  0.00%|        ...     model(x) # Warmup CUDA memory allocator and profiler\n",
      "   515|         0|            0|            0|  0.00%|        ...     with torch.autograd.profiler.emit_nvtx():\n",
      "   516|         0|            0|            0|  0.00%|        ...         model(x)\n",
      "   517|         0|            0|            0|  0.00%|\n",
      "   518|         0|            0|            0|  0.00%|    **Forward-backward correlation**\n",
      "   519|         0|            0|            0|  0.00%|\n",
      "   520|         0|            0|            0|  0.00%|    When viewing a profile created using :class:`emit_nvtx` in the Nvidia Visual Profiler,\n",
      "   521|         0|            0|            0|  0.00%|    correlating each backward-pass op with the corresponding forward-pass op can be difficult.\n",
      "   522|         0|            0|            0|  0.00%|    To ease this task, :class:`emit_nvtx` appends sequence number information to the ranges it\n",
      "   523|         0|            0|            0|  0.00%|    generates.\n",
      "   524|         0|            0|            0|  0.00%|\n",
      "   525|         0|            0|            0|  0.00%|    During the forward pass, each function range is decorated with ``seq=<N>``.  ``seq`` is a running\n",
      "   526|         0|            0|            0|  0.00%|    counter, incremented each time a new backward Function object is created and stashed for backward.\n",
      "   527|         0|            0|            0|  0.00%|    Thus, the ``seq=<N>`` annotation associated with each forward function range tells you that\n",
      "   528|         0|            0|            0|  0.00%|    if a backward Function object is created by this forward function,\n",
      "   529|         0|            0|            0|  0.00%|    the backward object will receive sequence number N.\n",
      "   530|         0|            0|            0|  0.00%|    During the backward pass, the top-level range wrapping each C++ backward Function's\n",
      "   531|         0|            0|            0|  0.00%|    ``apply()`` call is decorated with ``stashed seq=<M>``.  ``M`` is the sequence number that\n",
      "   532|         0|            0|            0|  0.00%|    the backward object was created with.  By comparing ``stashed seq`` numbers in backward with ``seq``\n",
      "   533|         0|            0|            0|  0.00%|    numbers in forward, you can track down which forward op created each backward Function.\n",
      "   534|         0|            0|            0|  0.00%|\n",
      "   535|         0|            0|            0|  0.00%|    Any functions executed during the backward pass are also decorated with ``seq=<N>``.  During\n",
      "   536|         0|            0|            0|  0.00%|    default backward (with ``create_graph=False``) this information is irrelevant, and in fact,\n",
      "   537|         0|            0|            0|  0.00%|    ``N`` may simply be 0 for all such functions.  Only the top-level ranges associated with\n",
      "   538|         0|            0|            0|  0.00%|    backward Function objects' ``apply()`` methods are useful, as a way to correlate these Function\n",
      "   539|         0|            0|            0|  0.00%|    objects with the earlier forward pass.\n",
      "   540|         0|            0|            0|  0.00%|\n",
      "   541|         0|            0|            0|  0.00%|    **Double-backward**\n",
      "   542|         0|            0|            0|  0.00%|\n",
      "   543|         0|            0|            0|  0.00%|    If, on the other hand, a backward pass with ``create_graph=True`` is underway (in other words,\n",
      "   544|         0|            0|            0|  0.00%|    if you are setting up for a double-backward), each function's execution during backward\n",
      "   545|         0|            0|            0|  0.00%|    is given a nonzero, useful ``seq=<N>``.  Those functions may themselves create Function objects\n",
      "   546|         0|            0|            0|  0.00%|    to be executed later during double-backward, just as the original functions in the forward pass did.\n",
      "   547|         0|            0|            0|  0.00%|    The relationship between backward and double-backward is conceptually the same as the relationship\n",
      "   548|         0|            0|            0|  0.00%|    between forward and backward: The functions still emit current-sequence-number-tagged ranges,\n",
      "   549|         0|            0|            0|  0.00%|    the Function objects they create still stash those sequence numbers, and during the eventual\n",
      "   550|         0|            0|            0|  0.00%|    double-backward, the Function objects' ``apply()`` ranges are still tagged with ``stashed seq``\n",
      "   551|         0|            0|            0|  0.00%|    numbers, which can be compared to `seq` numbers from the backward pass.\n",
      "   552|         0|            0|            0|  0.00%|\n",
      "   553|         0|            0|            0|  0.00%|    .. warning:\n",
      "   554|         0|            0|            0|  0.00%|        The sequence number is thread-local, and some forward functions don't create an associated\n",
      "   555|         0|            0|            0|  0.00%|        backward Function object (instead delegating that to sub-functions further down the call chain).\n",
      "   556|         0|            0|            0|  0.00%|        For these reasons, the correspondence of stashed sequence numbers in\n",
      "   557|         0|            0|            0|  0.00%|        backward Function ``apply()`` ranges with `seq` numbers in forward-pass ranges is\n",
      "   558|         0|            0|            0|  0.00%|        not guaranteed to be 1 to 1.  The sequence numbers alone may not be enough to fully\n",
      "   559|         0|            0|            0|  0.00%|        disambiguate which forward function created which\n",
      "   560|         0|            0|            0|  0.00%|        backward Function object.  You may need to make a judgment based on analytic knowledge of what\n",
      "   561|         0|            0|            0|  0.00%|        the expected correspondence should be.\n",
      "   562|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   563|         0|            0|            0|  0.00%|    def __init__(self, enabled=True, record_shapes=False):\n",
      "   564|         0|            0|            0|  0.00%|        self.enabled = enabled\n",
      "   565|         0|            0|            0|  0.00%|        self.entered = False\n",
      "   566|         0|            0|            0|  0.00%|        self.record_shapes = record_shapes\n",
      "   567|         0|            0|            0|  0.00%|\n",
      "   568|         0|            0|            0|  0.00%|    def __enter__(self):\n",
      "   569|         0|            0|            0|  0.00%|        if not self.enabled:\n",
      "   570|         0|            0|            0|  0.00%|            return\n",
      "   571|         0|            0|            0|  0.00%|        if self.entered:\n",
      "   572|         0|            0|            0|  0.00%|            raise RuntimeError(\"NVTX annotation context manager is not reentrant\")\n",
      "   573|         0|            0|            0|  0.00%|        self.entered = True\n",
      "   574|         0|            0|            0|  0.00%|        torch.cuda.synchronize()\n",
      "   575|         0|            0|            0|  0.00%|        _enable_profiler(\n",
      "   576|         0|            0|            0|  0.00%|            ProfilerConfig(\n",
      "   577|         0|            0|            0|  0.00%|                ProfilerState.NVTX,\n",
      "   578|         0|            0|            0|  0.00%|                self.record_shapes,\n",
      "   579|         0|            0|            0|  0.00%|                False,\n",
      "   580|         0|            0|            0|  0.00%|                False,\n",
      "   581|         0|            0|            0|  0.00%|                False,\n",
      "   582|         0|            0|            0|  0.00%|                False,\n",
      "   583|         0|            0|            0|  0.00%|                _ExperimentalConfig()),\n",
      "   584|         0|            0|            0|  0.00%|            set()\n",
      "   585|         0|            0|            0|  0.00%|        )\n",
      "   586|         0|            0|            0|  0.00%|        return self\n",
      "   587|         0|            0|            0|  0.00%|\n",
      "   588|         0|            0|            0|  0.00%|    def __exit__(self, exc_type, exc_val, exc_tb):\n",
      "   589|         0|            0|            0|  0.00%|        if not self.enabled:\n",
      "   590|         0|            0|            0|  0.00%|            return\n",
      "   591|         0|            0|            0|  0.00%|        torch.cuda.synchronize()\n",
      "   592|         0|            0|            0|  0.00%|        _disable_profiler()\n",
      "   593|         0|            0|            0|  0.00%|        return False\n",
      "   594|         0|            0|            0|  0.00%|\n",
      "   595|         0|            0|            0|  0.00%|\n",
      "   596|         0|            0|            0|  0.00%|def load_nvprof(path):\n",
      "   597|         0|            0|            0|  0.00%|    \"\"\"Opens an nvprof trace file and parses autograd annotations.\n",
      "   598|         0|            0|            0|  0.00%|\n",
      "   599|         0|            0|            0|  0.00%|    Args:\n",
      "   600|         0|            0|            0|  0.00%|        path (str): path to nvprof trace\n",
      "   601|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   602|         0|            0|            0|  0.00%|    return EventList(parse_nvprof_trace(path))\n",
      "   603|         0|            0|            0|  0.00%|\n",
      "   604|         0|            0|            0|  0.00%|\n",
      "   605|         0|            0|            0|  0.00%|class EnforceUnique(object):\n",
      "   606|         0|            0|            0|  0.00%|    \"\"\"Raises an error if a key is seen more than once.\"\"\"\n",
      "   607|         0|            0|            0|  0.00%|    def __init__(self):\n",
      "   608|         0|            0|            0|  0.00%|        self.seen = set()\n",
      "   609|         0|            0|            0|  0.00%|\n",
      "   610|         0|            0|            0|  0.00%|    def see(self, *key):\n",
      "   611|         0|            0|            0|  0.00%|        if key in self.seen:\n",
      "   612|         0|            0|            0|  0.00%|            raise RuntimeError('duplicate key: ' + str(key))\n",
      "   613|         0|            0|            0|  0.00%|        self.seen.add(key)\n",
      "   614|         0|            0|            0|  0.00%|\n",
      "   615|         0|            0|            0|  0.00%|\n",
      "   616|         0|            0|            0|  0.00%|def parse_nvprof_trace(path):\n",
      "   617|         0|            0|            0|  0.00%|    import sqlite3\n",
      "   618|         0|            0|            0|  0.00%|    conn = sqlite3.connect(path)\n",
      "   619|         0|            0|            0|  0.00%|    conn.row_factory = sqlite3.Row\n",
      "   620|         0|            0|            0|  0.00%|\n",
      "   621|         0|            0|            0|  0.00%|    # Parse strings table\n",
      "   622|         0|            0|            0|  0.00%|    strings = {}\n",
      "   623|         0|            0|            0|  0.00%|    for r in conn.execute(\"SELECT _id_ as id, value FROM StringTable\"):\n",
      "   624|         0|            0|            0|  0.00%|        strings[r[\"id\"]] = torch._C._demangle(r[\"value\"])\n",
      "   625|         0|            0|            0|  0.00%|\n",
      "   626|         0|            0|            0|  0.00%|    # First, find all functions and create FunctionEvents for them\n",
      "   627|         0|            0|            0|  0.00%|    marker_query = \"\"\"\n",
      "   628|         0|            0|            0|  0.00%|    SELECT\n",
      "   629|         0|            0|            0|  0.00%|        start.id AS marker_id, start.name, start.timestamp AS start_time, end.timestamp AS end_time\n",
      "   630|         0|            0|            0|  0.00%|    FROM\n",
      "   631|         0|            0|            0|  0.00%|        CUPTI_ACTIVITY_KIND_MARKER AS start INNER JOIN CUPTI_ACTIVITY_KIND_MARKER AS end\n",
      "   632|         0|            0|            0|  0.00%|        ON start.id = end.id\n",
      "   633|         0|            0|            0|  0.00%|    WHERE\n",
      "   634|         0|            0|            0|  0.00%|        start.name != 0 AND end.name = 0\n",
      "   635|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   636|         0|            0|            0|  0.00%|    functions = []\n",
      "   637|         0|            0|            0|  0.00%|    functions_map = {}\n",
      "   638|         0|            0|            0|  0.00%|    unique = EnforceUnique()\n",
      "   639|         0|            0|            0|  0.00%|    for row in conn.execute(marker_query):\n",
      "   640|         0|            0|            0|  0.00%|        unique.see(row['marker_id'])\n",
      "   641|         0|            0|            0|  0.00%|        evt = FunctionEvent(id=row['marker_id'],\n",
      "   642|         0|            0|            0|  0.00%|                            node_id=0,  # missing a node_id when calling FunctionEvent. This is just to ensure\n",
      "   643|         0|            0|            0|  0.00%|                                        # that pytorch doesn't crash when creating a FunctionEvent() object\n",
      "   644|         0|            0|            0|  0.00%|                            name=strings[row['name']],\n",
      "   645|         0|            0|            0|  0.00%|                            start_us=row['start_time'],\n",
      "   646|         0|            0|            0|  0.00%|                            end_us=row['end_time'],\n",
      "   647|         0|            0|            0|  0.00%|                            thread=0)  # TODO: find in sqlite database\n",
      "   648|         0|            0|            0|  0.00%|        functions.append(evt)\n",
      "   649|         0|            0|            0|  0.00%|        functions_map[evt.id] = evt\n",
      "   650|         0|            0|            0|  0.00%|\n",
      "   651|         0|            0|            0|  0.00%|    # Now, correlate all kernels with FunctionEvents\n",
      "   652|         0|            0|            0|  0.00%|    kernel_query = \"\"\"\n",
      "   653|         0|            0|            0|  0.00%|    SELECT\n",
      "   654|         0|            0|            0|  0.00%|        start.id AS marker_id, start.name, start.timestamp, end.timestamp,\n",
      "   655|         0|            0|            0|  0.00%|        runtime._id_ AS runtime_id, runtime.cbid, runtime.start AS runtime_start, runtime.end AS runtime_end,\n",
      "   656|         0|            0|            0|  0.00%|        kernel.start AS kernel_start, kernel.end AS kernel_end, kernel.name AS kernel_name\n",
      "   657|         0|            0|            0|  0.00%|    FROM\n",
      "   658|         0|            0|            0|  0.00%|        CUPTI_ACTIVITY_KIND_MARKER AS start\n",
      "   659|         0|            0|            0|  0.00%|        INNER JOIN CUPTI_ACTIVITY_KIND_MARKER AS end\n",
      "   660|         0|            0|            0|  0.00%|            ON start.id = end.id\n",
      "   661|         0|            0|            0|  0.00%|        INNER JOIN CUPTI_ACTIVITY_KIND_RUNTIME as runtime\n",
      "   662|         0|            0|            0|  0.00%|            ON (start.timestamp < runtime.start AND runtime.end < end.timestamp)\n",
      "   663|         0|            0|            0|  0.00%|        INNER JOIN CUPTI_ACTIVITY_KIND_CONCURRENT_KERNEL AS kernel\n",
      "   664|         0|            0|            0|  0.00%|            ON kernel.correlationId = runtime.correlationId\n",
      "   665|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   666|         0|            0|            0|  0.00%|    unique = EnforceUnique()\n",
      "   667|         0|            0|            0|  0.00%|    for row in conn.execute(kernel_query):\n",
      "   668|         0|            0|            0|  0.00%|        unique.see(row['marker_id'], row['runtime_id'])\n",
      "   669|         0|            0|            0|  0.00%|        # 211 is cudaKernelLaunch for cuda >= 9.2\n",
      "   670|         0|            0|            0|  0.00%|        assert (row['cbid'] == 211)\n",
      "   671|         0|            0|            0|  0.00%|        evt = functions_map[row['marker_id']]\n",
      "   672|         0|            0|            0|  0.00%|        evt.append_kernel(row['kernel_name'],\n",
      "   673|         0|            0|            0|  0.00%|                          0,\n",
      "   674|         0|            0|            0|  0.00%|                          row['kernel_end'] - row['kernel_start'])\n",
      "   675|         0|            0|            0|  0.00%|\n",
      "   676|         0|            0|            0|  0.00%|    functions.sort(key=lambda evt: evt.time_range.start)\n",
      "   677|         0|            0|            0|  0.00%|    return functions\n",
      "   678|         0|            0|            0|  0.00%|\n",
      "   679|         0|            0|            0|  0.00%|\n",
      "   680|         0|            0|            0|  0.00%|def kineto_step():\n",
      "   681|         0|            0|            0|  0.00%|    \"\"\" Notify kineto so it is aware of iteration boundaries for asynchronous\n",
      "   682|         0|            0|            0|  0.00%|        trace requests.\n",
      "   683|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   684|         0|            0|            0|  0.00%|    _kineto_step()\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_VF.py\n",
      "File duration: 0.417598s (0.07%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|\"\"\"\n",
      "     2|         0|            0|            0|  0.00%|This makes the functions in torch._C._VariableFunctions available as\n",
      "     3|         0|            0|            0|  0.00%|    torch._VF.<funcname>\n",
      "     4|         0|            0|            0|  0.00%|without mypy being able to find them.\n",
      "     5|         0|            0|            0|  0.00%|\n",
      "     6|         0|            0|            0|  0.00%|A subset of those functions are mapped to ATen functions in\n",
      "     7|         0|            0|            0|  0.00%|torch/jit/_builtins.py\n",
      "     8|         0|            0|            0|  0.00%|\n",
      "     9|         0|            0|            0|  0.00%|See https://github.com/pytorch/pytorch/issues/21478 for the reason for\n",
      "    10|         0|            0|            0|  0.00%|introducing torch._VF\n",
      "    11|         0|            0|            0|  0.00%|\n",
      "    12|         0|            0|            0|  0.00%|\"\"\"\n",
      "    13|         0|            0|            0|  0.00%|import torch\n",
      "    14|         0|            0|            0|  0.00%|import sys\n",
      "    15|         0|            0|            0|  0.00%|import types\n",
      "    16|         0|            0|            0|  0.00%|\n",
      "    17|         0|            0|            0|  0.00%|\n",
      "    18|         0|            0|            0|  0.00%|class VFModule(types.ModuleType):\n",
      "    19|         0|            0|            0|  0.00%|    vf: types.ModuleType\n",
      "    20|         0|            0|            0|  0.00%|\n",
      "    21|         0|            0|            0|  0.00%|    def __init__(self, name):\n",
      "    22|         0|            0|            0|  0.00%|        super(VFModule, self).__init__(name)\n",
      "    23|         0|            0|            0|  0.00%|        self.vf = torch._C._VariableFunctions\n",
      "    24|         0|            0|            0|  0.00%|\n",
      "    25|    112710|     0.171638|  1.52282e-06|  0.03%|    def __getattr__(self, attr):\n",
      "    26|    112710|      0.24596|  2.18224e-06|  0.04%|        return getattr(self.vf, attr)\n",
      "    27|         0|            0|            0|  0.00%|\n",
      "    28|         0|            0|            0|  0.00%|\n",
      "    29|         0|            0|            0|  0.00%|sys.modules[__name__] = VFModule(__name__)\n",
      "File: <string>_0\n",
      "File duration: 0.382184s (0.06%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|    199680|     0.382184|  1.91398e-06|  0.06%|\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/nn/parameter.py\n",
      "File duration: 0.381078s (0.06%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|import torch\n",
      "     2|         0|            0|            0|  0.00%|from torch._C import _disabled_torch_function_impl\n",
      "     3|         0|            0|            0|  0.00%|from collections import OrderedDict\n",
      "     4|         0|            0|            0|  0.00%|\n",
      "     5|         0|            0|            0|  0.00%|\n",
      "     6|         0|            0|            0|  0.00%|# Metaclass to combine _TensorMeta and the instance check override for Parameter.\n",
      "     7|         0|            0|            0|  0.00%|class _ParameterMeta(torch._C._TensorMeta):\n",
      "     8|         0|            0|            0|  0.00%|    # Make `isinstance(t, Parameter)` return True for custom tensor instances that have the _is_param flag.\n",
      "     9|     51090|    0.0804784|  1.57523e-06|  0.01%|    def __instancecheck__(self, instance):\n",
      "    10|    102180|     0.204512|  2.00148e-06|  0.03%|        return super().__instancecheck__(instance) or (\n",
      "    11|     51090|    0.0960882|  1.88076e-06|  0.02%|            isinstance(instance, torch.Tensor) and getattr(instance, '_is_param', False))\n",
      "    12|         0|            0|            0|  0.00%|\n",
      "    13|         0|            0|            0|  0.00%|\n",
      "    14|         0|            0|            0|  0.00%|class Parameter(torch.Tensor, metaclass=_ParameterMeta):\n",
      "    15|         0|            0|            0|  0.00%|    r\"\"\"A kind of Tensor that is to be considered a module parameter.\n",
      "    16|         0|            0|            0|  0.00%|\n",
      "    17|         0|            0|            0|  0.00%|    Parameters are :class:`~torch.Tensor` subclasses, that have a\n",
      "    18|         0|            0|            0|  0.00%|    very special property when used with :class:`Module` s - when they're\n",
      "    19|         0|            0|            0|  0.00%|    assigned as Module attributes they are automatically added to the list of\n",
      "    20|         0|            0|            0|  0.00%|    its parameters, and will appear e.g. in :meth:`~Module.parameters` iterator.\n",
      "    21|         0|            0|            0|  0.00%|    Assigning a Tensor doesn't have such effect. This is because one might\n",
      "    22|         0|            0|            0|  0.00%|    want to cache some temporary state, like last hidden state of the RNN, in\n",
      "    23|         0|            0|            0|  0.00%|    the model. If there was no such class as :class:`Parameter`, these\n",
      "    24|         0|            0|            0|  0.00%|    temporaries would get registered too.\n",
      "    25|         0|            0|            0|  0.00%|\n",
      "    26|         0|            0|            0|  0.00%|    Args:\n",
      "    27|         0|            0|            0|  0.00%|        data (Tensor): parameter tensor.\n",
      "    28|         0|            0|            0|  0.00%|        requires_grad (bool, optional): if the parameter requires gradient. See\n",
      "    29|         0|            0|            0|  0.00%|            :ref:`locally-disable-grad-doc` for more details. Default: `True`\n",
      "    30|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    31|         0|            0|            0|  0.00%|    def __new__(cls, data=None, requires_grad=True):\n",
      "    32|         0|            0|            0|  0.00%|        if data is None:\n",
      "    33|         0|            0|            0|  0.00%|            data = torch.empty(0)\n",
      "    34|         0|            0|            0|  0.00%|        if type(data) is torch.Tensor or type(data) is Parameter:\n",
      "    35|         0|            0|            0|  0.00%|            # For ease of BC maintenance, keep this path for standard Tensor.\n",
      "    36|         0|            0|            0|  0.00%|            # Eventually (tm), we should change the behavior for standard Tensor to match.\n",
      "    37|         0|            0|            0|  0.00%|            return torch.Tensor._make_subclass(cls, data, requires_grad)\n",
      "    38|         0|            0|            0|  0.00%|\n",
      "    39|         0|            0|            0|  0.00%|        # Path for custom tensors: set a flag on the instance to indicate parameter-ness.\n",
      "    40|         0|            0|            0|  0.00%|        t = data.detach().requires_grad_(requires_grad)\n",
      "    41|         0|            0|            0|  0.00%|        if type(t) is not type(data):\n",
      "    42|         0|            0|            0|  0.00%|            raise RuntimeError(f\"Creating a Parameter from an instance of type {type(data).__name__} \"\n",
      "    43|         0|            0|            0|  0.00%|                               \"requires that detach() returns an instance of the same type, but return \"\n",
      "    44|         0|            0|            0|  0.00%|                               f\"type {type(t).__name__} was found instead. To use the type as a \"\n",
      "    45|         0|            0|            0|  0.00%|                               \"Parameter, please correct the detach() semantics defined by \"\n",
      "    46|         0|            0|            0|  0.00%|                               \"its __torch_dispatch__() implementation.\")\n",
      "    47|         0|            0|            0|  0.00%|        t._is_param = True\n",
      "    48|         0|            0|            0|  0.00%|        return t\n",
      "    49|         0|            0|            0|  0.00%|\n",
      "    50|         0|            0|            0|  0.00%|    # Note: the 3 methods below only apply to standard Tensor. Parameters of custom tensor types\n",
      "    51|         0|            0|            0|  0.00%|    # are still considered that custom tensor type and these methods will not be called for them.\n",
      "    52|         0|            0|            0|  0.00%|    def __deepcopy__(self, memo):\n",
      "    53|         0|            0|            0|  0.00%|        if id(self) in memo:\n",
      "    54|         0|            0|            0|  0.00%|            return memo[id(self)]\n",
      "    55|         0|            0|            0|  0.00%|        else:\n",
      "    56|         0|            0|            0|  0.00%|            result = type(self)(self.data.clone(memory_format=torch.preserve_format), self.requires_grad)\n",
      "    57|         0|            0|            0|  0.00%|            memo[id(self)] = result\n",
      "    58|         0|            0|            0|  0.00%|            return result\n",
      "    59|         0|            0|            0|  0.00%|\n",
      "    60|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "    61|         0|            0|            0|  0.00%|        return 'Parameter containing:\\n' + super(Parameter, self).__repr__()\n",
      "    62|         0|            0|            0|  0.00%|\n",
      "    63|         0|            0|            0|  0.00%|    def __reduce_ex__(self, proto):\n",
      "    64|         0|            0|            0|  0.00%|        # See Note [Don't serialize hooks]\n",
      "    65|         0|            0|            0|  0.00%|        return (\n",
      "    66|         0|            0|            0|  0.00%|            torch._utils._rebuild_parameter,\n",
      "    67|         0|            0|            0|  0.00%|            (self.data, self.requires_grad, OrderedDict())\n",
      "    68|         0|            0|            0|  0.00%|        )\n",
      "    69|         0|            0|            0|  0.00%|\n",
      "    70|         0|            0|            0|  0.00%|    __torch_function__ = _disabled_torch_function_impl\n",
      "    71|         0|            0|            0|  0.00%|\n",
      "    72|         0|            0|            0|  0.00%|\n",
      "    73|         0|            0|            0|  0.00%|class UninitializedTensorMixin:\n",
      "    74|         0|            0|            0|  0.00%|    _allowed_methods = [\n",
      "    75|         0|            0|            0|  0.00%|        torch.Tensor.__hash__,\n",
      "    76|         0|            0|            0|  0.00%|        torch.Tensor.size,\n",
      "    77|         0|            0|            0|  0.00%|        torch.Tensor.copy_,\n",
      "    78|         0|            0|            0|  0.00%|        torch.Tensor.is_floating_point,\n",
      "    79|         0|            0|            0|  0.00%|        torch.Tensor.half,\n",
      "    80|         0|            0|            0|  0.00%|        torch.Tensor.float,\n",
      "    81|         0|            0|            0|  0.00%|        torch.Tensor.double,\n",
      "    82|         0|            0|            0|  0.00%|        torch.Tensor.char,\n",
      "    83|         0|            0|            0|  0.00%|        torch.Tensor.short,\n",
      "    84|         0|            0|            0|  0.00%|        torch.Tensor.int,\n",
      "    85|         0|            0|            0|  0.00%|        torch.Tensor.long,\n",
      "    86|         0|            0|            0|  0.00%|        torch.Tensor.cuda,\n",
      "    87|         0|            0|            0|  0.00%|        torch.Tensor.cpu,\n",
      "    88|         0|            0|            0|  0.00%|        torch.Tensor.to,\n",
      "    89|         0|            0|            0|  0.00%|        torch.Tensor.get_device,\n",
      "    90|         0|            0|            0|  0.00%|        torch._has_compatible_shallow_copy_type,\n",
      "    91|         0|            0|            0|  0.00%|    ]\n",
      "    92|         0|            0|            0|  0.00%|\n",
      "    93|         0|            0|            0|  0.00%|    def materialize(self, shape, device=None, dtype=None):\n",
      "    94|         0|            0|            0|  0.00%|        r\"\"\"Create a Parameter or Tensor with the same properties of the uninitialized one.\n",
      "    95|         0|            0|            0|  0.00%|        Given a shape, it materializes a parameter in the same device\n",
      "    96|         0|            0|            0|  0.00%|        and with the same `dtype` as the current one or the specified ones in the\n",
      "    97|         0|            0|            0|  0.00%|        arguments.\n",
      "    98|         0|            0|            0|  0.00%|\n",
      "    99|         0|            0|            0|  0.00%|        Args:\n",
      "   100|         0|            0|            0|  0.00%|            shape : (tuple): the shape for the materialized tensor.\n",
      "   101|         0|            0|            0|  0.00%|            device (:class:`torch.device`): the desired device of the parameters\n",
      "   102|         0|            0|            0|  0.00%|                and buffers in this module. Optional.\n",
      "   103|         0|            0|            0|  0.00%|            dtype (:class:`torch.dtype`): the desired floating point type of\n",
      "   104|         0|            0|            0|  0.00%|                the floating point parameters and buffers in this module. Optional.\n",
      "   105|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   106|         0|            0|            0|  0.00%|        if device is None:\n",
      "   107|         0|            0|            0|  0.00%|            device = self.data.device\n",
      "   108|         0|            0|            0|  0.00%|        if dtype is None:\n",
      "   109|         0|            0|            0|  0.00%|            dtype = self.data.dtype\n",
      "   110|         0|            0|            0|  0.00%|        self.data = torch.empty(shape, device=device, dtype=dtype)\n",
      "   111|         0|            0|            0|  0.00%|        self.__class__ = self.cls_to_become\n",
      "   112|         0|            0|            0|  0.00%|\n",
      "   113|         0|            0|            0|  0.00%|    @property\n",
      "   114|         0|            0|            0|  0.00%|    def shape(self):\n",
      "   115|         0|            0|            0|  0.00%|        raise RuntimeError(\n",
      "   116|         0|            0|            0|  0.00%|            'Can\\'t access the shape of an uninitialized parameter or buffer. '\n",
      "   117|         0|            0|            0|  0.00%|            'This error usually happens in `load_state_dict` when trying to load '\n",
      "   118|         0|            0|            0|  0.00%|            'an uninitialized parameter into an initialized one. '\n",
      "   119|         0|            0|            0|  0.00%|            'Call `forward` to initialize the parameters before accessing their attributes.')\n",
      "   120|         0|            0|            0|  0.00%|\n",
      "   121|         0|            0|            0|  0.00%|    def share_memory_(self):\n",
      "   122|         0|            0|            0|  0.00%|        raise RuntimeError(\n",
      "   123|         0|            0|            0|  0.00%|            'Can\\'t share memory on an uninitialized parameter or buffer. '\n",
      "   124|         0|            0|            0|  0.00%|            'Call `forward` to initialize the parameters before calling '\n",
      "   125|         0|            0|            0|  0.00%|            '`module.share_memory()`.')\n",
      "   126|         0|            0|            0|  0.00%|\n",
      "   127|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "   128|         0|            0|            0|  0.00%|        return f'<{self.__class__.__name__}>'\n",
      "   129|         0|            0|            0|  0.00%|\n",
      "   130|         0|            0|            0|  0.00%|    def __reduce_ex__(self, proto):\n",
      "   131|         0|            0|            0|  0.00%|        # See Note [Don't serialize hooks]\n",
      "   132|         0|            0|            0|  0.00%|        return (\n",
      "   133|         0|            0|            0|  0.00%|            self.__class__,\n",
      "   134|         0|            0|            0|  0.00%|            (self.requires_grad,)\n",
      "   135|         0|            0|            0|  0.00%|        )\n",
      "   136|         0|            0|            0|  0.00%|\n",
      "   137|         0|            0|            0|  0.00%|    @classmethod\n",
      "   138|         0|            0|            0|  0.00%|    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
      "   139|         0|            0|            0|  0.00%|        # method-wrapper is to detect access to Tensor properties that are\n",
      "   140|         0|            0|            0|  0.00%|        # wrapped in descriptors\n",
      "   141|         0|            0|            0|  0.00%|        if func in cls._allowed_methods or func.__class__.__name__ == 'method-wrapper':\n",
      "   142|         0|            0|            0|  0.00%|            if kwargs is None:\n",
      "   143|         0|            0|            0|  0.00%|                kwargs = {}\n",
      "   144|         0|            0|            0|  0.00%|            return super().__torch_function__(func, types, args, kwargs)\n",
      "   145|         0|            0|            0|  0.00%|        raise ValueError(\n",
      "   146|         0|            0|            0|  0.00%|            'Attempted to use an uninitialized parameter in {}. '\n",
      "   147|         0|            0|            0|  0.00%|            'This error happens when you are using a `LazyModule` or '\n",
      "   148|         0|            0|            0|  0.00%|            'explicitly manipulating `torch.nn.parameter.{}` '\n",
      "   149|         0|            0|            0|  0.00%|            'objects. When using LazyModules Call `forward` with a dummy batch '\n",
      "   150|         0|            0|            0|  0.00%|            'to initialize the parameters before calling torch functions'.format(func, cls.__name__))\n",
      "   151|         0|            0|            0|  0.00%|\n",
      "   152|         0|            0|            0|  0.00%|\n",
      "   153|         0|            0|            0|  0.00%|def is_lazy(param):\n",
      "   154|         0|            0|            0|  0.00%|    return isinstance(param, UninitializedTensorMixin)\n",
      "   155|         0|            0|            0|  0.00%|\n",
      "   156|         0|            0|            0|  0.00%|\n",
      "   157|         0|            0|            0|  0.00%|class UninitializedParameter(UninitializedTensorMixin, Parameter):\n",
      "   158|         0|            0|            0|  0.00%|    r\"\"\"A parameter that is not initialized.\n",
      "   159|         0|            0|            0|  0.00%|\n",
      "   160|         0|            0|            0|  0.00%|    Unitialized Parameters are a a special case of :class:`torch.nn.Parameter`\n",
      "   161|         0|            0|            0|  0.00%|    where the shape of the data is still unknown.\n",
      "   162|         0|            0|            0|  0.00%|\n",
      "   163|         0|            0|            0|  0.00%|    Unlike a :class:`torch.nn.Parameter`, uninitialized parameters\n",
      "   164|         0|            0|            0|  0.00%|    hold no data and attempting to access some properties, like their shape,\n",
      "   165|         0|            0|            0|  0.00%|    will throw a runtime error. The only operations that can be performed on a uninitialized\n",
      "   166|         0|            0|            0|  0.00%|    parameter are changing its datatype, moving it to a different device and\n",
      "   167|         0|            0|            0|  0.00%|    converting it to a regular :class:`torch.nn.Parameter`.\n",
      "   168|         0|            0|            0|  0.00%|\n",
      "   169|         0|            0|            0|  0.00%|    The default device or dtype to use when the parameter is materialized can be set\n",
      "   170|         0|            0|            0|  0.00%|    during construction using e.g. ``device='cuda'``.\n",
      "   171|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   172|         0|            0|            0|  0.00%|\n",
      "   173|         0|            0|            0|  0.00%|    cls_to_become = Parameter\n",
      "   174|         0|            0|            0|  0.00%|\n",
      "   175|         0|            0|            0|  0.00%|    def __new__(cls, requires_grad=True, device=None, dtype=None) -> None:\n",
      "   176|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "   177|         0|            0|            0|  0.00%|        data = torch.empty(0, **factory_kwargs)\n",
      "   178|         0|            0|            0|  0.00%|        return torch.Tensor._make_subclass(cls, data, requires_grad)\n",
      "   179|         0|            0|            0|  0.00%|\n",
      "   180|         0|            0|            0|  0.00%|\n",
      "   181|         0|            0|            0|  0.00%|class UninitializedBuffer(UninitializedTensorMixin, torch.Tensor):\n",
      "   182|         0|            0|            0|  0.00%|    r\"\"\"A buffer that is not initialized.\n",
      "   183|         0|            0|            0|  0.00%|\n",
      "   184|         0|            0|            0|  0.00%|    Unitialized Buffer is a a special case of :class:`torch.Tensor`\n",
      "   185|         0|            0|            0|  0.00%|    where the shape of the data is still unknown.\n",
      "   186|         0|            0|            0|  0.00%|\n",
      "   187|         0|            0|            0|  0.00%|    Unlike a :class:`torch.Tensor`, uninitialized parameters\n",
      "   188|         0|            0|            0|  0.00%|    hold no data and attempting to access some properties, like their shape,\n",
      "   189|         0|            0|            0|  0.00%|    will throw a runtime error. The only operations that can be performed on a uninitialized\n",
      "   190|         0|            0|            0|  0.00%|    parameter are changing its datatype, moving it to a different device and\n",
      "   191|         0|            0|            0|  0.00%|    converting it to a regular :class:`torch.Tensor`.\n",
      "   192|         0|            0|            0|  0.00%|\n",
      "   193|         0|            0|            0|  0.00%|    The default device or dtype to use when the buffer is materialized can be set\n",
      "   194|         0|            0|            0|  0.00%|    during construction using e.g. ``device='cuda'``.\n",
      "   195|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   196|         0|            0|            0|  0.00%|\n",
      "   197|         0|            0|            0|  0.00%|    cls_to_become = torch.Tensor\n",
      "   198|         0|            0|            0|  0.00%|\n",
      "   199|         0|            0|            0|  0.00%|    def __new__(cls, requires_grad=False, device=None, dtype=None) -> None:\n",
      "   200|         0|            0|            0|  0.00%|        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "   201|         0|            0|            0|  0.00%|        data = torch.empty(0, **factory_kwargs)\n",
      "   202|         0|            0|            0|  0.00%|        return torch.Tensor._make_subclass(cls, data, requires_grad)\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_ops.py\n",
      "File duration: 0.278955s (0.04%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|import torch._C\n",
      "     2|         0|            0|            0|  0.00%|\n",
      "     3|         0|            0|            0|  0.00%|import contextlib\n",
      "     4|         0|            0|            0|  0.00%|import ctypes\n",
      "     5|         0|            0|            0|  0.00%|import sys\n",
      "     6|         0|            0|            0|  0.00%|import types\n",
      "     7|         0|            0|            0|  0.00%|\n",
      "     8|         0|            0|            0|  0.00%|import torch.jit\n",
      "     9|         0|            0|            0|  0.00%|import torch._utils_internal\n",
      "    10|         0|            0|            0|  0.00%|\n",
      "    11|         0|            0|            0|  0.00%|# Query `hasattr` only once.\n",
      "    12|         0|            0|            0|  0.00%|_SET_GLOBAL_FLAGS = hasattr(sys, 'getdlopenflags') and hasattr(sys, 'setdlopenflags')\n",
      "    13|         0|            0|            0|  0.00%|\n",
      "    14|         0|            0|            0|  0.00%|\n",
      "    15|         0|            0|            0|  0.00%|@contextlib.contextmanager\n",
      "    16|         0|            0|            0|  0.00%|def dl_open_guard():\n",
      "    17|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    18|         0|            0|            0|  0.00%|    Context manager to set the RTLD_GLOBAL dynamic linker flag while we open a\n",
      "    19|         0|            0|            0|  0.00%|    shared library to load custom operators.\n",
      "    20|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    21|         0|            0|            0|  0.00%|    if _SET_GLOBAL_FLAGS:\n",
      "    22|         0|            0|            0|  0.00%|        old_flags = sys.getdlopenflags()\n",
      "    23|         0|            0|            0|  0.00%|        sys.setdlopenflags(old_flags | ctypes.RTLD_GLOBAL)\n",
      "    24|         0|            0|            0|  0.00%|    yield\n",
      "    25|         0|            0|            0|  0.00%|    if _SET_GLOBAL_FLAGS:\n",
      "    26|         0|            0|            0|  0.00%|        sys.setdlopenflags(old_flags)\n",
      "    27|         0|            0|            0|  0.00%|\n",
      "    28|         0|            0|            0|  0.00%|# Each OpOverload object contains pointer to a a specific operator overload, a pointer to the parent `OpOverloadPacket` object.\n",
      "    29|         0|            0|            0|  0.00%|# You can obtain an OpOverload object through attribute query on OpOverloadPacket.\n",
      "    30|         0|            0|            0|  0.00%|class OpOverload:\n",
      "    31|         0|            0|            0|  0.00%|    def __init__(self, overloadpacket, op, schema):\n",
      "    32|         0|            0|            0|  0.00%|        self._op = op\n",
      "    33|         0|            0|            0|  0.00%|        self._schema = schema\n",
      "    34|         0|            0|            0|  0.00%|        self._overloadpacket = overloadpacket\n",
      "    35|         0|            0|            0|  0.00%|        self._overloadname = 'default' if schema.overload_name == '' else schema.overload_name\n",
      "    36|         0|            0|            0|  0.00%|        self.__name__ = \"{}.{}\".format(self._schema.name.split(\"::\")[1], self._overloadname)\n",
      "    37|         0|            0|            0|  0.00%|        self.__module__ = overloadpacket.__module__\n",
      "    38|         0|            0|            0|  0.00%|        op.__module__ = overloadpacket.__module__\n",
      "    39|         0|            0|            0|  0.00%|\n",
      "    40|         0|            0|            0|  0.00%|    # it's a no-op since OpOverload object is immutable and must be unique for a given op overload.\n",
      "    41|         0|            0|            0|  0.00%|    def __deepcopy__(self, memo=None):\n",
      "    42|         0|            0|            0|  0.00%|        return self\n",
      "    43|         0|            0|            0|  0.00%|\n",
      "    44|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "    45|         0|            0|            0|  0.00%|        return \"<OpOverload(op='{}.{}', overload='{}')>\".format(*self._schema.name.split(\"::\"), self._overloadname)\n",
      "    46|         0|            0|            0|  0.00%|\n",
      "    47|         0|            0|            0|  0.00%|    def __call__(self, *args, **kwargs):\n",
      "    48|         0|            0|            0|  0.00%|        return self._op(*args, **kwargs or {})\n",
      "    49|         0|            0|            0|  0.00%|\n",
      "    50|         0|            0|            0|  0.00%|    def __getattr__(self, key):\n",
      "    51|         0|            0|            0|  0.00%|        return getattr(self._op, key)\n",
      "    52|         0|            0|            0|  0.00%|\n",
      "    53|         0|            0|            0|  0.00%|    def __hash__(self):\n",
      "    54|         0|            0|            0|  0.00%|        return hash(self._op)\n",
      "    55|         0|            0|            0|  0.00%|\n",
      "    56|         0|            0|            0|  0.00%|    # `my_namespace.my_op_name.overload_name`\n",
      "    57|         0|            0|            0|  0.00%|    def __str__(self):\n",
      "    58|         0|            0|            0|  0.00%|        return \"{}.{}.{}\".format(*self._schema.name.split(\"::\"), self._overloadname)\n",
      "    59|         0|            0|            0|  0.00%|\n",
      "    60|         0|            0|            0|  0.00%|    @property\n",
      "    61|         0|            0|            0|  0.00%|    def overloadpacket(self):\n",
      "    62|         0|            0|            0|  0.00%|        return self._overloadpacket\n",
      "    63|         0|            0|            0|  0.00%|\n",
      "    64|         0|            0|            0|  0.00%|    @property\n",
      "    65|         0|            0|            0|  0.00%|    def op(self):\n",
      "    66|         0|            0|            0|  0.00%|        return self._op\n",
      "    67|         0|            0|            0|  0.00%|\n",
      "    68|         0|            0|            0|  0.00%|    # TODO: add more methods to expose information about input and output arguments\n",
      "    69|         0|            0|            0|  0.00%|\n",
      "    70|         0|            0|            0|  0.00%|# OpOverloadPacket class contains pointer to a base unresolved operator that doesn't correspond to a specific operator\n",
      "    71|         0|            0|            0|  0.00%|# You can obtain an OpOverload object through attribute query.\n",
      "    72|         0|            0|            0|  0.00%|class OpOverloadPacket:\n",
      "    73|         0|            0|            0|  0.00%|    def __init__(self, qualified_op_name, op_name, op, overload_names):\n",
      "    74|         0|            0|            0|  0.00%|        # These attributes are accessible on the object through the properties\n",
      "    75|         0|            0|            0|  0.00%|        # defined below but are immutable\n",
      "    76|         0|            0|            0|  0.00%|        self._qualified_op_name = qualified_op_name\n",
      "    77|         0|            0|            0|  0.00%|        self.__name__ = op_name\n",
      "    78|         0|            0|            0|  0.00%|        self._op = op\n",
      "    79|         0|            0|            0|  0.00%|        self._overload_names = overload_names\n",
      "    80|         0|            0|            0|  0.00%|\n",
      "    81|         0|            0|            0|  0.00%|    # it's a no-op since OpOverloadPacket object is immutable and must be unique for a given op.\n",
      "    82|         0|            0|            0|  0.00%|    def __deepcopy__(self, memo=None):\n",
      "    83|         0|            0|            0|  0.00%|        return self\n",
      "    84|         0|            0|            0|  0.00%|\n",
      "    85|         0|            0|            0|  0.00%|    def __repr__(self):\n",
      "    86|         0|            0|            0|  0.00%|        return \"<OpOverloadPacket(op='{}.{}')>\".format(*self._qualified_op_name.split(\"::\"))\n",
      "    87|         0|            0|            0|  0.00%|\n",
      "    88|         0|            0|            0|  0.00%|    def __hash__(self):\n",
      "    89|         0|            0|            0|  0.00%|        return hash(self._op)\n",
      "    90|         0|            0|            0|  0.00%|\n",
      "    91|         0|            0|            0|  0.00%|    def __str__(self):\n",
      "    92|         0|            0|            0|  0.00%|        return \"{}.{}\".format(*self._qualified_op_name.split(\"::\"))\n",
      "    93|         0|            0|            0|  0.00%|\n",
      "    94|         0|            0|            0|  0.00%|    @property\n",
      "    95|         0|            0|            0|  0.00%|    def op(self):\n",
      "    96|         0|            0|            0|  0.00%|        return self._op\n",
      "    97|         0|            0|            0|  0.00%|\n",
      "    98|         0|            0|            0|  0.00%|    def __getattr__(self, key):\n",
      "    99|         0|            0|            0|  0.00%|        # It is not a valid op_name when __file__ is passed in\n",
      "   100|         0|            0|            0|  0.00%|        if key == '__file__':\n",
      "   101|         0|            0|            0|  0.00%|            return 'torch.ops'\n",
      "   102|         0|            0|            0|  0.00%|\n",
      "   103|         0|            0|            0|  0.00%|        # ensure that query for dunder attributes that does not exist on\n",
      "   104|         0|            0|            0|  0.00%|        # opoverloadpacket but instead exists on the self._op object does not unnecessarily call\n",
      "   105|         0|            0|            0|  0.00%|        # `_get_operation_overload` (which is an expensive operation).\n",
      "   106|         0|            0|            0|  0.00%|        # This is done to prevent any potential slowdown. This list can be extended\n",
      "   107|         0|            0|            0|  0.00%|        # if there exists other attributes like `__name__` that only exist on self._op and not on the\n",
      "   108|         0|            0|            0|  0.00%|        # opoverloadpacket.\n",
      "   109|         0|            0|            0|  0.00%|        # This is ok since we are guaranteed that an overload name for an aten op can't start with '__'\n",
      "   110|         0|            0|            0|  0.00%|        try:\n",
      "   111|         0|            0|            0|  0.00%|            if key.startswith('__'):\n",
      "   112|         0|            0|            0|  0.00%|                return getattr(self._op, key)\n",
      "   113|         0|            0|            0|  0.00%|        except AttributeError:\n",
      "   114|         0|            0|            0|  0.00%|            # for consistency because it seems weird to\n",
      "   115|         0|            0|            0|  0.00%|            # throw an attribute error with a message containing\n",
      "   116|         0|            0|            0|  0.00%|            # an object name different from the one the attribute\n",
      "   117|         0|            0|            0|  0.00%|            # query was performed on.\n",
      "   118|         0|            0|            0|  0.00%|            raise AttributeError(\"'{}' can't have an overload name beginning with '__' and the \"\n",
      "   119|         0|            0|            0|  0.00%|                                 \"underlying op {} has no attribute {} either.\"\n",
      "   120|         0|            0|            0|  0.00%|                                 .format(str(self), str(self._op), key)) from None\n",
      "   121|         0|            0|            0|  0.00%|\n",
      "   122|         0|            0|            0|  0.00%|        try:\n",
      "   123|         0|            0|            0|  0.00%|            # This is ok since we are guaranteed that an overload name for an aten op can't be 'default'\n",
      "   124|         0|            0|            0|  0.00%|            use_key = '' if key == 'default' else key\n",
      "   125|         0|            0|            0|  0.00%|            # TODO: disallow access to overloads registered by JIT\n",
      "   126|         0|            0|            0|  0.00%|            op_ = torch._C._get_operation_overload(\n",
      "   127|         0|            0|            0|  0.00%|                self._qualified_op_name, use_key)\n",
      "   128|         0|            0|            0|  0.00%|            schema = torch._C._get_schema(self._qualified_op_name, use_key)\n",
      "   129|         0|            0|            0|  0.00%|            overload = OpOverload(self, op_, schema)\n",
      "   130|         0|            0|            0|  0.00%|            # cache the overload object\n",
      "   131|         0|            0|            0|  0.00%|            setattr(self, key, overload)\n",
      "   132|         0|            0|            0|  0.00%|            return overload\n",
      "   133|         0|            0|            0|  0.00%|        except RuntimeError:\n",
      "   134|         0|            0|            0|  0.00%|            raise AttributeError(\n",
      "   135|         0|            0|            0|  0.00%|                \"The underlying op of '{}' has no overload name '{}'\".format(str(self), key)\n",
      "   136|         0|            0|            0|  0.00%|            ) from None\n",
      "   137|         0|            0|            0|  0.00%|\n",
      "   138|     24960|    0.0380278|  1.52355e-06|  0.01%|    def __call__(self, *args, **kwargs):\n",
      "   139|         0|            0|            0|  0.00%|        # overloading __call__ to ensure torch.ops.foo.bar()\n",
      "   140|         0|            0|            0|  0.00%|        # is still callable from JIT\n",
      "   141|         0|            0|            0|  0.00%|        # We save the function ptr as the `op` attribute on\n",
      "   142|         0|            0|            0|  0.00%|        # OpOverloadPacket to access it here.\n",
      "   143|     24960|     0.240927|  9.65253e-06|  0.04%|        return self._op(*args, **kwargs or {})\n",
      "   144|         0|            0|            0|  0.00%|\n",
      "   145|         0|            0|            0|  0.00%|    # TODO: use this to make a __dir__\n",
      "   146|         0|            0|            0|  0.00%|    def overloads(self):\n",
      "   147|         0|            0|            0|  0.00%|        return [n if n else \"default\" for n in self._overload_names]\n",
      "   148|         0|            0|            0|  0.00%|\n",
      "   149|         0|            0|            0|  0.00%|# Resolution of torch.fn is different from torch.ops.aten.fn\n",
      "   150|         0|            0|            0|  0.00%|# torch.fn uses the Python argparser, matches with the\n",
      "   151|         0|            0|            0|  0.00%|# appropriate schema, and calls into the unboxed version of the method\n",
      "   152|         0|            0|            0|  0.00%|# torch.ops.aten.fn resolution is done via the mechanism defined in JIT.\n",
      "   153|         0|            0|            0|  0.00%|# JIT creates a stack of all the overloads and then tries to match the\n",
      "   154|         0|            0|            0|  0.00%|# correct one at runtime and always calls into the boxed version of the method\n",
      "   155|         0|            0|            0|  0.00%|# Autograd codegen creates VariableType, TracerType,\n",
      "   156|         0|            0|            0|  0.00%|# inplace or view type and python bindings.\n",
      "   157|         0|            0|            0|  0.00%|# Aten codegen generates tensor methods for the the tensor class.\n",
      "   158|         0|            0|            0|  0.00%|\n",
      "   159|         0|            0|            0|  0.00%|# _OpNamespace is a subclass of ModuleType because the torch script\n",
      "   160|         0|            0|            0|  0.00%|# allows attribute lookups on modules only. Since we want torch.ops.foo.bar()\n",
      "   161|         0|            0|            0|  0.00%|# to work from script, we need to ensure ops and foo are modules\n",
      "   162|         0|            0|            0|  0.00%|\n",
      "   163|         0|            0|            0|  0.00%|\n",
      "   164|         0|            0|            0|  0.00%|class _OpNamespace(types.ModuleType):\n",
      "   165|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   166|         0|            0|            0|  0.00%|    An op namespace to dynamically bind Operators into Python.\n",
      "   167|         0|            0|            0|  0.00%|\n",
      "   168|         0|            0|            0|  0.00%|    Say a user has created a custom Operator called \"my_namespace::my_op\". To\n",
      "   169|         0|            0|            0|  0.00%|    call this op, the user will write torch.ops.my_namespace.my_op(...).\n",
      "   170|         0|            0|            0|  0.00%|    At startup, this operation will not yet be bound into Python. Instead, the\n",
      "   171|         0|            0|            0|  0.00%|    following sequence of magic tricks will occur:\n",
      "   172|         0|            0|            0|  0.00%|    1. `torch.ops.my_namespace` will invoke the `__getattr__` magic method\n",
      "   173|         0|            0|            0|  0.00%|       on the `torch.ops` object, which will create a new `_OpNamespace`\n",
      "   174|         0|            0|            0|  0.00%|       object called `my_namespace` and set it as an attribute on the `ops`\n",
      "   175|         0|            0|            0|  0.00%|       object.\n",
      "   176|         0|            0|            0|  0.00%|    2. `torch.ops.my_namespace.my_op` will then invoke `__getattr__` on\n",
      "   177|         0|            0|            0|  0.00%|       the `my_namespace` object, which will retrieve the operation via\n",
      "   178|         0|            0|            0|  0.00%|       `torch.get_operation`, a function bound from C++, and then in a similar\n",
      "   179|         0|            0|            0|  0.00%|       fashion bind this new object onto the `my_namespace` object.\n",
      "   180|         0|            0|            0|  0.00%|    3. `torch.ops.my_namespace.my_op(...)` then calls this new operation\n",
      "   181|         0|            0|            0|  0.00%|        and subsequent accesses will incur no further lookup (the namespace and\n",
      "   182|         0|            0|            0|  0.00%|        operation will already exist).\n",
      "   183|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   184|         0|            0|            0|  0.00%|    def __init__(self, name):\n",
      "   185|         0|            0|            0|  0.00%|        super(_OpNamespace, self).__init__('torch.ops.' + name)\n",
      "   186|         0|            0|            0|  0.00%|        self.name = name\n",
      "   187|         0|            0|            0|  0.00%|\n",
      "   188|         0|            0|            0|  0.00%|    def __getattr__(self, op_name):\n",
      "   189|         0|            0|            0|  0.00%|        # It is not a valid op_name when __file__ is passed in\n",
      "   190|         0|            0|            0|  0.00%|        if op_name == '__file__':\n",
      "   191|         0|            0|            0|  0.00%|            return 'torch.ops'\n",
      "   192|         0|            0|            0|  0.00%|\n",
      "   193|         0|            0|            0|  0.00%|        # Get the op `my_namespace::my_op` if available. This will also check\n",
      "   194|         0|            0|            0|  0.00%|        # for overloads and raise an exception if there are more than one.\n",
      "   195|         0|            0|            0|  0.00%|        namespace_name = self.name\n",
      "   196|         0|            0|            0|  0.00%|        qualified_op_name = '{}::{}'.format(namespace_name, op_name)\n",
      "   197|         0|            0|            0|  0.00%|        try:\n",
      "   198|         0|            0|            0|  0.00%|            op, overload_names = torch._C._jit_get_operation(qualified_op_name)\n",
      "   199|         0|            0|            0|  0.00%|        except RuntimeError as e:\n",
      "   200|         0|            0|            0|  0.00%|            # Turn this into AttributeError so getattr(obj, key, default)\n",
      "   201|         0|            0|            0|  0.00%|            # works (this is called by TorchScript with __origin__)\n",
      "   202|         0|            0|            0|  0.00%|            raise AttributeError(f\"'_OpNamespace' object has no attribute '{op_name}'\") from e\n",
      "   203|         0|            0|            0|  0.00%|\n",
      "   204|         0|            0|            0|  0.00%|        # let the script frontend know that op is identical to the builtin op\n",
      "   205|         0|            0|            0|  0.00%|        # with qualified_op_name\n",
      "   206|         0|            0|            0|  0.00%|        torch.jit._builtins._register_builtin(op, qualified_op_name)\n",
      "   207|         0|            0|            0|  0.00%|        op.__module__ = self.__module__ + \".\" + namespace_name\n",
      "   208|         0|            0|            0|  0.00%|        opoverloadpacket = OpOverloadPacket(qualified_op_name, op_name, op, overload_names)\n",
      "   209|         0|            0|            0|  0.00%|        opoverloadpacket.__module__ = self.__module__ + \".\" + namespace_name\n",
      "   210|         0|            0|            0|  0.00%|        # cache the opoverloadpacket to ensure that each op corresponds to\n",
      "   211|         0|            0|            0|  0.00%|        # a unique OpOverloadPacket object\n",
      "   212|         0|            0|            0|  0.00%|        setattr(self, op_name, opoverloadpacket)\n",
      "   213|         0|            0|            0|  0.00%|        return opoverloadpacket\n",
      "   214|         0|            0|            0|  0.00%|\n",
      "   215|         0|            0|            0|  0.00%|\n",
      "   216|         0|            0|            0|  0.00%|class _Ops(types.ModuleType):\n",
      "   217|         0|            0|            0|  0.00%|    __file__ = '_ops.py'\n",
      "   218|         0|            0|            0|  0.00%|\n",
      "   219|         0|            0|            0|  0.00%|    def __init__(self):\n",
      "   220|         0|            0|            0|  0.00%|        super(_Ops, self).__init__('torch.ops')\n",
      "   221|         0|            0|            0|  0.00%|        self.loaded_libraries = set()\n",
      "   222|         0|            0|            0|  0.00%|\n",
      "   223|         0|            0|            0|  0.00%|    def __getattr__(self, name):\n",
      "   224|         0|            0|            0|  0.00%|        # Here we are creating `torch.ops.my_namespace`\n",
      "   225|         0|            0|            0|  0.00%|        namespace = _OpNamespace(name)\n",
      "   226|         0|            0|            0|  0.00%|        setattr(self, name, namespace)\n",
      "   227|         0|            0|            0|  0.00%|        return namespace\n",
      "   228|         0|            0|            0|  0.00%|\n",
      "   229|         0|            0|            0|  0.00%|    def load_library(self, path):\n",
      "   230|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   231|         0|            0|            0|  0.00%|        Loads a shared library from the given path into the current process.\n",
      "   232|         0|            0|            0|  0.00%|\n",
      "   233|         0|            0|            0|  0.00%|        The library being loaded may run global initialization code to register\n",
      "   234|         0|            0|            0|  0.00%|        custom operators with the PyTorch JIT runtime. This allows dynamically\n",
      "   235|         0|            0|            0|  0.00%|        loading custom operators. For this, you should compile your operator\n",
      "   236|         0|            0|            0|  0.00%|        and the static registration code into a shared library object, and then\n",
      "   237|         0|            0|            0|  0.00%|        call ``torch.ops.load_library('path/to/libcustom.so')`` to load the\n",
      "   238|         0|            0|            0|  0.00%|        shared object.\n",
      "   239|         0|            0|            0|  0.00%|\n",
      "   240|         0|            0|            0|  0.00%|        After the library is loaded, it is added to the\n",
      "   241|         0|            0|            0|  0.00%|        ``torch.ops.loaded_libraries`` attribute, a set that may be inspected\n",
      "   242|         0|            0|            0|  0.00%|        for the paths of all libraries loaded using this function.\n",
      "   243|         0|            0|            0|  0.00%|\n",
      "   244|         0|            0|            0|  0.00%|        Args:\n",
      "   245|         0|            0|            0|  0.00%|            path (str): A path to a shared library to load.\n",
      "   246|         0|            0|            0|  0.00%|        \"\"\"\n",
      "   247|         0|            0|            0|  0.00%|        if sys.executable == \"torch_deploy\":\n",
      "   248|         0|            0|            0|  0.00%|            return\n",
      "   249|         0|            0|            0|  0.00%|\n",
      "   250|         0|            0|            0|  0.00%|        path = torch._utils_internal.resolve_library_path(path)\n",
      "   251|         0|            0|            0|  0.00%|        with dl_open_guard():\n",
      "   252|         0|            0|            0|  0.00%|            # Import the shared library into the process, thus running its\n",
      "   253|         0|            0|            0|  0.00%|            # static (global) initialization code in order to register custom\n",
      "   254|         0|            0|            0|  0.00%|            # operators with the JIT.\n",
      "   255|         0|            0|            0|  0.00%|            ctypes.CDLL(path)\n",
      "   256|         0|            0|            0|  0.00%|        self.loaded_libraries.add(path)\n",
      "   257|         0|            0|            0|  0.00%|\n",
      "   258|         0|            0|            0|  0.00%|\n",
      "   259|         0|            0|            0|  0.00%|# The ops \"namespace\"\n",
      "   260|         0|            0|            0|  0.00%|ops = _Ops()\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/_jit_internal.py\n",
      "File duration: 0.119938s (0.02%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|\"\"\"\n",
      "     2|         0|            0|            0|  0.00%|The weak_script annotation needs to be here instead of inside torch/jit/ so it\n",
      "     3|         0|            0|            0|  0.00%|can be used in other places in torch/ (namely torch.nn) without running into\n",
      "     4|         0|            0|            0|  0.00%|circular dependency problems\n",
      "     5|         0|            0|            0|  0.00%|\"\"\"\n",
      "     6|         0|            0|            0|  0.00%|\n",
      "     7|         0|            0|            0|  0.00%|import contextlib\n",
      "     8|         0|            0|            0|  0.00%|import collections\n",
      "     9|         0|            0|            0|  0.00%|import enum\n",
      "    10|         0|            0|            0|  0.00%|import inspect\n",
      "    11|         0|            0|            0|  0.00%|import ast\n",
      "    12|         0|            0|            0|  0.00%|import weakref\n",
      "    13|         0|            0|            0|  0.00%|import warnings\n",
      "    14|         0|            0|            0|  0.00%|from textwrap import dedent\n",
      "    15|         0|            0|            0|  0.00%|import torch\n",
      "    16|         0|            0|            0|  0.00%|import sys\n",
      "    17|         0|            0|            0|  0.00%|import builtins\n",
      "    18|         0|            0|            0|  0.00%|import typing\n",
      "    19|         0|            0|            0|  0.00%|import io\n",
      "    20|         0|            0|            0|  0.00%|import pickle\n",
      "    21|         0|            0|            0|  0.00%|import threading\n",
      "    22|         0|            0|            0|  0.00%|# This is needed. `torch._jit_internal` is imported before `torch.distributed.__init__`.\n",
      "    23|         0|            0|            0|  0.00%|# Explicitly ask to import `torch.distributed.__init__` first.\n",
      "    24|         0|            0|            0|  0.00%|# Otherwise, \"AttributeError: module 'torch' has no attribute 'distributed'\" is raised.\n",
      "    25|         0|            0|            0|  0.00%|import torch.distributed.rpc\n",
      "    26|         0|            0|            0|  0.00%|from torch._C import Future as CFuture\n",
      "    27|         0|            0|            0|  0.00%|from torch._sources import get_source_lines_and_file, parse_def, fake_range\n",
      "    28|         0|            0|            0|  0.00%|from torch.futures import Future\n",
      "    29|         0|            0|            0|  0.00%|import torch.package._mangling as package_mangling\n",
      "    30|         0|            0|            0|  0.00%|from typing import Any, Callable, Dict, Generic, List, Optional, Tuple, Type, TypeVar, Union  # noqa: F401\n",
      "    31|         0|            0|            0|  0.00%|\n",
      "    32|         0|            0|            0|  0.00%|if sys.version_info[:2] > (3, 7):\n",
      "    33|         0|            0|            0|  0.00%|    from typing import Final\n",
      "    34|         0|            0|            0|  0.00%|else:\n",
      "    35|         0|            0|            0|  0.00%|    from typing_extensions import Final\n",
      "    36|         0|            0|            0|  0.00%|\n",
      "    37|         0|            0|            0|  0.00%|LockType: Type\n",
      "    38|         0|            0|            0|  0.00%|try:\n",
      "    39|         0|            0|            0|  0.00%|    import _thread\n",
      "    40|         0|            0|            0|  0.00%|    LockType = _thread.LockType\n",
      "    41|         0|            0|            0|  0.00%|except ImportError:\n",
      "    42|         0|            0|            0|  0.00%|    import _dummy_thread\n",
      "    43|         0|            0|            0|  0.00%|    LockType = _dummy_thread.LockType\n",
      "    44|         0|            0|            0|  0.00%|\n",
      "    45|         0|            0|            0|  0.00%|# Wrapper functions that can call either of 2 functions depending on a boolean\n",
      "    46|         0|            0|            0|  0.00%|# argument\n",
      "    47|         0|            0|            0|  0.00%|boolean_dispatched: 'weakref.WeakKeyDictionary[Callable, Dict[str, Callable]]' = weakref.WeakKeyDictionary()  # noqa: T484\n",
      "    48|         0|            0|            0|  0.00%|\n",
      "    49|         0|            0|            0|  0.00%|\n",
      "    50|         0|            0|            0|  0.00%|def createResolutionCallbackFromEnv(lookup_base):\n",
      "    51|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    52|         0|            0|            0|  0.00%|    Creates a resolution callback that will look up qualified names in an\n",
      "    53|         0|            0|            0|  0.00%|    environment, starting with `lookup_base` for the base of any qualified\n",
      "    54|         0|            0|            0|  0.00%|    names, then proceeding down the lookup chain with the resolved object.\n",
      "    55|         0|            0|            0|  0.00%|\n",
      "    56|         0|            0|            0|  0.00%|    You should not use this directly, it should only be used from the other\n",
      "    57|         0|            0|            0|  0.00%|    createResolutionCallbackFrom* functions.\n",
      "    58|         0|            0|            0|  0.00%|    \"\"\"\n",
      "    59|         0|            0|            0|  0.00%|    def lookupInModule(qualified_name, module):\n",
      "    60|         0|            0|            0|  0.00%|        if '.' in qualified_name:\n",
      "    61|         0|            0|            0|  0.00%|            parts = qualified_name.split('.')\n",
      "    62|         0|            0|            0|  0.00%|            base = parts[0]\n",
      "    63|         0|            0|            0|  0.00%|            remaining_pieces = '.'.join(parts[1:])\n",
      "    64|         0|            0|            0|  0.00%|            module_value = getattr(module, base)\n",
      "    65|         0|            0|            0|  0.00%|            return lookupInModule(remaining_pieces, module_value)\n",
      "    66|         0|            0|            0|  0.00%|        else:\n",
      "    67|         0|            0|            0|  0.00%|            return getattr(module, qualified_name)\n",
      "    68|         0|            0|            0|  0.00%|\n",
      "    69|         0|            0|            0|  0.00%|    def parseNestedExpr(expr, module) -> Tuple[Any, int]:\n",
      "    70|         0|            0|            0|  0.00%|        i = 0\n",
      "    71|         0|            0|            0|  0.00%|        while i < len(expr) and expr[i] not in (',', '[', ']'):\n",
      "    72|         0|            0|            0|  0.00%|            i += 1\n",
      "    73|         0|            0|            0|  0.00%|\n",
      "    74|         0|            0|            0|  0.00%|        # Special case logic for the empty Tuple as a subscript (used\n",
      "    75|         0|            0|            0|  0.00%|        # in the type annotation `Tuple[()]`)\n",
      "    76|         0|            0|            0|  0.00%|        if expr[:i] == '()':\n",
      "    77|         0|            0|            0|  0.00%|            return (), i\n",
      "    78|         0|            0|            0|  0.00%|\n",
      "    79|         0|            0|            0|  0.00%|        base = lookupInModule(expr[:i].strip(), module)\n",
      "    80|         0|            0|            0|  0.00%|        assert base is not None, f\"Unresolvable type {expr[:i]}\"\n",
      "    81|         0|            0|            0|  0.00%|        if i == len(expr) or expr[i] != '[':\n",
      "    82|         0|            0|            0|  0.00%|            return base, i\n",
      "    83|         0|            0|            0|  0.00%|\n",
      "    84|         0|            0|            0|  0.00%|        assert expr[i] == '['\n",
      "    85|         0|            0|            0|  0.00%|        parts = []\n",
      "    86|         0|            0|            0|  0.00%|        while expr[i] != ']':\n",
      "    87|         0|            0|            0|  0.00%|            part_len = 0\n",
      "    88|         0|            0|            0|  0.00%|            i += 1\n",
      "    89|         0|            0|            0|  0.00%|            part, part_len = parseNestedExpr(expr[i:], module)\n",
      "    90|         0|            0|            0|  0.00%|            parts.append(part)\n",
      "    91|         0|            0|            0|  0.00%|            i += part_len\n",
      "    92|         0|            0|            0|  0.00%|        if len(parts) > 1:\n",
      "    93|         0|            0|            0|  0.00%|            return base[tuple(parts)], i + 1\n",
      "    94|         0|            0|            0|  0.00%|        else:\n",
      "    95|         0|            0|            0|  0.00%|            return base[parts[0]], i + 1\n",
      "    96|         0|            0|            0|  0.00%|\n",
      "    97|         0|            0|            0|  0.00%|    def parseExpr(expr, module):\n",
      "    98|         0|            0|            0|  0.00%|        try:\n",
      "    99|         0|            0|            0|  0.00%|            value, len_parsed = parseNestedExpr(expr, module)\n",
      "   100|         0|            0|            0|  0.00%|            assert len_parsed == len(expr), \"whole expression was not parsed, falling back to c++ parser\"\n",
      "   101|         0|            0|            0|  0.00%|            return value\n",
      "   102|         0|            0|            0|  0.00%|        except Exception:\n",
      "   103|         0|            0|            0|  0.00%|            \"\"\"\n",
      "   104|         0|            0|            0|  0.00%|            The python resolver fails in several cases in known unit tests, and is intended\n",
      "   105|         0|            0|            0|  0.00%|            to fall back gracefully to the c++ resolver in general.  For example, python 2 style\n",
      "   106|         0|            0|            0|  0.00%|            annotations which are frequent in our unit tests often fail with types e.g. int not\n",
      "   107|         0|            0|            0|  0.00%|            resolvable from the calling frame.\n",
      "   108|         0|            0|            0|  0.00%|            \"\"\"\n",
      "   109|         0|            0|            0|  0.00%|            return None\n",
      "   110|         0|            0|            0|  0.00%|\n",
      "   111|         0|            0|            0|  0.00%|    return lambda expr: parseExpr(expr, lookup_base)\n",
      "   112|         0|            0|            0|  0.00%|\n",
      "   113|         0|            0|            0|  0.00%|\n",
      "   114|         0|            0|            0|  0.00%|def createResolutionCallbackFromFrame(frames_up: int = 0):\n",
      "   115|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   116|         0|            0|            0|  0.00%|    Creates a function which, given a string variable name,\n",
      "   117|         0|            0|            0|  0.00%|    returns the value of the variable in the scope of the caller of\n",
      "   118|         0|            0|            0|  0.00%|    the function which called createResolutionCallbackFromFrame (by default).\n",
      "   119|         0|            0|            0|  0.00%|\n",
      "   120|         0|            0|            0|  0.00%|    This is used to enable access in-scope Python variables inside\n",
      "   121|         0|            0|            0|  0.00%|    TorchScript fragments.\n",
      "   122|         0|            0|            0|  0.00%|\n",
      "   123|         0|            0|            0|  0.00%|    frames_up is number of additional frames to go up on the stack.\n",
      "   124|         0|            0|            0|  0.00%|    The default value is 0, which correspond to the frame of the caller\n",
      "   125|         0|            0|            0|  0.00%|    of createResolutionCallbackFromFrame. Also for example, if frames_up is set\n",
      "   126|         0|            0|            0|  0.00%|    to 1, then the frame of the caller's caller of createResolutionCallbackFromFrame\n",
      "   127|         0|            0|            0|  0.00%|    will be taken.\n",
      "   128|         0|            0|            0|  0.00%|\n",
      "   129|         0|            0|            0|  0.00%|    For example, the following program prints 2::\n",
      "   130|         0|            0|            0|  0.00%|\n",
      "   131|         0|            0|            0|  0.00%|        def bar():\n",
      "   132|         0|            0|            0|  0.00%|            cb = createResolutionCallbackFromFrame(1)\n",
      "   133|         0|            0|            0|  0.00%|            print(cb(\"foo\"))\n",
      "   134|         0|            0|            0|  0.00%|\n",
      "   135|         0|            0|            0|  0.00%|        def baz():\n",
      "   136|         0|            0|            0|  0.00%|            foo = 2\n",
      "   137|         0|            0|            0|  0.00%|            bar()\n",
      "   138|         0|            0|            0|  0.00%|\n",
      "   139|         0|            0|            0|  0.00%|        baz()\n",
      "   140|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   141|         0|            0|            0|  0.00%|    frame = inspect.currentframe()\n",
      "   142|         0|            0|            0|  0.00%|    i = 0\n",
      "   143|         0|            0|            0|  0.00%|    while i < frames_up + 1:\n",
      "   144|         0|            0|            0|  0.00%|        assert frame is not None\n",
      "   145|         0|            0|            0|  0.00%|        frame = frame.f_back\n",
      "   146|         0|            0|            0|  0.00%|        i += 1\n",
      "   147|         0|            0|            0|  0.00%|\n",
      "   148|         0|            0|            0|  0.00%|    assert frame is not None\n",
      "   149|         0|            0|            0|  0.00%|    f_locals = frame.f_locals\n",
      "   150|         0|            0|            0|  0.00%|    f_globals = frame.f_globals\n",
      "   151|         0|            0|            0|  0.00%|\n",
      "   152|         0|            0|            0|  0.00%|    class env(object):\n",
      "   153|         0|            0|            0|  0.00%|        def __getattr__(self, key):\n",
      "   154|         0|            0|            0|  0.00%|            if key in f_locals:\n",
      "   155|         0|            0|            0|  0.00%|                return f_locals[key]\n",
      "   156|         0|            0|            0|  0.00%|            elif key in f_globals:\n",
      "   157|         0|            0|            0|  0.00%|                return f_globals[key]\n",
      "   158|         0|            0|            0|  0.00%|            elif key in dir(builtins):\n",
      "   159|         0|            0|            0|  0.00%|                return getattr(builtins, key)\n",
      "   160|         0|            0|            0|  0.00%|\n",
      "   161|         0|            0|            0|  0.00%|    return createResolutionCallbackFromEnv(env())\n",
      "   162|         0|            0|            0|  0.00%|\n",
      "   163|         0|            0|            0|  0.00%|\n",
      "   164|         0|            0|            0|  0.00%|def get_closure(fn):\n",
      "   165|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   166|         0|            0|            0|  0.00%|    Get a dictionary of closed over variables from a function\n",
      "   167|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   168|         0|            0|            0|  0.00%|    captures = {}\n",
      "   169|         0|            0|            0|  0.00%|    captures.update(fn.__globals__)\n",
      "   170|         0|            0|            0|  0.00%|\n",
      "   171|         0|            0|            0|  0.00%|    for index, captured_name in enumerate(fn.__code__.co_freevars):\n",
      "   172|         0|            0|            0|  0.00%|        captures[captured_name] = fn.__closure__[index].cell_contents\n",
      "   173|         0|            0|            0|  0.00%|\n",
      "   174|         0|            0|            0|  0.00%|    return captures\n",
      "   175|         0|            0|            0|  0.00%|\n",
      "   176|         0|            0|            0|  0.00%|# [local resolution in python]\n",
      "   177|         0|            0|            0|  0.00%|# Depending on where a variable is defined, and where it is used, we may\n",
      "   178|         0|            0|            0|  0.00%|# or may not be able to recover its value when recursively compiling a\n",
      "   179|         0|            0|            0|  0.00%|# script function. Remember in the general case, a module or function is\n",
      "   180|         0|            0|            0|  0.00%|# first defined and then later scripted. This means we do not have a\n",
      "   181|         0|            0|            0|  0.00%|# chance to capture the active frames when the function is defined. Hence any\n",
      "   182|         0|            0|            0|  0.00%|# name resolution has to happen later on the created closure. The way\n",
      "   183|         0|            0|            0|  0.00%|# python captures type annotations restricts what we can recover. The\n",
      "   184|         0|            0|            0|  0.00%|# follow example illustrates the different cases:\n",
      "   185|         0|            0|            0|  0.00%|#\n",
      "   186|         0|            0|            0|  0.00%|#         class MyGlobalClass:\n",
      "   187|         0|            0|            0|  0.00%|#         ...\n",
      "   188|         0|            0|            0|  0.00%|#         def my_local_scope():\n",
      "   189|         0|            0|            0|  0.00%|#             @torch.jit.script\n",
      "   190|         0|            0|            0|  0.00%|#             class MyClass:\n",
      "   191|         0|            0|            0|  0.00%|#                 ...\n",
      "   192|         0|            0|            0|  0.00%|#             @torch.jit.script\n",
      "   193|         0|            0|            0|  0.00%|#             class MyClassUsedAsVar:\n",
      "   194|         0|            0|            0|  0.00%|#                 ...\n",
      "   195|         0|            0|            0|  0.00%|#             def eg(x: MyClass, y: MyGlobalClass):\n",
      "   196|         0|            0|            0|  0.00%|#                 a_local_capture : Foo\n",
      "   197|         0|            0|            0|  0.00%|#                 return MyClassUsedAsVar(x)\n",
      "   198|         0|            0|            0|  0.00%|#\n",
      "   199|         0|            0|            0|  0.00%|# MyGlobalClass is defined in the __globals__ dictionary of function\n",
      "   200|         0|            0|            0|  0.00%|# 'eg', so it is always recoverable. my_local_scope introduces a new local\n",
      "   201|         0|            0|            0|  0.00%|# variable scope in the function. Classes defined here are only visible as\n",
      "   202|         0|            0|            0|  0.00%|# local variables. For the case of MyClassUsedAsVar, it is captured\n",
      "   203|         0|            0|            0|  0.00%|# because it is used as a variable inside the body of the function, and we\n",
      "   204|         0|            0|            0|  0.00%|# can resolve it using the captures returned from `get_closure`. However,\n",
      "   205|         0|            0|            0|  0.00%|# the type annotations are not captured by the closure. In Python\n",
      "   206|         0|            0|            0|  0.00%|# 3.0--3.9, the _value_ of MyClass and MyGlobalClass will be available as\n",
      "   207|         0|            0|            0|  0.00%|# annotations on `eg``, but starting in Python 4.0, they will represented as\n",
      "   208|         0|            0|            0|  0.00%|# strings and no longer present. Furthermore, since the body of `eg` does\n",
      "   209|         0|            0|            0|  0.00%|# not reference those names, they do not appear in the list of closed over\n",
      "   210|         0|            0|            0|  0.00%|# variables. In Python 2.x, type annotations are in comments, leading to a\n",
      "   211|         0|            0|            0|  0.00%|# similar situation where their definitions are not available. We anticipate\n",
      "   212|         0|            0|            0|  0.00%|# that most users will not run into this issue because their modules and\n",
      "   213|         0|            0|            0|  0.00%|# functions will be defined at a global scope like MyGlobalClass. In cases\n",
      "   214|         0|            0|            0|  0.00%|# where they are not, it is possible to work around issues by declaring the\n",
      "   215|         0|            0|            0|  0.00%|# values global in the function.\n",
      "   216|         0|            0|            0|  0.00%|# In Python 3.9 declaring class as global will make it invisible to\n",
      "   217|         0|            0|            0|  0.00%|# `inspect.getsource`, see https://bugs.python.org/issue42666 .\n",
      "   218|         0|            0|            0|  0.00%|# This could be worked around by manualy adding it to `global()` dictionary.\n",
      "   219|         0|            0|            0|  0.00%|\n",
      "   220|         0|            0|            0|  0.00%|\n",
      "   221|         0|            0|            0|  0.00%|\n",
      "   222|         0|            0|            0|  0.00%|def createResolutionCallbackFromClosure(fn):\n",
      "   223|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   224|         0|            0|            0|  0.00%|    Create a resolutionCallback by introspecting the function instead of\n",
      "   225|         0|            0|            0|  0.00%|    looking up the stack for the enclosing scope\n",
      "   226|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   227|         0|            0|            0|  0.00%|    closure = get_closure(fn)\n",
      "   228|         0|            0|            0|  0.00%|\n",
      "   229|         0|            0|            0|  0.00%|    class closure_lookup(object):\n",
      "   230|         0|            0|            0|  0.00%|        # This is a class since `closure` is a dict and it's easier in\n",
      "   231|         0|            0|            0|  0.00%|        # `env_helper` if everything just works with `getattr` calls\n",
      "   232|         0|            0|            0|  0.00%|        def __getattr__(self, key):\n",
      "   233|         0|            0|            0|  0.00%|            if key in closure:\n",
      "   234|         0|            0|            0|  0.00%|                return closure[key]\n",
      "   235|         0|            0|            0|  0.00%|            elif hasattr(typing, key):\n",
      "   236|         0|            0|            0|  0.00%|                return getattr(typing, key)\n",
      "   237|         0|            0|            0|  0.00%|            elif hasattr(builtins, key):\n",
      "   238|         0|            0|            0|  0.00%|                return getattr(builtins, key)\n",
      "   239|         0|            0|            0|  0.00%|            return None\n",
      "   240|         0|            0|            0|  0.00%|\n",
      "   241|         0|            0|            0|  0.00%|    return createResolutionCallbackFromEnv(closure_lookup())\n",
      "   242|         0|            0|            0|  0.00%|\n",
      "   243|         0|            0|            0|  0.00%|\n",
      "   244|         0|            0|            0|  0.00%|def can_compile_class(cls) -> bool:\n",
      "   245|         0|            0|            0|  0.00%|    # If any of the functions on a type don't have a code object, this type can't\n",
      "   246|         0|            0|            0|  0.00%|    # be compiled and is probably a builtin / bound from C\n",
      "   247|         0|            0|            0|  0.00%|    if is_ignored_fn(cls):\n",
      "   248|         0|            0|            0|  0.00%|        return False\n",
      "   249|         0|            0|            0|  0.00%|\n",
      "   250|         0|            0|            0|  0.00%|    # Ignore the following list of built-in classes.\n",
      "   251|         0|            0|            0|  0.00%|    ignored_builtin_classes = (torch.nn.Module, tuple, list, Exception)\n",
      "   252|         0|            0|            0|  0.00%|    if issubclass(cls, ignored_builtin_classes):\n",
      "   253|         0|            0|            0|  0.00%|        return False\n",
      "   254|         0|            0|            0|  0.00%|\n",
      "   255|         0|            0|            0|  0.00%|    names = cls.__dict__\n",
      "   256|         0|            0|            0|  0.00%|    fns = [getattr(cls, name) for name in names if inspect.isroutine(getattr(cls, name, None))]\n",
      "   257|         0|            0|            0|  0.00%|    has_code = [hasattr(fn, '__code__') for fn in fns]\n",
      "   258|         0|            0|            0|  0.00%|    return all(has_code)\n",
      "   259|         0|            0|            0|  0.00%|\n",
      "   260|         0|            0|            0|  0.00%|\n",
      "   261|         0|            0|            0|  0.00%|def get_callable_argument_names(fn) -> List[str]:\n",
      "   262|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   263|         0|            0|            0|  0.00%|    Gets names of all POSITIONAL_OR_KEYWORD arguments for callable `fn`.\n",
      "   264|         0|            0|            0|  0.00%|    Returns an empty list when other types of arguments are present.\n",
      "   265|         0|            0|            0|  0.00%|\n",
      "   266|         0|            0|            0|  0.00%|    This is used by `torch.jit.trace` to assign meaningful argument names to\n",
      "   267|         0|            0|            0|  0.00%|    traced functions and modules.\n",
      "   268|         0|            0|            0|  0.00%|\n",
      "   269|         0|            0|            0|  0.00%|    Args:\n",
      "   270|         0|            0|            0|  0.00%|        fn: A callable.\n",
      "   271|         0|            0|            0|  0.00%|    Returns:\n",
      "   272|         0|            0|            0|  0.00%|        Argument names: List[str]\n",
      "   273|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   274|         0|            0|            0|  0.00%|    # inspect.signature may fail, give up in that case.\n",
      "   275|         0|            0|            0|  0.00%|    try:\n",
      "   276|         0|            0|            0|  0.00%|        callable_signature = inspect.signature(fn)\n",
      "   277|         0|            0|            0|  0.00%|    except Exception:\n",
      "   278|         0|            0|            0|  0.00%|        return []\n",
      "   279|         0|            0|            0|  0.00%|\n",
      "   280|         0|            0|            0|  0.00%|    argument_names = []\n",
      "   281|         0|            0|            0|  0.00%|    for name, param in callable_signature.parameters.items():\n",
      "   282|         0|            0|            0|  0.00%|        # All four other types of arguments do not map to individual values\n",
      "   283|         0|            0|            0|  0.00%|        # with a keyword as name.\n",
      "   284|         0|            0|            0|  0.00%|        if not param.kind == param.POSITIONAL_OR_KEYWORD:\n",
      "   285|         0|            0|            0|  0.00%|            return []\n",
      "   286|         0|            0|            0|  0.00%|\n",
      "   287|         0|            0|            0|  0.00%|        argument_names.append(name)\n",
      "   288|         0|            0|            0|  0.00%|\n",
      "   289|         0|            0|            0|  0.00%|    return argument_names\n",
      "   290|         0|            0|            0|  0.00%|\n",
      "   291|         0|            0|            0|  0.00%|\n",
      "   292|         0|            0|            0|  0.00%|def get_annotation_str(annotation):\n",
      "   293|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   294|         0|            0|            0|  0.00%|    Convert an AST node containing a type annotation to the string present in the source\n",
      "   295|         0|            0|            0|  0.00%|    that represents the same annotation.\n",
      "   296|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   297|         0|            0|            0|  0.00%|    if isinstance(annotation, ast.Name):\n",
      "   298|         0|            0|            0|  0.00%|        return annotation.id\n",
      "   299|         0|            0|            0|  0.00%|    elif isinstance(annotation, ast.Attribute):\n",
      "   300|         0|            0|            0|  0.00%|        return '.'.join([get_annotation_str(annotation.value), annotation.attr])\n",
      "   301|         0|            0|            0|  0.00%|    elif isinstance(annotation, ast.Subscript):\n",
      "   302|         0|            0|            0|  0.00%|        # In Python3.9+ subscript indicies are not wrapped in ast.Index\n",
      "   303|         0|            0|            0|  0.00%|        subscript_slice = annotation.slice if sys.version_info >= (3, 9) else annotation.slice.value  # type: ignore[attr-defined]\n",
      "   304|         0|            0|            0|  0.00%|        return f\"{get_annotation_str(annotation.value)}[{get_annotation_str(subscript_slice)}]\"\n",
      "   305|         0|            0|            0|  0.00%|    elif isinstance(annotation, ast.Tuple):\n",
      "   306|         0|            0|            0|  0.00%|        return ','.join([get_annotation_str(elt) for elt in annotation.elts])\n",
      "   307|         0|            0|            0|  0.00%|    elif isinstance(annotation, ast.Constant) or isinstance(annotation, ast.NameConstant):\n",
      "   308|         0|            0|            0|  0.00%|        return f\"{annotation.value}\"\n",
      "   309|         0|            0|            0|  0.00%|\n",
      "   310|         0|            0|            0|  0.00%|    # If an AST node is not handled here, it's probably handled in ScriptTypeParser.\n",
      "   311|         0|            0|            0|  0.00%|    return None\n",
      "   312|         0|            0|            0|  0.00%|\n",
      "   313|         0|            0|            0|  0.00%|\n",
      "   314|         0|            0|            0|  0.00%|def get_type_hint_captures(fn):\n",
      "   315|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   316|         0|            0|            0|  0.00%|    Get a dictionary containing type resolution mappings necessary to resolve types\n",
      "   317|         0|            0|            0|  0.00%|    for the literal annotations on 'fn'. These are not considered to be closed-over by fn\n",
      "   318|         0|            0|            0|  0.00%|    and must be obtained separately (e.g. using this function).\n",
      "   319|         0|            0|            0|  0.00%|\n",
      "   320|         0|            0|            0|  0.00%|    Args:\n",
      "   321|         0|            0|            0|  0.00%|        fn: A callable.\n",
      "   322|         0|            0|            0|  0.00%|    Returns:\n",
      "   323|         0|            0|            0|  0.00%|        A Dict[str, Any] containing a mapping from the literal annotations used on\n",
      "   324|         0|            0|            0|  0.00%|        fn to the Python objects they refer to.\n",
      "   325|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   326|         0|            0|            0|  0.00%|    # Gather a dictionary of parameter name -> type, skipping any parameters whose annotated\n",
      "   327|         0|            0|            0|  0.00%|    # types are strings. These are only understood by TorchScript in the context of a type annotation\n",
      "   328|         0|            0|            0|  0.00%|    # that refers to a class in its own definition, but trying to include a mapping for this in the result\n",
      "   329|         0|            0|            0|  0.00%|    # function would cause infinite recursion because the class is currently being compiled.\n",
      "   330|         0|            0|            0|  0.00%|    # In addition, there is logic in ScriptTypeParser to handle this.\n",
      "   331|         0|            0|            0|  0.00%|    signature = inspect.signature(fn)\n",
      "   332|         0|            0|            0|  0.00%|    name_to_type = {\n",
      "   333|         0|            0|            0|  0.00%|        name: parameter.annotation\n",
      "   334|         0|            0|            0|  0.00%|        for name, parameter in signature.parameters.items()\n",
      "   335|         0|            0|            0|  0.00%|        if parameter.annotation is not inspect.Parameter.empty and not isinstance(parameter.annotation, str)\n",
      "   336|         0|            0|            0|  0.00%|    }\n",
      "   337|         0|            0|            0|  0.00%|\n",
      "   338|         0|            0|            0|  0.00%|    # Then, get the literal type annotations from the function declaration\n",
      "   339|         0|            0|            0|  0.00%|    # by source inspection. This accounts for the case in which aliases are used\n",
      "   340|         0|            0|            0|  0.00%|    # to annotate the arguments (e.g device_t = torch.device, and then d: device_t).\n",
      "   341|         0|            0|            0|  0.00%|    src = inspect.getsource(fn)\n",
      "   342|         0|            0|            0|  0.00%|\n",
      "   343|         0|            0|            0|  0.00%|    # frontend.py cannot be used here because it includes _jit_internal, so use ast instead.\n",
      "   344|         0|            0|            0|  0.00%|    a = ast.parse(dedent(src))\n",
      "   345|         0|            0|            0|  0.00%|    if len(a.body) != 1 or not isinstance(a.body[0], ast.FunctionDef):\n",
      "   346|         0|            0|            0|  0.00%|        raise RuntimeError(f\"Expected {fn} to be a function\")\n",
      "   347|         0|            0|            0|  0.00%|    f = a.body[0]\n",
      "   348|         0|            0|            0|  0.00%|\n",
      "   349|         0|            0|            0|  0.00%|    # Prepare a dictionary of source annotation -> type, which will be the final result of this function,\n",
      "   350|         0|            0|            0|  0.00%|    # by using the parsed AST (f) to reconstruct source annotations as strings for each parameter and mapping\n",
      "   351|         0|            0|            0|  0.00%|    # them to the type object corresponding to the annotation via name_to_type using the parameter name.\n",
      "   352|         0|            0|            0|  0.00%|    annotation_to_type = {}\n",
      "   353|         0|            0|            0|  0.00%|\n",
      "   354|         0|            0|            0|  0.00%|    for arg in f.args.args:\n",
      "   355|         0|            0|            0|  0.00%|        # Get the source type annotation string for this argument if possible.\n",
      "   356|         0|            0|            0|  0.00%|        arg_annotation_str = get_annotation_str(arg.annotation) if arg.annotation else None\n",
      "   357|         0|            0|            0|  0.00%|\n",
      "   358|         0|            0|            0|  0.00%|        # If the argument has no annotation or get_annotation_str cannot convert it to a string,\n",
      "   359|         0|            0|            0|  0.00%|        # arg_annotation_str will be None. Skip this arg; ScriptTypeParser will probably handle\n",
      "   360|         0|            0|            0|  0.00%|        # this in the latter case.\n",
      "   361|         0|            0|            0|  0.00%|        if arg_annotation_str is None:\n",
      "   362|         0|            0|            0|  0.00%|            continue\n",
      "   363|         0|            0|            0|  0.00%|\n",
      "   364|         0|            0|            0|  0.00%|        # Insert {arg_annotation_str: type} into annotation_to_type if possible. One reason arg_name may not\n",
      "   365|         0|            0|            0|  0.00%|        # be present in name_to_type is that the annotation itself is a string and not a type object\n",
      "   366|         0|            0|            0|  0.00%|        # (common for self-refential annotations in classes). Once again, let ScriptTypeParser handle this.\n",
      "   367|         0|            0|            0|  0.00%|        arg_name = arg.arg\n",
      "   368|         0|            0|            0|  0.00%|        if arg_name in name_to_type:\n",
      "   369|         0|            0|            0|  0.00%|            annotation_to_type[arg_annotation_str] = name_to_type[arg_name]\n",
      "   370|         0|            0|            0|  0.00%|\n",
      "   371|         0|            0|            0|  0.00%|    # If there is a valid return annotation, include it in annotation_to_type. As with argument annotations,\n",
      "   372|         0|            0|            0|  0.00%|    # the literal annotation has to be convertible to a string by get_annotation_str, and the actual type\n",
      "   373|         0|            0|            0|  0.00%|    # of the annotation cannot be a string.\n",
      "   374|         0|            0|            0|  0.00%|    literal_return_annotation = get_annotation_str(f.returns)\n",
      "   375|         0|            0|            0|  0.00%|    valid_literal_annotation = literal_return_annotation is not None\n",
      "   376|         0|            0|            0|  0.00%|    return_annotation = signature.return_annotation\n",
      "   377|         0|            0|            0|  0.00%|    valid_return_annotation_type = return_annotation is not inspect.Parameter.empty and not isinstance(return_annotation, str)\n",
      "   378|         0|            0|            0|  0.00%|    if valid_literal_annotation and valid_return_annotation_type:\n",
      "   379|         0|            0|            0|  0.00%|        annotation_to_type[literal_return_annotation] = return_annotation\n",
      "   380|         0|            0|            0|  0.00%|\n",
      "   381|         0|            0|            0|  0.00%|    return annotation_to_type\n",
      "   382|         0|            0|            0|  0.00%|\n",
      "   383|         0|            0|            0|  0.00%|\n",
      "   384|         0|            0|            0|  0.00%|def createResolutionCallbackForClassMethods(cls):\n",
      "   385|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   386|         0|            0|            0|  0.00%|    This looks at all the methods defined in a class and pulls their closed-over\n",
      "   387|         0|            0|            0|  0.00%|    variables into a dictionary and uses that to resolve variables.\n",
      "   388|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   389|         0|            0|            0|  0.00%|    # cls is a type here, so `ismethod` is false since the methods on the type\n",
      "   390|         0|            0|            0|  0.00%|    # aren't bound to anything, so Python treats them as regular functions\n",
      "   391|         0|            0|            0|  0.00%|    fns = [getattr(cls, name) for name in cls.__dict__ if inspect.isroutine(getattr(cls, name))]\n",
      "   392|         0|            0|            0|  0.00%|    captures = {}\n",
      "   393|         0|            0|            0|  0.00%|\n",
      "   394|         0|            0|            0|  0.00%|    for fn in fns:\n",
      "   395|         0|            0|            0|  0.00%|        captures.update(get_closure(fn))\n",
      "   396|         0|            0|            0|  0.00%|        captures.update(get_type_hint_captures(fn))\n",
      "   397|         0|            0|            0|  0.00%|\n",
      "   398|         0|            0|            0|  0.00%|    def lookup_in_class(key):\n",
      "   399|         0|            0|            0|  0.00%|        if key in captures:\n",
      "   400|         0|            0|            0|  0.00%|            return captures[key]\n",
      "   401|         0|            0|            0|  0.00%|        else:\n",
      "   402|         0|            0|            0|  0.00%|            return getattr(builtins, key, None)\n",
      "   403|         0|            0|            0|  0.00%|\n",
      "   404|         0|            0|            0|  0.00%|    return lookup_in_class\n",
      "   405|         0|            0|            0|  0.00%|\n",
      "   406|         0|            0|            0|  0.00%|\n",
      "   407|         0|            0|            0|  0.00%|def boolean_dispatch(arg_name, arg_index, default, if_true, if_false, module_name, func_name):\n",
      "   408|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   409|         0|            0|            0|  0.00%|    Dispatches to either of 2 script functions based on a boolean argument.\n",
      "   410|         0|            0|            0|  0.00%|    In TorchScript, the boolean argument must be constant so that the correct\n",
      "   411|         0|            0|            0|  0.00%|    function to use can be determined at compile time.\n",
      "   412|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   413|         0|            0|            0|  0.00%|    def fn(*args, **kwargs):\n",
      "   414|         0|            0|            0|  0.00%|        dispatch_flag = False\n",
      "   415|         0|            0|            0|  0.00%|        if arg_name in kwargs:\n",
      "   416|         0|            0|            0|  0.00%|            dispatch_flag = kwargs[arg_name]\n",
      "   417|         0|            0|            0|  0.00%|        elif arg_index < len(args):\n",
      "   418|         0|            0|            0|  0.00%|            dispatch_flag = args[arg_index]\n",
      "   419|         0|            0|            0|  0.00%|\n",
      "   420|         0|            0|            0|  0.00%|        if dispatch_flag:\n",
      "   421|         0|            0|            0|  0.00%|            return if_true(*args, **kwargs)\n",
      "   422|         0|            0|            0|  0.00%|        else:\n",
      "   423|         0|            0|            0|  0.00%|            return if_false(*args, **kwargs)\n",
      "   424|         0|            0|            0|  0.00%|\n",
      "   425|         0|            0|            0|  0.00%|    if if_true.__doc__ is None and if_false.__doc__ is not None:\n",
      "   426|         0|            0|            0|  0.00%|        doc = if_false.__doc__\n",
      "   427|         0|            0|            0|  0.00%|        if_true.__doc__ = doc\n",
      "   428|         0|            0|            0|  0.00%|    elif if_false.__doc__ is None and if_true.__doc__ is not None:\n",
      "   429|         0|            0|            0|  0.00%|        doc = if_true.__doc__\n",
      "   430|         0|            0|            0|  0.00%|        if_false.__doc__ = doc\n",
      "   431|         0|            0|            0|  0.00%|    elif if_false.__doc__ is None and if_true.__doc__ is None:\n",
      "   432|         0|            0|            0|  0.00%|        # neither function has a docstring\n",
      "   433|         0|            0|            0|  0.00%|        doc = None\n",
      "   434|         0|            0|            0|  0.00%|    else:\n",
      "   435|         0|            0|            0|  0.00%|        raise RuntimeError(\"only one function can have a docstring\")\n",
      "   436|         0|            0|            0|  0.00%|    fn.__doc__ = doc\n",
      "   437|         0|            0|            0|  0.00%|\n",
      "   438|         0|            0|            0|  0.00%|    if module_name is not None:\n",
      "   439|         0|            0|            0|  0.00%|        fn.__module__ = module_name\n",
      "   440|         0|            0|            0|  0.00%|    if func_name is not None:\n",
      "   441|         0|            0|            0|  0.00%|        fn.__name__ = func_name\n",
      "   442|         0|            0|            0|  0.00%|\n",
      "   443|         0|            0|            0|  0.00%|    boolean_dispatched[fn] = {\n",
      "   444|         0|            0|            0|  0.00%|        \"if_true\": if_true,\n",
      "   445|         0|            0|            0|  0.00%|        \"if_false\": if_false,\n",
      "   446|         0|            0|            0|  0.00%|        \"index\": arg_index,\n",
      "   447|         0|            0|            0|  0.00%|        \"default\": default,\n",
      "   448|         0|            0|            0|  0.00%|        \"arg_name\": arg_name\n",
      "   449|         0|            0|            0|  0.00%|    }\n",
      "   450|         0|            0|            0|  0.00%|    return fn\n",
      "   451|         0|            0|            0|  0.00%|\n",
      "   452|         0|            0|            0|  0.00%|\n",
      "   453|         0|            0|            0|  0.00%|class FunctionModifiers(object):\n",
      "   454|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   455|         0|            0|            0|  0.00%|    Used to denote the behavior of a function in TorchScript. See export() and\n",
      "   456|         0|            0|            0|  0.00%|    ignore() for details.\n",
      "   457|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   458|         0|            0|            0|  0.00%|    UNUSED = \"unused (ignored and replaced with raising of an exception)\"\n",
      "   459|         0|            0|            0|  0.00%|    IGNORE = \"ignore (leave as a call to Python, cannot be torch.jit.save'd)\"\n",
      "   460|         0|            0|            0|  0.00%|    EXPORT = \"export (compile this function even if nothing calls it)\"\n",
      "   461|         0|            0|            0|  0.00%|    DEFAULT = \"default (compile if called from a exported function / forward)\"\n",
      "   462|         0|            0|            0|  0.00%|    COPY_TO_SCRIPT_WRAPPER = \\\n",
      "   463|         0|            0|            0|  0.00%|        \"if this method is not scripted, copy the python method onto the scripted model\"\n",
      "   464|         0|            0|            0|  0.00%|\n",
      "   465|         0|            0|            0|  0.00%|\n",
      "   466|         0|            0|            0|  0.00%|def export(fn):\n",
      "   467|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   468|         0|            0|            0|  0.00%|    This decorator indicates that a method on an ``nn.Module`` is used as an entry point into a\n",
      "   469|         0|            0|            0|  0.00%|    :class:`ScriptModule` and should be compiled.\n",
      "   470|         0|            0|            0|  0.00%|\n",
      "   471|         0|            0|            0|  0.00%|    ``forward`` implicitly is assumed to be an entry point, so it does not need this decorator.\n",
      "   472|         0|            0|            0|  0.00%|    Functions and methods called from ``forward`` are compiled as they are seen\n",
      "   473|         0|            0|            0|  0.00%|    by the compiler, so they do not need this decorator either.\n",
      "   474|         0|            0|            0|  0.00%|\n",
      "   475|         0|            0|            0|  0.00%|    Example (using ``@torch.jit.export`` on a method):\n",
      "   476|         0|            0|            0|  0.00%|\n",
      "   477|         0|            0|            0|  0.00%|    .. testcode::\n",
      "   478|         0|            0|            0|  0.00%|\n",
      "   479|         0|            0|            0|  0.00%|        import torch\n",
      "   480|         0|            0|            0|  0.00%|        import torch.nn as nn\n",
      "   481|         0|            0|            0|  0.00%|\n",
      "   482|         0|            0|            0|  0.00%|        class MyModule(nn.Module):\n",
      "   483|         0|            0|            0|  0.00%|            def implicitly_compiled_method(self, x):\n",
      "   484|         0|            0|            0|  0.00%|                return x + 99\n",
      "   485|         0|            0|            0|  0.00%|\n",
      "   486|         0|            0|            0|  0.00%|            # `forward` is implicitly decorated with `@torch.jit.export`,\n",
      "   487|         0|            0|            0|  0.00%|            # so adding it here would have no effect\n",
      "   488|         0|            0|            0|  0.00%|            def forward(self, x):\n",
      "   489|         0|            0|            0|  0.00%|                return x + 10\n",
      "   490|         0|            0|            0|  0.00%|\n",
      "   491|         0|            0|            0|  0.00%|            @torch.jit.export\n",
      "   492|         0|            0|            0|  0.00%|            def another_forward(self, x):\n",
      "   493|         0|            0|            0|  0.00%|                # When the compiler sees this call, it will compile\n",
      "   494|         0|            0|            0|  0.00%|                # `implicitly_compiled_method`\n",
      "   495|         0|            0|            0|  0.00%|                return self.implicitly_compiled_method(x)\n",
      "   496|         0|            0|            0|  0.00%|\n",
      "   497|         0|            0|            0|  0.00%|            def unused_method(self, x):\n",
      "   498|         0|            0|            0|  0.00%|                return x - 20\n",
      "   499|         0|            0|            0|  0.00%|\n",
      "   500|         0|            0|            0|  0.00%|        # `m` will contain compiled methods:\n",
      "   501|         0|            0|            0|  0.00%|        #     `forward`\n",
      "   502|         0|            0|            0|  0.00%|        #     `another_forward`\n",
      "   503|         0|            0|            0|  0.00%|        #     `implicitly_compiled_method`\n",
      "   504|         0|            0|            0|  0.00%|        # `unused_method` will not be compiled since it was not called from\n",
      "   505|         0|            0|            0|  0.00%|        # any compiled methods and wasn't decorated with `@torch.jit.export`\n",
      "   506|         0|            0|            0|  0.00%|        m = torch.jit.script(MyModule())\n",
      "   507|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   508|         0|            0|            0|  0.00%|    fn._torchscript_modifier = FunctionModifiers.EXPORT\n",
      "   509|         0|            0|            0|  0.00%|    return fn\n",
      "   510|         0|            0|            0|  0.00%|\n",
      "   511|         0|            0|            0|  0.00%|\n",
      "   512|         0|            0|            0|  0.00%|def unused(fn):\n",
      "   513|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   514|         0|            0|            0|  0.00%|    This decorator indicates to the compiler that a function or method should\n",
      "   515|         0|            0|            0|  0.00%|    be ignored and replaced with the raising of an exception. This allows you\n",
      "   516|         0|            0|            0|  0.00%|    to leave code in your model that is not yet TorchScript compatible and still\n",
      "   517|         0|            0|            0|  0.00%|    export your model.\n",
      "   518|         0|            0|            0|  0.00%|\n",
      "   519|         0|            0|            0|  0.00%|        Example (using ``@torch.jit.unused`` on a method)::\n",
      "   520|         0|            0|            0|  0.00%|\n",
      "   521|         0|            0|            0|  0.00%|            import torch\n",
      "   522|         0|            0|            0|  0.00%|            import torch.nn as nn\n",
      "   523|         0|            0|            0|  0.00%|\n",
      "   524|         0|            0|            0|  0.00%|            class MyModule(nn.Module):\n",
      "   525|         0|            0|            0|  0.00%|                def __init__(self, use_memory_efficient):\n",
      "   526|         0|            0|            0|  0.00%|                    super(MyModule, self).__init__()\n",
      "   527|         0|            0|            0|  0.00%|                    self.use_memory_efficient = use_memory_efficient\n",
      "   528|         0|            0|            0|  0.00%|\n",
      "   529|         0|            0|            0|  0.00%|                @torch.jit.unused\n",
      "   530|         0|            0|            0|  0.00%|                def memory_efficient(self, x):\n",
      "   531|         0|            0|            0|  0.00%|                    import pdb\n",
      "   532|         0|            0|            0|  0.00%|                    pdb.set_trace()\n",
      "   533|         0|            0|            0|  0.00%|                    return x + 10\n",
      "   534|         0|            0|            0|  0.00%|\n",
      "   535|         0|            0|            0|  0.00%|                def forward(self, x):\n",
      "   536|         0|            0|            0|  0.00%|                    # Use not-yet-scriptable memory efficient mode\n",
      "   537|         0|            0|            0|  0.00%|                    if self.use_memory_efficient:\n",
      "   538|         0|            0|            0|  0.00%|                        return self.memory_efficient(x)\n",
      "   539|         0|            0|            0|  0.00%|                    else:\n",
      "   540|         0|            0|            0|  0.00%|                        return x + 10\n",
      "   541|         0|            0|            0|  0.00%|\n",
      "   542|         0|            0|            0|  0.00%|            m = torch.jit.script(MyModule(use_memory_efficient=False))\n",
      "   543|         0|            0|            0|  0.00%|            m.save(\"m.pt\")\n",
      "   544|         0|            0|            0|  0.00%|\n",
      "   545|         0|            0|            0|  0.00%|            m = torch.jit.script(MyModule(use_memory_efficient=True))\n",
      "   546|         0|            0|            0|  0.00%|            # exception raised\n",
      "   547|         0|            0|            0|  0.00%|            m(torch.rand(100))\n",
      "   548|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   549|         0|            0|            0|  0.00%|    if isinstance(fn, property):\n",
      "   550|         0|            0|            0|  0.00%|        prop = fn\n",
      "   551|         0|            0|            0|  0.00%|        setattr(prop.fget, \"_torchscript_modifier\", FunctionModifiers.UNUSED)  # noqa: B010\n",
      "   552|         0|            0|            0|  0.00%|\n",
      "   553|         0|            0|            0|  0.00%|        if prop.fset:\n",
      "   554|         0|            0|            0|  0.00%|            setattr(prop.fset, \"_torchscript_modifier\", FunctionModifiers.UNUSED)  # noqa: B010\n",
      "   555|         0|            0|            0|  0.00%|\n",
      "   556|         0|            0|            0|  0.00%|        return prop\n",
      "   557|         0|            0|            0|  0.00%|\n",
      "   558|         0|            0|            0|  0.00%|    fn._torchscript_modifier = FunctionModifiers.UNUSED\n",
      "   559|         0|            0|            0|  0.00%|    return fn\n",
      "   560|         0|            0|            0|  0.00%|\n",
      "   561|         0|            0|            0|  0.00%|# No op context manager from python side\n",
      "   562|         0|            0|            0|  0.00%|class _IgnoreContextManager(contextlib.AbstractContextManager):\n",
      "   563|         0|            0|            0|  0.00%|    def __init__(self, **kwargs):\n",
      "   564|         0|            0|            0|  0.00%|        pass\n",
      "   565|         0|            0|            0|  0.00%|\n",
      "   566|         0|            0|            0|  0.00%|    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n",
      "   567|         0|            0|            0|  0.00%|        pass\n",
      "   568|         0|            0|            0|  0.00%|\n",
      "   569|         0|            0|            0|  0.00%|def ignore(drop=False, **kwargs):\n",
      "   570|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   571|         0|            0|            0|  0.00%|    This decorator indicates to the compiler that a function or method should\n",
      "   572|         0|            0|            0|  0.00%|    be ignored and left as a Python function. This allows you to leave code in\n",
      "   573|         0|            0|            0|  0.00%|    your model that is not yet TorchScript compatible. If called from TorchScript,\n",
      "   574|         0|            0|            0|  0.00%|    ignored functions will dispatch the call to the Python interpreter. Models with ignored\n",
      "   575|         0|            0|            0|  0.00%|    functions cannot be exported; use :func:`@torch.jit.unused <torch.jit.unused>` instead.\n",
      "   576|         0|            0|            0|  0.00%|\n",
      "   577|         0|            0|            0|  0.00%|    Example (using ``@torch.jit.ignore`` on a method)::\n",
      "   578|         0|            0|            0|  0.00%|\n",
      "   579|         0|            0|            0|  0.00%|        import torch\n",
      "   580|         0|            0|            0|  0.00%|        import torch.nn as nn\n",
      "   581|         0|            0|            0|  0.00%|\n",
      "   582|         0|            0|            0|  0.00%|        class MyModule(nn.Module):\n",
      "   583|         0|            0|            0|  0.00%|            @torch.jit.ignore\n",
      "   584|         0|            0|            0|  0.00%|            def debugger(self, x):\n",
      "   585|         0|            0|            0|  0.00%|                import pdb\n",
      "   586|         0|            0|            0|  0.00%|                pdb.set_trace()\n",
      "   587|         0|            0|            0|  0.00%|\n",
      "   588|         0|            0|            0|  0.00%|            def forward(self, x):\n",
      "   589|         0|            0|            0|  0.00%|                x += 10\n",
      "   590|         0|            0|            0|  0.00%|                # The compiler would normally try to compile `debugger`,\n",
      "   591|         0|            0|            0|  0.00%|                # but since it is `@ignore`d, it will be left as a call\n",
      "   592|         0|            0|            0|  0.00%|                # to Python\n",
      "   593|         0|            0|            0|  0.00%|                self.debugger(x)\n",
      "   594|         0|            0|            0|  0.00%|                return x\n",
      "   595|         0|            0|            0|  0.00%|\n",
      "   596|         0|            0|            0|  0.00%|        m = torch.jit.script(MyModule())\n",
      "   597|         0|            0|            0|  0.00%|\n",
      "   598|         0|            0|            0|  0.00%|        # Error! The call `debugger` cannot be saved since it calls into Python\n",
      "   599|         0|            0|            0|  0.00%|        m.save(\"m.pt\")\n",
      "   600|         0|            0|            0|  0.00%|\n",
      "   601|         0|            0|            0|  0.00%|    Example (using ``@torch.jit.ignore(drop=True)`` on a method):\n",
      "   602|         0|            0|            0|  0.00%|\n",
      "   603|         0|            0|            0|  0.00%|    .. testcode::\n",
      "   604|         0|            0|            0|  0.00%|\n",
      "   605|         0|            0|            0|  0.00%|        import torch\n",
      "   606|         0|            0|            0|  0.00%|        import torch.nn as nn\n",
      "   607|         0|            0|            0|  0.00%|\n",
      "   608|         0|            0|            0|  0.00%|        class MyModule(nn.Module):\n",
      "   609|         0|            0|            0|  0.00%|            @torch.jit.ignore(drop=True)\n",
      "   610|         0|            0|            0|  0.00%|            def training_method(self, x):\n",
      "   611|         0|            0|            0|  0.00%|                import pdb\n",
      "   612|         0|            0|            0|  0.00%|                pdb.set_trace()\n",
      "   613|         0|            0|            0|  0.00%|\n",
      "   614|         0|            0|            0|  0.00%|            def forward(self, x):\n",
      "   615|         0|            0|            0|  0.00%|                if self.training:\n",
      "   616|         0|            0|            0|  0.00%|                    self.training_method(x)\n",
      "   617|         0|            0|            0|  0.00%|                return x\n",
      "   618|         0|            0|            0|  0.00%|\n",
      "   619|         0|            0|            0|  0.00%|        m = torch.jit.script(MyModule())\n",
      "   620|         0|            0|            0|  0.00%|\n",
      "   621|         0|            0|            0|  0.00%|        # This is OK since `training_method` is not saved, the call is replaced\n",
      "   622|         0|            0|            0|  0.00%|        # with a `raise`.\n",
      "   623|         0|            0|            0|  0.00%|        m.save(\"m.pt\")\n",
      "   624|         0|            0|            0|  0.00%|\n",
      "   625|         0|            0|            0|  0.00%|    .. testcleanup::\n",
      "   626|         0|            0|            0|  0.00%|\n",
      "   627|         0|            0|            0|  0.00%|        import os\n",
      "   628|         0|            0|            0|  0.00%|        os.remove('m.pt')\n",
      "   629|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   630|         0|            0|            0|  0.00%|\n",
      "   631|         0|            0|            0|  0.00%|    if callable(drop):\n",
      "   632|         0|            0|            0|  0.00%|        # used without any args, so drop is actually a function\n",
      "   633|         0|            0|            0|  0.00%|        #   @torch.jit.ignore\n",
      "   634|         0|            0|            0|  0.00%|        #   def fn(...):\n",
      "   635|         0|            0|            0|  0.00%|        fn = drop\n",
      "   636|         0|            0|            0|  0.00%|        fn._torchscript_modifier = FunctionModifiers.IGNORE\n",
      "   637|         0|            0|            0|  0.00%|        return fn\n",
      "   638|         0|            0|            0|  0.00%|\n",
      "   639|         0|            0|            0|  0.00%|    if not isinstance(drop, bool):\n",
      "   640|         0|            0|            0|  0.00%|        raise RuntimeError(\"Argument to @torch.jit.ignore must be a bool or \"\n",
      "   641|         0|            0|            0|  0.00%|                           f\"a function but got {drop}\")\n",
      "   642|         0|            0|            0|  0.00%|\n",
      "   643|         0|            0|            0|  0.00%|    # for backwards compat\n",
      "   644|         0|            0|            0|  0.00%|    drop_on_export = kwargs.pop(\"drop_on_export\", None)\n",
      "   645|         0|            0|            0|  0.00%|    if drop_on_export:\n",
      "   646|         0|            0|            0|  0.00%|        warnings.warn(\"ignore(drop_on_export=True) has been deprecated. TorchScript will now drop the function \"\n",
      "   647|         0|            0|            0|  0.00%|                      \"call on compilation. Use torch.jit.unused now. {}\", category=FutureWarning)\n",
      "   648|         0|            0|            0|  0.00%|\n",
      "   649|         0|            0|            0|  0.00%|        drop = drop_on_export\n",
      "   650|         0|            0|            0|  0.00%|    elif drop:\n",
      "   651|         0|            0|            0|  0.00%|        warnings.warn(\"ignore(True) has been deprecated. TorchScript will now drop the function \"\n",
      "   652|         0|            0|            0|  0.00%|                      \"call on compilation. Use torch.jit.unused now. {}\", category=FutureWarning)\n",
      "   653|         0|            0|            0|  0.00%|\n",
      "   654|         0|            0|            0|  0.00%|    def decorator(fn):\n",
      "   655|         0|            0|            0|  0.00%|        if drop:\n",
      "   656|         0|            0|            0|  0.00%|            fn._torchscript_modifier = FunctionModifiers.UNUSED\n",
      "   657|         0|            0|            0|  0.00%|        else:\n",
      "   658|         0|            0|            0|  0.00%|            fn._torchscript_modifier = FunctionModifiers.IGNORE\n",
      "   659|         0|            0|            0|  0.00%|        return fn\n",
      "   660|         0|            0|            0|  0.00%|    return decorator\n",
      "   661|         0|            0|            0|  0.00%|\n",
      "   662|         0|            0|            0|  0.00%|\n",
      "   663|         0|            0|            0|  0.00%|def _copy_to_script_wrapper(fn):\n",
      "   664|         0|            0|            0|  0.00%|    fn._torchscript_modifier = FunctionModifiers.COPY_TO_SCRIPT_WRAPPER\n",
      "   665|         0|            0|            0|  0.00%|    return fn\n",
      "   666|         0|            0|            0|  0.00%|\n",
      "   667|         0|            0|            0|  0.00%|def module_has_exports(mod):\n",
      "   668|         0|            0|            0|  0.00%|    for name in dir(mod):\n",
      "   669|         0|            0|            0|  0.00%|        if hasattr(mod, name):\n",
      "   670|         0|            0|            0|  0.00%|            item = getattr(mod, name)\n",
      "   671|         0|            0|            0|  0.00%|            if callable(item):\n",
      "   672|         0|            0|            0|  0.00%|                if get_torchscript_modifier(item) is FunctionModifiers.EXPORT:\n",
      "   673|         0|            0|            0|  0.00%|                    return True\n",
      "   674|         0|            0|            0|  0.00%|    return False\n",
      "   675|         0|            0|            0|  0.00%|\n",
      "   676|         0|            0|            0|  0.00%|\n",
      "   677|         0|            0|            0|  0.00%|# WARNING: should_drop is currently being used by our JIT code coverage plug-in to mark JIT'd code as covered. If you\n",
      "   678|         0|            0|            0|  0.00%|# rename this function, please update references in tools/coverage_plugins_package/src/coverage_plugins/jit_plugin.py to\n",
      "   679|         0|            0|            0|  0.00%|# allow JIT'd code to still be covered.\n",
      "   680|         0|            0|            0|  0.00%|def should_drop(fn) -> bool:\n",
      "   681|         0|            0|            0|  0.00%|    attr = get_torchscript_modifier(fn)\n",
      "   682|         0|            0|            0|  0.00%|    if attr is None:\n",
      "   683|         0|            0|            0|  0.00%|        return False\n",
      "   684|         0|            0|            0|  0.00%|    return attr is FunctionModifiers.UNUSED\n",
      "   685|         0|            0|            0|  0.00%|\n",
      "   686|         0|            0|            0|  0.00%|\n",
      "   687|         0|            0|            0|  0.00%|def is_ignored_fn(fn) -> bool:\n",
      "   688|         0|            0|            0|  0.00%|    mod = get_torchscript_modifier(fn)\n",
      "   689|         0|            0|            0|  0.00%|    return mod is FunctionModifiers.UNUSED or mod is FunctionModifiers.IGNORE\n",
      "   690|         0|            0|            0|  0.00%|\n",
      "   691|         0|            0|            0|  0.00%|\n",
      "   692|         0|            0|            0|  0.00%|def is_static_fn(cls, fn) -> bool:\n",
      "   693|         0|            0|            0|  0.00%|    return isinstance(inspect.getattr_static(cls, fn, default=None), staticmethod)\n",
      "   694|         0|            0|            0|  0.00%|\n",
      "   695|         0|            0|            0|  0.00%|def get_static_fn(cls, fn):\n",
      "   696|         0|            0|            0|  0.00%|    return inspect.getattr_static(cls, fn).__func__\n",
      "   697|         0|            0|            0|  0.00%|\n",
      "   698|         0|            0|            0|  0.00%|\n",
      "   699|         0|            0|            0|  0.00%|def get_torchscript_modifier(fn):\n",
      "   700|         0|            0|            0|  0.00%|    if not callable(fn):\n",
      "   701|         0|            0|            0|  0.00%|        return None\n",
      "   702|         0|            0|            0|  0.00%|    if hasattr(fn, '__func__'):\n",
      "   703|         0|            0|            0|  0.00%|        fn = fn.__func__\n",
      "   704|         0|            0|            0|  0.00%|    return getattr(fn, '_torchscript_modifier', FunctionModifiers.DEFAULT)\n",
      "   705|         0|            0|            0|  0.00%|\n",
      "   706|         0|            0|            0|  0.00%|def copy_torchscript_modifier(orig, new) -> None:\n",
      "   707|         0|            0|            0|  0.00%|    attr = get_torchscript_modifier(orig)\n",
      "   708|         0|            0|            0|  0.00%|    if attr is None:\n",
      "   709|         0|            0|            0|  0.00%|        return\n",
      "   710|         0|            0|            0|  0.00%|    new._torchscript_modifier = attr\n",
      "   711|         0|            0|            0|  0.00%|\n",
      "   712|         0|            0|            0|  0.00%|# overloading registration\n",
      "   713|         0|            0|            0|  0.00%|# overloads get registered in this file, and compiled in torch/jit/__init__.py\n",
      "   714|         0|            0|            0|  0.00%|# so that they can be imported in nn/functional.py without an import cycle\n",
      "   715|         0|            0|            0|  0.00%|\n",
      "   716|         0|            0|            0|  0.00%|# qualified_name => list[overload_functions]\n",
      "   717|         0|            0|            0|  0.00%|_overloaded_fns : Dict[str, List[Callable]] = {}  # noqa: T484\n",
      "   718|         0|            0|            0|  0.00%|\n",
      "   719|         0|            0|            0|  0.00%|\n",
      "   720|         0|            0|            0|  0.00%|_OVERLOAD_EXAMPLE = '''\n",
      "   721|         0|            0|            0|  0.00%|Example usage of overload function:\n",
      "   722|         0|            0|            0|  0.00%|@torch.jit._overload\n",
      "   723|         0|            0|            0|  0.00%|def my_function(x: type0) -> type0: # decl 1\n",
      "   724|         0|            0|            0|  0.00%|    pass\n",
      "   725|         0|            0|            0|  0.00%|\n",
      "   726|         0|            0|            0|  0.00%|@torch.jit._overload\n",
      "   727|         0|            0|            0|  0.00%|def my_function(x: type1) -> type1: # decl 2\n",
      "   728|         0|            0|            0|  0.00%|    pass\n",
      "   729|         0|            0|            0|  0.00%|\n",
      "   730|         0|            0|            0|  0.00%|def my_function(x):                 # implementation\n",
      "   731|         0|            0|            0|  0.00%|    if isinstance(x, type0):\n",
      "   732|         0|            0|            0|  0.00%|        return x\n",
      "   733|         0|            0|            0|  0.00%|    elif isinstance(x, type1):\n",
      "   734|         0|            0|            0|  0.00%|        return x\n",
      "   735|         0|            0|            0|  0.00%|'''\n",
      "   736|         0|            0|            0|  0.00%|\n",
      "   737|         0|            0|            0|  0.00%|def get_overload_no_implementation_error_message(kind, obj):\n",
      "   738|         0|            0|            0|  0.00%|    sourcelines, file_lineno, filename = get_source_lines_and_file(obj)\n",
      "   739|         0|            0|            0|  0.00%|    return (\n",
      "   740|         0|            0|            0|  0.00%|        f'Implementation for the {kind} \"{_qualified_name(obj)}\" is missing. Please make '\n",
      "   741|         0|            0|            0|  0.00%|        f'sure a definition is provided and defined after all overload declarations.\\n'\n",
      "   742|         0|            0|            0|  0.00%|        f'File \"{filename}\", line {file_lineno}:\\n' + ''.join(sourcelines) + \"\\n\" + _OVERLOAD_EXAMPLE\n",
      "   743|         0|            0|            0|  0.00%|    )\n",
      "   744|         0|            0|            0|  0.00%|\n",
      "   745|         0|            0|            0|  0.00%|def _check_overload_body(func):\n",
      "   746|         0|            0|            0|  0.00%|    try:\n",
      "   747|         0|            0|            0|  0.00%|        parsed_def = parse_def(func)\n",
      "   748|         0|            0|            0|  0.00%|    except OSError as e:\n",
      "   749|         0|            0|            0|  0.00%|        # Parsing the function definition can raise an OSError if source is unavailable.\n",
      "   750|         0|            0|            0|  0.00%|        # Since this is just an initial check, just raise a warning if this is the case.\n",
      "   751|         0|            0|            0|  0.00%|        warnings.warn(f\"Unable to retrieve source for @torch.jit._overload function: {func}.\")\n",
      "   752|         0|            0|            0|  0.00%|        return\n",
      "   753|         0|            0|            0|  0.00%|\n",
      "   754|         0|            0|            0|  0.00%|    body = parsed_def.ast.body[0].body\n",
      "   755|         0|            0|            0|  0.00%|\n",
      "   756|         0|            0|            0|  0.00%|    def is_pass(x):\n",
      "   757|         0|            0|            0|  0.00%|        return isinstance(x, ast.Pass)\n",
      "   758|         0|            0|            0|  0.00%|\n",
      "   759|         0|            0|            0|  0.00%|    def is_ellipsis(x):\n",
      "   760|         0|            0|            0|  0.00%|        return isinstance(x, ast.Expr) and isinstance(x.value, ast.Ellipsis)\n",
      "   761|         0|            0|            0|  0.00%|\n",
      "   762|         0|            0|            0|  0.00%|    if len(body) != 1 or not (is_pass(body[0]) or is_ellipsis(body[0])):\n",
      "   763|         0|            0|            0|  0.00%|        msg = \"Only `pass` statement or `...` can be the body of overload declaration:\\n\"\n",
      "   764|         0|            0|            0|  0.00%|        msg += '\\n'.join(parsed_def.source.split(\"\\n\")[:3])\n",
      "   765|         0|            0|            0|  0.00%|        msg += \" <- Expecting `pass` or `...` here!\\n\" + _OVERLOAD_EXAMPLE\n",
      "   766|         0|            0|            0|  0.00%|        raise RuntimeError(msg)\n",
      "   767|         0|            0|            0|  0.00%|\n",
      "   768|         0|            0|            0|  0.00%|def _overload(func):\n",
      "   769|         0|            0|            0|  0.00%|    _check_overload_body(func)\n",
      "   770|         0|            0|            0|  0.00%|    qual_name = _qualified_name(func)\n",
      "   771|         0|            0|            0|  0.00%|    global _overloaded_fns\n",
      "   772|         0|            0|            0|  0.00%|    fn_overload_list = _overloaded_fns.get(qual_name)\n",
      "   773|         0|            0|            0|  0.00%|    if fn_overload_list is None:\n",
      "   774|         0|            0|            0|  0.00%|        fn_overload_list = []\n",
      "   775|         0|            0|            0|  0.00%|        _overloaded_fns[qual_name] = fn_overload_list\n",
      "   776|         0|            0|            0|  0.00%|    fn_overload_list.append(func)\n",
      "   777|         0|            0|            0|  0.00%|    return func\n",
      "   778|         0|            0|            0|  0.00%|\n",
      "   779|         0|            0|            0|  0.00%|def _get_fn_overloads(qual_name):\n",
      "   780|         0|            0|            0|  0.00%|    return _overloaded_fns.get(qual_name)\n",
      "   781|         0|            0|            0|  0.00%|\n",
      "   782|         0|            0|            0|  0.00%|def _clear_fn_overloads(qual_name) -> None:\n",
      "   783|         0|            0|            0|  0.00%|    del _overloaded_fns[qual_name]\n",
      "   784|         0|            0|            0|  0.00%|\n",
      "   785|         0|            0|            0|  0.00%|def get_class_name_lineno(method) -> Tuple[str, int]:\n",
      "   786|         0|            0|            0|  0.00%|    current_frame = inspect.currentframe()\n",
      "   787|         0|            0|            0|  0.00%|\n",
      "   788|         0|            0|            0|  0.00%|    # one for the get_class_name call, one for _overload_method call\n",
      "   789|         0|            0|            0|  0.00%|    for i in range(2):\n",
      "   790|         0|            0|            0|  0.00%|        assert current_frame is not None  # assert current frame is not an Optional[FrameType]\n",
      "   791|         0|            0|            0|  0.00%|        current_frame = current_frame.f_back\n",
      "   792|         0|            0|            0|  0.00%|\n",
      "   793|         0|            0|            0|  0.00%|    assert current_frame is not None  # same here\n",
      "   794|         0|            0|            0|  0.00%|    class_name = current_frame.f_code.co_name\n",
      "   795|         0|            0|            0|  0.00%|    line_no = current_frame.f_code.co_firstlineno\n",
      "   796|         0|            0|            0|  0.00%|    return class_name, line_no\n",
      "   797|         0|            0|            0|  0.00%|\n",
      "   798|         0|            0|            0|  0.00%|# At the the point the decorator is applied to class methods the method\n",
      "   799|         0|            0|            0|  0.00%|# has no reference to its owning class. _qualified_name would not include\n",
      "   800|         0|            0|            0|  0.00%|# the class it is defined in, so any methods with the same name in the same file\n",
      "   801|         0|            0|            0|  0.00%|# would have the same _qualified_name, even if they were defined in different\n",
      "   802|         0|            0|            0|  0.00%|# classes. This problem only exists in python 2.\n",
      "   803|         0|            0|            0|  0.00%|# We get around this problem by looking at the stack frame and identifying\n",
      "   804|         0|            0|            0|  0.00%|# the class name, and throwing an error whenever overloads are used\n",
      "   805|         0|            0|            0|  0.00%|# when modules of the same name are in the same file\n",
      "   806|         0|            0|            0|  0.00%|\n",
      "   807|         0|            0|            0|  0.00%|# qualified_name => class name => list[overload_functions]\n",
      "   808|         0|            0|            0|  0.00%|_overloaded_methods : Dict[str, Dict[str, List[Callable]]] = {}  # noqa: T484\n",
      "   809|         0|            0|            0|  0.00%|\n",
      "   810|         0|            0|            0|  0.00%|\n",
      "   811|         0|            0|            0|  0.00%|# (qualified_name, class name) => class_fileno\n",
      "   812|         0|            0|            0|  0.00%|_overloaded_method_class_fileno = {}\n",
      "   813|         0|            0|            0|  0.00%|\n",
      "   814|         0|            0|            0|  0.00%|def _overload_method(func):\n",
      "   815|         0|            0|            0|  0.00%|    _check_overload_body(func)\n",
      "   816|         0|            0|            0|  0.00%|    qual_name = _qualified_name(func)\n",
      "   817|         0|            0|            0|  0.00%|    global _overloaded_methods\n",
      "   818|         0|            0|            0|  0.00%|    class_name_map = _overloaded_methods.get(qual_name, None)\n",
      "   819|         0|            0|            0|  0.00%|    if class_name_map is None:\n",
      "   820|         0|            0|            0|  0.00%|        class_name_map = {}\n",
      "   821|         0|            0|            0|  0.00%|        _overloaded_methods[qual_name] = class_name_map\n",
      "   822|         0|            0|            0|  0.00%|\n",
      "   823|         0|            0|            0|  0.00%|    class_name, line_no = get_class_name_lineno(func)\n",
      "   824|         0|            0|            0|  0.00%|    method_overloads = class_name_map.get(class_name, None)\n",
      "   825|         0|            0|            0|  0.00%|    if method_overloads is None:\n",
      "   826|         0|            0|            0|  0.00%|        method_overloads = []\n",
      "   827|         0|            0|            0|  0.00%|        class_name_map[class_name] = method_overloads\n",
      "   828|         0|            0|            0|  0.00%|        _overloaded_method_class_fileno[(qual_name, class_name)] = line_no\n",
      "   829|         0|            0|            0|  0.00%|    else:\n",
      "   830|         0|            0|            0|  0.00%|        existing_lineno = _overloaded_method_class_fileno[(qual_name, class_name)]\n",
      "   831|         0|            0|            0|  0.00%|        if existing_lineno != line_no:\n",
      "   832|         0|            0|            0|  0.00%|            raise RuntimeError(\"Cannot currently overload the same method name in two different\"\n",
      "   833|         0|            0|            0|  0.00%|                               \" classes with the same name in the same module\")\n",
      "   834|         0|            0|            0|  0.00%|\n",
      "   835|         0|            0|            0|  0.00%|    method_overloads.append(func)\n",
      "   836|         0|            0|            0|  0.00%|    return func\n",
      "   837|         0|            0|            0|  0.00%|\n",
      "   838|         0|            0|            0|  0.00%|def _get_overloaded_methods(method, mod_class):\n",
      "   839|         0|            0|            0|  0.00%|    # TODO: __name__ not set for submodules in recursive script\n",
      "   840|         0|            0|            0|  0.00%|    if not hasattr(method, \"__name__\"):\n",
      "   841|         0|            0|            0|  0.00%|        return None\n",
      "   842|         0|            0|            0|  0.00%|    qual_name = _qualified_name(method)\n",
      "   843|         0|            0|            0|  0.00%|    class_name_map = _overloaded_methods.get(qual_name, None)\n",
      "   844|         0|            0|            0|  0.00%|    if class_name_map is None:\n",
      "   845|         0|            0|            0|  0.00%|        return None\n",
      "   846|         0|            0|            0|  0.00%|    overloads = class_name_map.get(mod_class.__name__, None)\n",
      "   847|         0|            0|            0|  0.00%|    if overloads is None:\n",
      "   848|         0|            0|            0|  0.00%|        return None\n",
      "   849|         0|            0|            0|  0.00%|\n",
      "   850|         0|            0|            0|  0.00%|    method_line_no = get_source_lines_and_file(method)[1]\n",
      "   851|         0|            0|            0|  0.00%|    mod_class_fileno = get_source_lines_and_file(mod_class)[1]\n",
      "   852|         0|            0|            0|  0.00%|    mod_end_fileno = mod_class_fileno + len(get_source_lines_and_file(mod_class)[0])\n",
      "   853|         0|            0|            0|  0.00%|    if not (method_line_no >= mod_class_fileno and method_line_no <= mod_end_fileno):\n",
      "   854|         0|            0|            0|  0.00%|        raise Exception(\"Overloads are not useable when a module is redeclared within the same file: \" + str(method))\n",
      "   855|         0|            0|            0|  0.00%|    return overloads\n",
      "   856|         0|            0|            0|  0.00%|\n",
      "   857|         0|            0|            0|  0.00%|\n",
      "   858|         0|            0|            0|  0.00%|def is_tuple(ann) -> bool:\n",
      "   859|         0|            0|            0|  0.00%|    if ann is Tuple:\n",
      "   860|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing(\"Tuple\")\n",
      "   861|         0|            0|            0|  0.00%|\n",
      "   862|         0|            0|            0|  0.00%|    # For some reason Python 3.7 violates the Type[A, B].__origin__ == Type rule\n",
      "   863|         0|            0|            0|  0.00%|    if not hasattr(ann, '__module__'):\n",
      "   864|         0|            0|            0|  0.00%|        return False\n",
      "   865|         0|            0|            0|  0.00%|    return ann.__module__ == 'typing' and \\\n",
      "   866|         0|            0|            0|  0.00%|        (getattr(ann, '__origin__', None) is Tuple or\n",
      "   867|         0|            0|            0|  0.00%|            getattr(ann, '__origin__', None) is tuple)\n",
      "   868|         0|            0|            0|  0.00%|\n",
      "   869|         0|            0|            0|  0.00%|def is_list(ann) -> bool:\n",
      "   870|         0|            0|            0|  0.00%|    if ann is List:\n",
      "   871|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing(\"List\")\n",
      "   872|         0|            0|            0|  0.00%|\n",
      "   873|         0|            0|            0|  0.00%|    if not hasattr(ann, '__module__'):\n",
      "   874|         0|            0|            0|  0.00%|        return False\n",
      "   875|         0|            0|            0|  0.00%|    return ann.__module__ == 'typing' and \\\n",
      "   876|         0|            0|            0|  0.00%|        (getattr(ann, '__origin__', None) is List or\n",
      "   877|         0|            0|            0|  0.00%|            getattr(ann, '__origin__', None) is list)\n",
      "   878|         0|            0|            0|  0.00%|\n",
      "   879|         0|            0|            0|  0.00%|def is_dict(ann) -> bool:\n",
      "   880|         0|            0|            0|  0.00%|    if ann is Dict:\n",
      "   881|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing(\"Dict\")\n",
      "   882|         0|            0|            0|  0.00%|\n",
      "   883|         0|            0|            0|  0.00%|    if not hasattr(ann, '__module__'):\n",
      "   884|         0|            0|            0|  0.00%|        return False\n",
      "   885|         0|            0|            0|  0.00%|    return ann.__module__ == 'typing' and \\\n",
      "   886|         0|            0|            0|  0.00%|        (getattr(ann, '__origin__', None) is Dict or\n",
      "   887|         0|            0|            0|  0.00%|            getattr(ann, '__origin__', None) is dict)\n",
      "   888|         0|            0|            0|  0.00%|\n",
      "   889|         0|            0|            0|  0.00%|def is_union(ann):\n",
      "   890|         0|            0|            0|  0.00%|    if ann is Union:\n",
      "   891|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing(\"Union\")\n",
      "   892|         0|            0|            0|  0.00%|\n",
      "   893|         0|            0|            0|  0.00%|    return (hasattr(ann, '__module__') and\n",
      "   894|         0|            0|            0|  0.00%|            ann.__module__ == 'typing' and\n",
      "   895|         0|            0|            0|  0.00%|            (getattr(ann, '__origin__', None) is Union))\n",
      "   896|         0|            0|            0|  0.00%|\n",
      "   897|         0|            0|            0|  0.00%|def is_optional(ann):\n",
      "   898|         0|            0|            0|  0.00%|    if ann is Optional:\n",
      "   899|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing(\"Optional\")\n",
      "   900|         0|            0|            0|  0.00%|\n",
      "   901|         0|            0|            0|  0.00%|    def is_optional_as_optional(ann):\n",
      "   902|         0|            0|            0|  0.00%|        return (hasattr(ann, '__module__') and\n",
      "   903|         0|            0|            0|  0.00%|                ann.__module__ == 'typing' and\n",
      "   904|         0|            0|            0|  0.00%|                (getattr(ann, '__origin__', None) is Optional))\n",
      "   905|         0|            0|            0|  0.00%|\n",
      "   906|         0|            0|            0|  0.00%|    def is_union_as_optional(ann):\n",
      "   907|         0|            0|            0|  0.00%|        ann_args = ann.__args__\n",
      "   908|         0|            0|            0|  0.00%|        return len(ann_args) == 2 and None in ann_args\n",
      "   909|         0|            0|            0|  0.00%|\n",
      "   910|         0|            0|            0|  0.00%|    return is_optional_as_optional(ann) or (is_union(ann) and is_union_as_optional(ann))\n",
      "   911|         0|            0|            0|  0.00%|\n",
      "   912|         0|            0|            0|  0.00%|def is_future(ann) -> bool:\n",
      "   913|         0|            0|            0|  0.00%|    if ann is Future:\n",
      "   914|         0|            0|            0|  0.00%|        raise RuntimeError(\n",
      "   915|         0|            0|            0|  0.00%|            \"Attempted to use Future without a \"\n",
      "   916|         0|            0|            0|  0.00%|            \"contained type. Please add a contained type, e.g. \"\n",
      "   917|         0|            0|            0|  0.00%|            \"Future[int]\"\n",
      "   918|         0|            0|            0|  0.00%|        )\n",
      "   919|         0|            0|            0|  0.00%|    return getattr(ann, \"__origin__\", None) is Future\n",
      "   920|         0|            0|            0|  0.00%|\n",
      "   921|         0|            0|            0|  0.00%|if torch.distributed.rpc.is_available():\n",
      "   922|         0|            0|            0|  0.00%|    from torch.distributed.rpc import RRef\n",
      "   923|         0|            0|            0|  0.00%|    from torch._C._distributed_rpc import PyRRef\n",
      "   924|         0|            0|            0|  0.00%|\n",
      "   925|         0|            0|            0|  0.00%|    def is_rref(ann) -> bool:\n",
      "   926|         0|            0|            0|  0.00%|        if ann is RRef:\n",
      "   927|         0|            0|            0|  0.00%|            raise RuntimeError(\n",
      "   928|         0|            0|            0|  0.00%|                \"Attempted to use RRef without a \"\n",
      "   929|         0|            0|            0|  0.00%|                \"contained type. Please add a contained type, e.g. \"\n",
      "   930|         0|            0|            0|  0.00%|                \"RRef[int]\"\n",
      "   931|         0|            0|            0|  0.00%|            )\n",
      "   932|         0|            0|            0|  0.00%|        return getattr(ann, \"__origin__\", None) is RRef\n",
      "   933|         0|            0|            0|  0.00%|\n",
      "   934|         0|            0|            0|  0.00%|    def is_rref_instance(obj) -> bool:\n",
      "   935|         0|            0|            0|  0.00%|        return isinstance(obj, PyRRef)\n",
      "   936|         0|            0|            0|  0.00%|\n",
      "   937|         0|            0|            0|  0.00%|else:\n",
      "   938|         0|            0|            0|  0.00%|    def is_rref_instance(obj) -> bool:\n",
      "   939|         0|            0|            0|  0.00%|        # If the RPC module doesn't exist then RRefs don't exist either.\n",
      "   940|         0|            0|            0|  0.00%|        return False\n",
      "   941|         0|            0|            0|  0.00%|\n",
      "   942|         0|            0|            0|  0.00%|def is_final(ann) -> bool:\n",
      "   943|         0|            0|            0|  0.00%|    return ann.__module__ in {'typing', 'typing_extensions'} and \\\n",
      "   944|         0|            0|            0|  0.00%|        (getattr(ann, '__origin__', None) is Final or isinstance(ann, type(Final)))\n",
      "   945|         0|            0|            0|  0.00%|\n",
      "   946|         0|            0|            0|  0.00%|# allows BroadcastingList instance to be subscriptable\n",
      "   947|         0|            0|            0|  0.00%|class BroadcastingListCls(object):\n",
      "   948|         0|            0|            0|  0.00%|    def __getitem__(self, types):\n",
      "   949|         0|            0|            0|  0.00%|        return\n",
      "   950|         0|            0|            0|  0.00%|\n",
      "   951|         0|            0|            0|  0.00%|# mypy doesn't support parameters on types, so we have to explicitly type each\n",
      "   952|         0|            0|            0|  0.00%|# list size\n",
      "   953|         0|            0|            0|  0.00%|BroadcastingList1 = BroadcastingListCls()\n",
      "   954|         0|            0|            0|  0.00%|for i in range(2, 7):\n",
      "   955|         0|            0|            0|  0.00%|    globals()[f\"BroadcastingList{i}\"] = BroadcastingList1\n",
      "   956|         0|            0|            0|  0.00%|\n",
      "   957|         0|            0|            0|  0.00%|\n",
      "   958|     37830|    0.0554922|  1.46688e-06|  0.01%|def is_scripting() -> bool:\n",
      "   959|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "   960|         0|            0|            0|  0.00%|    Function that returns True when in compilation and False otherwise. This\n",
      "   961|         0|            0|            0|  0.00%|    is useful especially with the @unused decorator to leave code in your\n",
      "   962|         0|            0|            0|  0.00%|    model that is not yet TorchScript compatible.\n",
      "   963|         0|            0|            0|  0.00%|    .. testcode::\n",
      "   964|         0|            0|            0|  0.00%|\n",
      "   965|         0|            0|            0|  0.00%|        import torch\n",
      "   966|         0|            0|            0|  0.00%|\n",
      "   967|         0|            0|            0|  0.00%|        @torch.jit.unused\n",
      "   968|         0|            0|            0|  0.00%|        def unsupported_linear_op(x):\n",
      "   969|         0|            0|            0|  0.00%|            return x\n",
      "   970|         0|            0|            0|  0.00%|\n",
      "   971|         0|            0|            0|  0.00%|        def linear(x):\n",
      "   972|         0|            0|            0|  0.00%|           if torch.jit.is_scripting():\n",
      "   973|         0|            0|            0|  0.00%|              return torch.linear(x)\n",
      "   974|         0|            0|            0|  0.00%|           else:\n",
      "   975|         0|            0|            0|  0.00%|              return unsupported_linear_op(x)\n",
      "   976|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   977|     37830|    0.0644462|  1.70357e-06|  0.01%|    return False\n",
      "   978|         0|            0|            0|  0.00%|\n",
      "   979|         0|            0|            0|  0.00%|\n",
      "   980|         0|            0|            0|  0.00%|# Retrieves a fully-qualified name (module hierarchy + classname) for a given obj.\n",
      "   981|         0|            0|            0|  0.00%|def _qualified_name(obj, mangle_name=True) -> str:\n",
      "   982|         0|            0|            0|  0.00%|    # This special case allows us to override the qualified name on a type.\n",
      "   983|         0|            0|            0|  0.00%|    # It's currently used in conjunction with tracing, where we create a\n",
      "   984|         0|            0|            0|  0.00%|    # fake module to filter only supported attributes. However, since this\n",
      "   985|         0|            0|            0|  0.00%|    # new type is defined as a local class, we need a mechanism to override\n",
      "   986|         0|            0|            0|  0.00%|    # its qualname so it appears correctly in the TorchScript system. This,\n",
      "   987|         0|            0|            0|  0.00%|    # we set '_jit_override_qualname' with the original traced module's\n",
      "   988|         0|            0|            0|  0.00%|    # qualified name, which is picked up here\n",
      "   989|         0|            0|            0|  0.00%|    if hasattr(obj, '_jit_override_qualname'):\n",
      "   990|         0|            0|            0|  0.00%|        return obj._jit_override_qualname\n",
      "   991|         0|            0|            0|  0.00%|    # short-circuit in cases where the object already has a known qualified name\n",
      "   992|         0|            0|            0|  0.00%|    if isinstance(obj, torch._C.ScriptFunction):\n",
      "   993|         0|            0|            0|  0.00%|        return obj.qualified_name\n",
      "   994|         0|            0|            0|  0.00%|\n",
      "   995|         0|            0|            0|  0.00%|    if getattr(obj, \"__name__\", None):\n",
      "   996|         0|            0|            0|  0.00%|        name = obj.__name__\n",
      "   997|         0|            0|            0|  0.00%|    # Enum classes do not have `__name__` attr, instead they have `name`.\n",
      "   998|         0|            0|            0|  0.00%|    elif isinstance(obj, enum.Enum):\n",
      "   999|         0|            0|            0|  0.00%|        name = obj.name\n",
      "  1000|         0|            0|            0|  0.00%|    else:\n",
      "  1001|         0|            0|            0|  0.00%|        raise RuntimeError(\"Could not get name of python class object\")\n",
      "  1002|         0|            0|            0|  0.00%|\n",
      "  1003|         0|            0|            0|  0.00%|\n",
      "  1004|         0|            0|            0|  0.00%|    if name == '<lambda>':\n",
      "  1005|         0|            0|            0|  0.00%|        name = '_lambda'  # make name a valid identifier\n",
      "  1006|         0|            0|            0|  0.00%|\n",
      "  1007|         0|            0|            0|  0.00%|    module_name = obj.__module__\n",
      "  1008|         0|            0|            0|  0.00%|\n",
      "  1009|         0|            0|            0|  0.00%|    # If the module is actually a torchbind module, then we should short circuit\n",
      "  1010|         0|            0|            0|  0.00%|    if module_name == \"torch._classes\":\n",
      "  1011|         0|            0|            0|  0.00%|        return obj.qualified_name\n",
      "  1012|         0|            0|            0|  0.00%|\n",
      "  1013|         0|            0|            0|  0.00%|    # The Python docs are very clear that `__module__` can be None, but I can't\n",
      "  1014|         0|            0|            0|  0.00%|    # figure out when it actually would be.\n",
      "  1015|         0|            0|            0|  0.00%|    if module_name is None:\n",
      "  1016|         0|            0|            0|  0.00%|        raise RuntimeError(f\"Could not get qualified name for class '{name}': \"\n",
      "  1017|         0|            0|            0|  0.00%|                           \"__module__ can't be None.\")\n",
      "  1018|         0|            0|            0|  0.00%|\n",
      "  1019|         0|            0|            0|  0.00%|    # if getattr(sys.modules[module_name], name) is not obj:\n",
      "  1020|         0|            0|            0|  0.00%|    #     raise RuntimeError(f\"Could not get qualified name for class '{name}': \"\n",
      "  1021|         0|            0|            0|  0.00%|    #                        f\"the attr {name} on module {module_name} is not the the class\")\n",
      "  1022|         0|            0|            0|  0.00%|\n",
      "  1023|         0|            0|            0|  0.00%|    # torch.package and TorchScript have separate mangling schemes to avoid\n",
      "  1024|         0|            0|            0|  0.00%|    # name collisions from multiple packages. To avoid them interfering with\n",
      "  1025|         0|            0|            0|  0.00%|    # each other, normalize the package manging here.\n",
      "  1026|         0|            0|            0|  0.00%|    if package_mangling.is_mangled(module_name):\n",
      "  1027|         0|            0|            0|  0.00%|        module_name = module_name.replace(\"<\", \"_\")\n",
      "  1028|         0|            0|            0|  0.00%|        module_name = module_name.replace(\">\", \"_\")\n",
      "  1029|         0|            0|            0|  0.00%|\n",
      "  1030|         0|            0|            0|  0.00%|    # The PythonExceptionValue C++ class in torch/csrc/jit/python/python_sugared_value.h\n",
      "  1031|         0|            0|            0|  0.00%|    # does not need mangle the python class name.\n",
      "  1032|         0|            0|            0|  0.00%|    if mangle_name:\n",
      "  1033|         0|            0|            0|  0.00%|        # __main__ is a builtin module, so rewrite it to \"__torch__\".\n",
      "  1034|         0|            0|            0|  0.00%|        if module_name == \"__main__\":\n",
      "  1035|         0|            0|            0|  0.00%|            module_name = \"__torch__\"\n",
      "  1036|         0|            0|            0|  0.00%|        else:\n",
      "  1037|         0|            0|            0|  0.00%|            # Everything else gets a \"__torch__\" prefix to avoid name collisions\n",
      "  1038|         0|            0|            0|  0.00%|            # with the names of user values.\n",
      "  1039|         0|            0|            0|  0.00%|            module_name = \"__torch__.\" + module_name\n",
      "  1040|         0|            0|            0|  0.00%|\n",
      "  1041|         0|            0|            0|  0.00%|    if \".\" in name:\n",
      "  1042|         0|            0|            0|  0.00%|        raise RuntimeError(f\"Could not get qualified name for class '{name}': \"\n",
      "  1043|         0|            0|            0|  0.00%|                           f\"'{name}' is not a valid identifier\")\n",
      "  1044|         0|            0|            0|  0.00%|\n",
      "  1045|         0|            0|            0|  0.00%|    return module_name + \".\" + name\n",
      "  1046|         0|            0|            0|  0.00%|\n",
      "  1047|         0|            0|            0|  0.00%|\n",
      "  1048|         0|            0|            0|  0.00%|def _try_get_dispatched_fn(fn):\n",
      "  1049|         0|            0|            0|  0.00%|    if not callable(fn):\n",
      "  1050|         0|            0|            0|  0.00%|        return None\n",
      "  1051|         0|            0|            0|  0.00%|    return boolean_dispatched.get(fn)\n",
      "  1052|         0|            0|            0|  0.00%|\n",
      "  1053|         0|            0|            0|  0.00%|\n",
      "  1054|         0|            0|            0|  0.00%|def _get_named_tuple_properties(obj):\n",
      "  1055|         0|            0|            0|  0.00%|    assert issubclass(obj, tuple) and hasattr(obj, '_fields')\n",
      "  1056|         0|            0|            0|  0.00%|    if hasattr(obj, \"_field_defaults\"):\n",
      "  1057|         0|            0|            0|  0.00%|        defaults = [obj._field_defaults[field]\n",
      "  1058|         0|            0|            0|  0.00%|                    for field in obj._fields\n",
      "  1059|         0|            0|            0|  0.00%|                    if field in obj._field_defaults]\n",
      "  1060|         0|            0|            0|  0.00%|    else:\n",
      "  1061|         0|            0|            0|  0.00%|        defaults = []\n",
      "  1062|         0|            0|            0|  0.00%|    annotations = []\n",
      "  1063|         0|            0|            0|  0.00%|    has_annotations = hasattr(obj, '__annotations__')\n",
      "  1064|         0|            0|            0|  0.00%|    for field in obj._fields:\n",
      "  1065|         0|            0|            0|  0.00%|        if has_annotations and field in obj.__annotations__:\n",
      "  1066|         0|            0|            0|  0.00%|            the_type = torch.jit.annotations.ann_to_type(obj.__annotations__[field], fake_range())\n",
      "  1067|         0|            0|            0|  0.00%|            annotations.append(the_type)\n",
      "  1068|         0|            0|            0|  0.00%|        else:\n",
      "  1069|         0|            0|            0|  0.00%|            annotations.append(torch._C.TensorType.getInferred())\n",
      "  1070|         0|            0|            0|  0.00%|    return type(obj).__name__, obj._fields, annotations, defaults\n",
      "  1071|         0|            0|            0|  0.00%|\n",
      "  1072|         0|            0|            0|  0.00%|\n",
      "  1073|         0|            0|            0|  0.00%|def _create_named_tuple(t, unqual_name: str, field_names: List[str], defaults: Tuple[Any, ...]):\n",
      "  1074|         0|            0|            0|  0.00%|    # mypy: namedtuple() expects a string literal as the first argument\n",
      "  1075|         0|            0|            0|  0.00%|    if sys.version_info < (3, 7, 0):\n",
      "  1076|         0|            0|            0|  0.00%|        TupleType = collections.namedtuple(unqual_name, field_names)  # type: ignore[no-redef, misc]\n",
      "  1077|         0|            0|            0|  0.00%|        TupleType.__new__.__defaults__ = defaults    # type: ignore[attr-defined]\n",
      "  1078|         0|            0|            0|  0.00%|    else:\n",
      "  1079|         0|            0|            0|  0.00%|        TupleType = collections.namedtuple(unqual_name, field_names, defaults=defaults)  # type: ignore[call-arg, no-redef, misc]\n",
      "  1080|         0|            0|            0|  0.00%|    return TupleType(*t)\n",
      "  1081|         0|            0|            0|  0.00%|\n",
      "  1082|         0|            0|            0|  0.00%|@contextlib.contextmanager\n",
      "  1083|         0|            0|            0|  0.00%|def _disable_emit_hooks():\n",
      "  1084|         0|            0|            0|  0.00%|    hooks = torch._C._jit_get_emit_hooks()\n",
      "  1085|         0|            0|            0|  0.00%|    torch._C._jit_set_emit_hooks(None, None)\n",
      "  1086|         0|            0|            0|  0.00%|    yield\n",
      "  1087|         0|            0|            0|  0.00%|    torch._C._jit_set_emit_hooks(hooks[0], hooks[1])\n",
      "  1088|         0|            0|            0|  0.00%|\n",
      "  1089|         0|            0|            0|  0.00%|\n",
      "  1090|         0|            0|            0|  0.00%|def _disable_emit_hooks_decorator(_DecoratorContextManager) -> None:  # noqa: F811\n",
      "  1091|         0|            0|            0|  0.00%|    def __enter__(self) -> None:\n",
      "  1092|         0|            0|            0|  0.00%|        self.hooks = torch._C._jit_get_emit_hooks()\n",
      "  1093|         0|            0|            0|  0.00%|        torch._C._jit_set_emit_hooks(None, None)\n",
      "  1094|         0|            0|            0|  0.00%|\n",
      "  1095|         0|            0|            0|  0.00%|    def __exit__(self, *args) -> None:\n",
      "  1096|         0|            0|            0|  0.00%|        torch._C._jit_set_emit_hooks(self.hooks[0], self.hooks[1])\n",
      "  1097|         0|            0|            0|  0.00%|\n",
      "  1098|         0|            0|            0|  0.00%|def _is_exception(obj) -> bool:\n",
      "  1099|         0|            0|            0|  0.00%|    if not inspect.isclass(obj):\n",
      "  1100|         0|            0|            0|  0.00%|        return False\n",
      "  1101|         0|            0|            0|  0.00%|    return issubclass(obj, Exception)\n",
      "  1102|         0|            0|            0|  0.00%|\n",
      "  1103|         0|            0|            0|  0.00%|def raise_error_container_parameter_missing(target_type) -> None:\n",
      "  1104|         0|            0|            0|  0.00%|    if target_type == 'Dict':\n",
      "  1105|         0|            0|            0|  0.00%|        raise RuntimeError(\n",
      "  1106|         0|            0|            0|  0.00%|            \"Attempted to use Dict without \"\n",
      "  1107|         0|            0|            0|  0.00%|            \"contained types. Please add contained type, e.g. \"\n",
      "  1108|         0|            0|            0|  0.00%|            \"Dict[int, int]\"\n",
      "  1109|         0|            0|            0|  0.00%|        )\n",
      "  1110|         0|            0|            0|  0.00%|    raise RuntimeError(\n",
      "  1111|         0|            0|            0|  0.00%|        f\"Attempted to use {target_type} without a \"\n",
      "  1112|         0|            0|            0|  0.00%|        \"contained type. Please add a contained type, e.g. \"\n",
      "  1113|         0|            0|            0|  0.00%|        f\"{target_type}[int]\"\n",
      "  1114|         0|            0|            0|  0.00%|    )\n",
      "  1115|         0|            0|            0|  0.00%|\n",
      "  1116|         0|            0|            0|  0.00%|\n",
      "  1117|         0|            0|            0|  0.00%|def get_origin(target_type):\n",
      "  1118|         0|            0|            0|  0.00%|    return getattr(target_type, \"__origin__\", None)\n",
      "  1119|         0|            0|            0|  0.00%|\n",
      "  1120|         0|            0|            0|  0.00%|\n",
      "  1121|         0|            0|            0|  0.00%|def get_args(target_type):\n",
      "  1122|         0|            0|            0|  0.00%|    return getattr(target_type, \"__args__\", None)\n",
      "  1123|         0|            0|            0|  0.00%|\n",
      "  1124|         0|            0|            0|  0.00%|\n",
      "  1125|         0|            0|            0|  0.00%|def check_args_exist(target_type) -> None:\n",
      "  1126|         0|            0|            0|  0.00%|    if target_type is List or target_type is list:\n",
      "  1127|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing(\"List\")\n",
      "  1128|         0|            0|            0|  0.00%|    elif target_type is Tuple or target_type is tuple:\n",
      "  1129|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing(\"Tuple\")\n",
      "  1130|         0|            0|            0|  0.00%|    elif target_type is Dict or target_type is dict:\n",
      "  1131|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing(\"Dict\")\n",
      "  1132|         0|            0|            0|  0.00%|    elif target_type is None or target_type is Optional:\n",
      "  1133|         0|            0|            0|  0.00%|        raise_error_container_parameter_missing(\"Optional\")\n",
      "  1134|         0|            0|            0|  0.00%|\n",
      "  1135|         0|            0|            0|  0.00%|\n",
      "  1136|         0|            0|            0|  0.00%|def check_empty_containers(obj) -> None:\n",
      "  1137|         0|            0|            0|  0.00%|    if obj == [] or obj == {} or obj == ():\n",
      "  1138|         0|            0|            0|  0.00%|        warnings.warn(\"The inner type of a container is lost when \"\n",
      "  1139|         0|            0|            0|  0.00%|                      \"calling torch.jit.isinstance in eager mode. For \"\n",
      "  1140|         0|            0|            0|  0.00%|                      \"example, List[int] would become list and \"\n",
      "  1141|         0|            0|            0|  0.00%|                      \"therefore falsely return True for List[float] or\"\n",
      "  1142|         0|            0|            0|  0.00%|                      \" List[str].\")\n",
      "  1143|         0|            0|            0|  0.00%|\n",
      "  1144|         0|            0|            0|  0.00%|\n",
      "  1145|         0|            0|            0|  0.00%|# supports List/Dict/Tuple and Optional types\n",
      "  1146|         0|            0|            0|  0.00%|# TODO support future\n",
      "  1147|         0|            0|            0|  0.00%|def container_checker(obj, target_type) -> bool:\n",
      "  1148|         0|            0|            0|  0.00%|    origin_type = get_origin(target_type)\n",
      "  1149|         0|            0|            0|  0.00%|    check_args_exist(target_type)\n",
      "  1150|         0|            0|            0|  0.00%|    if origin_type is list or origin_type is List:\n",
      "  1151|         0|            0|            0|  0.00%|        check_empty_containers(obj)\n",
      "  1152|         0|            0|            0|  0.00%|        if not isinstance(obj, list):\n",
      "  1153|         0|            0|            0|  0.00%|            return False\n",
      "  1154|         0|            0|            0|  0.00%|        arg_type = get_args(target_type)[0]\n",
      "  1155|         0|            0|            0|  0.00%|        arg_origin = get_origin(arg_type)\n",
      "  1156|         0|            0|            0|  0.00%|        for el in obj:\n",
      "  1157|         0|            0|            0|  0.00%|            # check if nested container, ex: List[List[str]]\n",
      "  1158|         0|            0|            0|  0.00%|            if arg_origin:  # processes nested container, ex: List[List[str]]\n",
      "  1159|         0|            0|            0|  0.00%|                if not container_checker(el, arg_type):\n",
      "  1160|         0|            0|            0|  0.00%|                    return False\n",
      "  1161|         0|            0|            0|  0.00%|            elif not isinstance(el, arg_type):\n",
      "  1162|         0|            0|            0|  0.00%|                return False\n",
      "  1163|         0|            0|            0|  0.00%|        return True\n",
      "  1164|         0|            0|            0|  0.00%|    elif origin_type is Dict or origin_type is dict:\n",
      "  1165|         0|            0|            0|  0.00%|        check_empty_containers(obj)\n",
      "  1166|         0|            0|            0|  0.00%|        if not isinstance(obj, dict):\n",
      "  1167|         0|            0|            0|  0.00%|            return False\n",
      "  1168|         0|            0|            0|  0.00%|        key_type = get_args(target_type)[0]\n",
      "  1169|         0|            0|            0|  0.00%|        val_type = get_args(target_type)[1]\n",
      "  1170|         0|            0|            0|  0.00%|        for key, val in obj.items():\n",
      "  1171|         0|            0|            0|  0.00%|            # check if keys are of right type\n",
      "  1172|         0|            0|            0|  0.00%|            if not isinstance(key, key_type):\n",
      "  1173|         0|            0|            0|  0.00%|                return False\n",
      "  1174|         0|            0|            0|  0.00%|            val_origin = get_origin(val_type)\n",
      "  1175|         0|            0|            0|  0.00%|            if val_origin:\n",
      "  1176|         0|            0|            0|  0.00%|                if not container_checker(val, val_type):\n",
      "  1177|         0|            0|            0|  0.00%|                    return False\n",
      "  1178|         0|            0|            0|  0.00%|            elif not isinstance(val, val_type):\n",
      "  1179|         0|            0|            0|  0.00%|                return False\n",
      "  1180|         0|            0|            0|  0.00%|        return True\n",
      "  1181|         0|            0|            0|  0.00%|    elif origin_type is Tuple or origin_type is tuple:\n",
      "  1182|         0|            0|            0|  0.00%|        check_empty_containers(obj)\n",
      "  1183|         0|            0|            0|  0.00%|        if not isinstance(obj, tuple):\n",
      "  1184|         0|            0|            0|  0.00%|            return False\n",
      "  1185|         0|            0|            0|  0.00%|        arg_types = get_args(target_type)\n",
      "  1186|         0|            0|            0|  0.00%|        if len(obj) != len(arg_types):\n",
      "  1187|         0|            0|            0|  0.00%|            return False\n",
      "  1188|         0|            0|            0|  0.00%|        for el, el_type in zip(obj, arg_types):\n",
      "  1189|         0|            0|            0|  0.00%|            el_origin = get_origin(el_type)\n",
      "  1190|         0|            0|            0|  0.00%|            if el_origin:\n",
      "  1191|         0|            0|            0|  0.00%|                if not container_checker(el, el_type):\n",
      "  1192|         0|            0|            0|  0.00%|                    return False\n",
      "  1193|         0|            0|            0|  0.00%|            elif not isinstance(el, el_type):\n",
      "  1194|         0|            0|            0|  0.00%|                return False\n",
      "  1195|         0|            0|            0|  0.00%|        return True\n",
      "  1196|         0|            0|            0|  0.00%|    elif origin_type is Union:  # also handles Optional\n",
      "  1197|         0|            0|            0|  0.00%|        if obj is None:  # check before recursion because None is always fine\n",
      "  1198|         0|            0|            0|  0.00%|            return True\n",
      "  1199|         0|            0|            0|  0.00%|        inner_types = get_args(target_type)\n",
      "  1200|         0|            0|            0|  0.00%|        for t in inner_types:\n",
      "  1201|         0|            0|            0|  0.00%|            t_origin = get_origin(t)\n",
      "  1202|         0|            0|            0|  0.00%|            if (t_origin):\n",
      "  1203|         0|            0|            0|  0.00%|                return container_checker(obj, t)\n",
      "  1204|         0|            0|            0|  0.00%|            elif isinstance(obj, t):\n",
      "  1205|         0|            0|            0|  0.00%|                return True\n",
      "  1206|         0|            0|            0|  0.00%|    return False\n",
      "  1207|         0|            0|            0|  0.00%|\n",
      "  1208|         0|            0|            0|  0.00%|\n",
      "  1209|         0|            0|            0|  0.00%|def _isinstance(obj, target_type) -> bool:\n",
      "  1210|         0|            0|            0|  0.00%|    if isinstance(target_type, collections.abc.Container):\n",
      "  1211|         0|            0|            0|  0.00%|        if not isinstance(target_type, tuple):\n",
      "  1212|         0|            0|            0|  0.00%|            raise RuntimeError(\"The second argument to \"\n",
      "  1213|         0|            0|            0|  0.00%|                               \"`torch.jit.isinstance` must be a type \"\n",
      "  1214|         0|            0|            0|  0.00%|                               \"or a tuple of types\")\n",
      "  1215|         0|            0|            0|  0.00%|        for t_type in target_type:\n",
      "  1216|         0|            0|            0|  0.00%|            if _isinstance(obj, t_type):\n",
      "  1217|         0|            0|            0|  0.00%|                return True\n",
      "  1218|         0|            0|            0|  0.00%|        return False\n",
      "  1219|         0|            0|            0|  0.00%|\n",
      "  1220|         0|            0|            0|  0.00%|    origin_type = get_origin(target_type)\n",
      "  1221|         0|            0|            0|  0.00%|    if origin_type:\n",
      "  1222|         0|            0|            0|  0.00%|        return container_checker(obj, target_type)\n",
      "  1223|         0|            0|            0|  0.00%|\n",
      "  1224|         0|            0|            0|  0.00%|    # Check to handle non-typed optional origin returns as none instead\n",
      "  1225|         0|            0|            0|  0.00%|    #    of as optional in 3.7-3.8\n",
      "  1226|         0|            0|            0|  0.00%|    check_args_exist(target_type)\n",
      "  1227|         0|            0|            0|  0.00%|\n",
      "  1228|         0|            0|            0|  0.00%|    # handle non-containers\n",
      "  1229|         0|            0|            0|  0.00%|    return isinstance(obj, target_type)\n",
      "  1230|         0|            0|            0|  0.00%|\n",
      "  1231|         0|            0|            0|  0.00%|\n",
      "  1232|         0|            0|            0|  0.00%|class _TensorExtractor(pickle.Pickler):\n",
      "  1233|         0|            0|            0|  0.00%|    def __init__(self, *args, tensors: List[torch.Tensor], **kwargs):\n",
      "  1234|         0|            0|            0|  0.00%|        super().__init__(*args, **kwargs)\n",
      "  1235|         0|            0|            0|  0.00%|        self.tensors = tensors\n",
      "  1236|         0|            0|            0|  0.00%|\n",
      "  1237|         0|            0|            0|  0.00%|    def persistent_id(self, obj):\n",
      "  1238|         0|            0|            0|  0.00%|        if isinstance(obj, torch.Tensor):\n",
      "  1239|         0|            0|            0|  0.00%|            self.tensors.append(obj)\n",
      "  1240|         0|            0|            0|  0.00%|            return \"\"\n",
      "  1241|         0|            0|            0|  0.00%|        # Since we just want to extract tensors, we don't mind if an object is\n",
      "  1242|         0|            0|            0|  0.00%|        # unpicklable if it doesn't contain tensors, as we can just ignore/skip\n",
      "  1243|         0|            0|            0|  0.00%|        # it. To play it safe, we only do so for common objects that we're sure\n",
      "  1244|         0|            0|            0|  0.00%|        # don't contain tensors. Feel free to add new types here. Note also that\n",
      "  1245|         0|            0|            0|  0.00%|        # even if a type isn't listed here this won't block users, since thet\n",
      "  1246|         0|            0|            0|  0.00%|        # can just add a __getstate__ or __reduce__ method to their class.\n",
      "  1247|         0|            0|            0|  0.00%|        if isinstance(obj, LockType):\n",
      "  1248|         0|            0|            0|  0.00%|            return \"\"\n",
      "  1249|         0|            0|            0|  0.00%|        # Futures and RRefs don't technically contain a value, they just offer\n",
      "  1250|         0|            0|            0|  0.00%|        # the means to access a value.\n",
      "  1251|         0|            0|            0|  0.00%|        if isinstance(obj, CFuture) or is_rref_instance(obj):\n",
      "  1252|         0|            0|            0|  0.00%|            return \"\"\n",
      "  1253|         0|            0|            0|  0.00%|        if isinstance(obj, torch.cuda.Event):\n",
      "  1254|         0|            0|            0|  0.00%|            return \"\"\n",
      "  1255|         0|            0|            0|  0.00%|        if isinstance(obj, threading.Thread):\n",
      "  1256|         0|            0|            0|  0.00%|            return \"\"\n",
      "  1257|         0|            0|            0|  0.00%|        return None\n",
      "  1258|         0|            0|            0|  0.00%|\n",
      "  1259|         0|            0|            0|  0.00%|\n",
      "  1260|         0|            0|            0|  0.00%|def _extract_tensors(obj):\n",
      "  1261|         0|            0|            0|  0.00%|    r\"\"\"\n",
      "  1262|         0|            0|            0|  0.00%|    This function is exclusively called from C++.\n",
      "  1263|         0|            0|            0|  0.00%|    See ``torch/csrc/jit/python/python_ivalue.h``.\n",
      "  1264|         0|            0|            0|  0.00%|\n",
      "  1265|         0|            0|            0|  0.00%|    It extracts the tensors contained in the given object, through pickling.\n",
      "  1266|         0|            0|            0|  0.00%|    \"\"\"\n",
      "  1267|         0|            0|            0|  0.00%|    tensors: List[torch.Tensor] = []\n",
      "  1268|         0|            0|            0|  0.00%|    extractor = _TensorExtractor(io.BytesIO(), protocol=-1, tensors=tensors)\n",
      "  1269|         0|            0|            0|  0.00%|    extractor.dump(obj)\n",
      "  1270|         0|            0|            0|  0.00%|    return tensors\n",
      "File: /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/cuda/__init__.py\n",
      "File duration: 0.0394785s (0.01%)\n",
      "Line #|      Hits|         Time| Time per hit|      %|Source code\n",
      "------+----------+-------------+-------------+-------+-----------\n",
      "     1|         0|            0|            0|  0.00%|r\"\"\"\n",
      "     2|         0|            0|            0|  0.00%|This package adds support for CUDA tensor types, that implement the same\n",
      "     3|         0|            0|            0|  0.00%|function as CPU tensors, but they utilize GPUs for computation.\n",
      "     4|         0|            0|            0|  0.00%|\n",
      "     5|         0|            0|            0|  0.00%|It is lazily initialized, so you can always import it, and use\n",
      "     6|         0|            0|            0|  0.00%|:func:`is_available()` to determine if your system supports CUDA.\n",
      "     7|         0|            0|            0|  0.00%|\n",
      "     8|         0|            0|            0|  0.00%|:ref:`cuda-semantics` has more details about working with CUDA.\n",
      "     9|         0|            0|            0|  0.00%|\"\"\"\n",
      "    10|         0|            0|            0|  0.00%|\n",
      "    11|         0|            0|            0|  0.00%|import contextlib\n",
      "    12|         0|            0|            0|  0.00%|import os\n",
      "    13|         0|            0|            0|  0.00%|import torch\n",
      "    14|         0|            0|            0|  0.00%|from torch.types import Device\n",
      "    15|         0|            0|            0|  0.00%|import traceback\n",
      "    16|         0|            0|            0|  0.00%|import warnings\n",
      "    17|         0|            0|            0|  0.00%|import threading\n",
      "    18|         0|            0|            0|  0.00%|from typing import List, Optional, Tuple, Union, Any\n",
      "    19|         0|            0|            0|  0.00%|from ._utils import _get_device_index, _dummy_type\n",
      "    20|         0|            0|            0|  0.00%|from .._utils import classproperty\n",
      "    21|         0|            0|            0|  0.00%|from .graphs import CUDAGraph, graph_pool_handle, graph, \\\n",
      "    22|         0|            0|            0|  0.00%|    make_graphed_callables, is_current_stream_capturing\n",
      "    23|         0|            0|            0|  0.00%|from .streams import ExternalStream, Stream, Event\n",
      "    24|         0|            0|            0|  0.00%|from .. import device as _device\n",
      "    25|         0|            0|            0|  0.00%|import torch._C\n",
      "    26|         0|            0|            0|  0.00%|\n",
      "    27|         0|            0|            0|  0.00%|try:\n",
      "    28|         0|            0|            0|  0.00%|    from torch._C import _cudart  # type: ignore[attr-defined]\n",
      "    29|         0|            0|            0|  0.00%|except ImportError:\n",
      "    30|         0|            0|            0|  0.00%|    _cudart = None\n",
      "    31|         0|            0|            0|  0.00%|\n",
      "    32|         0|            0|            0|  0.00%|_initialized = False\n",
      "    33|         0|            0|            0|  0.00%|_tls = threading.local()\n",
      "    34|         0|            0|            0|  0.00%|_initialization_lock = threading.Lock()\n",
      "    35|         0|            0|            0|  0.00%|_queued_calls = []  # don't invoke these until initialization occurs\n",
      "    36|         0|            0|            0|  0.00%|_is_in_bad_fork = getattr(torch._C, \"_cuda_isInBadFork\", lambda: False)\n",
      "    37|         0|            0|            0|  0.00%|_device_t = Union[_device, str, int, None]\n",
      "    38|         0|            0|            0|  0.00%|\n",
      "    39|         0|            0|            0|  0.00%|\n",
      "    40|         0|            0|            0|  0.00%|class _LazySeedTracker:\n",
      "    41|         0|            0|            0|  0.00%|    # Since seeding is memory-less, only track the latest seed.\n",
      "    42|         0|            0|            0|  0.00%|    # Note: `manual_seed_all` followed by `manual_seed` overwrites\n",
      "    43|         0|            0|            0|  0.00%|    # the seed on current device. We track the order of **latest**\n",
      "    44|         0|            0|            0|  0.00%|    # calls between these two API.\n",
      "    45|         0|            0|            0|  0.00%|    def __init__(self):\n",
      "    46|         0|            0|            0|  0.00%|        self.manual_seed_all_cb = None\n",
      "    47|         0|            0|            0|  0.00%|        self.manual_seed_cb = None\n",
      "    48|         0|            0|            0|  0.00%|        self.call_order = []\n",
      "    49|         0|            0|            0|  0.00%|\n",
      "    50|         1|   2.6226e-06|   2.6226e-06|  0.00%|    def queue_seed_all(self, cb, traceback):\n",
      "    51|         1|  3.33786e-06|  3.33786e-06|  0.00%|        self.manual_seed_all_cb = (cb, traceback)\n",
      "    52|         0|            0|            0|  0.00%|        # update seed_all to be latest\n",
      "    53|         1|  7.15256e-06|  7.15256e-06|  0.00%|        self.call_order = [self.manual_seed_cb, self.manual_seed_all_cb]\n",
      "    54|         0|            0|            0|  0.00%|\n",
      "    55|         0|            0|            0|  0.00%|    def queue_seed(self, cb, traceback):\n",
      "    56|         0|            0|            0|  0.00%|        self.manual_seed_cb = (cb, traceback)\n",
      "    57|         0|            0|            0|  0.00%|        # update seed to be latest\n",
      "    58|         0|            0|            0|  0.00%|        self.call_order = [self.manual_seed_all_cb, self.manual_seed_cb]\n",
      "    59|         0|            0|            0|  0.00%|\n",
      "    60|         0|            0|            0|  0.00%|    def get_calls(self) -> List:\n",
      "    61|         0|            0|            0|  0.00%|        return self.call_order\n",
      "    62|         0|            0|            0|  0.00%|\n",
      "    63|         0|            0|            0|  0.00%|\n",
      "    64|         0|            0|            0|  0.00%|_lazy_seed_tracker = _LazySeedTracker()\n",
      "    65|         0|            0|            0|  0.00%|\n",
      "    66|         0|            0|            0|  0.00%|# Define dummy _CudaDeviceProperties type if PyTorch was compiled without CUDA\n",
      "    67|         0|            0|            0|  0.00%|if hasattr(torch._C, '_CudaDeviceProperties'):\n",
      "    68|         0|            0|            0|  0.00%|    _CudaDeviceProperties = torch._C._CudaDeviceProperties\n",
      "    69|         0|            0|            0|  0.00%|else:\n",
      "    70|         0|            0|            0|  0.00%|    _CudaDeviceProperties = _dummy_type('_CudaDeviceProperties')  # type: ignore[assignment, misc]\n",
      "    71|         0|            0|            0|  0.00%|\n",
      "    72|         0|            0|            0|  0.00%|# Global variables dynamically populated by native code\n",
      "    73|         0|            0|            0|  0.00%|has_magma: bool = False\n",
      "    74|         0|            0|            0|  0.00%|has_half: bool = False\n",
      "    75|         0|            0|            0|  0.00%|default_generators: Tuple[torch._C.Generator] = ()  # type: ignore[assignment]\n",
      "    76|         0|            0|            0|  0.00%|\n",
      "    77|      6240|   0.00976253|  1.56451e-06|  0.00%|def is_available() -> bool:\n",
      "    78|         0|            0|            0|  0.00%|    r\"\"\"Returns a bool indicating if CUDA is currently available.\"\"\"\n",
      "    79|      6240|    0.0147533|  2.36432e-06|  0.00%|    if not hasattr(torch._C, '_cuda_getDeviceCount'):\n",
      "    80|         0|            0|            0|  0.00%|        return False\n",
      "    81|         0|            0|            0|  0.00%|    # This function never throws and returns 0 if driver is missing or can't\n",
      "    82|         0|            0|            0|  0.00%|    # be initialized\n",
      "    83|      6240|    0.0149076|  2.38904e-06|  0.00%|    return torch._C._cuda_getDeviceCount() > 0\n",
      "    84|         0|            0|            0|  0.00%|\n",
      "    85|         0|            0|            0|  0.00%|def is_bf16_supported():\n",
      "    86|         0|            0|            0|  0.00%|    r\"\"\"Returns a bool indicating if the current CUDA device supports dtype bfloat16\"\"\"\n",
      "    87|         0|            0|            0|  0.00%|    cu_vers = torch.version.cuda\n",
      "    88|         0|            0|            0|  0.00%|    if cu_vers is not None:\n",
      "    89|         0|            0|            0|  0.00%|        cuda_maj_decide = int(cu_vers.split('.')[0]) >= 11\n",
      "    90|         0|            0|            0|  0.00%|\n",
      "    91|         0|            0|            0|  0.00%|    else:\n",
      "    92|         0|            0|            0|  0.00%|        cuda_maj_decide = False\n",
      "    93|         0|            0|            0|  0.00%|    return torch.cuda.get_device_properties(torch.cuda.current_device()).major >= 8 and cuda_maj_decide\n",
      "    94|         0|            0|            0|  0.00%|\n",
      "    95|         0|            0|            0|  0.00%|def _sleep(cycles):\n",
      "    96|         0|            0|            0|  0.00%|    torch._C._cuda_sleep(cycles)\n",
      "    97|         0|            0|            0|  0.00%|\n",
      "    98|         0|            0|            0|  0.00%|\n",
      "    99|         0|            0|            0|  0.00%|def _check_capability():\n",
      "   100|         0|            0|            0|  0.00%|    incorrect_binary_warn = \"\"\"\n",
      "   101|         0|            0|            0|  0.00%|    Found GPU%d %s which requires CUDA_VERSION >= %d to\n",
      "   102|         0|            0|            0|  0.00%|     work properly, but your PyTorch was compiled\n",
      "   103|         0|            0|            0|  0.00%|     with CUDA_VERSION %d. Please install the correct PyTorch binary\n",
      "   104|         0|            0|            0|  0.00%|     using instructions from https://pytorch.org\n",
      "   105|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   106|         0|            0|            0|  0.00%|\n",
      "   107|         0|            0|            0|  0.00%|    old_gpu_warn = \"\"\"\n",
      "   108|         0|            0|            0|  0.00%|    Found GPU%d %s which is of cuda capability %d.%d.\n",
      "   109|         0|            0|            0|  0.00%|    PyTorch no longer supports this GPU because it is too old.\n",
      "   110|         0|            0|            0|  0.00%|    The minimum cuda capability supported by this library is %d.%d.\n",
      "   111|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   112|         0|            0|            0|  0.00%|\n",
      "   113|         0|            0|            0|  0.00%|    if torch.version.cuda is not None:  # on ROCm we don't want this check\n",
      "   114|         0|            0|            0|  0.00%|        CUDA_VERSION = torch._C._cuda_getCompiledVersion()\n",
      "   115|         0|            0|            0|  0.00%|        for d in range(device_count()):\n",
      "   116|         0|            0|            0|  0.00%|            capability = get_device_capability(d)\n",
      "   117|         0|            0|            0|  0.00%|            major = capability[0]\n",
      "   118|         0|            0|            0|  0.00%|            minor = capability[1]\n",
      "   119|         0|            0|            0|  0.00%|            name = get_device_name(d)\n",
      "   120|         0|            0|            0|  0.00%|            current_arch = major * 10 + minor\n",
      "   121|         0|            0|            0|  0.00%|            min_arch = min((int(arch.split(\"_\")[1]) for arch in torch.cuda.get_arch_list()), default=35)\n",
      "   122|         0|            0|            0|  0.00%|            if current_arch < min_arch:\n",
      "   123|         0|            0|            0|  0.00%|                warnings.warn(old_gpu_warn % (d, name, major, minor, min_arch // 10, min_arch % 10))\n",
      "   124|         0|            0|            0|  0.00%|            elif CUDA_VERSION <= 9000 and major >= 7 and minor >= 5:\n",
      "   125|         0|            0|            0|  0.00%|                warnings.warn(incorrect_binary_warn % (d, name, 10000, CUDA_VERSION))\n",
      "   126|         0|            0|            0|  0.00%|\n",
      "   127|         0|            0|            0|  0.00%|def _check_cubins():\n",
      "   128|         0|            0|            0|  0.00%|    incompatible_device_warn = \"\"\"\n",
      "   129|         0|            0|            0|  0.00%|{} with CUDA capability sm_{} is not compatible with the current PyTorch installation.\n",
      "   130|         0|            0|            0|  0.00%|The current PyTorch install supports CUDA capabilities {}.\n",
      "   131|         0|            0|            0|  0.00%|If you want to use the {} GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "   132|         0|            0|            0|  0.00%|\"\"\"\n",
      "   133|         0|            0|            0|  0.00%|    if torch.version.cuda is None:  # on ROCm we don't want this check\n",
      "   134|         0|            0|            0|  0.00%|        return\n",
      "   135|         0|            0|            0|  0.00%|    arch_list = get_arch_list()\n",
      "   136|         0|            0|            0|  0.00%|    if len(arch_list) == 0:\n",
      "   137|         0|            0|            0|  0.00%|        return\n",
      "   138|         0|            0|            0|  0.00%|    supported_sm = [int(arch.split('_')[1]) for arch in arch_list if 'sm_' in arch]\n",
      "   139|         0|            0|            0|  0.00%|    for idx in range(device_count()):\n",
      "   140|         0|            0|            0|  0.00%|        cap_major, cap_minor = get_device_capability(idx)\n",
      "   141|         0|            0|            0|  0.00%|        # NVIDIA GPU compute architectures are backward compatible within major version\n",
      "   142|         0|            0|            0|  0.00%|        supported = any([sm // 10 == cap_major for sm in supported_sm])\n",
      "   143|         0|            0|            0|  0.00%|        if not supported:\n",
      "   144|         0|            0|            0|  0.00%|            device_name = get_device_name(idx)\n",
      "   145|         0|            0|            0|  0.00%|            capability = cap_major * 10 + cap_minor\n",
      "   146|         0|            0|            0|  0.00%|            warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "   147|         0|            0|            0|  0.00%|\n",
      "   148|         0|            0|            0|  0.00%|\n",
      "   149|         1|  5.72205e-06|  5.72205e-06|  0.00%|def is_initialized():\n",
      "   150|         0|            0|            0|  0.00%|    r\"\"\"Returns whether PyTorch's CUDA state has been initialized.\"\"\"\n",
      "   151|         1|   2.6226e-06|   2.6226e-06|  0.00%|    return _initialized and not _is_in_bad_fork()\n",
      "   152|         0|            0|            0|  0.00%|\n",
      "   153|         0|            0|            0|  0.00%|\n",
      "   154|         1|  3.09944e-06|  3.09944e-06|  0.00%|def _lazy_call(callable, **kwargs):\n",
      "   155|         1|  1.09673e-05|  1.09673e-05|  0.00%|    if is_initialized():\n",
      "(call)|         1|  8.34465e-06|  8.34465e-06|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:149 is_initialized\n",
      "   156|         0|            0|            0|  0.00%|        callable()\n",
      "   157|         0|            0|            0|  0.00%|    else:\n",
      "   158|         0|            0|            0|  0.00%|        # TODO(torch_deploy): this accesses linecache, which attempts to read the\n",
      "   159|         0|            0|            0|  0.00%|        # file system to get traceback info. Patch linecache or do something\n",
      "   160|         0|            0|            0|  0.00%|        # else here if this ends up being important.\n",
      "   161|         0|            0|            0|  0.00%|        global _lazy_seed_tracker\n",
      "   162|         1|  2.86102e-06|  2.86102e-06|  0.00%|        if kwargs.get(\"seed_all\", False):\n",
      "   163|         1|  1.66893e-05|  1.66893e-05|  0.00%|            _lazy_seed_tracker.queue_seed_all(callable, traceback.format_stack())\n",
      "(call)|         1|    0.0052073|    0.0052073|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/lib/python3.8/traceback.py:193 format_stack\n",
      "(call)|         1|   1.3113e-05|   1.3113e-05|  0.00%|# /home/ubuntu/.pyenv/versions/3.8.2/envs/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:50 queue_seed_all\n",
      "   164|         0|            0|            0|  0.00%|        elif kwargs.get(\"seed\", False):\n",
      "   165|         0|            0|            0|  0.00%|            _lazy_seed_tracker.queue_seed(callable, traceback.format_stack())\n",
      "   166|         0|            0|            0|  0.00%|        else:\n",
      "   167|         0|            0|            0|  0.00%|            # Don't store the actual traceback to avoid memory cycle\n",
      "   168|         0|            0|            0|  0.00%|            _queued_calls.append((callable, traceback.format_stack()))\n",
      "   169|         0|            0|            0|  0.00%|\n",
      "   170|         0|            0|            0|  0.00%|_lazy_call(_check_capability)\n",
      "   171|         0|            0|            0|  0.00%|_lazy_call(_check_cubins)\n",
      "   172|         0|            0|            0|  0.00%|\n",
      "   173|         0|            0|            0|  0.00%|\n",
      "   174|         0|            0|            0|  0.00%|class DeferredCudaCallError(Exception):\n",
      "   175|         0|            0|            0|  0.00%|    pass\n",
      "   176|         0|            0|            0|  0.00%|\n",
      "   177|         0|            0|            0|  0.00%|\n",
      "   178|         0|            0|            0|  0.00%|def init():\n",
      "   179|         0|            0|            0|  0.00%|    r\"\"\"Initialize PyTorch's CUDA state.  You may need to call\n",
      "   180|         0|            0|            0|  0.00%|    this explicitly if you are interacting with PyTorch via\n",
      "   181|         0|            0|            0|  0.00%|    its C API, as Python bindings for CUDA functionality will not\n",
      "   182|         0|            0|            0|  0.00%|    be available until this initialization takes place.  Ordinary users\n",
      "   183|         0|            0|            0|  0.00%|    should not need this, as all of PyTorch's CUDA methods\n",
      "   184|         0|            0|            0|  0.00%|    automatically initialize CUDA state on-demand.\n",
      "   185|         0|            0|            0|  0.00%|\n",
      "   186|         0|            0|            0|  0.00%|    Does nothing if the CUDA state is already initialized.\n",
      "   187|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   188|         0|            0|            0|  0.00%|    _lazy_init()\n",
      "   189|         0|            0|            0|  0.00%|\n",
      "   190|         0|            0|            0|  0.00%|\n",
      "   191|         0|            0|            0|  0.00%|def _lazy_init():\n",
      "   192|         0|            0|            0|  0.00%|    global _initialized, _queued_calls\n",
      "   193|         0|            0|            0|  0.00%|    if is_initialized() or hasattr(_tls, 'is_initializing'):\n",
      "   194|         0|            0|            0|  0.00%|        return\n",
      "   195|         0|            0|            0|  0.00%|    with _initialization_lock:\n",
      "   196|         0|            0|            0|  0.00%|        # We be double-checked locking, boys!  This is OK because\n",
      "   197|         0|            0|            0|  0.00%|        # the above test was GIL protected anyway.  The inner test\n",
      "   198|         0|            0|            0|  0.00%|        # is for when a thread blocked on some other thread which was\n",
      "   199|         0|            0|            0|  0.00%|        # doing the initialization; when they get the lock, they will\n",
      "   200|         0|            0|            0|  0.00%|        # find there is nothing left to do.\n",
      "   201|         0|            0|            0|  0.00%|        if is_initialized():\n",
      "   202|         0|            0|            0|  0.00%|            return\n",
      "   203|         0|            0|            0|  0.00%|        # It is important to prevent other threads from entering _lazy_init\n",
      "   204|         0|            0|            0|  0.00%|        # immediately, while we are still guaranteed to have the GIL, because some\n",
      "   205|         0|            0|            0|  0.00%|        # of the C calls we make below will release the GIL\n",
      "   206|         0|            0|            0|  0.00%|        if _is_in_bad_fork():\n",
      "   207|         0|            0|            0|  0.00%|            raise RuntimeError(\n",
      "   208|         0|            0|            0|  0.00%|                \"Cannot re-initialize CUDA in forked subprocess. To use CUDA with \"\n",
      "   209|         0|            0|            0|  0.00%|                \"multiprocessing, you must use the 'spawn' start method\")\n",
      "   210|         0|            0|            0|  0.00%|        if not hasattr(torch._C, '_cuda_getDeviceCount'):\n",
      "   211|         0|            0|            0|  0.00%|            raise AssertionError(\"Torch not compiled with CUDA enabled\")\n",
      "   212|         0|            0|            0|  0.00%|        if _cudart is None:\n",
      "   213|         0|            0|            0|  0.00%|            raise AssertionError(\n",
      "   214|         0|            0|            0|  0.00%|                \"libcudart functions unavailable. It looks like you have a broken build?\")\n",
      "   215|         0|            0|            0|  0.00%|        # This function throws if there's a driver initialization error, no GPUs\n",
      "   216|         0|            0|            0|  0.00%|        # are found or any other error occurs\n",
      "   217|         0|            0|            0|  0.00%|        torch._C._cuda_init()\n",
      "   218|         0|            0|            0|  0.00%|        # Some of the queued calls may reentrantly call _lazy_init();\n",
      "   219|         0|            0|            0|  0.00%|        # we need to just return without initializing in that case.\n",
      "   220|         0|            0|            0|  0.00%|        # However, we must not let any *other* threads in!\n",
      "   221|         0|            0|            0|  0.00%|        _tls.is_initializing = True\n",
      "   222|         0|            0|            0|  0.00%|\n",
      "   223|         0|            0|            0|  0.00%|        for calls in _lazy_seed_tracker.get_calls():\n",
      "   224|         0|            0|            0|  0.00%|            if calls:\n",
      "   225|         0|            0|            0|  0.00%|                _queued_calls.append(calls)\n",
      "   226|         0|            0|            0|  0.00%|\n",
      "   227|         0|            0|            0|  0.00%|        try:\n",
      "   228|         0|            0|            0|  0.00%|            for queued_call, orig_traceback in _queued_calls:\n",
      "   229|         0|            0|            0|  0.00%|                try:\n",
      "   230|         0|            0|            0|  0.00%|                    queued_call()\n",
      "   231|         0|            0|            0|  0.00%|                except Exception as e:\n",
      "   232|         0|            0|            0|  0.00%|                    msg = (f\"CUDA call failed lazily at initialization with error: {str(e)}\\n\\n\"\n",
      "   233|         0|            0|            0|  0.00%|                           f\"CUDA call was originally invoked at:\\n\\n{orig_traceback}\")\n",
      "   234|         0|            0|            0|  0.00%|                    raise DeferredCudaCallError(msg) from e\n",
      "   235|         0|            0|            0|  0.00%|        finally:\n",
      "   236|         0|            0|            0|  0.00%|            delattr(_tls, 'is_initializing')\n",
      "   237|         0|            0|            0|  0.00%|        _initialized = True\n",
      "   238|         0|            0|            0|  0.00%|\n",
      "   239|         0|            0|            0|  0.00%|\n",
      "   240|         0|            0|            0|  0.00%|def cudart():\n",
      "   241|         0|            0|            0|  0.00%|    _lazy_init()\n",
      "   242|         0|            0|            0|  0.00%|    return _cudart\n",
      "   243|         0|            0|            0|  0.00%|\n",
      "   244|         0|            0|            0|  0.00%|\n",
      "   245|         0|            0|            0|  0.00%|class cudaStatus(object):\n",
      "   246|         0|            0|            0|  0.00%|    SUCCESS: int = 0\n",
      "   247|         0|            0|            0|  0.00%|    ERROR_NOT_READY: int = 34\n",
      "   248|         0|            0|            0|  0.00%|\n",
      "   249|         0|            0|            0|  0.00%|class CudaError(RuntimeError):\n",
      "   250|         0|            0|            0|  0.00%|    def __init__(self, code: int) -> None:\n",
      "   251|         0|            0|            0|  0.00%|        msg = _cudart.cudaGetErrorString(_cudart.cudaError(code))\n",
      "   252|         0|            0|            0|  0.00%|        super(CudaError, self).__init__('{0} ({1})'.format(msg, code))\n",
      "   253|         0|            0|            0|  0.00%|\n",
      "   254|         0|            0|            0|  0.00%|\n",
      "   255|         0|            0|            0|  0.00%|def check_error(res: int) -> None:\n",
      "   256|         0|            0|            0|  0.00%|    if res != _cudart.cudaError.success:\n",
      "   257|         0|            0|            0|  0.00%|        raise CudaError(res)\n",
      "   258|         0|            0|            0|  0.00%|\n",
      "   259|         0|            0|            0|  0.00%|\n",
      "   260|         0|            0|            0|  0.00%|class device(object):\n",
      "   261|         0|            0|            0|  0.00%|    r\"\"\"Context-manager that changes the selected device.\n",
      "   262|         0|            0|            0|  0.00%|\n",
      "   263|         0|            0|            0|  0.00%|    Args:\n",
      "   264|         0|            0|            0|  0.00%|        device (torch.device or int): device index to select. It's a no-op if\n",
      "   265|         0|            0|            0|  0.00%|            this argument is a negative integer or ``None``.\n",
      "   266|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   267|         0|            0|            0|  0.00%|\n",
      "   268|         0|            0|            0|  0.00%|    def __init__(self, device: Any):\n",
      "   269|         0|            0|            0|  0.00%|        self.idx = _get_device_index(device, optional=True)\n",
      "   270|         0|            0|            0|  0.00%|        self.prev_idx = -1\n",
      "   271|         0|            0|            0|  0.00%|\n",
      "   272|         0|            0|            0|  0.00%|    def __enter__(self):\n",
      "   273|         0|            0|            0|  0.00%|        if self.idx == -1:\n",
      "   274|         0|            0|            0|  0.00%|            return\n",
      "   275|         0|            0|            0|  0.00%|        self.prev_idx = torch.cuda.current_device()\n",
      "   276|         0|            0|            0|  0.00%|        if self.prev_idx != self.idx:\n",
      "   277|         0|            0|            0|  0.00%|            torch.cuda.set_device(self.idx)\n",
      "   278|         0|            0|            0|  0.00%|        if not torch.jit.is_scripting():\n",
      "   279|         0|            0|            0|  0.00%|            _lazy_init()\n",
      "   280|         0|            0|            0|  0.00%|\n",
      "   281|         0|            0|            0|  0.00%|    def __exit__(self, type: Any, value: Any, traceback: Any):\n",
      "   282|         0|            0|            0|  0.00%|        if self.prev_idx != self.idx:\n",
      "   283|         0|            0|            0|  0.00%|            torch.cuda.set_device(self.prev_idx)\n",
      "   284|         0|            0|            0|  0.00%|        return False\n",
      "   285|         0|            0|            0|  0.00%|\n",
      "   286|         0|            0|            0|  0.00%|\n",
      "   287|         0|            0|            0|  0.00%|class device_of(device):\n",
      "   288|         0|            0|            0|  0.00%|    r\"\"\"Context-manager that changes the current device to that of given object.\n",
      "   289|         0|            0|            0|  0.00%|\n",
      "   290|         0|            0|            0|  0.00%|    You can use both tensors and storages as arguments. If a given object is\n",
      "   291|         0|            0|            0|  0.00%|    not allocated on a GPU, this is a no-op.\n",
      "   292|         0|            0|            0|  0.00%|\n",
      "   293|         0|            0|            0|  0.00%|    Args:\n",
      "   294|         0|            0|            0|  0.00%|        obj (Tensor or Storage): object allocated on the selected device.\n",
      "   295|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   296|         0|            0|            0|  0.00%|\n",
      "   297|         0|            0|            0|  0.00%|    def __init__(self, obj):\n",
      "   298|         0|            0|            0|  0.00%|        idx = obj.get_device() if obj.is_cuda else -1\n",
      "   299|         0|            0|            0|  0.00%|        super(device_of, self).__init__(idx)\n",
      "   300|         0|            0|            0|  0.00%|\n",
      "   301|         0|            0|            0|  0.00%|\n",
      "   302|         0|            0|            0|  0.00%|def set_device(device: _device_t) -> None:\n",
      "   303|         0|            0|            0|  0.00%|    r\"\"\"Sets the current device.\n",
      "   304|         0|            0|            0|  0.00%|\n",
      "   305|         0|            0|            0|  0.00%|    Usage of this function is discouraged in favor of :any:`device`. In most\n",
      "   306|         0|            0|            0|  0.00%|    cases it's better to use ``CUDA_VISIBLE_DEVICES`` environmental variable.\n",
      "   307|         0|            0|            0|  0.00%|\n",
      "   308|         0|            0|            0|  0.00%|    Args:\n",
      "   309|         0|            0|            0|  0.00%|        device (torch.device or int): selected device. This function is a no-op\n",
      "   310|         0|            0|            0|  0.00%|            if this argument is negative.\n",
      "   311|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   312|         0|            0|            0|  0.00%|    device = _get_device_index(device)\n",
      "   313|         0|            0|            0|  0.00%|    if device >= 0:\n",
      "   314|         0|            0|            0|  0.00%|        torch._C._cuda_setDevice(device)\n",
      "   315|         0|            0|            0|  0.00%|\n",
      "   316|         0|            0|            0|  0.00%|\n",
      "   317|         0|            0|            0|  0.00%|def get_device_name(device: Optional[_device_t] = None) -> str:\n",
      "   318|         0|            0|            0|  0.00%|    r\"\"\"Gets the name of a device.\n",
      "   319|         0|            0|            0|  0.00%|\n",
      "   320|         0|            0|            0|  0.00%|    Args:\n",
      "   321|         0|            0|            0|  0.00%|        device (torch.device or int, optional): device for which to return the\n",
      "   322|         0|            0|            0|  0.00%|            name. This function is a no-op if this argument is a negative\n",
      "   323|         0|            0|            0|  0.00%|            integer. It uses the current device, given by :func:`~torch.cuda.current_device`,\n",
      "   324|         0|            0|            0|  0.00%|            if :attr:`device` is ``None`` (default).\n",
      "   325|         0|            0|            0|  0.00%|\n",
      "   326|         0|            0|            0|  0.00%|    Returns:\n",
      "   327|         0|            0|            0|  0.00%|        str: the name of the device\n",
      "   328|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   329|         0|            0|            0|  0.00%|    return get_device_properties(device).name\n",
      "   330|         0|            0|            0|  0.00%|\n",
      "   331|         0|            0|            0|  0.00%|\n",
      "   332|         0|            0|            0|  0.00%|def get_device_capability(device: Optional[_device_t] = None) -> Tuple[int, int]:\n",
      "   333|         0|            0|            0|  0.00%|    r\"\"\"Gets the cuda capability of a device.\n",
      "   334|         0|            0|            0|  0.00%|\n",
      "   335|         0|            0|            0|  0.00%|    Args:\n",
      "   336|         0|            0|            0|  0.00%|        device (torch.device or int, optional): device for which to return the\n",
      "   337|         0|            0|            0|  0.00%|            device capability. This function is a no-op if this argument is\n",
      "   338|         0|            0|            0|  0.00%|            a negative integer. It uses the current device, given by\n",
      "   339|         0|            0|            0|  0.00%|            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``\n",
      "   340|         0|            0|            0|  0.00%|            (default).\n",
      "   341|         0|            0|            0|  0.00%|\n",
      "   342|         0|            0|            0|  0.00%|    Returns:\n",
      "   343|         0|            0|            0|  0.00%|        tuple(int, int): the major and minor cuda capability of the device\n",
      "   344|         0|            0|            0|  0.00%|    \"\"\"\n",
      "   345|         0|            0|            0|  0.00%|    prop = get_device_properties(device)\n",
      "   346|         0|            0|            0|  0.00%|    return prop.major, prop.minor\n",
      "   347|         0|            0|            0|  0.00%|\n",
      "   348|         0|            0|            0|  0.00%|\n",
      "   349|         0|            0|            0|  0.00%|def get_device_properties(device: _device_t) -> _CudaDeviceProperties:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profiler.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edf0f859-123b-45b3-9eb8-f53f79259d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.dump_stats('ouptut_jan12.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21336e81-26b9-4721-ae46-805d4f6fed62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
