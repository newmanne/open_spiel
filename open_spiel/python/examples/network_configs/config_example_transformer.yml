anticipatory_param: 0.2
batch_size: 256
epsilon_end: 0.01
epsilon_start: 0.5
eval_every: 10000
learn_every: 64
min_buffer_size_to_learn: 5000
num_training_episodes: 3000000
optimizer_str: sgd
replay_buffer_capacity: 50000
reservoir_buffer_capacity: 2000000
rl_learning_rate: 5.0e-05
rl_model: transformer
rl_model_args:
    d_model: 16
    nhead: 2
    feedforward_dim: 128 
    dropout: 0.0
    num_layers: 2
seed: 1234
sl_learning_rate: 0.01
sl_model: transformer
sl_model_args:
    d_model: 16
    nhead: 2
    feedforward_dim: 128 
    dropout: 0.0 
    num_layers: 2
