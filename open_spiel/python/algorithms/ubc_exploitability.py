# Copyright 2019 DeepMind Technologies Ltd. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Compute the exploitability of a bot / strategy in a 2p sequential game.

This computes the value that a policy achieves against a worst-case opponent.
The policy applies to both player 1 and player 2, and hence we have a 2-player
symmetric zero-sum game, so the game value is zero for both players, and hence
value-vs-best-response is equal to exploitability.

We construct information sets, each consisting of a list of (state, probability)
pairs where probability is a counterfactual reach probability, i.e. the
probability that the state would be reached if the best responder (the current
player) played to reach it. This is the product of the probabilities of the
necessary chance events and opponent action choices required to reach the node.

These probabilities give us the correct weighting for possible states of the
world when considering our best response for a particular information set.

The values we calculate are values of being in the specific state. Unlike in a
CFR algorithm, they are not weighted by reach probabilities. These values
take into account the whole state, so they may depend on information which is
unknown to the best-responding player.
"""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections

import numpy as np

from open_spiel.python.algorithms import best_response as pyspiel_best_response
import pyspiel

def expected_value_of_best_responder(state, policy, best_responder_player_index, cut_prob=0.):
  if state.is_terminal():
    return state.player_return(best_responder_player_index)
  elif state.current_player() == best_responder_player_index:
    return max([
      expected_value_of_best_responder(state.child(action), policy, best_responder_player_index) for action in state.legal_actions()
    ])
  elif state.is_chance_node(): # Another player or chance
    return sum((p * expected_value_of_best_responder(state.child(a), policy, best_responder_player_index) for (a, p) in state.chance_outcomes() if p >= cut_prob))
  else:
    return sum((p * expected_value_of_best_responder(state.child(a), policy, best_responder_player_index) for (a, p) in policy.action_probabilities(state).items() if p >= cut_prob))


def game_tree_size(state):
  if state.is_terminal():
    return 1
  elif state.is_chance_node():
    return 1 + sum([game_tree_size(state.child(a)) for (a, _) in state.chance_outcomes()])
  else:
    return 1 + sum([game_tree_size(state.child(a)) for a in state.legal_actions()])